{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_ZafXO_4YAd"
      },
      "source": [
        "## Install libs and import them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-YbjCkzw0yU",
        "outputId": "560645b1-6530-42ea-c0c2-057332bb137f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n"
          ]
        }
      ],
      "source": [
        "# A dependency of the preprocessing for BERT inputs\n",
        "\n",
        "!pip install gensim\n",
        "!pip install -q bertopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_XgTpm9ZxoN9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "309da3f3-1394-4c12-fde0-3694e06d5524"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/hdbscan/plots.py:448: SyntaxWarning: invalid escape sequence '\\l'\n",
            "  axis.set_ylabel('$\\lambda$ value')\n",
            "/usr/local/lib/python3.12/dist-packages/hdbscan/robust_single_linkage_.py:175: SyntaxWarning: invalid escape sequence '\\{'\n",
            "  $max \\{ core_k(a), core_k(b), 1/\\alpha d(a,b) \\}$.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from google.colab import drive\n",
        "\n",
        "from bertopic import BERTopic\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6MugfEgDRpY"
      },
      "source": [
        "## Topic Extraction and analysis\n",
        "\n",
        "<!-- About topic modeling from blog-->\n",
        "\n",
        "In this demo we use BerTopic to extract topics from a dataset, then we do some ts analysis. The dataset used in this notebook is open source and it was selected from github (https://github.com/linanqiu/reddit-dataset/tree/master). It's the reddit threads and comments dataset.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vnvd4mrtPHHV"
      },
      "source": [
        "### Load dataset\n",
        "\n",
        "We first load the dataset from google drive and we execute some light EDA.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J34vY-N2BMFH",
        "outputId": "0352bda5-2aa2-4061-885f-737ee26ff0d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "pOdqCMoQDRJL",
        "outputId": "158ecc14-904f-431b-81c5-9f77279320ec"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 79478,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1551,\n        \"samples\": [\n          \"excellent  excellent advice as patients  we feel awkward questioning or secondguessing a doctor  or even getting a second opinion while some doctors get annoyed  asking questions and getting secondopinions are essential to getting a good diagnosis  figuring out a good treatment  and getting good care  unfortunately  even with insurance  doing all of that can be hard making appointments is difficult  as the economics of healthcare encourages doctors to overbook and to spend as little time as possible with patients many doctors do nt take any insurance or have moved into extremely expensive vip practices  and others  who do take insurance  wo nt see patients who do nt have the particular insurance that the doctor accepts  moreover  finding reliable data and ratings on doctors is nearly impossible  people will say   just google the doctor   but that rarely  if ever  gives reliable results  even once you scroll down below the paidemptyform web sites that inevitably come up first in those searches  and doing all of this while sick is even harder  but completely agree with op s point \",\n          \"the biggest downside to this method is you are missing out on a lot of possible deductions that would only be found by paying an accountant  100 to tell you what you missed\",\n          \"awesome  i ll remember this next time i m in the 1990s \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1591,\n        \"samples\": [\n          \"czhctfe\",\n          \"4308fc\",\n          \"cyol91u\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subreddit\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"youshouldknow\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"meta\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"learning\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1172957.9968120265,\n        \"min\": 1442934810.0,\n        \"max\": 1455685605.0,\n        \"num_unique_values\": 1573,\n        \"samples\": [\n          1455682913.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"author\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1444,\n        \"samples\": [\n          \"freshtrax\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ups\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 326.13382239168374,\n        \"min\": -64.0,\n        \"max\": 3957.0,\n        \"num_unique_values\": 398,\n        \"samples\": [\n          2197.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"downs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authorlinkkarma\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 22047,\n        \"min\": 0,\n        \"max\": 621795,\n        \"num_unique_values\": 969,\n        \"samples\": [\n          10402\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authorcommentkarma\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 32087,\n        \"min\": -29,\n        \"max\": 301427,\n        \"num_unique_values\": 1349,\n        \"samples\": [\n          1171\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authorisgold\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.26288606861604,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-1b224c06-5a74-448e-a2d6-be57c7b971bb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>id</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>meta</th>\n",
              "      <th>time</th>\n",
              "      <th>author</th>\n",
              "      <th>ups</th>\n",
              "      <th>downs</th>\n",
              "      <th>authorlinkkarma</th>\n",
              "      <th>authorcommentkarma</th>\n",
              "      <th>authorisgold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>informative post and truly helpful it s easy t...</td>\n",
              "      <td>czctg99</td>\n",
              "      <td>youshouldknow</td>\n",
              "      <td>learning</td>\n",
              "      <td>1.453833e+09</td>\n",
              "      <td>doverthere</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>705</td>\n",
              "      <td>4511</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>actually that is such a good tip it s not what...</td>\n",
              "      <td>czrpd2k</td>\n",
              "      <td>youshouldknow</td>\n",
              "      <td>learning</td>\n",
              "      <td>1.454896e+09</td>\n",
              "      <td>innercirclepost</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>do nt put it in the fridge with the peel on   ...</td>\n",
              "      <td>czs3rfq</td>\n",
              "      <td>youshouldknow</td>\n",
              "      <td>learning</td>\n",
              "      <td>1.454931e+09</td>\n",
              "      <td>agent-99</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3490</td>\n",
              "      <td>9171</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>this probably wo nt come up that often  but i ...</td>\n",
              "      <td>44h2zv</td>\n",
              "      <td>youshouldknow</td>\n",
              "      <td>learning</td>\n",
              "      <td>1.454779e+09</td>\n",
              "      <td>traunks</td>\n",
              "      <td>33.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>341</td>\n",
              "      <td>1012</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>just in general with usbc be careful what you ...</td>\n",
              "      <td>czzqpho</td>\n",
              "      <td>youshouldknow</td>\n",
              "      <td>learning</td>\n",
              "      <td>1.455473e+09</td>\n",
              "      <td>xsvfan</td>\n",
              "      <td>33.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>383</td>\n",
              "      <td>25495</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79473</th>\n",
              "      <td>also  windows displays file sizes in kibimebig...</td>\n",
              "      <td>czc9rbl</td>\n",
              "      <td>youshouldknow</td>\n",
              "      <td>learning</td>\n",
              "      <td>1.453790e+09</td>\n",
              "      <td>[deleted]</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79474</th>\n",
              "      <td>deleted</td>\n",
              "      <td>czexg90</td>\n",
              "      <td>youshouldknow</td>\n",
              "      <td>learning</td>\n",
              "      <td>1.453969e+09</td>\n",
              "      <td>[deleted]</td>\n",
              "      <td>-4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79475</th>\n",
              "      <td>there are people who do nt know this  perhaps ...</td>\n",
              "      <td>cywryit</td>\n",
              "      <td>youshouldknow</td>\n",
              "      <td>learning</td>\n",
              "      <td>1.452704e+09</td>\n",
              "      <td>[deleted]</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79476</th>\n",
              "      <td>deleted</td>\n",
              "      <td>cys6qrw</td>\n",
              "      <td>youshouldknow</td>\n",
              "      <td>learning</td>\n",
              "      <td>1.452376e+09</td>\n",
              "      <td>[deleted]</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79477</th>\n",
              "      <td>deleted</td>\n",
              "      <td>cyoefaz</td>\n",
              "      <td>youshouldknow</td>\n",
              "      <td>learning</td>\n",
              "      <td>1.452110e+09</td>\n",
              "      <td>[deleted]</td>\n",
              "      <td>422.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>79478 rows × 11 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b224c06-5a74-448e-a2d6-be57c7b971bb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1b224c06-5a74-448e-a2d6-be57c7b971bb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1b224c06-5a74-448e-a2d6-be57c7b971bb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-1e81ae8e-81cf-479b-991b-f56a195776d2\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1e81ae8e-81cf-479b-991b-f56a195776d2')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-1e81ae8e-81cf-479b-991b-f56a195776d2 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_5d60ccad-857a-4995-900e-c40fad1f69b0\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_5d60ccad-857a-4995-900e-c40fad1f69b0 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                    text       id  \\\n",
              "0      informative post and truly helpful it s easy t...  czctg99   \n",
              "1      actually that is such a good tip it s not what...  czrpd2k   \n",
              "2      do nt put it in the fridge with the peel on   ...  czs3rfq   \n",
              "3      this probably wo nt come up that often  but i ...   44h2zv   \n",
              "4      just in general with usbc be careful what you ...  czzqpho   \n",
              "...                                                  ...      ...   \n",
              "79473  also  windows displays file sizes in kibimebig...  czc9rbl   \n",
              "79474                                           deleted   czexg90   \n",
              "79475  there are people who do nt know this  perhaps ...  cywryit   \n",
              "79476                                           deleted   cys6qrw   \n",
              "79477                                           deleted   cyoefaz   \n",
              "\n",
              "           subreddit      meta          time           author    ups  downs  \\\n",
              "0      youshouldknow  learning  1.453833e+09       doverthere    2.0    0.0   \n",
              "1      youshouldknow  learning  1.454896e+09  innercirclepost    6.0    0.0   \n",
              "2      youshouldknow  learning  1.454931e+09         agent-99    1.0    0.0   \n",
              "3      youshouldknow  learning  1.454779e+09          traunks   33.0    0.0   \n",
              "4      youshouldknow  learning  1.455473e+09           xsvfan   33.0    0.0   \n",
              "...              ...       ...           ...              ...    ...    ...   \n",
              "79473  youshouldknow  learning  1.453790e+09        [deleted]    7.0    0.0   \n",
              "79474  youshouldknow  learning  1.453969e+09        [deleted]   -4.0    0.0   \n",
              "79475  youshouldknow  learning  1.452704e+09        [deleted]    1.0    0.0   \n",
              "79476  youshouldknow  learning  1.452376e+09        [deleted]    1.0    0.0   \n",
              "79477  youshouldknow  learning  1.452110e+09        [deleted]  422.0    0.0   \n",
              "\n",
              "       authorlinkkarma  authorcommentkarma  authorisgold  \n",
              "0                  705                4511           1.0  \n",
              "1                    1                   8           0.0  \n",
              "2                 3490                9171           0.0  \n",
              "3                  341                1012           0.0  \n",
              "4                  383               25495           0.0  \n",
              "...                ...                 ...           ...  \n",
              "79473                0                   0           NaN  \n",
              "79474                0                   0           NaN  \n",
              "79475                0                   0           NaN  \n",
              "79476                0                   0           NaN  \n",
              "79477                0                   0           NaN  \n",
              "\n",
              "[79478 rows x 11 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "path = '/content/drive/MyDrive/Drive Blog NLP/reddit-dataset-master/learning_youshouldknow.csv'\n",
        "\n",
        "df=pd.read_csv(path)\n",
        "\n",
        "df.pop(\"Unnamed: 0\")\n",
        "df.columns=['text', 'id', 'subreddit', 'meta', 'time', 'author', 'ups', 'downs', 'authorlinkkarma', 'authorcommentkarma', 'authorisgold']\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHAUV0uct-Eb"
      },
      "outputs": [],
      "source": [
        "df['time'] = pd.to_datetime(df['time'],unit='s')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frWKhkBPP7lZ",
        "outputId": "87d0cf4a-2877-46a3-d25c-d14d0338a405"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 79478 entries, 0 to 79477\n",
            "Data columns (total 11 columns):\n",
            " #   Column              Non-Null Count  Dtype         \n",
            "---  ------              --------------  -----         \n",
            " 0   text                78678 non-null  object        \n",
            " 1   id                  79478 non-null  object        \n",
            " 2   subreddit           79478 non-null  object        \n",
            " 3   meta                79478 non-null  object        \n",
            " 4   time                78678 non-null  datetime64[ns]\n",
            " 5   author              78678 non-null  object        \n",
            " 6   ups                 78678 non-null  float64       \n",
            " 7   downs               78678 non-null  float64       \n",
            " 8   authorlinkkarma     79478 non-null  int64         \n",
            " 9   authorcommentkarma  79478 non-null  int64         \n",
            " 10  authorisgold        77444 non-null  float64       \n",
            "dtypes: datetime64[ns](1), float64(3), int64(2), object(5)\n",
            "memory usage: 6.7+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "RT2rD-4hqn__",
        "outputId": "df1eaf65-ee13-464e-d3f1-505e2c201cc1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 11,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1551,\n          \"1199\",\n          \"78678\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1591,\n          \"50\",\n          \"79478\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subreddit\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"79478\",\n          1,\n          \"youshouldknow\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"meta\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"79478\",\n          1,\n          \"learning\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"time\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"1970-01-01 00:00:00.000078678\",\n        \"max\": \"2016-02-17 05:06:45\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"78678\",\n          \"2016-01-26 03:23:44.901802240\",\n          \"2016-02-04 22:45:59\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"author\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1444,\n          \"1748\",\n          \"78678\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ups\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 27633.829285629523,\n        \"min\": -64.0,\n        \"max\": 78678.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          64.59111822872975,\n          10.0,\n          78678.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"downs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 27816.87366509759,\n        \"min\": 0.0,\n        \"max\": 78678.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          78678.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authorlinkkarma\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 216022.2190923959,\n        \"min\": 0.0,\n        \"max\": 621795.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          4511.927652935404,\n          2684.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authorcommentkarma\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 102394.42347913355,\n        \"min\": -29.0,\n        \"max\": 301427.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          15623.533455799088,\n          15954.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authorisgold\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 27380.52122563953,\n        \"min\": 0.0,\n        \"max\": 77444.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.07468622488507826,\n          0.26288606861604\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-a1d3bc77-3633-46f4-8dcb-b8a1ec77fff1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>id</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>meta</th>\n",
              "      <th>time</th>\n",
              "      <th>author</th>\n",
              "      <th>ups</th>\n",
              "      <th>downs</th>\n",
              "      <th>authorlinkkarma</th>\n",
              "      <th>authorcommentkarma</th>\n",
              "      <th>authorisgold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>78678</td>\n",
              "      <td>79478</td>\n",
              "      <td>79478</td>\n",
              "      <td>79478</td>\n",
              "      <td>78678</td>\n",
              "      <td>78678</td>\n",
              "      <td>78678.000000</td>\n",
              "      <td>78678.0</td>\n",
              "      <td>79478.000000</td>\n",
              "      <td>79478.000000</td>\n",
              "      <td>77444.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>1551</td>\n",
              "      <td>1591</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1444</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>deleted</td>\n",
              "      <td>czctg99</td>\n",
              "      <td>youshouldknow</td>\n",
              "      <td>learning</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[deleted]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>1199</td>\n",
              "      <td>50</td>\n",
              "      <td>79478</td>\n",
              "      <td>79478</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1748</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2016-01-26 03:23:44.901802240</td>\n",
              "      <td>NaN</td>\n",
              "      <td>64.591118</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4511.927653</td>\n",
              "      <td>15623.533456</td>\n",
              "      <td>0.074686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-09-22 15:13:30</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-64.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-29.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2016-01-15 16:48:17</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>978.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2016-01-26 14:23:56</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>475.000000</td>\n",
              "      <td>4669.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2016-02-04 22:45:59</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2684.000000</td>\n",
              "      <td>15954.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2016-02-17 05:06:45</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3957.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>621795.000000</td>\n",
              "      <td>301427.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>326.133822</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22047.198393</td>\n",
              "      <td>32087.383573</td>\n",
              "      <td>0.262886</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a1d3bc77-3633-46f4-8dcb-b8a1ec77fff1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a1d3bc77-3633-46f4-8dcb-b8a1ec77fff1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a1d3bc77-3633-46f4-8dcb-b8a1ec77fff1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-8bda6e3f-4f0f-42b4-81b5-c73ffa0fec19\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8bda6e3f-4f0f-42b4-81b5-c73ffa0fec19')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-8bda6e3f-4f0f-42b4-81b5-c73ffa0fec19 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "             text       id      subreddit      meta  \\\n",
              "count       78678    79478          79478     79478   \n",
              "unique       1551     1591              1         1   \n",
              "top      deleted   czctg99  youshouldknow  learning   \n",
              "freq         1199       50          79478     79478   \n",
              "mean          NaN      NaN            NaN       NaN   \n",
              "min           NaN      NaN            NaN       NaN   \n",
              "25%           NaN      NaN            NaN       NaN   \n",
              "50%           NaN      NaN            NaN       NaN   \n",
              "75%           NaN      NaN            NaN       NaN   \n",
              "max           NaN      NaN            NaN       NaN   \n",
              "std           NaN      NaN            NaN       NaN   \n",
              "\n",
              "                                 time     author           ups    downs  \\\n",
              "count                           78678      78678  78678.000000  78678.0   \n",
              "unique                            NaN       1444           NaN      NaN   \n",
              "top                               NaN  [deleted]           NaN      NaN   \n",
              "freq                              NaN       1748           NaN      NaN   \n",
              "mean    2016-01-26 03:23:44.901802240        NaN     64.591118      0.0   \n",
              "min               2015-09-22 15:13:30        NaN    -64.000000      0.0   \n",
              "25%               2016-01-15 16:48:17        NaN      1.000000      0.0   \n",
              "50%               2016-01-26 14:23:56        NaN      2.000000      0.0   \n",
              "75%               2016-02-04 22:45:59        NaN     10.000000      0.0   \n",
              "max               2016-02-17 05:06:45        NaN   3957.000000      0.0   \n",
              "std                               NaN        NaN    326.133822      0.0   \n",
              "\n",
              "        authorlinkkarma  authorcommentkarma  authorisgold  \n",
              "count      79478.000000        79478.000000  77444.000000  \n",
              "unique              NaN                 NaN           NaN  \n",
              "top                 NaN                 NaN           NaN  \n",
              "freq                NaN                 NaN           NaN  \n",
              "mean        4511.927653        15623.533456      0.074686  \n",
              "min            0.000000          -29.000000      0.000000  \n",
              "25%           38.000000          978.000000      0.000000  \n",
              "50%          475.000000         4669.000000      0.000000  \n",
              "75%         2684.000000        15954.000000      0.000000  \n",
              "max       621795.000000       301427.000000      1.000000  \n",
              "std        22047.198393        32087.383573      0.262886  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.describe(include=\"all\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzJbbN4GP9L7",
        "outputId": "448d1242-cce9-444b-a22e-a2e328253cc2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "800"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.text.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6C5aCxyTYhQ"
      },
      "outputs": [],
      "source": [
        "df.text=df.text.fillna(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEwMP1Q51_nI"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oipmWZ6o2ERC",
        "outputId": "adc55247-c40a-4542-eae9-5535c1f3ba4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting better_profanity\n",
            "  Downloading better_profanity-0.7.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Downloading better_profanity-0.7.0-py3-none-any.whl (46 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/46.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: better_profanity\n",
            "Successfully installed better_profanity-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install better_profanity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evxlU27U6v-N"
      },
      "source": [
        "Comments in social media sources contains occasionally bad language. For that reason we remove such words for better topics quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2VesIB17DP4"
      },
      "outputs": [],
      "source": [
        "from better_profanity import profanity\n",
        "df[\"text\"] = df[\"text\"].apply(lambda x: profanity.censor(x, ' '))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewuUmDgX43Oq"
      },
      "outputs": [],
      "source": [
        "df.to_csv('reddit_cleaned.csv',index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h16Qpmj-b_bp"
      },
      "source": [
        "if you have already cleaned the data (long cleaning process) you can skip the previous two lines and execute the following two ones instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYfw7F9aYp1L"
      },
      "outputs": [],
      "source": [
        "path=\"/content/drive/MyDrive/data/reddit_cleaned.csv\"\n",
        "df=pd.read_csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiorTJdGb3E3"
      },
      "outputs": [],
      "source": [
        "df.text=df.text.fillna(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGQgCffxckNn"
      },
      "outputs": [],
      "source": [
        "comments=df.text.unique().tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vjrf2GvJwPR1"
      },
      "source": [
        "We also execute some basic data cleaning by removing malformed text such as malformed domains and urls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbzmgQAIwE2R"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\b(?:www)?[a-z0-9\\-]{3,}\\.(?:[a-z]{2,})?\\b', '', text)\n",
        "    text = re.sub(r'\\b(?:www)?[a-z0-9\\-]{5,}\\b', '', text)\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LZtPZzkw1IB"
      },
      "outputs": [],
      "source": [
        "comments=[clean_text(c) for c in comments]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scU9fotnfmIg"
      },
      "outputs": [],
      "source": [
        "\n",
        "topic_model = BERTopic( low_memory=True,calculate_probabilities=True,verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clTbAzocuafO",
        "outputId": "017c3d10-125d-4e30-c5de-2f5419662e54"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['informative post and truly helpful it s easy to miss something so small as clearing your exhaust pipe  especially after you spent hours shoveling the snow and you just want it to be over',\n",
              " 'actually that is such a good tip it s not what i would have thought of even though i may have done it before ',\n",
              " 'do nt put it in the fridge with the peel on   it will turn black   http  indianapublicmediaorgamomentofsciencefiles200907banana940x633jpg  ',\n",
              " 'this probably wo nt come up that often  but i do find sometimes i only need half of a banana i used to try to save the other half by kind of wrapping the peel around it but it never worked  here s the trick  cut the banana in half before you peel it that way the peel will stay sealed to the uneaten half you simply then just wrap saran wrap around the exposed end of the banana you do nt even have to keep it in the fridge  although that will allow it to last longer i ve done this a few times and it s remained perfectly preserved ',\n",
              " 'just in general with usbc be careful what you buy there a lot of faulty cheap knockoffs that can be damaging there is a good engineer who tests them to make sure they re good ',\n",
              " 'that cant be real ',\n",
              " 'what weak security ',\n",
              " 'anybody know what an entire a  z english wikipedia britannica would look like in hardcover  how many pages ',\n",
              " 'i wish i could still find a good  new  and current encyclopedia set ',\n",
              " 'is there a way to make this work with the wikitravel sites  it would be a great way to put together a little pdf travel guide ',\n",
              " 'i ve seen this multiple times on various social media not a good idea if one of your  friends  has malicious intent ',\n",
              " 'yes  i still trust chipotle and their food ',\n",
              " 'the post says there is writing on the cable  but my cable has no writing  is it potentially faulty  anyone else have a blank cable ',\n",
              " 'chipotle was closed today  2816  for lunch so they are giving a coupon to those who text raincheck to 888222 for a free burrito  you know if you still trust chipotle and their food  source  http  timecom4212129chipotleclosedfreeburritoraincheck',\n",
              " 'so i can get book of all my anime waifu ',\n",
              " 'expired',\n",
              " 'is nt that one of the main reasons to pay ',\n",
              " 'with the snowstorm in the northeast  it may be tempting to start your car and let it heat up to help melt the deep snow covering it and surrounding it however  if snow is covering the tailpipe to your car  it will cause the carbon monoxide exhaust to back up and fill the car  which is deadly  a man was found dead in brooklyn yesterday he had actually cleared the area behind his car  but a passing snowplow blocked it back up again  http  wwwnydailynewscomnewyorkbrooklynbrooklynmanmissingstormfounddeadcararticle12509309 it s something that might not be obvious because exhaust is typically hot and it seems that it would melt snow with ease but that is not the case stay safe everyone ',\n",
              " 'if you re not smart enough to click advanced your not smart enough to know what to change ',\n",
              " '',\n",
              " 'your best bet would be to buy a cheap  generic usbc cable off of amazon  and then set the date to 1970 on your iphone ',\n",
              " 'the cables shipped with a design fault which has since been fixed  so apple are replacing cables at no cost this only applies to macbooks  not macbook airs or macbook pros http  forumsmacrumorscomthreadsapplelaunchesusbcchargecablereplacementprogramforretinamacbook1956078',\n",
              " 'i ll just use my phone ',\n",
              " 'explanation of the   7   rule  since it took me a minute to understand  your number  say  889  is split into 10x  y  or 880  9   we want to know if 10x  y divides by 7     so we multiply by 5   10x  y   5  50x  5y  multiplying a number by 5 does nt affect whether it is divisible by 7  example  assume m and n are integers a number   7m  n    where n is the remainder when the number is divided evenly by 7  multiplies by 5 to become   35m  5n    dividing evenly by 7 must remove the 35m  leaving 5n 5n will not divide by 7 unless n  is  7  therefore if   7m  n   is not divisible by 7  neither is the product of it and 5     we then subtract 49x  which is   always   divisible by 7  and 7y  which is also   always   divisible by 7    50x  5y    49x  7y   x  2y  subtracting a multiple of 7 does nt affect whether it is divisible by 7  example  assume p  q  and r are integers a number   7p  r    where r is the remainder when the number is divided evenly by 7  subtracts some value   7q    the difference remaining is 7p  7q  r  or   7  pq   r    because p and q are integers  pq must also be an integer  meaning 7  pq  is a multiple of 7 the remainder when 7  pq   r is therefore r as with the original number  meaning if   7p  r   is not divisible by 7  neither is the difference between it and a number that  is  divisible by 7     therefore  to see if a number of the form 10x  y    88   0    9    is divisible by 7  we may test x  2y    88      18    instead  hope this was helpful      ps this method is way easier than the godawful  1024  method  or however it worked   plus  this one chains    889  7   8818   7  70  7   70   7  7  7 ',\n",
              " 'so for four  halve the last two digits twicethat s the same as dividing by four you lpt to see if something is divisible by four is dividing it by four ',\n",
              " 'google is making all users switch over to google play music starting today thankfully  there s a way to circumvent this  simply add this line into adblock or ublock origin or whatever you use to block ads   212016  8  47  09 pm http  songzacom songzacom    colorbox if you do nt have adblock  simply inspect element on the box and delete  this line   https  imgurcomisbjwi4  ',\n",
              " 'you should also know that they often get reenabled by default during updates and that despite turning everything off  your pc still hemorrages data back over the net ',\n",
              " 'not anymore',\n",
              " 'greatkeep it up',\n",
              " 'i do nt understand this bot it s on every subreddit and most people do nt care about how many subscribers the subreddit has ',\n",
              " 'now i know',\n",
              " '  hey  i m doing  subscript',\n",
              " 'sometimes in this life you need to be hardcore you need to      it up and deal with it  and just eat the other half of the      banana ',\n",
              " ' relevant xkcd   https  whatifxkcdcom59 ',\n",
              " '  ryoushouldknow metrics    total subscribers  500028 subreddit rank  75 subreddit growth amp  milestones  http  redditmetricscomryoushouldknow',\n",
              " 'i edit it out for exactly that reason too easy to plug that into the united app  i m a standby pass rider anyways  so i m constantly looking at it  but it never hurts to practice good opsec ',\n",
              " 'can you reschedule their destination  and send an ex girlfriend to thailand ',\n",
              " 'or just carry a lock pick kit  like i do never been locked out  ever ',\n",
              " 'they are broken down into categories and you can find your niche in there i listen to a few different podcasts  dont have enough time for all of them most are pretty good  everything from lgbt  gaming  humor  and information about health or science  http  wwwpodcastawardscom',\n",
              " 'i m a bot   bleep    bloop   someone has linked to this thread from another place on reddit    rknowyourshit   ysk about the podcast awards  and here s the list of all the winners and runners up  youshouldknow   https  npredditcomrknowyourshitcomments451bt6ysk_about_the_podcast_awards_and_heres_the_list      footer     if you follow any of the above links  please respect the rules of reddit and do nt vote in the other threads      info   rtotesmessenger      contact   messagecompose  to  rtotesmessenger        bot ',\n",
              " 'also 11 subtract the sum of the even digits  in position  from the sum of the odd digits and divide that number by 11  example    9685423     9843    652   2413  11  gt  9685423 is divisible by 11  a little more on 11  to multiple a twodigit number by 11 add the two digits and stick that in the middle if the sum is more than 9 add one to the hundreds  example    89  11     89  17 8 17 9  gt    979  ',\n",
              " ' deleted ',\n",
              " 'this is wrong  you should nt always select advanced or custom too often  this can result in not all the correct things being installed correctly choose this option if you know what you are doing or if the program or application requires it this is not applicable to everything ',\n",
              " '2  1 last digit 2  2 2 last digits 2  3 3 last digits did nt notice this until now',\n",
              " 'depends on what you re installing and where you got it from  if it s off a website on the internet then yes ',\n",
              " 'go install visual studio 2015 with custom and azure sdk now let me sit with you for an hour whilst i fix itl',\n",
              " 'many times programs will install addons or unnecessary plugins without permission through the  full  or  typical  installation  note that these installations usually say  recommended  and that s how they get you  edit  because there have been so many people saying it  you must keep in mind that there are some advanced softwares that you should not use custom installations for unless you know what you do nt need  generally  the unnecessary addons are pretty easy to spot  but if anything do a little googling while installing if you re unsure ',\n",
              " 'good advice unless you love bing  toolbar and thousands of things even worse  more criminal is how they sometimes make you click  cancel  to not install a toolbar  making normal people think they would be cancelling the entire installation ',\n",
              " 'for nine if you add everything together over and over it eventually turns into nine',\n",
              " 'and now i have a headache ',\n",
              " 'this is definitely not a good ysk  normal people should not      around with the custom install option  anybody that  should  already knows about it  normal people ca nt handle the responsibility ',\n",
              " 'the 4 and 8 tips are a bit silly if the last bit is divisible by 4  the whole number is that makes sense if not sure  divide it by 4 well that s really not a trick then is it  that s like if i said i know a trick to peeling potatoes first off  hold it this way then  if you do nt know the trick  peel your own      potato  everything else was great  but 4 and 8 felt like a substitute teacher put it in while you were nt looking ',\n",
              " 'on  what kind of phone ',\n",
              " ' takes out smart phone  uses calculator  that was pretty simple ',\n",
              " 'you can also use callin  oates to hear hall amp  oates music 71926oates i dial it every once in a while ',\n",
              " 'in a near future  gt    til   that google will increase your google drive online storage limit by 2gb if you do this safety checkup today',\n",
              " 'and you can bump it up to 100 gb for just  199 i use that service to keep multiple pc s in sync it works great ',\n",
              " 'finally something i should actually know thanks ',\n",
              " 'turns out this is nt an automated thing some guy just looks up whatever people text him  and he texts back the wikipedia article ',\n",
              " 'i often see text on videos or images that s unreadable due to using the incorrect colour hope this helps ',\n",
              " 'the most surprising part of this is that they re willing to replace them free of charge ',\n",
              " 'this would have been great 10 years ago when we could nt just look it up on our phones ',\n",
              " 'would nt it also work to have black text and white outline ',\n",
              " 'if you do nt have data and wifi  how are you going to open the article  just google search it or use sirigoogle now that s like 20 times faster ',\n",
              " ' removed ',\n",
              " '844piewiki',\n",
              " 'they always try to sell the protection plan   what if you open it and it does nt work  this plan will allow you to replace it free of charge  i always say   if i open this box and it does nt work  trust me  you re replacing it free of charge ',\n",
              " 'piggybacking on this  microsoft is also  replacing many of its surface chargers sold before july 2015   https  wwwmicrosoftcomsurfaceenausupportwarrantyserviceandrecoverypowercord  ',\n",
              " 'gt     the vacuum manufacturer says your warranty is void because you used thirdparty bags instead of the company s moreexpensive  genuine  ones    gt     the law    gt  whether it s vacuum bags  printer cartridges  or parts or service for your car  federal law generally prevents you from being  tied in  to using any particular parts or services as a condition to maintaining your warranty coverage  gt     you should know    gt  an exception is allowed if the company can show that its product will not work properly without a specified item or service the company does not need to cover thirdparty service or parts  or damages that result  this one seemed useful to me ',\n",
              " 'wait so i work for a large electronics retailer  and most items in the store can not be returned after two weeks  the return policy   things like tvs and computers can be repaired for no charge  but smaller items like ipads  ipods  headphones  etc can not our company does not allow us to process it as a return after two weeks  there is no option in our pos system to fulfill that without a manager   we have to tell the customer to contact the manufacturer and that we cant do much else  is the wrong or against the law considering what this article has written ',\n",
              " 'example  http  imgurcomgallery0vmxo',\n",
              " 'a quick word about implied warranties this is not a free extended warranty almost every warranty i have read has language saying there is no implied warranty you are much better off using a credit card that has extended warranty coverage instead ',\n",
              " 'i am shocked that citizens have to pay to file taxes that s like paying someone so they can get some of your money ',\n",
              " 'i used turbo tax last year and it was completely free for me ',\n",
              " 'til  62k is poor in the us ',\n",
              " 'a lot of very good information  but i need to add one thing  as the owner of a small ecommerce business  i implore you  try to calmly discuss the issues you have  with the business  first often  if possible  we will try our best to accommodate  whatever the situation may be if you just initiate a chargeback  as the first course of action  there s no way that the actual issues will get resolved  and in the end  it will wind up costing the business at least  25 up to hundreds of dollars in bank fees for  processing the chargeback ',\n",
              " 'could you explain what the title means please ',\n",
              " 'another advantage is that it weeds out those who only want      and are nt interested in an actual relationship ',\n",
              " 'wow i thought this was a bot nice play on title ',\n",
              " 'i need advise like this like i need another whole in my head ',\n",
              " 'woot  took advantage of this and found that someone in the town over had been accessing my account definitely do this  even if you do nt need the extra 2 gb ',\n",
              " 'thanks op  now i have 19gigs ',\n",
              " 'woot  just ran through this with all my google accounts ',\n",
              " 'i did this last year and now i have 19 gigs ',\n",
              " 'awesome thank you  i was at 1493gb used  so this extra 2 really helps ',\n",
              " 'the advantage of using an addon  at least the better ones  is that the database of blocked ips is more easily and frequently updated when you modify your hosts file  it s up to you to keep it uptodate ',\n",
              " 'very nice  i did this not long ago  a few months i think  edit maybe it was a year  and was nt sure i could get more free storage again  but i did indeed the extra 2gb showed up right away for me  you can check your storage here  https  wwwgooglecomsettingsstorage',\n",
              " 'i used hamp  r block to joint file with my husband and it was free our household is above that cutoff ',\n",
              " 'thanks so much for this ',\n",
              " ' this topic   https  wwwredditcomryoushouldknowcomments2sviwgysk_you_can_most_likely_file_your_taxes_for_free   has totally not   https  wwwredditcomryoushouldknowcomments43jpykysk_that_if_you_make_below_62000_you_can_file   been posted about before   https  wwwredditcomryoushouldknowcomments2ttussysk_that_you_can_efile_your_taxes_for_free_if ',\n",
              " 'turbo tax federal free edition is free for 1040a or ez forms also state is free ',\n",
              " ' kiwix   http  wwwkiwixorgwikimain_page  is a free and opensource offline wikipedia app for windows  mac  linux  android  and ios it uses pretty serious compression  so a textonly version of all of english wikipedia  as of may 2015  only takes up 16gb 64gb and 128gb microsd cards are pretty affordable it comes in handy  because my smart phone only has texts  calls  and wifi  no data   it s also useful for poor countries that do nt have readily available internet access  there are other offline wikipedia apps can anyone recommend another  i d like to use one that is nt still stuck on may 2015 ',\n",
              " 'thanks for this information  2 gb added to my google drive ',\n",
              " 'i just run an extension cord directly from the generator to the appliances i want to run am i doing it wrong ',\n",
              " 'same with android ',\n",
              " 'i have successfully made a book about the sun and all the planets including pluto this is really greate',\n",
              " 'this tool is basically broken  but you can still download all of the text of wikipedia  here   https  metawikimediaorgwikidata_dump_torrents  enwiki  ',\n",
              " 'is the free tax filing with jackson hewitt subject to your income ',\n",
              " 'according to chipotle official website  offer is only valid until 6 pm est https  wwwchipotlecomraincheck',\n",
              " 'you can do so by rightclicking the video and clicking on the  loop  option ',\n",
              " 'i m a bot   bleep    bloop   someone has linked to this thread from another place on reddit    rbuffalo   ysk  you can get spent grains from your local craft brewery and distillery to make bread  pretezels  etc   https  npredditcomrbuffalocomments40j5qlysk_you_can_get_spent_grains_from_your_local      footer     if you follow any of the above links  please respect the rules of reddit and do nt vote in the other threads      info   rtotesmessenger      contact   messagecompose  to  rtotesmessenger        bot ',\n",
              " 'ysk that you are pedantic ',\n",
              " 'suhposedly say it again goood supposably isnt real ',\n",
              " 'if you speak americanglish  is it advize ',\n",
              " 'also  in my spelling   practice n  practise v  licence n  license v  edit  italics',\n",
              " 'here is a good example  http  youtubewr1yowesw8e',\n",
              " ' gt  voting  gt  2016',\n",
              " 'just found this brilliant website  https  wwwcommonsensemediaorg i was deciding whether to let my son play just cause 3 and found this via google  instead of just blindly trusting the contentage ratings  this website gives information about various media  video games  movies  books  music  apps etc   what it s content is like  ratings from both parents and kids as well as tips on what families can discuss in relation to the content  eg for jc3 you can discuss governments and evil dictators  should you wish to let your child be exposed ',\n",
              " 'common sense media is      if their  expert reviewers  actually played the games and watched the movies they reviewed in depth  then maybe it would actually be useful ',\n",
              " 'i find that site useful to look up good movies for young kids ',\n",
              " 'my suspicion is that this goes on with canned goods as well often when you buy a  125 can of wolf chili at fred s or dollar general  it has fewer beans  less meat and more liquid than the  175 one at albertson s  i ve also long suspected that the computers and consumer electronics bought at walmart may look the same and be the same model as you d purchase at best buy or target but they cost less and the materials used are shoddier  suspicions  mind you  not backed up by investigation results on my part ',\n",
              " 'my mbp 2011 was having these issues like 6 month before they introduced the program  so i could nt use it i fixed it day one of this program',\n",
              " 'i ll keep this short and sweet  as i ve attracted some haters for my previous privacyconscious advice regarding windows 10  if you open the windows settings  and navigate to privacy  you ll find about 13 subsections regarding various privacy options  i d recommend looking through all of it and deciding onebyone what you d like to keep enabled  vs what you d like to turn off vs what you d like to enable if it s already off  you can do whatever you d like  but to stop leaking personal data  i turn everything off except for the smartscreen filter  under general it still has its privacy implications  but i feel that it s a reasonably good feature and worth that tradeoff  when an app needs access to something that s been disabled  you can simply choose to reenable it ',\n",
              " 'i wouldif i did nt live in a      dry county  thanks arkansas ',\n",
              " 'this might come in handy thanks',\n",
              " 'thanks for sharing this excellent tip  can you also suggest what apps support viewing these hidden files ',\n",
              " 'any files you place in such a folder  images  videos  audio  txt  anything  will be hidden throughout the system to access those files you need a file explorer capable of viewing hidden files',\n",
              " 'relatedly  songza is saying we ll get  the same music experts crafting your playlists  but i do nt see anything of the sort on google play any ideas ',\n",
              " 'is nt this common knowledge by now ',\n",
              " '     the one for 7 is completely new to me definitely gon na be useful',\n",
              " 'ysk  do nt install shadyass software',\n",
              " 'this marks the folder as  hidden   an alternative would be to place an empty file with the name  nomedia  into the folder ',\n",
              " 'and here are proofs for all the rules  proof of the 3 rule  represent our number as a_1  10 a_2    10  n a_n we have a_1  10 a_2    10  n a_n  0 mod 3 iff a_1  a_2    a_n  0 mod 3  because 10  k  1 mod 3 so the number is divisible by 3 iff the sum of its digits are  proof of the 4 rule  represent our number as 100x10yz  where y and z are the last 2 digits since 100 is divisible by 4  we have that the entire number is divisible by 4 iff 10yz is  proof of the 6 rule  since 6  3   2  a number is divisible by 6 iff it is divisible by both 2 and 3 so we apply both rules 2 and 3  proof of the 7 rule  let our number be represented as 10xy so the number  after removing the last digit and subtracting it twice from the result  equals x2y we need to show that 7  10xy iff 7  x2y but this is true  since x  2y  0 mod       gt  x5y  0 mod       gt  10x50y  0 mod       gt  10xy  0 mod 7  using the facts 7x  7y  0 mod 7  the notation a  b means  a divides b    proof of the 8 rule  since 1000 is divisible by 8  equaling 125   the same proof as in the rule for 4 applies  but for the last 3 digits instead of the last 2  proof of the 9 rule  same as for rule 3  noting that 10  k  1 mod 9 ',\n",
              " 'ysk  you do nt have to deal with this      on leenooks',\n",
              " 'the only problem with this advice is that there are still so many users who do nt know how to educate themselves on the configuration selections  i d say the same users who are installing software infected with  addonware  or similar are probably users who can risk doing some actual damage when selecting configuration that writes log files  selects default file associations or changes directory permissions  i agree with your point  but actually i commonly suggest to my least  mature  users to go with the defalut settings  that typically assures the installation footprint is at least confined to something easily investigated or replicated  give those users 10 different options to decide on and someone could conceivably  foul  a device pretty badly lt  imogt ',\n",
              " 'contrary to popular belief  they are not only found in dirty homes infestations can also develop very quickly in clean areas all it takes to harbor an infestation is a single human host and a mattress learning how to get rid of bed bugs naturally is not an easy task however  you are definitely capable of doing these treatments yourself with the proper guidance  https  wwwyoutubecomwatch  v  pzlzmjzqqya',\n",
              " 'anyone know anything about 2011 macbook pro battery issues  whether or not they are covered  i just went ot use my macbook and the battery has hemorrhaged and i really need that battery to work ',\n",
              " 'awesome ',\n",
              " '19 has always been my favorite very similar to 7  except you remove the last digit  multiply by 2  then   add   it back to the remaining number  example    19    12  9   118  19 example    437    432  7   4314  57 52  7   19',\n",
              " 'this saved me just before christmas  i had a logic board failure and got it repaired for free though whenever i did anything cpu intensive the board would break again this happened 3 times eventually they replaced my old 2011 17 inch with a brand new top spec 15 inch  i would suggest doing something cpu heavy as i think this is what breaks the boardgraphics  i work in 3d design so have a lot of render intensive programs ',\n",
              " 'thanks  almost forgot this ',\n",
              " 'i ca nt be alone in this i have most of these burned into memory from final fantasy tactics ',\n",
              " 'rlearnuselesstalents',\n",
              " 'more info  https  wwwapplecomsupportmacbookprovideoissues saved me 300 bucks last week edit  for clarification   15inch and 17inch macbook pro models manufactured in 2011 15inch macbook pro with retina models manufactured from mid 2012 to early 2013 ',\n",
              " 'wait  what    back in august of 2011  i brought my 15 inch macbook pro in for repair  an early 2011 model  specifically for this issue at the time it cost me a whopping  600  cad  to get the logic board replaced  is there any way i can get that refunded  i understand it s a long shot  but the fact that the symptoms were exactly what was described there  and that the fix was what they offered for free is super frustrating ',\n",
              " 'this is not true for all garages  but you can find out usually by checking their website or social media pages  it was devised to help clear out snow emergency routes  but is also a great way to to avoid having to shovel out your car ',\n",
              " 'long division never hurt anyone ',\n",
              " 'why  for the love of       did i not learn about this in school ',\n",
              " 'if i have a 2011 17  mbp  which has shown the video issues  but i have since upgraded the ram myself  and also swapped out optical drive for an ssd i m out of warranty anyway right ',\n",
              " 'there is a program  ca nt remember the name  that sets these  applies other mods and rechecks on every boot to ensure they stay on ',\n",
              " 'you could use this to look up a term using no wifi or data  edit  i found their website if you want to learn more  http  weepowernet',\n",
              " 'throwback to chacha b4 the smartphone days',\n",
              " 'http  wwwtextengineinfo is a really nice tool for those who do nt have a smartphone  limited data etc it can give you local weather  directions  news etc it s worth knowing about if you re into using that kind of stuff ',\n",
              " 'this is good advice  i used all of these methods and successfully eradicated the little bloodsuckers in my apartment i brought them home from my chair at the office and they made a nest in the window curtain above my bed true story',\n",
              " 'i would be really interested in playing the dave game  i used play it a lot when i was kid ',\n",
              " 'there used to be a library service where you could text a question and some guy would look it up for you and reply could take ages though cause it was literally a guy',\n",
              " 'very cold weather makes road surfaces cold they tend to stay cold  and coupled with the precipitation that often comes with that temperature rise  can often result in the roads becoming ice rinks of death drive carefully  or do nt if you can avoid it ',\n",
              " 'what a looser',\n",
              " 'this tip would be cool about 15 years ago ',\n",
              " 'one of the bosses here at the office always writes  please advice  on her emails to one of our service providers when she is troubleshooting issues for our clients  which is her main duty  so this happens multiple times a day   not wanting to be  that guy  when i first came here  i would write it correctly on emails i sent to the same places that this boss needed to be copied on she has nt picked up on it in over ten years it s now an injoke among a few of us here   please advice  is the least of her writing and communications issues  really  but it s funny as      ',\n",
              " 'i feel like even if i did nt already have a smart phone i would still rather wait until i was near a real computer instead of dealing with text limiting in 99  of circumstances i m not sure what would be so urgent that you ca nt wait to research and would resort to a scrap of an entry  i would guess this is primarily used for settling arguments at a bar ',\n",
              " 'the grammar on their website and the service itself is concerning also  it returns no results or very limited results in most searches it s phonetic alphabet and foreign language results are terrible most things it returns are not exceptionally informative  however  it is a free service it gives you a simple summary of what some things are in layman s terminology it can be used anywhere that you have mobile coverage  i will continue to use it in hopes that it improves ',\n",
              " '9454 is wiki  but i ca nt figure out what 743 is supposed to be rid  she ',\n",
              " 'if you have android kiwix basically let s you download wikipedia ',\n",
              " 'it s  please advise  vs  give me advice  ca nt count how often i ve seen this wrong and it reflects poorly on your grammar to leadership ',\n",
              " 'it s going to text me an entire wiki article  ',\n",
              " 'i started last night but it said it could nt confirm my identity am i      come taxes or is there something else i can still do ',\n",
              " 'true for montgomery county  maryland  http  patchcommarylandsilverspringsnowemergencydeclaredfreeparkingincountylotsgaragessilverspring',\n",
              " 'fenriz radio is not on here absolutely rigged ',\n",
              " 'parking either costs money or time so ymmv  in every sense of the phrase ',\n",
              " 'let s throw then  vs than  in here too  then  indicates temporality or timing first you do something  then  you do something else  than  used for comparison i d rather do this  than  that  these are two completely different words that just happen to share a few letters if you mix them up your sentence means something completely different it may sound similar in your head but it is not  rant',\n",
              " 'my macbook shorted out and burned down my house  killing my entire family i had to move to a homeless shelter where i was anally      and caught      and now i m missing most of my limbs it s a sad sad world tell your loved ones that you love them and hold them tight because you never know when it will be the last time you see them  ',\n",
              " 'almost  p http  iimgurcomkfym7xojpg',\n",
              " 'for chicago this sounds like a reason to double prices for parking in garages ',\n",
              " 'emergemcies ',\n",
              " 'good to know down here in south florida ',\n",
              " 'philadelphians looking for carshelter  ppa lots will be  5 for 24 hours starting at 5pm today  source   http  wwwphillycomphillynews20160122_snowstorm__what_you_need_to_knowhtml ',\n",
              " 'ysk has gone to     ',\n",
              " 'that website is for the united states only  here are some other countries    australia   https  wwwacccgovauconsumersconsumerrightsguarantees    united kingdom   https  wwwcitizensadviceorgukconsumer    canada   http  wwwconsumerinformationcaeicsite032nsfeng01173html    european union   http  europaeueulifeconsumerrightsindex_enhtm  edit  some more    new zealand   http  wwwconsumeraffairsgovtnzforconsumers    ireland   http  wwwcitizensinformationieenconsumer_affairsconsumer_protectionconsumer_rightsconsumers_and_the_law_in_irelandhtml  i wanted to do france  germany  brazil  and a few others  but unfortunately i only speak english so i ca nt navigate their sites to make sure i m linking to a page that s actually useful ',\n",
              " 'yeah  disagree ',\n",
              " 'you can also open your browser app and type the same search term into google wo using mobile data ',\n",
              " 'does nt work on my galaxy s4  android 501 ',\n",
              " 'so you just tried to post something helpful and get a shitstorm of rude comments yay reddit  way to be super cool whatevs though  i thought it was useful  i m in america though lol  ',\n",
              " 'in any new relationship  people are infatuated with their so because their hormones are going crazy people are more likely to perceive the things they do in a positive light it s takes about 36 months for these initial feelings to die down      only intensifies these feelings for a person you really might not know that well  waiting on      gives you a chance to figure out if you really like that person and want to continue a relationship with them  rather than getting caught up in everything and prolonging something that is nt going to work in the end this also gives you an opportunity to build your relationship with verbal communication and other activities and if you do decide that this person is worth going further with  and have taken a few months to build a serious foundation  the first time will be that much better and worth while ',\n",
              " 'i was just thinking about the game syndicate today  going to play this as soon as i get home ',\n",
              " 'this reminds me of when i was a lot youngergee it might even be 10 years ago now  it s hard to say you could send texts to google  466453  i always used the  define  insert word you want defined   to cheat on tests  the were more tricks than this one just not sure what they were my presmartphone hack ',\n",
              " 'i do nt believe this for a second sorry i have never ever seen this before in any city  any examples ',\n",
              " 'this did not work ',\n",
              " 'i recommend zeliard kinda zelda iiish  but still good ',\n",
              " '    the law is often on your side when you do battle with a retailer here  s what you need to know to win the day  published  september 2010 it s important to know your rights when you re trying to resolve a dispute with a retailer but people often make the wrong assumptions about what the laws allow  or they rely on misinformation from friends  family  or merchants here are some common scenarios  the laws that apply  and advice on how to get satisfaction  this link is for us laws see the top comment and its replies below for other countries  including  germany   https  wwwredditcomryoushouldknowcomments45pccvysk_your_rights_as_a_consumerczzgqg1  and  brazil   https  wwwredditcomryoushouldknowcomments45pccvysk_your_rights_as_a_consumerczzjl3e   http  wwwconsumerreportsorgcro201009yourrightsasaconsumerindexhtm',\n",
              " 'or you could just google it ',\n",
              " 'why is there a  new  category ',\n",
              " 'what about games where can save your game  edit  i just looked at their faq and save function is not implemented that wipes out rpgs for example ',\n",
              " '     is really important everything leading up to you had a keen interest in      things ambivalent about it did nt have offspring to give any genetic ambivalence to a next generation even one ambivalence meant that line died out  so  when you decide to take a partner  you had better be sure you re compatible sexually putting off such a discovery for a long time  when you re committed in other ways  is a terrible idea ',\n",
              " 'my dad has a model that falls into those production dates but he is a rather light user  usually for just web and email probably as a result of that  it has nt shown any signs of failing  will apple replace the part even if it is nt defective because it s still anticipated to fail at some point  sort of like the xbox 360 s rrod ',\n",
              " 'people know you can text a public library for answers  right  you do nt even need to have a card or be local  my new england city gets questions all the time for a west coast city of the same name  and usually it s not until after the question is complete if we find out their location  if at all   http  wwwtextalibrariancom is one popular platform ',\n",
              " 'no slipstream 5000 though  ',\n",
              " '      thank you so much for this  incredible time waster but i love it   ',\n",
              " 'i had this problem before apple issued the extended warranty they made me pay 330 to repair the logic board a year later they mailed me a refund check without me having to ask for it great support ',\n",
              " 'did they not have 13  mbp back then ',\n",
              " 'well now i m obviously going to waste my life away on sim ant ',\n",
              " 'what s the best way to play a windows 98 game  i ve tried a vm but have nt been able to make it work ',\n",
              " 'i got my 2gb immediately i did nt use google apps though ',\n",
              " 'thank you so much ',\n",
              " 'only about 13  of people are even aware this virus exists and even more alarming is that some medical professionals never even heard of it  https  wwwnationalcmvorg edit  great article on how other viruses are similar to zika http  wwwnytimescom20160209sciencezikavirusmicrocephalybirthdefectsrubellacytomegalovirushtml  _r  0 edit 2  another good article http  qzcom613327thecmvviruscausesmicrocephalyinbabiesanditsmuchwiderspreadthanzika',\n",
              " 'thanks man  i always like getting extra storage in google drive even though i have 230 gbs in onedrive  ',\n",
              " 'good luck turning the tables on these collectors though just like phone solicitors are nt allowed to call your cell but it happens anyway without any penalty ',\n",
              " 'things like this makes me wonder how many other opportunities do i miss to sue people for large sums of money even if it was only a minor inconvenience to myself granted  this is probably a major inconvenience if they continue calling as debt collectors typically do nt stop  even when you die ',\n",
              " 'i discovered someone in my state  capital city  even  was apparently logged into my acct via linux because i wanted 2gb of drive storage  thanks google   d',\n",
              " 'i totally forgot goggle my drive existed  i found a bunch of before  pics of the underside of my camper van steering components  i am replacing a bunch of these bits  so this could be useful  i have no idea how the pics of the old underside of my camper van are in there ',\n",
              " 'i just got my 2gb immediately after doing it  remindme  one year  do 2017 google security checkup for an additional 2gb ',\n",
              " 'cool  thanks for the tip ',\n",
              " 'done  2 extra gb  just like that ',\n",
              " 'did it twice nothing happened  no extra space is it only for usa users ',\n",
              " 'did it found an app i do nt use any more wanting permissions not anymore you do nt ',\n",
              " 'apparently it stacks if you did it last year  then it adds on to the 17gb you had the previous year  https  iimgurcomdnzhmrrpng',\n",
              " 'as someone who uses google drive to archive everything in life that could potentially be important  i really appreciate you sharing this  op thanks ',\n",
              " 'reindme  one year  do 2017 google security checkup for 2gb free ',\n",
              " 'for those who prefer a more official source  the irs also has  details   https  wwwirsgovuacfreefile  doyourfederaltaxesforfree  ',\n",
              " 'ysk that this only really works on      and that mythbusters did some stuff on this and concluded that it was cheaper to buy the good stuff to begin with they also figured out that moderately cheap       but not bottom shelf       was basically as good as very expensive      ',\n",
              " 'anyone know why some people have gotten near instant returns into their banking account but i m about a week or two in still waiting ',\n",
              " 'discovered this  https  wwwgooglecomsettingsaccountinactive basically a google will if i do nt log into anything google for 3 months clearly i am dead dead dead  or in a coma  and allows my wife to get into my contacts  email  youtube account etc i have all of our family videos in my account and my password is randomly generated from a password manager ',\n",
              " 'thank you it worked like a charm ',\n",
              " 'thanks  up to 19g now ',\n",
              " '     it still says 15 gb  ',\n",
              " 'wow  this security thing might have actually saved me  a few months ago i had charges from a town 3 hours north of us  naperville  il  on my debit card that clearly were not mine i reported it to my bank  got a refund and a new card  checked this today and saw that my google account is currently logged into a computer in naperville  il not sure if it s a coincidence or something that i definitely need to address anyway  i changed my google password so it should be ok  any advice is appreciated ',\n",
              " 'thanks op found out someone accessed my account as recently as yesterday changed passwords and reenabled 2 step verification ',\n",
              " 'hamp  r block s normal website allows you to file federal free regardless of income and does not force you to use a 1040ez personal taxes only though  they charge for business i believe state was  10 for me  ca in case that matters   ca has a free self file on the franchise tax board site  but it was worth the  10 for me to not have to enter the info twice ',\n",
              " 'thank you  ',\n",
              " 'here are some sources http  wwwpropublicaorgarticlehowthemakerofturbotaxfoughtfreesimpletaxfiling http  techcrunchcom20130327turbotaxmakerfunnelsmillionstolobbyagainsteasiertaxreturns i personally use jackson hewitt  who i wo nt link per the rules but there are other similar sites that are just as easy and free as turbo ',\n",
              " 'so  what s to stop someone from lying and saying  wrong number i m not him  please no call   ',\n",
              " '     it  i m without a job right now and i was really hoping for that check ',\n",
              " 'great and i just filed earlier this week  i wonder if they got ddosed ',\n",
              " 'i m so glad my boss does nt      around amp  got me my w2 on january 16th i just got my federal back this morning amp  had my state return this past tuesday  ',\n",
              " 'on the standard website this feature is usually only available through subscription fees for a   pro   wolfram alpha account the ios app is just a small one off payment for the purchase of the app to unlock this feature and possibly others    edit    this works on android too  utaelsil',\n",
              " 'hey  this is right up my alley  i did a couple of ama s on this topic  see  here   https  wwwredditcomriamacomments401lmsi_am_attorney_jeremy_glapion_and_i_still_sue  and  here   https  wwwredditcomriamacomments3cthh6i_am_attorney_jeremy_glapion_and_i_sue_companies   that said  you did not get this exactly right  first  the  one free wrong number call  applies whether or not you tell them they have the wrong number for example  if they call and you do nt answer  and they call again  the second call is in violation of the tcpa  even though you never told them  second  this part of the law and rule only covers calls to cell phones  or paypercallminute voip type services   if debt collectors are calling your landline  you re sol  third  while most debt collectors use the equipment covered by the statute  the statute is equipment dependent only calls made using an automatic telephone dialing system or prerecorded voice are covered that said  99  of calls i ve seen use one or both of these systems but it s theoretically possible a one man collection agency is literally manually dialing numbers from its rotary phone  finally  this is nt something that only the fcc can enforce if you re getting debt collection calls on your cell phone for someone that s not you  you can sue for  500  1500 per call  better argument for  1500 if you told them to stop or that they had the wrong number and they kept at it  my example i gave in the  first  paragraph would likely be  500call   most attorneys wo nt charge you anything out of pocket either i take all of these on contingency fee  13  my hard costs   if i lose  you owe nothing this can get pretty big i have seen people receive thousands of wrong number calls from the same agency  not my case  unfortunately   in short  the tcpa is awesome  source  tcpa attorney ',\n",
              " 'windows 10 has a buried p2p feature which is on by default not only is this a sneaky way for microsoft to avoid higher bandwidth fees on their end  but it s using your bandwidth and cpu cycles  which for older computers can have a detrimental effect on whatever else you re trying to do  to change this  open settings gt  update amp  security gt  advanced options  you might want to pause here to change the top dropdown option selection from automatically restart to notify you to schedule a restart  gt  choose how updates are delivered  here you will see that there s an onoff option and two choices  to share updates with pcs on your network  or the default option to share updates with pcs on your network and pcs on the internet  if you first choose pcs on your network and then change the top option to off  then even if this p2p service is reenabled in a future update  which i believe i ve observed happening   it should at least only operate within your own network ',\n",
              " 'just to be clear  i m not a windows 10 hater  sure i ve seen some issues with it  but mostly they go away on their own with updates i ve been using windows 10 for quite some time  and it s been mostly good to me  default behaviour may have changed  or it could depend on the flavour of windows  x64 vs x32  home vs pro  dell vs acer vs asus  etc   being that i do remote technical support for a living  i ve seen many installs of windows 10  and from my observations  these options have been on by default  i m not saying to disable updates here  that would be a dumb move considering how many fixes and security patches are released all the time  but i do nt like the  observed  default behaviour of rebooting whenever it wants to after updating  and i do nt like the  observed  default behaviour of having windows update torrents being shared with the internet  i dislike this happening because a  my cpu is a pos i had from a bitcoin mining rig  so i need them cycles and b  isps in canada are now enforcing bandwidth caps  and streaming netflix is pushing it as it is ',\n",
              " 'i just recently found out i have this virius ',\n",
              " 'some personal experience i was diagnosed with this in december  having most likely caught it at 29 weeks pregnant i knew i had been negative years ago  despite a history of working with children so when my doctor was pretty useless upon my reporting two weeks of 102 fevers and monolike fatigue  i knew i had to take matters into my own hands i found an infectious disease doctor and asked to be tested and while he insisted it could nt be cmv  he added tests for mono  lyme disease  and      it was  of course  cmv  a primary infection  meaning my first if i had caught the virus in the first or second trimester  i shudder to think what might of become of the pregnancy  a recent fetal mri showed that the baby has no calcifications on the brain but an enlarged abdomen and liver  i m expecting he caught it but hopeful he does nt have neurological damage      be tested at birth and monitored for two years for hearing loss the crazy thing is that many countries routinely screen for this in pregnant women  but not in the us despite many initiatives to do so ',\n",
              " 'this would substantiate my claim  that it ca nt possibly be zika causeing microcephaly  here s why look at the cases of infants born with both microcephaly and having zika zika takes a week to pass  which means that the child would ve gained immunity by then if it had been contracted during the period of headbrain development  not in the week before birth so in those cases you know with certainty it was nt zika causing the microcephaly  it was something else i m guessing that it is more likely relate to one of the chemical spills in the region last summer or pesitcides  but corporate power is using the media to shape the narrative to avoid criminal or civil lawsuits  or now i m considering this cytomegalovirus ',\n",
              " 'i did nt find this site very informative under prevention all it says is  if you have young kids are are around young kids you have a high chance of getting it  that s not exactly preventative advice ',\n",
              " 'cmv is common and any obgyn will know about it there is an umbrella of diseases known as torch  toxoplasmosis  other aka varicella  syphilis  parvovirus b19   rubella  cytomegalovirus  cmv   and       that are common causes of congenital anomalies there are quite a few diseases that can cause microcephaly it can also happen in the absence of any identifiable cause ',\n",
              " 'http  mobilenelsonmullinscomnewslettersfccissues so basically if you are getting annoying collections calls for someone that is nt you  the collection agency is in violation of this rule  they have the leeway of being asked to remove the number once if they do nt remove the number after being asked every call after is a violation if you record the calls you could lead to the company facing huge fines via a forfeiture order  here is an example of this  http  wwwballardspahrcomalertspublicationslegalalerts20150814fccfinescompanies296millionfortcparobocallviolationsnewinterpretationsaspx edit  read uglaplaw comment for morebetter info',\n",
              " 'as a former debt collector  can confirm ',\n",
              " 'i actually caught cytomegalovirus while working at an elementary school as an aide  i had chills  fever  and at one point in the night i was too afraid to go to the bathroom because i was hallucinating that i was surrounded by lions  i did nt even know i was a carrier for life though  til i do nt recommend it as an experience  everyone wash your hands ',\n",
              " 'sadly  this fix is probably only a temporary measure still nice to know it can be used in the meantime  btw  you ll also have to remove   cboxoverlay right above   colorbox  if you re familiar with the console or tampergreasemonkey  you can use  jquery    colorbox   cboxoverlay   empty   remove   ',\n",
              " 'this is part of the torch group of diseases that can be devastating to fetuses but in the us  it is only screened for if the mother had a suspected exposure  not sure about in other countries ',\n",
              " 'i m negative for this and i worked in hospitals and day area ',\n",
              " 'what if the call is a robot  my old number was a bakery and the debt collectors called me every other day  twice a day it never gave me an option for a live person no matter what i did',\n",
              " 'i m sorry i missed that part the way op worded it was vague and i kept referring back to it  that being said  i d love to see how this works when people stop using landlines ',\n",
              " 'if you shipped something with ups and it arrived damaged  they will fight your damage claim tooth and nail they make a lot of money off of packaging  insurance   and they do not like to pay it follow these steps to make ups pay for what they did  amp  nbsp  amp  nbsp  before you ship  amp  nbsp    step 1  take pictures of everything   the item you are shipping  the packaging  the item in the packaging  you mother since she s getting kinda old  amp  nbsp    step 2  pack your item according to the  packaging guidelines   https  wwwupscomcontentusenresourcesshippackagingguidelineshow_tohtml     these are not guidelines  there are shipping law that will be used to      you later choosing the correct box is important  amp  nbsp    step 3  declare the cost of your item and ship   they may say things like  how much insurance would you like   or  would you like to insure your package    but do nt be fooled these vague words  this is not insurance this is a declared value  and as far as ups is concerned  you will not see this money  amp  nbsp  amp  nbsp  after your package has arrived in hundreds of pieces  amp  nbsp    step 1  take pictures immediately   do nt pull everything apart keep every shard plastic and splinter of wood  amp  nbsp    step 2  file a claim  and ask for the phone interview   this is the most important step you will open up the  packaging guidelines   https  wwwupscomcontentusenresourcesshippackagingguidelineshow_tohtml  that you used earlier   when asked about your packaging  read your guidelines word for word   when asked about damage to the box  tell them there is damage to the box that coincides with the item damage   when asked if there was at least 2 inches of wrap surrounding your item  you will say yes  amp  nbsp    step 3  dispute the denied claim   yes  there is a very good chance they will automatically deny your first claim for some      reason  we did nt do it  or  we ca nt see how the leg broke off  dispute the claim and fight it further you have met all their shipping guidelines  so should win eventually  amp  nbsp    step last resort  fax a demand letter to the ups store you shipped from   hopefully it does nt get to this step  but you can write up a letter detailing the entire process  explaining you want compensation for the cost of shipping and the declared value of the item  here is a sample demand letter   https  iimgurcomuwfxjb4png   shipping through staples with ups is the same  just extra       they will call you within 10 minutes of faxing  be polite and cooperate  but follow through  amp  nbsp  this may also apply to fedex or usps  but i can not speak for them ',\n",
              " 'if you ca nt drop your package repeatedly 10  onto concrete without damage  you packaged it wrong source  i have repaired ups  sorting equipment before  also  fragile  means throw harder ',\n",
              " 'this should be loose versus lose i see that typo here all the      thyme ',\n",
              " 'ys also k that shouting into the phone  wrong       you  and the thousands of variations of this will not stop calls ',\n",
              " 'what frustrates me is that microsoft word ca nt tell the difference and always wants to change advise to advice and vice versa ',\n",
              " 'we re not 8 years old  bub ',\n",
              " 'i wish ysk would ban these dumb basic english grammar posts ',\n",
              " 'ticket barcode as well ',\n",
              " 'no     ',\n",
              " 'i hope they both have the same pronunciation',\n",
              " 'i have a reassigned number from some lady named rosa i got calls from her kid s school district saying they were nt attending  then calls from automated debt collectors with no number to call back every day there was no  hold on the line  or anything credit card companies  then real people asking for her from different companies  who knows what  i blocked the numbers but it tells me they called under a tab in my phone annoying ',\n",
              " ' 1 part mayo  1 part ketchup  14 part worcestershire sauce  garlic salt to taste  dashes of black pepper mix them all together well  it s the real deal  edit  for those that do nt know what raising cane s is  it s this chicken      place that s prevalent in the southern states and other parts of the us they re known more for their sauce than the chicken they fry  this sauce will go good with fries  chicken and other stuff you may feel compelled to try it on ',\n",
              " 'the only canes in new jersey shut down i miss it so much from my time in louisiana ',\n",
              " 'does nt matter if you re technically correct or not  everyone s going to misunderstand you if you do this and if you try explaining in person you re just going to look like a        never tell me the odds   https  wwwyoutubecomwatch  v  ea2hs8nl4s4 ',\n",
              " '  tl  dr  you should probably be using bitdefender   hello  professional tech guy here i run a business where a big part of my job is doing virus removals  fixingremoving   norton and mcafee   malfunctioning antivirus  and securing systems when clean i ve been doing it for a long time and i m very good at what i do given the recent  avast controversy   https  supportmozillaorgenusquestions1028987  i d like to clear some some very outdated misconceptions and give some up to date advice as to what you should be doing to protect your pc without spending a bunch of money on software licensing my opinions will be opinions and my facts will be sourced  the major misconception that i want to clear up is the one that microsoft security essentialswindows defender  mse from here on out  is a good antivirus or even a good enough antivirus solution based on tests from the two leading independent antivirus benchmark companies mse barely qualifies as antivirus at all it consistently rates at or near the bottom in terms of protection from not only zeroday attacks  which is what you really want your av to protect against  but also wellknown malware that has been in the wild for a while  here are the basic ratings from avtestorg for mse protection   date  os  rating6           february 2014   https  wwwavtestorgenantivirushomewindowswindows7february2014microsoftsecurityessentials44140671   win 7  00    august 2014   https  wwwavtestorgenantivirushomewindowswindows7august2014microsoftsecurityessentials45143171   win 7  00    december 2014   https  wwwavtestorgenantivirushomewindowswindows7december2014microsoftsecurityessentials46144986   win 7  00    april 2015   https  wwwavtestorgenantivirushomewindowswindows7april2015microsoftsecurityessentials47151547   win 7  05    august2015   https  wwwavtestorgenantivirushomewindowswindows7august2015microsoftsecurityessentials48153247   win 7  30    jun 2015   https  wwwavtestorgenantivirushomewindowswindows8june2015microsoftwindowsdefender47152347   win 8  05    oct 2015   https  wwwavtestorgenantivirushomewindowswindows10october2015microsoftwindowsdefender48153747   win 10  35  if you look at the actual detections rates you can see they go as high as 999  in some tests the problem is the consistency there are dips as low as 49   you want an antivirus that is capable of good protection  especially against zeroday attacks  from one month to the next mse has been improving as of late  but it is still far below competitors in terms of protection   take a look at the october numbers for windows 10   https  wwwavtestorgenantivirushomewindows   see the little icon to the right of the product names  that is the avtest certification icon microsoft lost their certification from avtest in  october   2012     https  wwwavtestorgenantivirushomewindowswindows7october2012  and has nt been able to earn it back since  avtest makes easy to digest numbers for bloggers and end users a much more detailed source for techies is avcomparativesorg  avc   we re going to look at the results of the most important test  the realworld protection test the following data is sourced from  this chart   http  chartavcomparativesorgchart1php   note that mse is not even on that list avc does nt even consider mse an option instead they use it as a baseline for the absolute bare minimum protection that is represented by the white dotted line it s basically just a metric to embarrass other av providers for being worse than mse     month    mar  apr  may  jun  jul  aug  sep  oct  nov  dec                            blocked    846   899   909   918   896   928   931   957   970   945   in the  november 2015 realworld protection report   pdf   http  wwwavcomparativesorgwpcontentuploads201512avc_prot_2015b_enpdf  mse ranked 18th place 4th from last  in the  june 2015 realworld protection report   pdf   http  wwwavcomparativesorgwpcontentuploads201507avc_prot_2015a_enpdf  mse ranked 21st place dead last the same is true for  november   pdf   http  wwwavcomparativesorgwpcontentuploads201412avc_prot_2014b_enpdf  and  june   pdf   http  wwwavcomparativesorgwpcontentuploads201407avc_prot_2014a_enpdf  2014  gt  but what do all these numbers mean  8497100 seems like pretty good results  there s no such thing as a perfect antivirus as such when we score them we score them against others in the industry if you look at the green bars  malware actually blocked from infecting the machine using realtime scanning  in the avc chart you ll see nearly all of them are considerably higher than the  baseline  which represents mse now we re going to look at those numbers to make a few recommendations   there are a lot of options for free antivirus out there the numbers are nt everything  but they re very important let s look at some of the nonstatistical drawbacks of the top contenders so you wo nt rag on me for not recommending your favorite free av    avira   is out of the running because it includes a nag screen popup asking you to buy the full version there are ways to disable which involve hackery with file system permissions but that s for advanced users in my eyes this makes avira as  free  as winzip it s just an indefinite free trial    avg    while having respectable detection rates  has become an extremely bloated mess over the years it includes a bunch of      you do nt need it is also prone to breaking things like your internet connection    malwarebytes   is not a comprehensive antivirus solution and the free version offers no realtime protection    avast   attempts to manipulate user s email without their consent by adding a   signature   advertisement to the bottom of all outbound emails it detects    comodo   sucks at detection rates  so what are we left with  here are the two best free antivirus programs as of the end of 2015    panda   http  wwwpandasecuritycomusahomeuserssolutionsfreeantivirus  panda has consistent toptier detection rates there is a nag screen but you can disable it permanently in the settings the drawback to panda comes in the performance hit panda ranks somewhere in the middle of the road for performance impact by both  avtest   https  wwwavtestorgencomparemanufacturerresults  and  avc   pdf   http  wwwavcomparativesorgwpcontentuploads201511avc_per_201510_enpdf   this performance impact is not going to be noticeable to your average user but if ricing your system is extremely important to you  move on to the final recommendation  on my windows 10 work laptop i run panda because i regularly connect it to potentially infected networks and sometimes must plug in possibly infected storage devices panda has nice usbscanning features like i said  computer repair guy here infected machines are a way of life for me not something i can avoid  if you re of the tinfoil hat variety  panda was founded by a scientologist but he is no longer the ceo there was some  controversy  in france in the late 90s about the thenceo making financial contributions to scientology no other allegations have been made relating to panda and scientology    bitdefender   http  wwwbitdefendercomsolutionsfreehtml    direct download link since some are having problems with redirects   http  downloadbitdefendercomnpdfreeantivirus_free_editionexe   if we re going purely by the numbers  bitdefender not only wins out over all the other free antivirus programs  but almost all of the paid versions as well the lowest score bitdefender received since march 2015 is 998  in the avc realworld test while regularly pulling in 100   in  october 2015 bitdefender got perfect scores   https  wwwavtestorgenantivirushomewindowswindows10october2015bitdefenderinternetsecurity20152016153791  for all metrics that avtest measures  i have a secondary windows 10 install on my main desktop and if i ran antivirus on it  i would run bitdefender my home antivirus is built into the network and i run linux 999  of the time so i do nt bother     but i m smart and i do nt visit shady sites or download sketchy files first off  quit acting like you do nt watch streaming      you re not convincing me or anyone else i ve had dozens of customers look me square in the eye and insist they do nt watch      while i have their      pornriddled   bookmarks bar   open in front of them if i do nt buy it from people who are paying me  i m not buying it from strangers already on the internet  second and most importantly  shady sites and downloads are not really what you should be concerned about the most what you should be concerned about are zeroday vulnerabilities in the software you run like flash  java  browser extensions  your browsers themselves   third party software and libraries used in other software you run   https  wwwopensslorgnewsvulnerabilitieshtml  y2015   and yes  even your  graphics drivers   http  wwwsecurityweekcomnvidiareleasesfixdangerousdisplaydriverexploit  which are exploited every day to serve malware to unsuspecting users via otherwise benign seeming means there are many completely trustworthy and legitimate sites and services that have been compromised to serve malware over the years  i do nt care how many science      made us more smarter  you should be running antivirus on your windows machine unless you re in it and know what you re doing no  building a couple of gaming rigs and running tron on your grandma s computer does not make you an it expert no   i ve never had an infection before and i do nt  is not an argument     but is there any reason to buy an antivirus program  short answer  no the primary reason for paying for antivirus is support that is  a number you can call where someone will remote into your machine for free and repair any issues that may come up as a result of viruses or the antivirus program breaking that can be handy for grandma who lives three hundred miles away and you do nt want to constantly remote into her machine to fix things your average user does nt need it  paid commercial antivirus programs usually are part of  security suites  that have a bunch of bloat features you do nt need or can get from other better  potentially free sources like password management  phishing protection  or browser filtering  and the final kick in the pants  more bloat  more opportunities to break kaspersky is considered the gold standard for detection rates it has been consistently at the top of the charts for many years however i spend an inordinate amount of time unfucking kaspersky because some part of it breaks  usually the updater norton is another antivirus that consistently breaks the machines it s installed on and can be a pain to remove if you do nt have the right tools one example is norton installs shell extensions  the custom options that show up when you right click on a file in file explorer  which regularly break in a way that causes file explorer to go into a launchcrash loop at boot making your pc unusable the less a piece of software does  the better chance it has of doing everything it does well    do i need a firewall to go with my antivirus  the answer is typically  no despite what hollywood has tried to teach us  a firewall is not some sophisticated piece of software with intelligently blocks hackers from your computer that kind of software exists  but is far outside the scope of even your typical power user a firewall simply blocks incoming and outgoing ports based on predefined rules windows comes with a completely passable firewall built in  as does every consumer router sold in the last decade if you re running windows behind a router  you re already behind two firewalls furthermore  the idea of someone  hacking  into your computer by breaking into it over a network is more or less  hollywood nonsense that does nt belong anywhere outside of bad ncis episodes   https  wwwyoutubecomwatch  v  u8qgehh3keq   if your computer is compromised in a way to let an attacker in  it s because you ve been infected not because they broke through your two firewalls from behind seven proxies if you re actually at risk of being truly and properly  hacked  over a network  you re running servers on your network and hopefully know what you re doing already  i hope that was informative every time the discussion of free antivirus comes up on it turns into a huge confusing round of anecdotes and mud slinging hopefully this can address the common question of  well then what should i use   next time that discussion is raised  securing your computer is much more than just having the best antivirus  but that s not in the scope of this post maybe next time  happy computing ',\n",
              " 'i used to swear by avast until they went over to the dark side after a lot of research i started using webroot it s very lightweight and its real time protection is superb it s not free but it s cheap enough if you look for deals ',\n",
              " 'i m a bot   bleep    bloop   someone has linked to this thread from another place on reddit    rbestof    u  hittingsmoke details why microsoft security essentials is currently nearuseless  and which free antivirus programs actually work   https  npredditcomrbestofcomments4127gwuhittingsmoke_details_why_microsoft_security      footer     if you follow any of the above links  please respect the rules of reddit and do nt vote in the other threads      info   rtotesmessenger      contact   messagecompose  to  rtotesmessenger        bot ',\n",
              " 'i m dumb hamp  r block did mine on the 27th popped in my account today via direct deposit it says  pending direct deposit  it shows my available balance is including my return money is it available ',\n",
              " 'is this why it s taking so long to get my return ',\n",
              " 'i just checked this on both of my computers not only was it set to  off  by default  the radio was set to  pcs on my local network  by default i have never changed these settings  i did nt even know they existed sooooo huh ',\n",
              " 'ysk this post is     ',\n",
              " 'there needs to be a class action lawsuit filed against them based on this garbage ',\n",
              " 'the best part about it is the detail instead of a generic  some language  or  some violence  it will actually say what the violence or language is  so you can judge it for yourself ',\n",
              " 'this is      ',\n",
              " 'yay    bring reading back   ',\n",
              " 'relevant story  http  thegaragejalopnikcomhowupsscrewedapopularyoutubecarguyoutof10001738255686 this brings in to question some recommended methods in this thread',\n",
              " 'p2p on your local      network not the internet ',\n",
              " 'main breaker  turn off ',\n",
              " 'i ll probably get flak for this and a bunch of downvotes  oh well if an item is very important to you and you are going to pack it yourself  because everyones  good enough  differs  over pack it guidelines say one layer of bubble wrap  put 2 layers on guidelines specify 3 inches of peanuts surrounding the item  put 5 or 6 inches of peanuts around the item when you fill the box with peanuts  overload the box  so when you close the flaps on top to seal the box up  it packs the peanuts tight around your item i have even seen a box in a box pack you item like i said  then get an even bigger box  drop a layer of peanuts into the bottom of it  put the small box into the bigger box  dump more peanuts into the bigger box and surround the inner box with them  again overfill the box with peanuts so when you close the lid  it packs tight  now a lot of people will call me out for over doing it yeah it s overkill  however if the item means a lot to you or is one of a kind  would nt you rather go above and beyond to give it a better chance of arriving  years ago i worked for a ups store  i learned a lot and we had a great team that packed boxes well  which is another option if you really deeply care about your item  break out the wallet and have the ups store folks pack your item now days i think they even guarantee their work now i m not saying every single package will always make it unharmed you would nt believe how many people thought that marking a box as  fragile  would be its armor fragile may be good for the drivers picking up and delivering your items  but once at a terminal or hub  large machines built for efficiency and not made to look for the word fragile will be bumping  sliding and moving your package around at a fast rate  in closing  ups is a good company towards its customers but as with any group there are bad apples they are my preferred carrier and ever since i shipped my first box and to this day  i have never had an issue  something like 15 years  i will give op credit for covering their      on stuff  my advice here is in hopes that it never have to make it to that level ',\n",
              " 'i do nt get step 3 before you ship how much  insurance  i get is the declared value  essentially  i m paying for my item again with ups ',\n",
              " 'usps will try and      you over on insurance claims as well i shipped a pc and it arrived smashed to      i had enough insurance on it to cover everything but they refused to honor it at first it was one i built myself so i did nt have a single invoice for the system i had to document the purchasereplacement price of every component  they applied depreciation  and paid me that amount following your steps might have helped but i ca nt say for sure ',\n",
              " 'offtopic but i have a hard time picturing what circumstances would lead someone to pay  67 to ship his own  100 computer to himself ',\n",
              " 'irs was ridiculously fast this year efiled last friday and had it in my bank yesterday ',\n",
              " 'i work for a company that received dozens of packages a day from ups  and ships out hundreds each day the biggest weapon you have in getting a successful damage claim is having the recipient deny the shipment due to damage  when the ups guy shows up at your door with a package  inspect the package before you sign for it if there is even a hint at damage  tell him that you want to either inspect the contents before signing  or just refuse the package because of damage especially if you are dealing with a mediumsized business  this will usually lead to a replacement being shipped  often before they even get the damaged back if you are shipping personal items  this obviously does nt help as much also  be careful  because they may try redeliver the same package again the next business day ',\n",
              " 'this could not have come at a more opportune time  i m studying for my comprehensive exams ',\n",
              " 'my best tips for shipping   if you can lower the weight and or value of a single package by separating them into different parcels  do this instead of paying for insuranceextra weight costs   if it is nt summer  be sure to add a little extra tape to seal all the seams of the box and even wrap a few around the entire circumference of the box in a    pattern to secure its shape   only ship through official upsusps locations  not any third partyaffiliated storefront because their package handling training and package liability are both garbage   if there ever is a problem  go to the location it was shipped  or the primary shipping center near you  and start demanding to speak with someone who can help you with your claim until you get help or speak with a manager and you can escalate things as needed from there their phoneemail techs seem trained to avoid liability for damage claims and i ve literally had them hang up midcall with me before not as easy to hang up on someone inperson  i ve worked with ups package handlers before  they do nt care what s inside the box you want to package your parcel in a way that makes it okay to toss on the ground or inout the back of a vehicle ',\n",
              " 'i had good luck calling the company out  fedex in my case  on twitter i think that helped me get more attention and urgency to my case ',\n",
              " 'it s done through host file instead of extensionsadd ons my brother showed me  probably a lot of you already have it good luck  http  someonewhocaresorghostshosts',\n",
              " 'use a dns server that does the same thing but is administered by someone who updates it ',\n",
              " 'dang  thought it said irs',\n",
              " 'or just say may the odds be ever in your favor',\n",
              " 'if you re a canadian  the solution to not getting      over by ups is not to use them at all i got charged  30 for bordercrossing fees on an  8 item ups can go to      ',\n",
              " 'what about malwarebytes ',\n",
              " 'using webroot av here  it has no nag screens  all i need ',\n",
              " 'if it s a framed picture  you can take it to a frame shop and get them to write an estimate for reframing or replacing glass ',\n",
              " 'any call that starts with  yes  hello i m calling from the windows  withwithout an accent is a call to hang up on without thinking twice  if you had continued  you ll get   to alert you of a virus on your computer we would like to offer you a complementary fix by visiting treeple dhabowlyou dawht  this story ends with your computer being controlled and then ransom ware  warn your elders as well as yourselves ',\n",
              " 'person to the right would be anticlockwise  not clockwise ',\n",
              " 'dogs like to eat dog      too  they do nt care ',\n",
              " 'why do nt you guys have traffic lights or round a bouts at those intersections  a four way stop seems like a really good way to have an accident',\n",
              " 'so do i just take said grains  grind them up in my vitamix and use it like reg flour ',\n",
              " 'rbreadit might like this',\n",
              " 'dogs are nt really meant to digest grains  just fyi they re carnivores be careful with portions when feeding them anything with spent grains  or even regular dog food with filler  grains  ',\n",
              " 'i met a guy who built a company based on this idea he makes granola bars from spent grain they re really good  but still overpriced  i think  http  wwwregrainedcom',\n",
              " 'what file explorer do you use so that you can see them ',\n",
              " 'i ve wanted to get one of these calls i d run their      on a virtual machine and then terminate it as they re trying to do something  or  i d run their      on linux and listen to their frustration as they can not gain control ',\n",
              " 'https  enwikipediaorgwindexphp  title  special  bookamp  bookcmd  book_creator edit  noticed there s some comments saying this thing is kinda broken hey  wikipeople  could you try to tighten this      up  it s a great feature that has been nonpromoted and neglected printed books on demand could be huge for the wikimedia foundation ',\n",
              " 'i ve actually been using this to study for the      exam portion of my seminar lol',\n",
              " 'will this help with their debt issues ',\n",
              " 'the usps discontinues priority mail express flat rate boxes  so is this all flat rate boxes  or a specific kind ',\n",
              " 'well considering i have to ship a package across the country tomorrow       timing',\n",
              " 'not if you ship from china ',\n",
              " 'it s not an apple extended warranty it s a repair extension program there s a big difference ',\n",
              " 'does anybody know if media mail prices staying the same ',\n",
              " 'where do you go for service that would only charge you  300 total partslabor for a logic board replacment ',\n",
              " 'canada post raised their parcel shipping as well for all you canadians out there ',\n",
              " 'they are now also making you pay 4  for a full roll of clear tape instead of providing a piece of it for free like they used to',\n",
              " 'as someone that does a lot of live bird shipping all over the us  this is going to hurt business it s already insanely expensive ',\n",
              " 'that weight increase makes too much sense much better to set it at a round pound than randomly around 34 a pound ',\n",
              " 'hopefully  they will take this chance to also stop being so      terrible with their service at least  in my area ',\n",
              " 'this is more obscure and helps more with multiplication than division  but also with six  six times any even  will always have a product with the same number in the ones place as there was in the number you multiplied 6 by eg 6  1   2    7   2    6  3   8    13   8  ',\n",
              " 'daily usps shipper here maybe this was nt publicized to the general public much  but pretty much every sales amp  fulfillment channel has had a posting or email alert on this for weeks amazon  ebay  shipstation  etc        even our dazzle app alerted us last week or the week before most of our stuff is first class padded envelopes  so it does nt affect us too much  but it s hardly  out of the blue  news ',\n",
              " 'this again  and now on ysk instead of lpt lt  sighgt  i eventually blocked lpt because it was just getting ridiculous and i hope this is not happening here too  okay  you need to clarify that this applies to standard installations  not advanced software if you choose to do advanced things when installing things like sql and you do nt know what you re doing  you will bork your install and good luck getting rid of it also  if you have no idea what you re doing when installing office  you re going to      that up too  alternatively  always check ninitecom to see if what you want is there it will automagically do the install without crapware ',\n",
              " 'try rcostamesa',\n",
              " 'it s not working anymore ',\n",
              " 'can i bring in professional tools at an urban workshop for my diy project ',\n",
              " 'if you do nt know that  what are the odds you know a verb from a noun ',\n",
              " 'while we re on the topic of english grammar and spelling  i ll leave this here http  iimgurcomhl1zrjpg ps  it s an entertaining read  pps  i know about the  in a sentence   p apparently the author was too busy ranting to notice it ',\n",
              " 'ysk basic grammar ',\n",
              " 'i once had this as a shower thought and kept thinking weather i have been using it correctly in my correspondence all this time ',\n",
              " 'after that they now have your phone number and know what you may be interested in because of your texted question i would be willing to bet that is the whole idea ',\n",
              " 'i followed this guide   the ultimate guide to killing bedbugs   http  wwwpestcontrolinfo   theultimateguidetokillingbedbugsaqgb0  and i m glad i did  as the exterminator quote was over  900 it ended up costing me just under  50 and we have nt seen any in over 4 months',\n",
              " 'like  practice  and  practise  ',\n",
              " 'every brazilian business are required by law to have their own policy  but the second paragraph under the manufacturer s warranty  its against the law backing the customer up ',\n",
              " 'keep fighting the good fight  guys just know it s a losing battle the people who care already do it correctly the rest have no interest in learning such things  i assume that every post i see will have the same errors that way  i m never disappointed  how come i cant loose weight its so discouraging when my waste never gets smaller then it was before  what are the foods your eating every day   it never      stops and it never will  because most people have no issue presenting themselves as uneducated ',\n",
              " 'gt  it was devised to help clear out snow emergency routes so basically localstate government requires that they do this if an emergency is declared  reminds me of the time i had to ask the guy at the ritz carlton in montreal to please garage my      car during a blizzard they were actually pretty nice about it  even though i was nt a guest  ',\n",
              " 'not in canada',\n",
              " 'i live in the midwest what is a snow emergency ',\n",
              " 'well      i feel like an idiot i thought this was actually a constructive post ',\n",
              " 'i got really excited about this  and then i remembered that i live in mn there s no such thing as a  snow emergency  here  only a way of life ',\n",
              " 'interesting i work for a national parking company and i have never heard of this any examples of where this occurs ',\n",
              " 'the same can be said for  prophecy   noun  and  prophesy   verb   this mistake is made in  bruce almighty  ',\n",
              " 'many casual internet users consider 25 mbps to be very fast when in reality it is only about 325 mbs knowing this is extremely important when shopping for isps edit  missing a word ',\n",
              " 'i m from canada what s a snow emergency ',\n",
              " 'it s one of the best ways to get moderately involved in politics it also allows you to evaluate how well your representative is doing my house rep is an old man almost 3 times my age but his voting pattern is about 90  in line with how i would vote  so i will vote for him in the upcoming election the senators are more around 60   but their position on committees makes it worth keeping them around ',\n",
              " 'you can also call 7124324213 to listen to emergency weather radio you can also call 7124324201 and listen to childrens ebooks you can also call 7124324215 and listen to white noise to help your baby sleep ',\n",
              " 'it says you cant text with atamp  tor am i the only one that s having this problem ',\n",
              " 'the did nt have  the playroom   http  wwwmyabandonwarecomgametheplayroomqi   but another site did  i played that game for hours upon hours when i was a young child ',\n",
              " 'ysk that in the uk if you wear a works uniform with their name on it or logo you can claim tax relief because they still own it and you wash it on their behalf  its not much but every little helps no i do nt work for tesco ',\n",
              " 'leisure suit larry       yes ',\n",
              " 'i looked for dark sun  shattered lands  and it did nt find anything  i m not sad for myself i ve played the game many times  its where my username comes from   i m sad others ca nt play that masterpiece ',\n",
              " 'it s a fact  proof  http  imgurcomaorm3x nb that sometimes  you need to add a couple of lines for the screen to be clean ',\n",
              " 'i use this to remove overlays  works extremely well  https  chromegooglecomwebstoredetailfckoverlaysppedokobpbdajgiejhnjfbdjlgobcpkp',\n",
              " 'what can i do about audio pop ups on a radio app  i use tuneincom radio on my pc it is fine on my android phone i have been getting longish ads like infomercials playing over the radio station stream and i have no idea how to get rid of it  tunein is the only path for me to get to a specific brit radio station that i like',\n",
              " 'psa for people outside the us  gt  our services are completely free using our search messaging system wo nt cost you a dime  gt   unless you re outside the usa ',\n",
              " 'ahw master of orion does nt seem to work   ',\n",
              " 'i need to know how to play microsoft bob now ',\n",
              " 'i m absolutely floored that i can play zork right now  guys  i know i ve maybe had a little too much wine  but cmon can i get a little excitement over this  ',\n",
              " 'no one is self diagnosing themselves with ocd when people say  i am going crazy today  they do nt mean they are actually going crazy it is hyperbole no one knows about ocpd  nor are they actually trying to diagnose a medical problem when they say that out of place tile is aggravating their ocd  this really reads like you ve just read your textbook for your intro to abnormal psych class and your ocd is triggering this response ',\n",
              " 'amazing  i found some old classics that i remember from back when we got our first pc in 1984 or 85  jumpman  congo bongo  etc  but it does nt have everything  at least not yet one of the first pc games i ever purchased was the hitchhiker s guide to the galaxy text adventure game  not trivia challenge  and they do nt seem to have it very old school  super tough adventure game with zero graphics  just text the puzzles seemed impossible at the time but would love to take another crack at it ',\n",
              " 'no tie fighter  ',\n",
              " '100mbs is 125mbs  great reference point mathematically speaking ',\n",
              " ' lil  biiiiits ',\n",
              " 'awesome  i ll remember this next time i m in the 1990s ',\n",
              " 'i ve been meaning to go over to the city and get a card from seattle public library  so i could get free lyndacom access on a whim  i noticed that i could apply online  giving me the library card and pin i needed to use this thing i ve been wanting for months  your library probably offers other resources  like audiobooks and databases online do nt let not having time to go to the library or not living in the  right  city stop you either  local links   apply for an spl card   http  wwwsplorgusingthelibrarygetstartedgetalibrarycard   using your card as a museum pass   http  wwwsplorglibrarycollectionmuseumpass   borrow a 4glte hotspot   http  wwwsplorglibrarycollectionsplhotspot   lyndacom   https  wwwlyndacomportalsip  org  splorgamp  triedlogout  true ',\n",
              " 'i can relate to this  it s an important difference i was made aware of long ago  but still some colleagues who i have respected for being really adept with computer usage and electronics have very much fallen for this ploy of the isp market  it borders on being untruthful ',\n",
              " 'i remember my cousin playing one a long  long time ago and we have nt been able to track it you were captain of a ship and had to park it in a harbor basically you had to steer the  seen from above  ship in a series of pathways without hitting the walls ',\n",
              " 'i think it is less important to know when shopping but more important when diagnosing issues what your browser and other software likely measures speed as megabytes ',\n",
              " 'i could swear that last time i called my isp they told me it stood for megapixels ',\n",
              " 'coming from hawaii  this is actually useful knowledge for me a cold snap there meant it got below 70 degrees  now  we re in washington state  but south of seattle  so not too cold we had a half inch of snow and it lasted almost a day that was winter  i guess we avoid driving when it gets much under 40 degrees  but some days  the temp drops after we are already at work it s still a new experience to us ',\n",
              " 'at first i thought this was a little lame for a ysk but so many people      this up that they really should know the difference ',\n",
              " 'to clarify on op s post  1  this is an rep program for macbooks with video issues that are purchased within a certain time period  2011 to 2013 irrc   2  the title seems to say that all macbooks warranty is over in feb which is nt quite right  if your device was bought in the last year it s still coveted under the limited 1 year hardware warranty or the protection plan if you bought it with your macbooks ',\n",
              " 'my life is a lie',\n",
              " 'i have  had  a 2010 one that did this to the point that it was causing even windows to bsod took it in  sorry sir this is nt covered  grr ',\n",
              " 'from their own website   to ensure that we  re providing a great experience for our customers  customers who use more than 23 gb of data in a bill cycle will have their data usage deprioritized compared to other customers for that bill cycle at times and locations when competing network demands occur  resulting in relatively slower speeds ',\n",
              " 'this may be really dumb  but what if i bought it online from best buy i never really got asked about insurance just more of a click and purchase type thing some how does this still apply to me ',\n",
              " 'lol just got a 2011 macbook yesterday  got vertical lines and they went awayhmmm',\n",
              " 'how does one claim this ',\n",
              " 'abp now has an option to automatically block antiblocker notifications you do nt need to manually do it most of the time ',\n",
              " 'unless you bought it in australia where consumer laws actually exist and apple ca nt      people with their mistakes   ',\n",
              " 'wait is this why      congress is saying no one needs 25mbps   can we please survey that  it did nt even occur to me  for some reason  that it was nt common knowledge ',\n",
              " 'i wonder if this works for the emergency services i have a few nursing and paramedic friends that this might be useful for ',\n",
              " 'mine  early 2011  stopped working and when i brought it in they said it was a graphics card issue and fixed it for free is this the same thing ',\n",
              " 'right here http  playdosgamesonlinecom',\n",
              " 'if in bexar county  you can get a bibliotech card online  http  bexarbibliotechorg we have linda too as well as other resources',\n",
              " 'that precisely describes the symptoms my mid2012 rmbp suffers i just called the applecare guy and he told me my model was nt eligible for any repair programmes so it would have to be a paid repair   that page does seem to suggest that the models listed are affected i wonder if there s anything i can do about that ',\n",
              " 'ocd is defined as  excessive thoughts  obsessions  that lead to repetitive behaviors  compulsions    i hear people say all the time  this picture frame is rotated very slightly  my ocd is annoying me so much right now   or something like that first of all  this is not ocd  which has symptoms such as excessive hand washing  hoarding  extreme fear of pathogens  and excessive checking  like making sure a door is locked every 5 minutes  for example   this is a   mental disorder   that needs to be   diagnosed   by a physician  and is usually treated with ssris ocd causes major disruption in someone s everyday life secondly  being an extreme perfectionist like in the quote i said earlier better matches symptoms of ocpd  the p is for personality   this is still a disorder  but the traits that individuals with ocpd have  extreme perfectionism to the point where it severely hinders someone at finishing tasks  are common and are part of our brain in a more mild form  as we evolved this to help us with survival so although i dislike when people self diagnose themselves with disorders  unless they ca nt see a doctor and their symptoms are perfectly accurate  and are nt of low intelligence  cough  trump  cough    i d much rather have someone say they have ocpd when they get annoyed  not crazed like in ocpd  by something slightly off than selfdiagnosed ocd  although preferably perfectionist is best word   tl  dr  edit   the mainstream or slang definition of ocd better matches the clinical definition of ocpd  i m not saying perfectionists have ocpd  but ocd is nt that close to how mentally healthy people use it as an adjective to describe a personality trait  edit2  for people saying that they do nt see the purpose of this post  my main opinion is how using actual mental illnesses and disorders as simple personality traits  hyperboles  etc waters down how most people think the severity of the disorder is the mainstream definition of ocd has shifted to that of perfectionism   depressed  is now sad in the mainstream  and  bipolar  makes the average person think of switching moods or opinions rapidly as a result  people do nt understand the suffering of people with any of these like me ',\n",
              " 'thanks for posting this  i just looked at the anchorage library website and saw that they also offer free lynda access ',\n",
              " 'what about opp ',\n",
              " 'my psychologist diagnosed me with ocpd i ve been going to a therapist for years and i ve come to the conclusion that i m alright with ocpd i m a perfectionist but rarely to a fault  i work from home when i m not seeing customers so my      is organized the way i like it the only negatives are that when big changes happen it stresses me out and i need to be careful about things i get involved in  practiceskill based games are really dangerous for me  because i want to winbe the best i can be an example is that when i was 22ish i joined a bar pool league i practiced pool literally 56 hours of pool for 7 months until my team won the state championship and got a paid vacation to vegas since we won states there was nt anything else to accomplish so i have nt picked up a pool cue since  ocpd also contributes to binge drinking for me because the only time i m not being in control of myself is when i m wasted and it s relaxing ',\n",
              " 'you should know this in case you ever get a cold sore and want it to heal quickly  i only get one every year or two  but they       and i always cheaped out and got some  5 thing  never realizing there was an actual medicine for it most people i know who get cold sores did nt know this either i have no affiliation with the company  other than appreciating they have the only over the counter med that actually works ',\n",
              " 'a quick comment and clarification   it sounds like you folks know a lot more than i do about this subject  but i m glad it s started a discussion  and  i use it when i feel a tingling around my lip and it seems to actually  prevent  a cold sore that s the part i m raving about if one started  it seems to be very mild and heal very quickly prior to when i finally listened to a pharmacists and spent the  23 or whatever for it ',\n",
              " 'rhailcorporate',\n",
              " 'most library systems offer ebooks  kindle books are mostly downloaded from the amazon website  since you are in seattlebtw king county library system and seattle public library have a reciprocal agreement that lets seattle residents also have a king county library card',\n",
              " 'so are you saying you can have access to lynda via your library card ',\n",
              " 'op  please add canada too on the list we get free lynda access along with tonnes of xboxps4 and ps3 games in my local library here in saskatoon  aka spl   ',\n",
              " 'i have tmobile s unlimited plan and since my phone is my only way to access the internet since i live in a very rural area and isp s do nt come out here i easily rocket past the 23gb mark and still get the same 3045 mbps i always do the thing thats great about the plan is that you get deprioritized not throttled throttling means once you hit the set mark of gigs your speeds no matter what are slowed down to 2gedge speeds this is how atamp  t treats their customers that have the grandfathered unlimited plan after they reach only 3gb deprioritization means that if you go past the mark you still get access to full lte speeds but if a lot  i mean a lot  of other tmobile users start using their data then you get deprioritized to slightly slower lte speeds i never get deprioritized where i live since there are few tmobile customers on the two towers that handle the area i live in while after reaching 23gb and being in the various cities i frequent each with lots of tmobile customers i barely ever notice being deprioritized i still can watch and fully buffer 1080p videos on youtube in seconds  load pages speedily  and download files or apps with speeds that are only a couple mbps slower not megabytes persecond slower but megabits persecond honestly though anyone living in a market crowded with other subscibers will never achieve the full network speed provided by the few towers covering the market area you are always sharing the towers with other subscribers which means the more people using a tower the more everyone using data from that tower deprioritize each other anyways the main point to take from all this text is that you are more likely to notice speed slowdowns on any carriers network thanks to you and your fellow subscribers hogging data thoroughput from each other  especially during lunch hours   i m at 847 gigs this month of data use and i m still rocking the same speeds i get while not being  deprioritized  ',\n",
              " 'having fun is nt hard when you ve got a library card ',\n",
              " 'i have  unlimited  with tmobile never had an issue until recently when i would use my mobile hot spot  they start throttling that very  very quickly i m still pretty      about that guess i ca nt complain though about getting 23g s per month for only  10 ',\n",
              " 'delivered through this extension is the simplest and most convenient way to remove overlays  you do nt have to spend time looking through the webpage s elements  nor do you have to add filters to anything  it s simply a case of right clicking and deleting any intrusive overlays  hope you find it as useful as i have   f  ck overlays   https  chromegooglecomwebstoredetailfckoverlaysppedokobpbdajgiejhnjfbdjlgobcpkp ',\n",
              " 'nah  if the site owner wants to be an      by having these  i just blacklist the site it s not worth my time to reward them with my eyeball count if they do this ',\n",
              " 'i use opera mini on my phone this      drives me crazy is there anything for opera ',\n",
              " '23gb on mobile is a lot of data people will take up torches and pitchforks over this and the number of people it will truly affect is so small ',\n",
              " 'for me  abreva caused the cold sore to spread and worsen it started as a small thing on the left side of my lip each application of abreva seemed to worsen it by the end  and i stopped using it  it ran the from beneath my nose to the corner of my mouth  and up from the lip  in a sort of lopsided triangle shape  camphophenique is what worked in childhood it s what works now for me  anyway we re all different  i guess figure out what works for you ',\n",
              " 'pharmacist here whether you choose a topical antiviral like abreva or a prescription drug like valtrex the most important thing is to    use them at the very first sign of an outbreak     after the 12 day window closes the medicines lose a lot of their efficacy for some people that could mean starting when you feel that telltale tingling on your lip ',\n",
              " 'i used to use abreva as i get a cold sore every year  but this year i got a cvs brand thing and it worked so much faster than abreva i do nt know the difference but i know it works ',\n",
              " 'the other half of the equation is to have an antiviral  such as acyclovir  on hand and use it and the abreva at the first sign of the cold sore  usually a tingly feeling in the lip   the combination can drastically shorten the duration and damage that the cold sore causes  the earlier the better  once the virus has caused a lesion that lesion has to heal  regardless of the course of the virus itself ',\n",
              " 'abreva  lysine  very few cold sores in the last few years',\n",
              " 'i ve had cold sores at random times throughout most of my life so far  and for actually getting rid of them quickly i would definitely recommend getting a prescription for medicine in pill form works much faster than the lip balmy types and you do nt as much of the crusty surface  the one i use is called aciclovir  but there are probably other brands that does the same thing ',\n",
              " 'abreva s clinical trials are mediocre at best shortened duration by something like 10 hrs easily attributable to a flawed definition of  duration   statistical malfeasance  or a hydratingdehydrating effect of one of the excipients ',\n",
              " 'sending electricity into the grid during an outage could cause the inadvertent injury of repair personel or others ',\n",
              " 'it s never worked for me valtrex is a lifesaver though ',\n",
              " 'mom was getting these calls for several years on her mobile we d told them that they have the wrong person  but they would nt listen i would end up adding the numbers to sprint s block list  but spoofed numbers make blocking in such a manner almost pointless  we finally got a rep on the phone the other month that apparently had some sort of supervisory position and she took us off their call list no calls  to knowledge  for a month or two  wish i d known about this a while back ',\n",
              " 'where the      was this 5 years ago ',\n",
              " 'dang  i wish i had this information 15 years ago  when i was harassed for over a year by someone claiming to be calling on behalf of spring long distance  for someone who used to have my number nothing i could do could convince them that not only was i not that person  but i had never heard of them  nor did i have any idea how to get in touch with them ',\n",
              " 'remindme ',\n",
              " 'i ve used abreva  but personally  bactine works much better for me it does nt even form a blister sometimes with bactine if i catch it early ',\n",
              " 'do you own a hobby drone in the us  the faa requires that you register it with them by february 19  2016 it only costs  5  but today is the last day to register it for free  http  wwwfaagovuasregistration  cid  tw379',\n",
              " 'up to 80  of adults are infected with cmv  as it is a herpesvirus  all the      viruses are lifelong infections   but fear not friends  in most immunocompetent  normal people  it does nt cause serious disease when you first get infected  it presents just like a normal cold in fact  in most people  it does nt cause any problems at all however  not all of us are so lucky pregnant mothers are screened because fetus  and newborns do not have a developed immune system it is a common nightmare for those with aids or have other conditions that weaken their immune system  in other words  do nt worry about it unless you have aids or are otherwise immunocompromised or thinking about getting pregnant all the more reason to go see an obgyn when you are thinking about having a baby  source  i am a med student and this is common medical knowledge',\n",
              " 'i m positive for cmv my blood and platelets are marked as to not be given to children or those with compromised immune systems outside of that  i hang out with my nieces and nephews and work in the public ',\n",
              " 'what s the advantage over  block   block   clarification   origin  also lets you pick and delete elements  in addition to blocking ads  while being cooler than adblock ',\n",
              " 'throttling  limitedor am i missing something  it s still unlimited data  just a bit slower ',\n",
              " 'glassdoor here i come',\n",
              " 'ysk not all countries are the same  please specify when posting   as a nonamerican this is pretty annoying and happens all the time  ',\n",
              " 'except most of us already got cmv long ago when we were kids it s  extremely  rare to live until adulthood without getting it ',\n",
              " 'would be easier if ublock origin had a block overlay filters list you could subscribe to ',\n",
              " 'i test your donated blood for cmv and syphilis you can blame me if we reject your donation  ',\n",
              " 'figured i would post this over there  as i posted it in til some people may know this  but i  like many  actually had no idea this has been perpetuated over the last 30 odds years  but it turns out to be nothing more then a mere falsehood  here is the link i provided in til   http  wwwsciencefridaycomarticlesismsgbadforyourhealth ',\n",
              " 'this is a little off color  in more ways than one  but with all this talk of caucuses  i wish i could find a vey old daily show segment in which a faux reporter actually went to dc and interviewed some politician coming out of a black caucus rallymeeting at one point she puts the question to the man  the question for which she obviously made the whole trip   did you ever see such a big black caucus   ca nt find it anywhere ',\n",
              " 'so they do nt all just run in a giant circle and everyone wins ',\n",
              " 'they count      penises that s why it s called a cockus ',\n",
              " 'the correct way to do this is to have a transfer or interlock switch installed ',\n",
              " 'i feel like if the generator is professionally installed it would take care of this for you automatically',\n",
              " 'read as  systems outrage  thought  what are they so upset about  ',\n",
              " 'i had no idea there was a difference  i thought it was just a matter of terminology but it seems like  basically  caucus  are completely ridiculous ',\n",
              " 'this is great information  i had no idea forgive me for asking a      question but i googled voting day which is scheduled for november 8  2016 is this day for everyone else that is not on the schedule in the article ',\n",
              " 'can someone explain to me just why exactly i should know the difference between the two ',\n",
              " 'so open caucus can just show up and vote right ',\n",
              " 'got mine deposited into my bank this morning ',\n",
              " 'use freetaxusacom  super easyas good as a paid program  and free for federal taxes state is only like  12  and it is super quick i got my return direct deposited on feb 3rd ca nt beat itused it for 6 years ',\n",
              " 'that sucks  filed last thursday  refund posted today ',\n",
              " 'how does this work  not that i have malicious intent or anything  i just want to know for you know fun ',\n",
              " 'you want my money  your      problem  irs ',\n",
              " 'i am an audiophile and have been for about 20 years i have bought and sold more gear than you can imagine fedex ground is by far the best and lots of people like me will tell you the same thing i have lots of bad stories about ups  but usps are absolute crooks never ever ship anything with them ',\n",
              " 'for starters  there are multiple free options outside of turbotax that you could use to file your taxes but in case you are going to use their paid version i do nt want anyone spending an extra  35 just to have them take additional money from your refund   irsgov   https  wwwirsgovuacfreefile  doyourfederaltaxesforfree ',\n",
              " 'i use taxslayer for the first time it s very thorough and i like it ',\n",
              " 'taxact does this as well  do nt think it s just with turbotax ',\n",
              " 'for canadians  there s http  wwwstudiotaxcom i ve been using them for years absolutely free  very simple to use just fill in the blanks  print and mail or efile  i print and mail myself takes a little longer  but it gets deposited in my account ',\n",
              " 'but i ve used turbotax 5 years in a row and have yet to pay for fedstate taxes we make less than 40k per year so maybe they pity us but i ve never had turbotax actively take my money they gave me a choice and i chose free  i also liked tax slayer but tt is crazy easy ',\n",
              " 'used turbotax and got my refund direct deposited for free so maybe i m the best reader of all of you',\n",
              " 'if you have to file in multiple states they also charge you about  35 per additional state they only allow the first state for free       that      here is how you bypass it  install turbotax on multiple computers on the second computer pick the second state as the free one  then whenever you have to work on state 1 use computer 1 save your work and email it to yourself and open up on computer 2 to work on state 2  it has worked for me multiple times now ',\n",
              " 'you should also know that they will comically still advertise this to you if you owe money to the government at the end of the year ',\n",
              " 'taxhawk  switched years ago and never looked back ',\n",
              " 'a possible      question  that i could nt answer with a simple google  why are nt the primaries and caucused all held on the same day  so that one state does nt have more influence than another ',\n",
              " 'thats why i always stick a potato in it before any snow fall  caution  forgotten potatoes in a tail pipe can be deadly ',\n",
              " 'alternatively  go to a ups store and have them pack your items  it may cost a little more  but liability is essentially on the stores hands they have a  pack and ship  promise basically  if a ups store packaged your item  your guaranteed claim approval because it was packaged by a  certified ups packer  or some      like that ',\n",
              " 'update  everything is fixed',\n",
              " '  3 years working for a ups store   hey guys  i can give more specific information about step 3 so if you ship through a certified ups store and use a box purchased from that store it will automatically come with  100 of declared value like he said above thats not insurance but in the case that it does break and they find out it was ups s fault and you can prove that the item cost at least  100 they will give it backmaybe at my store its  2 for every  100 you add to your package so its a pretty good idea to get it  but only if your paying the store to pack it for you i have never seen someone pack their own package and win a claim  ups lawyers are insanely good at throwing your claim away and saying it was your fault the sad thing is ups is miles better than any other courier service and i would never trust my stuff with any other service well thats about it  any questions feel free to ask ',\n",
              " 'my direct deposit hit this morning ',\n",
              " 'here s the page on  help  books   https  enwikipediaorgwikihelp  books   gt  warning  the book creator software has been crippled since it was rebuilt in 2014  the management process is inactive  and no fixes are in sight ',\n",
              " 'because i did nt until about 5 minutes ago https  supportspotifycomuslearnmoreguides   articleifyoureapremiumuseryoucantryhighqualitystreaming  in  search',\n",
              " 'better yet  use 3rd party insurance follow rules file a claim payout in 30 days less hassle less headache  also fyi  if your package is expensive  even if only  100 declaring value to the carrier has resulted in numerous package thefts  3000 item is very attractive to a guy making  9hr not saying all do it but the risk is there if you declare value',\n",
              " 'i had a filter for an aquarium get damaged and made a claim with usps they made me ship the filter to their facility in atlanta so they can verify the damage 3 days after usps shipped it there  a guy calls me and asks me what is wrong with it and how to demonstrate it  i told him to get a 20 gallon aquarium and fill it with water then go to the hagen youtube channel and watch the video on how to set up a fluval canister filter once you have it running  you ll see it leaking from 3 different places  there was a pause for about 10 seconds and the guy says  um we ll mail you a check can you fax me a receipt  ',\n",
              " 'tldr pack it right if you re shipping  if you ca nt jump on it  it s not good enough and  deny accepting it if you re receiving and it has visible damage',\n",
              " 'wow  thank you for this  so instead of a  secret  voting booth  people just sit in a room and the head of the place just asks  who votes for bernie sanders    who votes for hilary clinton   and people just raise their hands accordingly  edit  so i was looking online for more info on how a caucus works and found this interesting video of a district during the caucus process  talk about playing politics within politics   https  wwwyoutubecomwatch  v  fldytfrkiim  ',\n",
              " 'i just found out about this the other day  it s pretty cool ',\n",
              " 'further lpt  ups stores are franchise stores  and are basically rebranded  mailboxes  etc  i worked at an mbe franchise during the switchover if you had the items packaged by the folks at the store  just go straight to them to file the claim they have access to a higher tier of customer service agent at ups  and  at least at my store  would handle everything on the rare event that something we packed arrived damaged  we would work with ups on the customer s behalf  and in the once instance where ups denied the claim  the store owner cut a check himself for the damage  basically  our policy was this  if you want to put a dv on an item that you packed  we would inform you that ups can and does deny claims based on the quality of the packaging  that the entire sorting system was automated  so the 14  fragile  stickers did nt mean a      thing  and that the box could be subject to being thrown  dropped  crushed  etc if they were confident that the item was packaged properly  and the dv was above  500  we had them sign a waiver stating that they were informed of the above info  some stores even offered a doubleyourmoneyback guarantee  if it was packaged by the associates  in my two years of working there  i packed literally everything you could imagine  teddy bears  firearms  a complete 24 place setting china set  and a frosted chocolate cake  no kidding   i never had anything arrive broken by following the packaging guidelines ',\n",
              " 'i read this as systems outrage ',\n",
              " 'my watch was signed for by some other person idk who i was nt even home so i filed a claim on fedex motorola and fedex both denied it and gave me a finger i m out  350  ',\n",
              " 'i used to work for a company that gave up on ups insurance  and started charging every customer a  1 insurance fee when a customer called with a damaged shipment  we would just reship  no questions asked the  1 fee ended up being a money maker for the company and the customers appreciated the no questions asked policy ',\n",
              " 'do nt bother with the threat of a lawsuit if you re really going to do it  just do it nobody will believe you or care until it happens ',\n",
              " 'i recall someone on here stating that when they worked for ups  they suggested packing an item as if you were going to drop it from 10000 feet the safety and integrity of the item is essentially your responsibility ',\n",
              " 'just as there is a difference between the common usage of  theory  and the narrow definition of the term in science  the mere existence of a meaning in a scientific or similar field for a word does not make the technical meaning correct and the lay definition wrong the meaning of such terms is simply dependent on context  in english  a hoe dare is a gardening challenge  but in spanish it s  to        words do nt carry their meaning with them  so  no word ever  really  means any thing nor ever truly  does nt mean  some thing either  so you say a word means a certain thing and not another thing  how do you know that  oh  you read it in a book  maybe   then all you know is that that book said that that s what that word means  and you still do nt  know  that that word means that certain thing  what you re doing is prescriptivism  and it s devoid of scientific validity  being based entirely on unjustified authority  and entirely lacking empirical backing ',\n",
              " 'this is genius i wish i had known this a long time ago',\n",
              " 'what if you re listed as an associate  of the person of interest i ve had calls from debt collectors trying to collect from a relative and an old company of mine  i am the person they intended to call  but i have nothing to do with the debt  if it applies what if they call multiple times on my cell phone  but i have nt picked up to tell them that i am not the person of interest  if that applies  what steps do i take to sue ',\n",
              " 'i m the person at the till in a shipping place i do nt deal with claims  i do nt deal with packaging standards  i take your      and ship it  okay  it s up to you to do all of the above       do nt expect the cashier to know how much it costs to ship something somewhere off of the top of their head i ask you what the system asks me and that s it we ve never once been told from higher up that we need to make sure packaging is this and is nt this  we literally just take it and send it i m sorry it s that       but it is   you are responsible for your      when you ship it  not the cashier   i do nt get paid anything more because i scan envelopes instead of groceries ',\n",
              " 'also  guys  be wary of shipping things with ups over the border i live in canada  and once i ordered a graphics card from the us on ebay the value of the item was about  200  and when the ups guy came  i was told i had to pay  90 in  customs brokerage fees  in order to receive the item i paid it because i did nt really have a choice and i really wanted that card       ups ',\n",
              " 'the ups store is not part of ups it really has nothing to do with ups it s the old mailboxesetc chain that pays ups to use the name  there s no benefit at all in using a ups store they will point the finger at one another ',\n",
              " 'important step  package your      really well it s gon na get tossed and thrown around bubble wrap the      out of any breakable ',\n",
              " 'also  if you decide to buy the paid version and pay for it before you finish your taxes  you ll be asked again as you finish your taxes how you would like to pay and are forced to choose an option it does nt know that you paid previously and you ll pay for it twice the  contact  section does nt offer any help and i still have nt heard a peep back from them ',\n",
              " 'you should know that paying for software to do your taxes is a waste of money all together freetaxusa is good enough ',\n",
              " 'could you talk a little bit about how resourceheavy the different alternatives are  i realize that should not really be at the top of the list of criterias when choosing antivirus  but it would be nice to get a sense of it anyway i ve been using avast for some time  and i feel like it takes up a lot of resources when it s running the realtime stuff in the background is bitdefender more or less intrusive in this regard ',\n",
              " 'my      you hero',\n",
              " 'is nt the only reason the government does nt just do 90  of your taxes because of turbo tax  i thought i read a til about the company lobbying against the government doing it for you ',\n",
              " 'ysk how to read the option you pick before completing the transaction i was on the same path and i read it and said nah  i d rather pay myself',\n",
              " 'good to know the only reason i continue to use it is because i do taxes for about 20 people and i m too lazy to import all that information into another program i hate dealing with the company and while their software is very good it s full of ads and annoying tips that ca nt be turned off ',\n",
              " 'the best thing to do is take your item to be shipped to the store and make them pack it for you costs a little extra  but when it s broke  they will just pay it  i am a ups volume shipper and so far their claims process has been a cake walk ',\n",
              " 'i know it s a thread about windows  but what about macs  do i need an antivirus for my macos and if so  which one ',\n",
              " 'it allows you to get a good idea what an item you own is worth or if you are getting a good deal on an item ',\n",
              " 'ysk if you make less than like 55k a year you can do your taxes completely for free with wwwmyfreetaxescom done it for the past three years',\n",
              " 'i meanit literally discloses that when it asks you how you want to pay ',\n",
              " 'and if 4 cars stop at the same time  rockpaperscissors for who goes first ',\n",
              " 'i ve looked into the free options if there is one that can actually handle mortgages  real estate sales  charitable donations  children  brokerage accounts  mutual funds sales  dividends  and all the little deductions that come from being somewhat wealthy in our modern world  i d gladly use them but so far  they all have gaps in what they are able to handle  so i just pay turbotax cheaper than an accountant  and much faster and easier than doing all that myself ',\n",
              " 'i mainly pay now because i just discovered if you have a local audio file in a playlist and save the playlist offline on a phone or separate device  the song is downloaded along with the nonlocal tracks ',\n",
              " 'and there has been no scientific evidence of the existence of msg allergies it s either in their head  or they re allergic to some other ingredient in chinese food ',\n",
              " 'thoughts on glary utilities ',\n",
              " 'also  when you arrive at the 4way stop  look to see who s already waiting they go before you do  as opposed to the  welp  i ve waited long enough   rule that seems to exist in some drivers  heads ',\n",
              " 'i personally prefer eset i did not know bitdefender offered a free version ',\n",
              " 'as an actuary major  this is super nice and helpful thanks',\n",
              " 'i keep seeing  bitdefender antivirus free edition  on their site but each time i click on it  it takes me to their current set of paid offers i call bollocks on that  good sir  bollocks',\n",
              " 'how does msg compare with table salt for dietary purposes  anyway  is it like the junk food equivalent of salt  empty sodium your body gets little from even if it does nt cause active harm  or is the body just as fine with that as with nacl ',\n",
              " 'there are people who do nt know this  perhaps it is different in other countries but in my country we learned this pretty early in school ',\n",
              " 'is there a reason that you ca nt submit this as a link like a regular reddit post ',\n",
              " 'i do nt like using the hosts file harder to deactivate when you need something to work  also plugins like ublock allow you to block elements of pages like annoying popovers or auto playing videos  etc to me  the plugin is superior ',\n",
              " 'i had to direct traffic at a 4 way stop from my car because people did nt know what to do i pointed at each car by right of way to go more people need to know this it s like they just stop and stare at each other ',\n",
              " 'similarly  nonceliac gluten intolerance is not real however  some people will benefit from a low fodmap diet ',\n",
              " 'would nt recommend keeping them on the line as some are reverse charging the longer you are on the more they get from you just hang up ',\n",
              " 'this post was removed for violating rule 3 andor rule 4 posts about reddit or social media sites are not allowed please  message the mods   http  wwwredditcommessagecompose  to   2fr  2fyoushouldknow  and include a link to your post if you believe otherwise thank you   i am a bot  and this action was performed automatically please  contact the moderators of this subreddit   messagecompose  to  ryoushouldknow  if you have any questions or concerns ',\n",
              " 'i use gas station gas  premix it with a stablizer and run it out after every use then i add a small amount of ethanol free gas  expensive  to the tank and run that out too it s a pain  but better and cheaper than getting all of my small motors repaired every year  few years ',\n",
              " 'i wo nt argue with your science  nor with your method  if you re willing to go through all that but neither my snowthrower nor my lawnmower has given me any trouble for twenty years  i add stabil and run them dry at the end of  each season  but that s it running them dry after every use seems like an insane amount of trouble or maybe i m just lazy ',\n",
              " 'here in canada high test  high octane 94  generally does nt have ethanol that s all you should run in high revving engines and small motors ',\n",
              " 'im not gon na argue one way or another but i work at refineries from time to time and i brought up using stable to a operator and he assured me that it was needed in the 70 s but all the additives they add these days theirs no reason to bother any more now i know he s worked there his whole career and i know he believed what he was saying ',\n",
              " 'being a minnesotan i can assure you the correct term is snowblower  not snowthrower',\n",
              " ' charity navigator   http  wwwcharitynavigatororg  is a non profit charity which rates other charities comprehensively in two categories  1  financial health and 2  transparency and accountability sadly  not all charities are created equal  and it is imperative to make sure that the hardearned money you donate actually gets put to good and efficient use ',\n",
              " 'did nt work for me i searched  flan  ',\n",
              " 'i ve been getting a ton of calls from these bastards i was getting calls that showed the actual dell support number in caller id however  this was spoofed and there is information at http  800notescom you can search the phone number and read comments from others yes  beware ',\n",
              " 'got like 10 of these calls in the past month or past 2 weeks or so    do you speek inglish    nein  nur german    eet is verry emportent  get somwon ho speeks enlgish pleese    was     pleese yuo hafta get someone who speeks english it is verry verry emportent    hang up  next time i ll have some fun with them i ve only gotten the indian dude but i hear others have gotten a woman he s got this really thick indian accent part of me wants them to be bored tech support guys prank calling people for fun but that s sadly not the case  protip  google the number do nt go to a phonebook site just straight up drop the number in google you ll find countless  who called me   websites that show if these numbers are fake  which they are ',\n",
              " 'there is a sentiment out there that it does nt matter what cables you get and they are all the same this is only partially true   unbalanced analogue cables like rca  red  and white vcr type cable  or ts  unbalanced headphone cable  can easily pick up noise in the wire as it acts like an antennae this is nt just an only audiophiles can hear the difference thing if the cable is nt properly shielded you can get a sound similar to booting up aol in 95 or an amfm radio interference  depending on what signals are getting picked up   this happens on every tape cassette headphone adapter that i ve bought  and every time my phone sent or received data i would hear it   balanced cables like xlr  microphone  and trs  balanced headphone jack  have a  and  signal running opposite phases if the signals do nt match how they started  because they are opposites it can  figure out  what the interference is by adding the signals to each other terrible explanation  i hope it makes some sense  if the cable is longer than 100ft and is balanced or not  or if you are using multiple connectors on the same line  this could be the source of added noise if you hear any  data cables are a whole new ballpark for hdmi  as most of you know  get whichever is the cheapest unless there is possibility for a lot of strain on the cable  hardly ever happens   in which case get a robust one should nt be more than  15 at the very most if there are other data cable pitfalls to look out for hopefully someone here can point them out  it s not my expertise  this   https  wwwredditcomrandroidcomments43urbqbenson_leung_usb_type_c_tester_has_had_his_pixel  was posted today and inspired my post ',\n",
              " 'a friend who works at a distillery told me it removes some of the impurities for a nicersmoother taste ',\n",
              " 'where the      you gon na find a veinvascular access ',\n",
              " 'winterize with seafoam every time ',\n",
              " 'what the      is a snowthrower  is that a less badass alternative to a snowblower ',\n",
              " 'for all you wondering why  ethanol and water go together better than gas and water what i mean is that ethanol will mix with water readily  like food coloring in water  whereas standard gas will not mix with water  but rather will float on top of water also ethanol will absorb   50   times as much water as gas  meaning you now have watered down fuel this water will promote rust and negatively affect the oil on the piston sleeve  causing it to not be as well lubricated  which in turn causes premature wear on your engine  destroying it  ninja edit  source http  wwwfueltesterscomexpiration_of_ethanol_gashtml',\n",
              " 'i have yet to see a gas station that sells nonethanol fuel where are these mysterious places ',\n",
              " 'or use the expensive synthetic redi mix stuff if you do nt use your small engine stuff a lot it works out in convenience and even warranty coverage ',\n",
              " 'depending on what you want  you can go to a local airport and get a few gallons for your equipment for my 2stroke engines  i get a premixed solution it might cost more  but replacing a chainsaw  leafblower ever few years is worse ',\n",
              " 'pretty sure i ve run gas with ethanol in it for years and my  150 mower still runs just fine is there a paper out there that justifies the cost of fuel stabilizer for something so cheap  maybe if we ran a mowing business or golf course the downtime would have enough impact to justify the expense ',\n",
              " 'ysk that the same thing goes for older cars ',\n",
              " 'that also means that a gigabit connection is 125mbps so no one second gig download ',\n",
              " 'use premium it does nt have ethanol ',\n",
              " 'po box rates also went up  but i do nt know how much i received a notice around the 12th or so that if i renewed my po box before rates went up  mine was up on the 17th  i would get the current rate when i went to the counter to ask how much it was going up no one could tell me ',\n",
              " 'yeah  good      luck getting gasoline without 10  ethanol in pennsylvania ',\n",
              " 'b  bit  b  byte 8b  1b',\n",
              " 'i ve talked to many cabledslinternet providers where the salesperson insists i will get something like 10 megabytes per second  and even though i point out that speed is measured in bits  and they mean megabits  they stand by their flawed knowledge i have had to have the internet installed and then run a speed test to realize it s barely 1 megabyte per second  and then cancel they then tried to charge me for install and several months sales people selling internet should be required to understand basic data conversion ',\n",
              " 'but expect to buy a new filter because your water will taste like liquor if you do it s probably cheaper in the long run to just pay a few extra bucks for some decent booze  alternatively  have a booze specific filter ',\n",
              " 'we have done this for two cats with kidney problems and they lived happy and comfortable for a long while after most older cats due to to kidney issues so this type of care is familiar to those with many cats it s important to develop a trusting relationship with your cat when they are young so you can hold them still when they are older and need this special care',\n",
              " 'b is bit  b is byte those bits also include any parity and checksum values  example  dsl may use 10 bits to a byte  meaning you get less data than expected ',\n",
              " 'i work at a lawn equipment shop and around 80  of the repairs that come in are from carburetor issues due to bad gas i live in florida so the humidity with ethanol is a bad combination they also will not warranty any issues regarding fuel so do nt use any gas that s been sitting around in your shed for months  significantly cheaper to just buy a new gallon of gas than have the carb rebuilt or replaced on your machine  some gas stations have rec 90 fuel around here  mostly around a race track but you can also buy premixed oilethanol free gas like  this   http  wwwechousacompowerfuel  at your local lawn equip shop if you re a homeowner just trying to do some maintenance around the house that s definitely your best bet  it has a year shelf life once opened  if your equipment has to sit for an extended period of time run it dry and then remove the float bowl from the carb as some gas will still be in there ',\n",
              " 'i was calling them millabops for years ',\n",
              " 'mr layhey does nt believe in this      post  is nt that right  bobandy ',\n",
              " 'i do nt know      about       but i have a metric      ton of leafs in the fall  so i bought myself a backpack blower not knowing      about       i read the actual manual that came with it and it suggested not using ethanol gas period so i bought the fancy straight gasoline that costs like 15 a gallon worked fine being the lazy      that i am i then left leaf blower sitting for two years and a bit while paying a high schooler to blow my leafs then this year decided to do it myself again started right up and works as well as the day i bought it ',\n",
              " 'mythbusters used coffee filters for their       iirc ',\n",
              " 'ysk that when converting from   bytes   to   bits   there are  8 bits  in a  byte   so divide by 8 for the conversion to megabytes  in which case it s 313mibs not 325mbs  semantics  anywho  ysak about  binary prefixes   https  enwikipediaorgwikibinary_prefix  and why you should know the difference between kibi  mebi  gibi  tebi  and kilo  mega  giga  tera  hint  the  terabyte  tb  that s displayed when your purchase your hard drives is actually a tebibyte ',\n",
              " 'i had to argue with a roger employee about this her argument was that she has worked there for over ten years and she knows everything ',\n",
              " 'as your income increases into a new tax bracket   only  the money in that new bracket  not your total income  is taxed at the higher rate it s a myth and a fallacy that anyone would ever bring home less money after transitioning into a higher tax bracket  for example  with bernie s tax plan  someone making  250001 would have an increase in taxes in their  highest  tax bracket from 33  to 37   but it s not going from 33  to 37  of  total  income  it s going from 33  to 37  of income  between  250001 to  500000   because we use a marginal tax rate system so they re paying the same percent as everyone else on the first few brackets of their income  and only income  at or above  the  250001 mark would be taxed at a higher rate someone making  250001 a year would then have an increase in taxes in that bracket from  033 to  037  for a total increase of  004 a year  with  current tax rates   here s the total of what a single person with no children making  250001 a year pays in taxes  bracket ratetax  01845010    184500  184517490015    846750  7490115120025    1907500  15120123045028    2219000  23045141150033    645183  total taxes paid  5802933 as you can see  the tax in the 33  bracket is pretty low that s because they re only paying taxes on the  19550   250001230451  that they made  in that bracket   under bernie s plan  for example  the tax rates will change slightly here s how  bracket ratetax  01845010    184500  184517490015    846750  7490115120025    1907500  15120123045028    2219000  23045125000033    645150  25000150000037    037  total taxes paid  5802937 as you can see   only  the money  at or above   250001 is taxed in the highest bracket in this example  this person s taxes increased by  004 per year by switching to bernie s tax plan  you can use  this   http  wwwbernietaxcom  calculator to see how bernie s plan would affect what you take home every year ',\n",
              " 'http  urbanworkshopnetequipment so cool this place has so much stuff for you to utilize for all your diy dreams the membership is nothing in comparison to how much money all this stuff costs plus the space ',\n",
              " 'that s great if you live in costa mesa',\n",
              " 'a megabit is an eighth of a megabyte right ',\n",
              " 'lots of people have avast but may not have noticed yet i only noticed after looking back at an email i had previously sent  edit  you can turn off the signature in settings but it apparently comes back with next update as setting all go back to default signature only happens in emails sent from device with avast  so if you use a smartphone without avast to send an email the signature wo nt show up ',\n",
              " ' here is a link that shows how to disable that   http  wwwgetavastnetsupportremoveemailsignature ',\n",
              " 'previous post about this in rpcmasterrace https  wwwredditcomrpcmasterracecomments3zc7o2i_found_out_that_avast_was_putting_its_own',\n",
              " 'go for webroot antivirus for gamers  yes it cost but it is the most quiet av i have ever had ',\n",
              " 'and it s perfectly legal  at least in the us unionizing is the process of joining forces with coworkers in order to make your workplace work for you  often times we associate unions with industrial employment  but even if you re an engineer  desk jockey  or service industries  you can unionize your workplace  working 60 hours a week  being called in on your days off regularly  are you asked to do jobs that are usually for another position  that you feel you should nt be doing  then you might need to talk to your coworkers and see if you re all on the same page  unions in the us get a bad rep for having strong armed industries  but we often take for granted what they ve accomplished there are other unions that are nt all about beauracracy and collecting dues  such as the iww  that are more focused on giving you the tools to unionize your own workplace if you need further help from such an organization you only need one person to pay dues to them  and they are on a sliding scale based on what you think you should pay  further reading  http  wwwiwworgorganize http  wwwueunionorgorg_stepshtml http  uaworgorganizenounionnorights',\n",
              " 'this was actual tried at my current work place management and upper management pretty much said if you do nt like it there s the door  it s gone so far to say the owner of the company would sooner dismantle it in its entirety than allow a unionization history shows he was nt and is nt kidding as he shut down and cleaned out two plants that were trying this exact thing opened them up again a year later with all new staff  edit guess what i m saying is your milage may vary ',\n",
              " 'for the record fedex and ups have increased as well just before christmas when all the packages were heavy ',\n",
              " 'is this legit  first year of doing taxes  do nt know what to use to file taxes ',\n",
              " '  only if you ve bothered updating recently otherwise  that box probably has nt ticked itself on  here s what yrsk to disable it  1 settings 2 active protection 3  customize  on mail shield 4 behavior 5 untick  insert note into clean message  outgoing   6 hit the ok buttons 7 you re done',\n",
              " 'if this is a ysk  you should tell us  why  this is the case ',\n",
              " 'avast is      terrible ',\n",
              " 'i have been very pleased with avira for years ',\n",
              " 'that was very insightful thank you',\n",
              " 'ysk if you quit using your admin account for daily use  you do nt need an av only use your admin account to do things that require administrative privileges ',\n",
              " 'it also might be useful to know that avast  software updater uses opencandy adware old news  and they might not bundle it anymore ',\n",
              " 'just happened to a friend which is what brought it to my attention her insurance demanded notes from her sessions  which obviously made both her and her therapist very uncomfortable the therapist rewrote them as a summary so the word for word versions would nt be handed over  but they did nt accept those  i do nt know how widespread this practice is from state to state  company to company  but in philadelphia under aetna  it s legal and necessary under the terms of their coverage    edit  misspelled it s aetna not etna edit 2  it s aetna not aetna ',\n",
              " 'i am seeing a therapist on the sly in ontario  canada it is paid for by my wife s benefits yet she is unaware my therapist even had me read over and sign a legal document  that he signed first  that specifically disallows him from ever talking to a third party about our sessions  that is how you get someone to open up not by informing them that their talks are not private  insurance companies are mostly assholes imho ',\n",
              " 'ysk personal medical info is shared way more extensively than many ppl realize medical information bureau group  aka mib  collects health information from diagnostic tests and codes to drinking  smoking and driving habits then provides that info to insurance companies when you apply for insurance like health  life  disability  long term care  etc it s similar to a credit report but with your health info and you re now allowed by law to request your file i discovered their existence about 25 yrs ago when i was denied health insurance i d been diagnosed with rheumatoid arthritis a few months prior and this company knew more about my health than i did it s unsettling and disturbing that no docs  hospitals  etc ever mention this company even exists  wiki link   https  enmwikipediaorgwikimib_group   mib site   https  wwwmibcom ',\n",
              " 'i m a bot   bleep    bloop   someone has linked to this thread from another place on reddit    rprivacy    xpost  from r  youshouldknow  insurance companies can demand your session records from your therapists   https  npredditcomrprivacycomments459esnxpost_from_ryoushouldknow_insurance_companies_can      footer     if you follow any of the above links  please respect the rules of reddit and do nt vote in the other threads      info   rtotesmessenger      contact   messagecompose  to  rtotesmessenger        bot ',\n",
              " 'we really need to change our cultural outlook on mental health  if we wo nt get rid of insurance companies as middle men between us and health care  we should at least try to sheer these bastards for as much as we can and mental health is the least covered  most stigmatized arm of the health care industry  especially if you count drug addiction and rehabilitation as mental health care  one thing i d love to see our society do is adopt a policy towards mental health that we have towards dental health  socially speaking everyone should go see a mental health professional twice a year for a routine check up the same way they should go see a dental health professional twice a year more importantly  the stigma towards mental health should be similar as dental health in that a person is chastised or thought of as strangeweird for    not    going to see a mental health professional twice a year the same way we may think someone is unclean for not having gone to a dentist for a number of years ',\n",
              " ' avast no   http  treasurediylolcomuploadspostimage352642resized_obiwanthechosenonememegeneratoryouwerethechosenoneitrustedyou4d4e12jpg ',\n",
              " 'unot_an_apple made a compilation of avs he s tried and his thoughts on them  here   https  wwwredditcomrbuildapccomments3e16h7slugctamthw   for those who are uninstalling avast right now ',\n",
              " 'thank you my good sir ',\n",
              " 'you can also seek out your local homebrew club or local homebrew store plenty of people would be willing to give you their spent grain ',\n",
              " 'the site explains in easytounderstand summaries the bills and resolutions that congress is considering it includes information such as the bill s status  current actions  motions  and votes  and even includes an easy way to send your support or opposition to your representatives  wwwopencongressorg',\n",
              " 'doctor here  generally  if your doctor ca nt give you a good answer with a couple of tries  it s worth asking for a consultation that s my general ruleofthumb anyway more than two visits without a diagnosis or satisfactory resolution is worth asking for help from a fresh set of eyes ',\n",
              " 'my wife is having foot issues in the past 4 months she has seen 4 different doctors and two pas besides the diagnoses  all six of them have told her something different it sucks because it would of been easier for them to say the same thing and tell us what to do  but on the plus side  we now know all our options and can make the decision ourselves instead of being told what to do ',\n",
              " 'what you also really have to do is tell a doctor everything they ca nt do anything with the information  and an additional symptom could drastically change a doctor s thought process  tell your doctor if you have diarrhea  have a smelly discharge  your      fell off  etc i ve met a lot of people who do nt mention the  embarrassing  symptoms and it killed a few of them ',\n",
              " 'what i find really disturbing is receiving dated treatment that is no longer valid due to new research or change in guidelines',\n",
              " 'been through similar experiences one of the biggest problems i m healthcare are the doctors and insurance providers who ca nt admit being wrong without culpability but that s made worse by those doctors who communicate from a position of confidence and certainty  as a result  patients are led to believe providers are certain and thus responsible ',\n",
              " 'health care in america  half as useful for twice the cost ',\n",
              " 'you should also know never to touch the tailpipe if your car is on or was recently on  as it can burn the skin right off of your hand ',\n",
              " 'as others have said this is so the insurance company can review for medical necessity for additional services or visits this is a way to cut down on rampant fraud that has ended up costing billions for insurance companies across the country  edit  disclaimer  the below is true for the specific company and unit i was a part of i am in no way saying this is the policy of every insurance company  this is just my own personal experience with one such company  what i want to add is this  those who are reviewing the notes  are not regular employees of the insurance company they are licensed doctors and nurses  depending on the service being reviewed  who are employed with the insurance company for the specific purpose of reviewing clinical notes for medical necessity   so the person answering your phone calls is not the person reviewing this information not to mention any insurance employee will be fired on the spot if they are looking into materials with phi that have nothing to do with their current assignment they take that      seriously  source  worked for a major insurance company as a customer service agent whos unit had a medical review team very few people were allowed to access that documentation and only licensed medical professionals read the clinical notes to make a decision on medical necessity the entire insurance company does   not   have access to those records if your doctor has to submit them  only the people who are absolutely necessary to complete a review are allowed to review them  believe me  the insurance company does nt want a nonmedical professional to review clinical notes and make determinations of medical necessity as much as the doctors and patients dont want it too much of a liability from many different angles ',\n",
              " 'for those that do nt know 1 byte  8 bits and ironically enough  1 nibble  4 bits ',\n",
              " 'also  windows displays file sizes in kibimebigibietcbytes  not kilomegagigaetcbytes  hence why your 3tb hard drive does nt show up as 3tb ',\n",
              " 'aww this is so cute if we did nt live on the third floor i d totally get bloomberg an orange collar',\n",
              " 'gt  less than 5  of lost cats are returned home  i do nt believe this statistic for a second i d guess something more like 90  of  lost  cats return home on their own it really depends on how  lost  is defined and where those figures are coming from i remember the first time i  lost  my cat  we certainly did nt report it to anyone i knocked on doors for a couple hours  got sad  and then that night he was clawing at our front door  edit  downvoting me does nt change the fact that cats are intelligent and fantastically independent ',\n",
              " 'i put a bell on mine  there are coyotes around here gt   ',\n",
              " 'kbmbgb  kbitmbitgbit kbmbgb  kbytembytegbyte small b is a bit  big b is a byte easy ',\n",
              " 'it s easy  if you have a      15mbps internet like i do  multiply 15 x 1024mb  then divide by 8  8 bits in a byte   you should get your average download speed ',\n",
              " 'does the goverment get them though hippa ',\n",
              " 'relevant to me our customers are required to submit individual session notes to support their claim this job sucks for all parties involved  i do nt want to look at your records  but i have to customers do nt want to give us their records  but they have to creates a lot of strife for everyone and makes coming into work absolutely miserable  warning  i m going to get political here  this is why we need universal healthcare and a universal basic income that way people do nt need to hand over their private medical files to an insurance company in order to file a claim so they can get an income to pay their bills ',\n",
              " 'i m guessing it s in more states than just pennsylvania here s a blog post about a similar issue  apparently in texas  written from the perspective of a counseling service they suggest it is indeed a hipaa violation  and did nt want to respond to the request with the requested information  http  wwwdavidsoncounselinggroupcomwantyourinsuranceclaimpaidtheywantyourrecordsfirst',\n",
              " 'the magic number is to divide by 8',\n",
              " 'also backfeeding it into your dryer outlet is not a good idea ',\n",
              " '      i  can demand the records  the question is  is the therapist legally required to give those over to the insurance company and is the insurance company allowed to withhold payment  assuming mental health is a covered thing  if they do nt get the records ',\n",
              " 'mb  milli bit mb  mega byte  ',\n",
              " 'keep in mind the same thing is true for any type of insurancepaid healthcare in the us when you go to a doctor  one of the first things they have you sign is a piece of paper that says they may release your medical records to your insurance company in order to secure payment for services it is not a consent  it is a notice that they have the right to do it  gt  a covered entity may  without the individual  s authorization  gt  use or disclose protected health information for its own treatment  payment  and health care operations activities for example  gt  gt    gt  gt   a health care provider may disclose protected health information about an individual as part of a claim for payment to a health plan   source   http  wwwhhsgovhipaaforprofessionalsprivacyguidancedisclosurestreatmentpaymenthealthcareoperationsindexhtml ',\n",
              " 'aussie here  25mbps is very fast please spare some of your download for us poor aussies ',\n",
              " ' example of this actually happening   http  onemileatatimeboardingareacom20110601someonedecidedtocancelmyticketsremembertoprotectyourtravelplans ',\n",
              " ' shopping  for isps right  like people have a choice ',\n",
              " 'i really do nt get the point of this post first of all whether or not you describe yourself with a disease term should nt really matter in general if you re going to get offended or annoyed by people misusing words in their exact denotative fashion i would expect  retardmentally       to be the primary example to which there is some validity but i still personally think you can express yourself however you like  especially if you are getting your point across effectively  second i really do nt understand why you think ocpd is more accurate because i believe it s less accurate in describing what people are doing when they refer to their behavior in a comparative way to ocd  with your example someone is getting annoyed with a picture frame not being parallel to the floor  probably   there is an obsession  things need to be organized in this or that way  and a compulsion  i will take this or that action to alleviate my obsession   ocpd on the other hand tends to revolve more around the strictness of a lifestyle this means scheduling is so important that alternative means can never justify the ends this means not trusting other people to do things for you because they could straight up      your whole day by being slightly late or doing something slightly out of the ordinary additionally people with ocpd tend not to see the problems they have as problems but more of guidelines that a person needs to religiously keep in line with to live correctly  so correcting a slightly crocked picture frame  at least in the example of people misusing ocd  is nt the act of religiously keeping to a specific organization in order to function it s just something that s bothering a person and they want to act  have a compulsion  to relieve that irritation i do understand where you are coming from in terms of the perfection angle but that s not really the main component i was taught to focus on when learning ocpd in terms of differentiating from ocd ',\n",
              " 'i ve been diagnosed with both actually i do agree with your post it s more helpful to know the distinction  and know that ocd and ocpd are nt fun you just watch yourself      up your life over and over again  struggling with perfectionism and my grades show it i ca nt break out of that cycle it does nt make me a good student i ve tied in perfectionism with my self worth somehow i would nt hold anyone to the standards i ve set for myself i m setting up myself  and others  for disappointmentand doing it repetitively  i also have severe issues with contamination my hands bleed and i go into an outright panic attack if i do nt have lysol wipes on me i doubt every action  every intention i ve ever made i need to know if it was right uncertainty is an uncomfortable  perpetual presence i need answers logical  factbased  and concrete  add in adhd  some general anxiety and depression  and idiopathic hypersomnia it s a wonder i get  anything  done ',\n",
              " 'ysk that you should switch to ublock origin anyway ',\n",
              " 'if this actually affects you  you should be worrying more about your social life than you cell phone data plan ',\n",
              " 'run noscript never see most of them in the first place ',\n",
              " 'also  try overdrive  i use it on android  do nt know if it s available on iphone  to borrow ebooks from your library once you get a card number i have been doing it for almost two years  and have not set foot in the actual library ',\n",
              " 'yes  but importantly to compliment this  doctors know the human body better than you  but you know your body better than doctors i ve had two tumors  brain and adrenal  removed both times i knew something was wrong even when the doctors did nt when that happens  be your own advocate ',\n",
              " 'what i really love about numbers is that everything just adds up',\n",
              " 'good friend of mine married and went to belize on her honeymoon came back and they moved into the house they were buying 2 months later she started having gastrointestinal issues accompanied by being really tired and lack of appetite  she lost 15 lbs and visited her md  he diagnosed her with ibs gave her some medication and sent her on her way 3rd month she was in excruciating pain and had to be rushed to the hospital  part of her intestine had literally necrotized and they had to section her intestine  hospital recommended a gastroenterologist that specialised in ibs  did a full work up on her  changed up her meds and another month went by she was down 40 lbs  still having pains and her husband was going crazy finding other md s to give her a second opinion  finally she met one md that did his homework  asked the right questions and finally nailed it down when he found out she was in south america he recommended a doctor in california  they flew out there and did some preliminary checks and found that she had a parasites living in her intestinal tract treatment for whatever she had required her to be in the hospital for a few weeks  then follow up s over the next 6 months with additional treatments  they sold the house  moved to california  she was successfully treated and is back to her normal weight 3 years later she s also expecting their first child  never give up  doctors are people and people always make mistakes ',\n",
              " 'not perfect by any means  and often they are downright incompetent we have hospital bills for being released in worse condition than when admitted and without diagnosis many misdiagnosis many that did nt refer to a smarter  more educated dr when they should have all they way up the chain until mayo this game is expensive  degrading and set up to fail the majority of the time if you do nt have a simple cold or a visible lump but yeah  keep on asking questions and pray that you find a good one at some point before you go broke  or die  because hospital errors are the 3rd leading cause of death in the us source  http  wwwhospitalsafetyscoreorgnewsroomdisplayhospitalerrorsthirdleadingcauseofdeathinusimprovementstooslow  rant carry on ',\n",
              " 'or better  if you already use adblock   rmb gt  hide an ad on this page gt  select the overlay gt  confirm gt  boom  gone',\n",
              " 'oh and do nt trust dentists',\n",
              " 'here s my story  at 12 i discovered fist sized lymphnodes under each armpit region when i went to the clinic  low income  they did nt even draw blood they did an ultrasound  useless  and sent me home with penicillin after a few years  they got bigger and my body got more tired and bruised  at 17 i got diagnosed with lymphoma  then evan s syndrome  then lupus i ve been in and out of the hospital  i ve dealt with medical professionals and if you do nt advocate  speak up for yourself  they can run you right over  ask questions get second opinions if something is wrong  insist on tests doctors are not infallible and they do get lazy and misanthropic like us all  this does nt mean try to diagnose yourself  it means what it means  get second opinions and do nt put all your eggs in one basket ',\n",
              " 'ysk   library card  has other meanings on the internet',\n",
              " 'as long as this is nt interpreted as  you saw an advertisement for medicine and you know you need it  even if your doctor says you do nt   then it s great advice  also remember  webmd does nt make you an expert if you think your physician might be in the wrong  get a second  medical  opinion not the internet ',\n",
              " 'yes i went to the er in unbearable pain  morphine did nt even make a dent in it they could nt see anything on the sonogram so they tried to send me home i told them i d be right back as soon as the pain meds wore off and they needed to figure out what was wrong with me  turns out i had ovarian torsion  if i had gone home  i would have lost the      ',\n",
              " 'could be useful if it works for quora',\n",
              " 'i told my doctor i had bad luck with abreva and he said it was        and perscribed me something else lol',\n",
              " 'i got misdiagnosed in 2014 for multiple things and spent five months in intensive rehab and still have treatment twice a week to this day for what could have been a mix of fixes that are highly successful after 23 months ugh  edit  i say this because since that time  having seen over 30 doctors  50 med professionals  over the last 14 months  i ve learned to question everything responsibly not to doubt them  but to keep your curiosity up at all appointments and ask questions to perhaps spark an idea to them my current doctors are awesome but in the end  remember you care more about your body and health more than anybody in this world does ',\n",
              " 'also  you do nt even need an extension for this or  if you use an extension  you can select the overlay with ublock origin  press ctrlshifti of f12 to open the developer console  it  works on chrome   https  developerchromecomdevtools  and  also on firefox   https  developermozillaorgenusdocstools    it opens a pane on the bottom of the screen click the pointing icon on the top left of this pane  click on the element on the screen  it will select it then press the delete key  if there are another element below it  perhaps a fullwidth  graying out  screen   then select it again and press delete then  close the developer tools  this does nt change the page permanently  the overlay will be back when you visit the page again or refresh but that s okay if you just want to read an article and never go back to this website  note  when selecting an element with the developer tools  it shows a css selector on the bottom that matches it exactly this can be used in  ublock  adblock rules   https  adblockplusorgfilters  elemhide_css   for example  the ublock rule  sitecom   divtextad  filters the element selected by the  divtextad  css rule on the  sitecom  website  of course  ublock comes with its own  selection tool   https  githubcomgorhillublockwikielementpicker   click on the ublock icon  then click on the  picker  tool  then click on the element  and in practice you should use it to select an element in screen and then block it forever  it adds the rule to your  my filters  tab on ublock options   i mean  everyone should already have ublock origin installed  but  knowing how to do this stuff in a browser that does nt have any extension installed is also useful  edit  as a side note  once you have the css selector you can also hide the element with a greasemonkey script  just add  a script to hide the element   https  stackoverflowcomquestions22020833howdoiusegreasemonkeyandjquerytohideanhtmlelementwithacertainclas    greasemonkey is kind of a generalpurpose tool to locally alter the webpages you visit  that go beyond than simple filtering  ',\n",
              " 'is there a firefox alternative for this type of addon ',\n",
              " 'https  myoutubecomwatch  v  khz8ek6ccc',\n",
              " 'but when i turn the main breaker off  and then if i plug my home made doublemale extension cord into my generator and any wall outlet then only half of the outlets in my house work  you need the breaker on for all the outlets to work besides  it s neighborly what if my neighbors do nt have a generator  then they can use some electricity too  see  winwin ',\n",
              " 'there have been a few posts around reddit today  and everyday  about people losing their most precious photoswork projectshomework assignments  etc because their phone was dropped in the toilet  or their  computer was thrown across the room   https  npredditcomrpcmasterracecomments400rojfriends_dad_threw_his_pc_on_the_floor   or their  laptop was stolen   https  npredditcomrtodayilearnedcomments3zxt33til_in_2011_skrillex_lost_an_entire_unreleased   these incidents  while tragic  do not have to mean the end of all your data if you follow the simple 321 rule   3 copies of your important files  2 different formats  or places   1 of those being offsite one copy of your data is never enough a theft  a hardware failure  or any other tragedy could be the end of all your data once it s gone  it s gone  onsite backups  like what you get with time machine or other services like that are great  and can protect you from a hardware failure they re especially nice to have because it can take a long time to restore data over the internet  especially if there s lots of it but in case of a house fire or thorough burglary  you re going to be out of luck yet again  that third  offsite  copy is where you get the final bit of protection you want that backup to be far from you  so just in case your home gets  hit by a fighter jet   https  a248eakamainetf120215794midailymailcoukipix20120406article2126223127f0a1c000005dc304_964x709jpg   or just a regular fire   there are lots of services that you can use to pay a minimal monthly fee to backup your data here s a short list    crashplan   https  wwwcode42comcrashplan    backblaze   https  wwwbackblazecom    mozy   https  mozycom    carbonite   http  wwwcarbonitecom  i personally use crashplan  and pay about  4month for the peace of mind that  should anything happen to my home  and my laptop  and my desktop  i wo nt lose everything i started paying when i was working on my dissertation  because there was no way i was going to be the guy who got 150 pages into it and lost it all to some freak accident  edit  clarify  4month for crashplan ',\n",
              " 'add to that  if you have nt tested your backups to make sure they really restore  you do nt have backups ',\n",
              " 'why do nt file sync services get mentioned when discussing backups  or remote version control systems  most people do nt have to save huge important files ',\n",
              " 'another tip  find an app for your phone that will sync the data to your pc or backup provider  at least once a week someone posts how they droppedlostbroke their phone  and are desperate to save the files  i use an app called syncme on android and it just copies all my picsvideos to my pc  which is then backed up to crashplan  this is especially great if you get a new phone  just copy all your photos over in a flash ',\n",
              " 'to seed drive or not with offsite stuff ',\n",
              " 'i have barely any data  but i use an external hard drive  a flash drive  and google drive to back everything up most of my photos are on photobucket too  thinking about switching sites but whatever  ',\n",
              " 'each case would be different  security and confidentiality come into play ',\n",
              " 'in addition to crash plan and other services like that for off site backup  you could even go with a free alternative such as dropbox or google drive provided it fits the space requirements you have ',\n",
              " 'you know  pneumonic devices like  321 rule  are completely useless if they require that much explanation  and this is a completely paranoia level of backup something my grandpa or dad would do because they read it on the internet  you really only need one backup of data you are never in your life going to have 2 hdds or ssds go out at the same time houses burning down is exceedingly rare these days if you are preparing for things like your house getting hit a fighter jet  come on really   then you should probably just go live in a cave get one automated backup  put it in a discrete but easily accessible place by the front door that way you can just grab it on the way out if you need to  stop being such a      ',\n",
              " 'many of us have been paying turbo tax or hamp  r block to file our taxes for a while and they charge you a fee to file  myfreetaxescom   http  wwwunitedwayorgmyfreetaxes  in conjunction with unitedway and hamp  r block allow you file for free if you make below  62000 a year they even upgrade you to  premium  if you have a small business for free  happy filing   edit  to specify this is for usa filing only ',\n",
              " 'the biggest downside to this method is you are missing out on a lot of possible deductions that would only be found by paying an accountant  100 to tell you what you missed',\n",
              " 'is it true noodles are danger to eat ',\n",
              " 'cool exactly when did they implement this ',\n",
              " 'i thought it said cactus i was genuinely interested in learning about cacti',\n",
              " 'that looks like the beginning of kingdom hearts 1 when you choose your weapon for the island',\n",
              " 'just a regular  housefire lol ',\n",
              " 'actually i do nt need to know this i m one of the many people on reddit that is nt american ',\n",
              " 'i truly do nt have anything digital that i care about or need that much ',\n",
              " 'i can vouch for crashplan i had a hardrive die and i was back up and running with no data loss because of it it also does version which was super helpful when some of my files got messed up and i could pull out an older version ',\n",
              " 'my saying has always been 3 copies in 2 places for every file ',\n",
              " 'butters i do nt backup my data ',\n",
              " 'everyone should back up their stuff my sd card crapped out  and i would have lost all my photos on my phone if i had nt had dropbox set up to upload all of my photos ',\n",
              " 'also  you can use mdisks these are like super tough dvds  and they advertise your data will last 1000 years or so ',\n",
              " 'would this cover someone with 1099 s as well ',\n",
              " 'few questions if you do nt mind  do you not get nervous trusting some service with all your data  i know they say encrypted  but how can you be sure  can i instead use my own encrypted backup for any of those services you mentioned to store in a cloud  how long do you reckon it would take to backup about 150 gb from a mac  thanks for those recommendations btw think ill go with crash plan  although i admit it kinda bothers me letting someone u dont know keep all your data ',\n",
              " 'is there any online backup that will  1  allow me to select a just folder to back up 2  does not have crazy limitations on how much or how big 3  doesnt throttle bandwidth and take months to back up and restore data 4  only runs when i want it to run   tried carbonite before slow 6 months to back up  no thanks always on  just tried backblaze as i read they do not throttle but it wont let me select a folder it wouldnt let me unselect my c drive it wants to back up all my files on every drive i dont want all that      backed up control freaks  i just want to be able to back up my backup folder and all the      in it that is important to me the rest i dont care about  i have money please take my money just give me what i want for      sake it simple if synctoy had an online drive it would be perfect ',\n",
              " 'does this apply to both federal and state ',\n",
              " 'in canada  most online tax filing services will process your file  then at the end will give you the option to pay and file  or cancel if your income is low  they ll say the price is 0',\n",
              " 'i m surprised taxact is nt mentioned in this thread more for any complicated filling they re just as good as turbotax at a fraction of the price',\n",
              " 'not everyone has a  simple  tax filing yes there are free ways  even with turbotax but that s based on your annual income and forms needed to file  if you re married and making over their cap  and own your own business  then no it ai nt free and all the fees are in fairly large print on their website or software ',\n",
              " 'not if you re active duty military and get the basic refund package  d edit  words are hard',\n",
              " 'stop using turbotax and use  myfreetaxescom   https  myfreetaxescom  bc its 100  free for everything',\n",
              " 'small engine repair businesses hate him for this one simple trick click here to find out ',\n",
              " 'i used  free tax usa   http  wwwfreetaxusacom  this year  it allowed me to file my schedule c  self employeed   hsa form  and itemized deductions for free state cost  13 to file tax act would have cost me  30 and found  100 less in deductions  i had previously used tax act since 2009 and i do nt think i ll be going back i do nt mess with turbotax  it s way too expensive  ftusa also allows you to upload a pdf of your last years returns and auto pulls the info into this year  highly recommended ',\n",
              " 'tax act does the same thing ',\n",
              " 'hamp  r block does this too  i m guessing this      fee became standard after whoever was the first one to do it and realized  oh       ppl will actually do this and just give us  35 for nothing  and generated millions of dollars of revenue for them and its inventor is now on his yacht somewhere  i mean  it s very clear that they re charging it so i m do nt think it s too shady or anything  but it s at that airlines level where they just keep gating things behind fees ',\n",
              " 'itt  reverybodydidthemaths',\n",
              " 'ups stores will often provide insurance for products and when you close the flaps on top to seal the box with them  again overfill the box with peanuts  overload the box  so when you declare a value they ll charge you anything over  100 ',\n",
              " 'but if youre in the military they give you it right back which is nice',\n",
              " 'oltcom is super easy and super cheap i ve used it for all my clients for many years ',\n",
              " 'will void the warranty if you use gas from a gas station  fuel with ethanol  for some small engine mfrs so  read the fine print learned this the hard way ',\n",
              " 'ok what is stabilizer  is sea foam a stabilizer  or is that just good for gummed up stuff ',\n",
              " 'post a source  damnit ',\n",
              " 'note  in the eu  certain fact in germany and the uk  all fuel contains ethanol most is 5   on the continent they do have 10  ethanol  do not use this unless your cars manual says you can always use the lowest available ethanol in motorbikes ',\n",
              " 'i have a hunch you had armchair immunization experts or gmo naysayers in mind when you posted this should also note that the podcast or website that was found was not properly vetted since the knowledge required to properly vet has not been acquired yet ',\n",
              " 'you just made a bold statement with absolute zero evidence to back it up  well done champ ',\n",
              " 'i would assume that this does nt apply to engines  cars in particular  that have an e 85 rating  flexfuel or otherwise named  ',\n",
              " 'first time i ve ever seen someone call it a snowthrower ',\n",
              " 'it s all i run in my small engines  there is nothing good for your engine in ethanol use this site to find nonethanol stations near you http  wwwpuregasorg  ethanol fuel burns less efficiently  retains more water in your fuel  causes gas to go bad faster  and will rot rubber and plastic seals and hoses ',\n",
              " 'snowthrower ',\n",
              " 'is it too late to add stabilizer to my engines now that they have been sitting with ethanol fuel unused for a few months  or should i drain them and put fresh fuel in with some stabilizer ',\n",
              " 'it is pronounced snowblower   ',\n",
              " 'use high test if you ca nt find anything better to run in your small engines ',\n",
              " 'it seems to be working just fine for me thanks for the link ',\n",
              " 'the fuel eats things from the pickup tube screen in the tank  to gaskets and orings it also ends up going bad when stored for a while and will eventually gum up the carb to the point where you ll have issues startingrunning run premium gas and you should avoid the problems  also if you think you have water in the fuel  do nt just throw any kind of additive in check the owner s manual some additives sold at a gas station for cars will cause the snowblower or other small engine to run too hot  and possibly burn a hole through the piston ',\n",
              " 'my way  1  pull out calculator 2  divide by number you re trying to see if it s divisible by',\n",
              " 'i ve done this if you re going to as well  my advice would be to get selective about what you put in it my book is 400 mb as a pdf file and would have cost more than 200 dollars to print ',\n",
              " 'a general way to determine whether two numbers are divisible is by using the  euclidean algorithm   https  enwikipediaorgwikieuclidean_algorithm   which is fairly efficient  o   log a  log b   2   at determining the greatest common divisor  and checking if the result equals the smaller number  this has been reformulated to  only require subtraction and division by 2   https  enwikipediaorgwikigreatest_common_divisor  binary_method   and iteration and powers of 2  likely to be small powers   as follows  the  binary gcd algorithm   https  enwikipediaorgwikibinary_gcd_algorithm    input values  a  and  b  to check  0  define auxiliary variable  d   0 1  while  a  and  b  are both even  easy to tell   divide both by 2 and increment  d  by 1  2  while  a  is not equal to  b  21  if  a  is even  then half it   a    a   2  22  otherwise if  b  is even  then half it   b    b   2  23  otherwise  take their difference divided by two and set that as the value for the larger of  a  and  b   so if  a  gt   b   then set  a     a    b    2   their difference  and sum   is guaranteed to be even because the preceding clauses imply that  a  and  b  are both odd  3  return  a   2   d   or  b   2   d  since  a    b  at this point   the greatest common divisor  remember that the value of  a  has been modified by the algorithm   4  compare this result to the smaller of the input values of  a  and  b   your numbers to check   if they are equal  your answer is  yes   otherwise the greatest common divisor which you just found will be smaller than both  a  and  b   if the gcd is 1  that means the two numbers are  coprime   https  enwikipediaorgwikicoprime_integers   and are not even  close   to being divisible      share no prime factors  but changing a or b by 1 might change everything    edit   yeah this is quite useless for anyone to learn that is nt training to become a calculator  calculators started as people fyi  ',\n",
              " 'holy      op  you should know better to not post this low effort      ',\n",
              " 'idk what you re talking about ipod 5 ios 902',\n",
              " 'this webpage is not available',\n",
              " 'my mower hasnt showed any abnormal wear from using gas from the pump',\n",
              " 'if you need to fine pure gas near you and do nt know where   check this site out   http  wwwpuregasorg ',\n",
              " 'go to wwwpuregasorg to find gas stations that have gas with no ethanol added ',\n",
              " '     there goes my day off',\n",
              " '     ethanol is the biggest scam in history uses tons of water and food and destroys engines ',\n",
              " 'uh what about a tiller  is a tiller etc ',\n",
              " 'this is crazy overkill unless you re using stupidly expensive yard equipment to begin with buy a cheap      sears mower  beat the      out of it  dump some stabil in it for winter and continue to beat the      out of it replace every 56 years oh yeah sharpen your blade at least once a year ',\n",
              " ' this page   http  wwwfelinecrforgsubcutaneous_fluids_tipshtm  shows the technique and all you need is an rx from your vet for the fluids which can be supplied by any pharmacy  costco is usually the cheapest and you do nt need a membership for the pharmacy   our cat was diagnosed with crf at the age of ten and lived a happy and otherwise healthy life until he died 11years later at the ripe old age of 21 ',\n",
              " 'what is considered fast ',\n",
              " 'i feel like there should be an  on average  impact on small engines in the post or guess maybe this is anecdotal have had the same honda motor mower and have used standard ethanol fuel for 15 years now ',\n",
              " 'for philly folks  due to the snow emergency  which is set to begin at 9 pm tonight  january 22  meter and time limit violations will not be enforced however  we will be enforcing all snow emergency and safety violations normal enforcement of parking violations will continue when the snow emergency is lifted vehicles illegally parked on snow emergency routes will be subject to ticketing and towing if your vehicle has been relocated  please call 215686snow  additionally  starting at 5 pm tonight  january 22  we will be offering 24 hour  5 flat rate parking at the following center city garages the  5 snow rate will remain in effect until the snow emergency is lifted ',\n",
              " 'itt  ermehgerd errple surks',\n",
              " 'which is why i just put in a fake email for when i register my copy of free avast every time  problem solved ',\n",
              " 'is there an easy way to completely uninstall avast  leaving nothing behind ',\n",
              " 'stop using avast  seriously it used to be the standard for nonintrusive  well working av  i had to remove it for using 30  of my laptops resources  and even after uninstalling it left bits running  inclueing its main executsble avastsvcexe  which was the real deal  not a zombieworm or other virus  now it is just a pile of ads and its quite sad  i used them for over 7 years ',\n",
              " 'why ca nt i see this  signature  in my most recently sent emails  i do nt really have any issues with avast  unless i m somehow being naive',\n",
              " 'my thinking from an it perspective is that if it can write to it  it can read from it that prospect bothers me ',\n",
              " 'i really do nt know why this is such a big deal to people i understand it may be a nuisance and undesirable  but if you do nt like that feature  just disable it why stop using avast  just over that  seems silly to me ',\n",
              " 'well i guess i m moving to avira thanks for the heads up ',\n",
              " 'as a former chemistry student  this title confused me for several moments ',\n",
              " 'itt  ysk if you are nt paying for the product  you are the product  we have all come to expect things for free or next to nothing  this is the price you have to pay for free at the end of the day  these free products need to turn a profit somehow it is just a shame that it inevitably leads to intrusive behaviour like this ',\n",
              " 'time to get a new one i paid for avast this upsets me greatly and its lost them my buisiness i need a new one  i ve heard panda is good from uhittingsmoke s post but i m wary about cloud based anything the only cloud service i use is steam s are there any other good ones out there  microsofts tools are a joke ',\n",
              " 'stop agitating and get back to work  just kidding  throw on some pete seeger and get some power back ',\n",
              " 'i had a mid 2009 macbook pro it lasted me up until a few months ago i had two major issues under warranty  one including the touch pad which they replaced immediately no questions that laptop was upgradable updated ram and added an ssd around 34 years in  but now this  on top of the newer models having everything saudered to the motherboard making them not upgradable  yeah i have no regrets getting a windows laptop recently instead i ll admot the service will never live up to apples  ever though',\n",
              " 'i wonder if anyone has averaged how long it takes decent freeware companies to resort to infection g their customers ',\n",
              " 'i switched to sophos home free for up to 10 devices per account  and it s enterprise grade av no nag screens or popups my only complaint has been that i have to log in to the web interface to add exceptions ',\n",
              " 'awesome  i m playing  alone in the dark  first time in 20 years  thanks  op ',\n",
              " 'why not just eat the other banana half then its not going to go bad it s only a banana  unless your eating huge bananas  in which case i think we need to see one for scale ',\n",
              " 'april is coming up there s a free tax program you can use that s absolutely free  http  wwwstudiotaxcom i ve been using them for years the program is very very easy to use it s really just fill in the boxes with numbers and see what your return is  it even has forms for others which i never filled out so it may have all that you need for your tax purposes ',\n",
              " 'is there a version of the etherkiller for minidisplayport  daddy needs a new macbook ',\n",
              " 'i got that when i was in in college 5 yrs ago and still have it on my old laptop for some reason i ve never been able to use keyboard shortcuts to copy and paste which made it unbearable  they also gave me win7 os  access and visual studio ',\n",
              " 'i ve worked in medical records for a long time providers do not need a patients consent to release records for the purposes of treatment  payment  or in office operations if you were asked to sign a privacy policy at the beginning of your treatment it was outlined there ',\n",
              " 'i am a therapist insurance companies can ask us for clinical information about a patient to continue to authorize services and if we do nt provide it then they stop paying i do nt even have to get a signed release from the patient like in other cases when an agency is asking for such info  like the court system  cys  or another medical provider   and a summary would nt be accepted it definitely has to be the actual clinical notes unless they are ok with speaking on the phone this is usually required when clients move to different levels of care  verbal auth usually  but when i ve had clients on long term disability insurance always wants tons of my notes and it s all insurances not just aetna   edit  since this kinda blew up  other agencies can look at records too dept of public welfare  state level  and joint commission of hospitals  national level  regularly audit organizations and look at notes and records to ensure proper treatment is being conducted also supervisors in an organization will audit charts and notes to do the same i work for a large organization so many levels of administration above me can look at my patients records moral of the story here is that nothing is private nowadays but as many have stated below this stuff is being looked at to make sure good care is being provided and that fraud is not being committed   edit  2 i d like to add as well that  at least in my case  each and every time i have given records over to insurance  or provided verbal info  is to advocate for a patient to receive a type of treatment that i feel would benefit them  never to take away something so for example i turn over records in order for them to continue to get longterm disability payments and insurance for when their mh symptoms prevent them from working  or when they need a higher level of treatment if a situation arises where i feel an entity or organization is working against my patient  then you had better      well believe i m invoking hipaa  clientclinician confidentiality  and throwing up a wall of  i ca nt talk due to privacy rights  statements ',\n",
              " 'til not everyone is in the usa  ysk about til ',\n",
              " 'what reason did they give for demanding the notes ',\n",
              " 'knowing this really rustles my jimmies ',\n",
              " 'my roommates and i did this in college with skol      it worked  but it ruined the brita filter after spending money to replace the filter it would have been cheaper to buy better      in the first place ',\n",
              " 'gizmos amp  gadgets  all the super solvers  number munchers  reader rabbit  well there goes the rest of my night ',\n",
              " 'fyi it s  aetna   not  aetna   since it is not an acronym looks like it might come from a latin word ',\n",
              " 'ca nt find chips challenge  010',\n",
              " 'never thought to do it any other way it s much easier to peel the  half  banana after you cut it anyway i also do nt bother to wrap it  just slice off the very little dried up bit when i eat the other half ',\n",
              " 'doing a great job getting rid of the stigma on mental health problems and getting therapy for it yup  great job s',\n",
              " 'you are a saint almost had to buy it for school ',\n",
              " 'holy       i ve been looking for dangerous dave for years  i love you for this op ',\n",
              " 'if you fresh install windows 10 on a gpt disk  uefi boot mode enabled   it does create an efi  mrs  wo nt show in disk management   recovery  and c  drive if you install windows 10 on an mbr disk  it only needs system reserved partition and c  drive when you upgrade windows 81 to 10  windows 10 will create a recovery partition if your system reserved partition  mbr disk  or recovery partition does nt have enough disk space as you know  windows can only shrink a partition from the right side  so the 450 mb recovery partition that is the closest to c  drive is new created  which should not be deleted if you want to use recovery options if you really want to delete it  you can first create a recovery drivedisc in disk management  you can not delete recovery partition you can either use diskpart or third party software to see more details  you can read  the recovery partition after upgrading to windows 10 from windows 78 http  wwwdiskpartitioncomwindows10recoverypartitionafterupgradingtowindows104348html',\n",
              " 'the overarching law here in the states is hippa states must comply with this as a minimum health insurers must comply with this law they are allowed access to certain information in order to determine benefits and pay  but anything over and above cpt codes or icd information would be excessive under the law your privacy rights should be defined and on file with the provider as you are required to sign it annually i would ask what information is provided  why it is required  what will be done with the information  and how it is to transmitted and stored asking questions such as that usually forces the carrier to back off being an informed patient helps in this case ',\n",
              " 'how on earth could you know whether 13  of all people know about it  did they conduct an international poll or something  please ',\n",
              " '  bangs head repeatedly on desk   n  x   0',\n",
              " 'this post surely wont backfire ',\n",
              " 'i think this is something you have to be careful with yes  doctors are humans  they do nt have all the answers  and they make mistakes but at the same time  they have years of training and experience in the medical field  so their opinion is likely more authoritative than yours keep in mind that they re professionals  trying to do what s best for your health  not customer service whose job is to appease you in general i d say seek a second opinion if you have a specific reason not to trust the first one  but do nt ignore a doctor s advice because you do nt like it ',\n",
              " 'it boggles my mind how many people take others advice as gospel and do nt think to yourself i m no physician  but i am educated and you better      believe i know exactly what medication i am taking  what the side effects are  what contraindications exist  and dosing schedules ',\n",
              " 'at one point in time people s entire income consisted off of scrap people turned to collecting  sometimes stealing  scrap steel and selling it for up to  025 a pound  today if get a nickel a pound you ve got some luck  the average price of scrap steel is 003 cents  you could feed an entire family of 4  own a home  and still have enough to work toward a better tomorrow  now you re lucky is you can pay for the gas just to get to the scrap yard  i year ago today i got  016 a pound and it was luxurious http  imgurcomgallery2xauttv',\n",
              " 'welcome to the wonderful world of deflation  which i m sure we ll all get to know intimately in the next few years ',\n",
              " 'here s some scrap prices as at 122515 bare bright copper is at   200lb that s down from around  250 a year ago i think  http  wwwscrapregistercomscrappricesunitedstates260',\n",
              " 'not anywhere as serious as your condition by any means however i produce kidney stones in an insane amount i went years with urologist that just said nothing could be done and they would just hand me pain killers as they happened and blasted out the ones that were too big to pass i just kind of figured this is what life was going to be for me after close to a decade or so and well in to the 200 range of stones i got fed up quit seeing my urologist and started trying diet options for months at a time seeing what might have an effect beyond the few things they would just tell me to steer clear of i eventually landed on a diet that i felt helped me greatly reduce the amount i produced  but i still needed a urologist it took me weeks of searching and calling to find a urologist that was nt part of the same giant urology conglomerate in my area he turned out to be awesome  i was so lucky he cleared out the 4 or 5 big ones i had  helped me fine tune my diet and started me on meds to slow down production 2015 was the first calendar year i ve gone since 2003 with out a stone  i mean i had years where i passed 20 and finally a stone free year there s still one large one in a weird place that he ca nt get to with the laser but      it who cares i can deal with that monster when it moves  what s one more when my kidneys are all clear besides that one find a doctor that works with you  that works for you  do nt just assume they know what they are doing  or that they care  most do for sure but make sure they work well with you and what you are trying to accomplish  stay vigilant and stay healthy ',\n",
              " 'there s an old saying that goes  the one who scored the lowest at a doctor s graduate ceremony is still called a doctor ',\n",
              " 'i came to see a simpler version',\n",
              " 'a 2nd opinion has saved or prolonged a life twice in my immediate family   first  my halfbrother had renal cancer that had spread all over his doctor said that he probably had a few months to live  with no treatment options  he asked for a 2nd opinion and went to james cancer hospital in columbus  oh  and they said they had a treatment that had a shot of helping him  though it was pretty slim  it cured him it s been 4 years  and he s been cancer free the whole time   my mother had smallcell lung cancer that had spread before it was found the doctor told me we should just make her comfortable since she probably had weeks left they would nt even drain the fluid from around her lungs so she could breath while lying down  so i asked the doctor not to tell her that if she heard a doctor say   we think you re done   she would ve just accepted it because a doctor said it since my brother had seen results  i said to instead go tell her  we think your best option is to be seen at james cancer hospital  the doctor said   that s a good idea  i thought  _  why did i have to think of it    _ she went up there  and they said   sure  we can treat this  they drained her lungs  in her room   and she immediately felt much better then they gave her chemo and cranial radiation  and she lived another 15 months in fairly good conditionspirits   so yeah get a 2nd opinion doctors are just people  and they get stuff wrong the whole field of medicine is kind of a crapshoot ',\n",
              " 'last month the right side of my face started swelling up and got worse when i ate i immediately went to the doctor at my university and he told me i had a salivary stone and that i should      on lemons to get it to pass on its own meanwhile he got me a referral to a specialist that was a month away over the next 10 days i went back and saw 2 different doctors and was told the same thing as it was getting worse and worse i do nt know how they expected me to wait a monthbut eventually my mom got worried enough to force the specialist to see me immediately turns out my salivary glands were infected and if i had waited a month i could have died as it would have spread to my jaw bone and then my brain ',\n",
              " 'why should i know this  i believe it s better to be able to figure things out then have a bunch of rules memorized when it comes to stuff like math honestly  from taing physics throughout undergrad those were the people who frustrated me the most they could nt take a piece of information and see how it could be manipulated to get another piece several steps removed they just wanted to be able to plug some numbers into a rule they memorized and quickly spit out the number they needed i think teaching math this way makes people think that math is some archaic set of rules that s not applicable to their lives when it s actually a pretty powerful architecture for solving quantitative problems  basically these are cool tricks but i think there are easier ways to factor things like using the computers all around us also few numbers you encounter factoring in life will ever be  a  important enough to not round to the nearest power of 10  b  have 7 as the largest prime number they are divisible by ',\n",
              " 'things like this do nt really work if they do nt catch on',\n",
              " 'my mother has been sick for many years now i ve had to do this watching it from the side is so frustrating i m an only child so i am the one to go to ask of the appointments ',\n",
              " 'escaped convict  armed and dangerous  known on the streets as senor fluffbutt ',\n",
              " 'i wish there was a well known way to indicate that your cat has fiv ',\n",
              " 'can confirm worked for hospitals my entire life  now retired   dr s do the best they can while having to answer to hospitals and insurance companies it s not enough to be a good dr  you have to know coding  meet hospital quotas and still keep up to date on treatments and meds i ve worked for dr s who have given free care to veterans  or people in need  cried over cancer patients medical personnel are human and in this day and age it s more important than ever to be your own advocate when people come in and say  yeah  i m not gon na tell you my symptoms because you are the dr and you should know  it makes me want to scream or  i take the yellow pillyou know the one    okay  venting done ',\n",
              " 'reminds me of the oatmeal',\n",
              " 'i wish this would really take off but i m afraid that right now  no one would do anything about an outdoor cat with an orange color thanks for spreading the word though ',\n",
              " 'some breweries use the spent grains themselves  or sell them to farmers as feed  but many just compost them or throw them out call ahead and ask the brewer if they have grains available a few of my local breweries give them away for free   here is the pretzel recipe i just used that prompted this post   http  foodbullycompost29832190061spentgrainpretzels   here are some other awesome recipes   http  brooklynbrewshopcomthemashcategoryspentgrainchef ',\n",
              " 'as much as doctors hate this  bring up things you have read on the internet to them not anecdotal stories  but webmd would be fine doctors do not know everything  in fact that they often vary drastically by skill and knowledge bring up all of the things you may be worried you have based on what you read  because its possible the doctor just completely overlooked it ',\n",
              " 'c does nothing for colds zinc helps  see here  http  wwwbusinessinsidercomvitaminssupplementsthathelpwithcoldsandflu20161',\n",
              " ' blood net   http  playdosgamesonlinecombloodnethtml     syndicate   http  playdosgamesonlinecomsyndicatehtml      but no  a mind forever voyaging   https  enmwikipediaorgwikia_mind_forever_voyaging   man  that game had some great  feelies   http  infocomelsewhereorggalleryamfvamfvhtml  ',\n",
              " 'gt  they will run right over you so are you saying that doctors are human and make mistakes  as in the title   or that doctors are knowitalls who think they always know best  as implied  ',\n",
              " 'this is       just microchip them like any responsible pet owner ',\n",
              " 'just tried on a galaxy note 5 turns my ring tone volume up or down does not work ',\n",
              " 'holy      man forget about the hundreds or thousands of dollars you have spent on forms  applications  and appointments to get this far  your foreign girlfriend comes over on a k1 visa  you guys marry  and now it should just be a quick adjustment of status form to get everything finalized  right  wrong like donkey kong prepare to get bureaucracyraped as you discover that the fee  just to file  a request to change your new wife s legal resident status costs a bank breaking  1490  this is of course  also  assuming you choose to prepare and file the hundred pages of paperwork and forms yourself  without paying an additional stack of thousands for a lawyer   it should not be so difficult or expensive to adjust the resident status for someone already in country   ',\n",
              " 'would nt any collar indicate ownership ',\n",
              " 'how about my cat  we only let him out at night to get      he s always home by morning  so if you see him during the day  he s awol ',\n",
              " 'our 2 indoor cats have dog collars  we do nt like the bells on cat collars they both look like safety vests almost one is neon green with a reflective stripe  the other is orange with the reflective stripe we just got them because they sort of matched and it was the only one in tiny size the store had plus side that we did nt expect is we can see them better when they hide under the bed or couch these collars have also resisted their nails  unlike the collars we had on them before 2 days in their old collars  they were ripped to shreds  they ve had these reflective ones since christmas and not a scratch on them',\n",
              " 'great post  well written and researched  thank you ',\n",
              " 'although i noticed this url may have been featured before i recently discovered that it has been completely updated when i was trying to get my ap bio score if you are a teenager  the site lets you get potential  ap  advanced placement  scores ib  international bac  scores actsat scores psat scores sat subject test scores gpafinal grade calculators url   ap calculator   http  wwwapcalculatorcom  pretty cool and useful ',\n",
              " 'really obscure  11  take the difference between the sum of alternating numbers  if the result is divisible by 11  then the number is divisible by 11  eg 8393   89    33   8933  176  11  hence 8393 is divisible by 11',\n",
              " 'why should i know this  again ',\n",
              " 'i am cmv negative when i was looking for a      doner the doner s cmv status was listed along with the blood type in the interest of avoiding a baby with a birth defect i found a cmv negative doner odd thing was most american cryobank s doners were cmv positive i had to import from out if the country ',\n",
              " 'my 52 year old uncle has had lung disease for six years  and his lungs only get around 45 mmhg i ve heard people commenting when he gets out of the car  he does nt look disabled  insinuating that he s faking a disease or has a fake placard  lung disease  cardiovascular issues  and other nonvisible issues qualify a person  rightfully  for a disabled placard ',\n",
              " 'otherwise known as mind your own      business and move on with your life  if you are not handicapped then whoever is in the handicapped parking area is not your problem  if you feel like you have to insert yourself into every single situation then you need counseling ',\n",
              " 'for the victims of this misplaced judgment and disdain  please remember that the people giving dirty looks are simply ignorant of the finer details of the matter they resent and shame someone that is apparently abusing the situation their heart is in the right place  they are  in a small way  fighting for the assistance that an available space provides you  of course  it d be better if they understood the situation  which is why this post is helpful but considering their perspective  you might not be as angry or offended ',\n",
              " 'http  wwwapplecomnzsupportacwallplugadapter',\n",
              " 'the up votes this is getting really makes me worry about basic math education why use party tricks when we ve got mathematics ',\n",
              " 'whats the benefit doing this  does it taste differentbetter ',\n",
              " 'i filed mine on the 14th still just stuck at accepted  ',\n",
              " 'hey everyone  just wanted to let you guys know that if you re ever traveling out from toronto you should visit not just tourists at wwwnjttorontocom  also available in st catherines and ottawa  and take an extra luggage or two of medical supplies with you  you re already going somewhere  why not help save some lives while you re at it ',\n",
              " 'blue the answer is always blue ',\n",
              " 'that is what my mom said ',\n",
              " 'i tried this on my whole family  unfortunately i did nt have zinc  so i substituted vitamin c and it worked wonders  thanks for the tip ',\n",
              " 'having made more than my fair share of spent grain stuff let me recommend  this granola   http  wwwfitmamarealfoodcomcinnamoncoconutcrunchspentgraingranola   it is so good also better base grain makes better bread the grain i save for making barley flour out of is marris otter because its awesome and more nutty from  op s other awesome recipe link   http  brooklynbrewshopcomthemashcategoryspentgrainchef  i can say good things about the turnovers  thin mints  popovers  coconut pecan cookies  bbq burgers and bad things to say about the corn dogs and oven fried chicken the thin mints were super super hard to make but so delicious ',\n",
              " 'this was based on the thread below  extolling the virtues of using listerine and how its good for you since it eliminates 99  of bacteria etc one user even lists tables with bacterial growth rates showing why listerine is  a must use  thing these responses are assuming that having bacteria in your mouth is always a bad thing  and killing those bacteria with alcohol is good  https  mredditcomrexplainlikeimfivecomments44s60seli5_listerine_kills_bacteria_in_my_mouth_but_i except all bacteria in your mouth is nt  bad   your saliva has its own antibacterial agents and nuking your mouth is nt a very good idea it s a bit similar to why you would nt inject alcohol into your bloodstream  ie because they cause too much collateral damage  the same would apply for your mouth too that s why listerine  or any alcohol containing mouthwash  can have adverse effects just stop using listerine for a month  have a healthy diet  brush and flush thoroughly  and your breath would nt smell that bad at the end of the day  this is because it kills all the helpful bacteria  dries your mouth out and only promotes more bacteria growth since your saliva has antibacterial agents in it noticed how when you stay up all night  your mouth does nt smell as bad but take a couple hours sleep and it smells like a sarlacc  it s because saliva production goes down when you sleep and you end up with a dry mouth  alcohol does the same  combined with smoke  it can even contribute to      cancer as alcohol is synergistic with smoke  ie it compromises the integrity of the cell membranes and can lead to the carcinogens  such as the ones in smoke  that are in your mouth to more easily go into the cells in your mouth and thus interact with the dna and cause mutations   http  onlinelibrarywileycomdoi101111j18347819200800070xfull this is all assuming that the alcohol concentrations are enough to      999  bacteria in the first place the alcohol concentration is nt high enough and you do nt hold it in your mouth long enough for it to do anything really it s been engineered so that it starts to sting a little bit after swishing it around and  therefore  you think its working it s not that useful unless you brushed before to break the plaque biofilms biofilms in general offer protection against alcohol  recent evidence from international research even suggests that the most effective strategy for beating back bad breath may be more about nurturing helpful bacteria in the mouth than about destroying the offending germs and their byproducts wiping out too many of the mouth s native bacteria could disrupt the usual checks and balances  making way for opportunistic species responsible for gum disease and other infections to move in and take over  your mouth is a kind of densely populated tide pool fresh breath reflects a healthy mouth  which is not necessarily one that lacks  bad  bacteria  but rather one in which overlapping bacterial colonies hold one another in check  http  wwwscientificamericancomarticlebeatbadbreathkeepmouthbacteriahappy tl  dr  if you want to have good breath and a healthy mouth  junk your alcoholic mouthwash  have a healthy diet  floss and brush twice a day ',\n",
              " 'i had my refund split into two different accounts it was deposited into one off those accounts and not the other this is day 3',\n",
              " 'if i remember correctly it costs a lot more than this total for the entire naturalization process there are several fees throughout the fiancee visa process source  i brought over my current wife from china ',\n",
              " 'i reccomend you still have      with your so  and reevaluate at 3 and 6 months ',\n",
              " 'this is neat i am in a ap course right now and have a midterm coming up ',\n",
              " 'i do nt know why this works  but it s super handy when i forget whether or not i locked my car ',\n",
              " 'the range extends even more if you press it against your      ',\n",
              " 'i ve tried this before with different cars i think it s nonsense meant to make you look ridiculous ',\n",
              " 'good to know  never heard of them  my tip  if you re a university student using ufileca  make sure you re getting the service for free  the canadian federation of students has a deal with them ',\n",
              " 'it will be very easy for us ',\n",
              " 'is this for when i ve lost my cell phone and ca nt say ok google 889 divided by 7 ',\n",
              " 'i worked with someone who had worked in ups management he told some horror tales on how they treated packages  like conveyor to unload plane was nt immediately available so they just dropped the packages onto the tarmac or a truck was in a traffic accident related fire and they grabbed wet charred boxes and sent them on their way   his advice was  if it arrived damaged but no one signed for it  report it as not delivered they pay non delivery claims but it is a      to collect damage claims ',\n",
              " 'the other day i ordered a package from ups  but forgot to put my apartment number after two failed delivery attempts  the package ended up getting delivered at my parent s house across town  my dad and i share the same name   they had no way of knowing my parents  address  which means they had to look up my name in a directory and find the actual address  it was super bizarre and probably could have backfired  but in my specific situation it was a very good thing ',\n",
              " 'is there a system that would sync with someone you trust  as in backup to another computer on a different network that way if you wanted to you could bring your computer to the other person s house and it would sync over lan ',\n",
              " 'my work computer was recently hacked i got a message stating my files were encrypted and i had to pay to get them back the ransom would double from  700 to  1400 if i did nt pay within one week the ransom had to be paid in bitcoin on the dark web my company decided we would not pay i lost everything i ca nt put into words the feeling of losing years worth of files i almost put in my 2 weeks notice now i back up on multiple hds and networks ',\n",
              " 'just filed with my state and federal with hamp  r i m happy that it was free and sad because i make well below the  62k cut off ',\n",
              " 'my two cents guys is to use google drive for all your important files and anything that can be easily downloaded again such as movies and games stay on a drive i have no need to back anything up if everything is on github and google drive ',\n",
              " 'i m lazy and i just back all my text data up by uploading it to my email account it s not terribly secure  but it s not terribly sensitive data  either larger data i put on an external hard drive i should probably put it in the cloud  too  but i m too cheap and it would take foreeeeever to upload it s probably hundreds of gigs of data at this point that said  it s mostly old projects  so if they get deleted it s not like it s going to      me over  just be a bit of a letdown ',\n",
              " 'i make more than that and filed my federal and state taxes for free with turbo tax absolute zero just click no every time they ask you to upgrade for  30  they ask about 5 times throughout the process ',\n",
              " 'a few tragic mishaps with video game saves taught be the importance of data security early on in life i still know people who do nt back up their data you have to really get burned in order to take it seriously ',\n",
              " '2 days ago i bought a 2 tb external drive to back up another one  vacation pictures and videos   when partitioning the new drive  i accidently partitioned the original one because its the same brand fml i bought recover software that actually worked though so thats nice ',\n",
              " 'why not just use google drive ',\n",
              " 'everyone should back up we got burgled a month after our dauther was born and lost all our computers  along with extremely precious photos i now backup religiouslywell  automatically synced backups ',\n",
              " 'i have a regular job and a small business i just started i really did nt want to go and pay hamp  r block for a small buisness that has nt even turned a profit yet this is awesome ',\n",
              " 'try wwwfreetaxusacom instead state is only  13 ',\n",
              " 'my dad has hearing problems and that entitles him to a disabled railcard in the uk  which gives him a discount he once had a train conductor that was checking his ticket demand to know his disability  even after my dad presented his railcard which you need to provide medical evidence to get he said that he could nt see my dad s disability  probably because my dad never wears his hearing aid   my dad told him to       that he was nt going to discuss his health with the conductor and later made a formal complaint ',\n",
              " 'similarly  not everyone with limited mobility has a disability placard i see elderly people circling parking lots for a close spot all the time i try to park toward the back and be thankful that my body is up for the walk ',\n",
              " 'in high school my little brother destroyed both his ankles in a fall i would park in the handicap spot and walk in to get him i was frequently yelled at by students and teachers i was even pulled into the principal s office  who instead of calling my brother in to verify my story called my mother to tell her i stole a handicap placard ',\n",
              " 'my good friend is 28 and has fibro  ra  thyroid cancer  and some autoimmune thing that makes her joints really loose  so she uses a wheelchair she ca nt get the wheelchair in and out of her car by herself  so anytime she wants to go somewhere she has to find someone to take her so i take her lots of places every single time i pull into a handicapped spot and get out i get a      look from someone it usually only lasts till the moment i reach into my backseat and start pulling out a      wheelchair for my passenger  but it still feels bad i m just trying to help this girl who is a really awesome person and deserves to be able to run her errands on her own as much as she wants i do nt deserve the looks  do nt be a judgy mcjudgerson ',\n",
              " 'my mom has been holding off on getting a placard for several years now because she s afraid of what people would think it s sad that some people think that way and even sadder that she lets it prevent her from getting one ',\n",
              " '     you  i was scared enough as it was ',\n",
              " 'i ve made this mistake before i ve also asked a       not pregnant  lady if she was having a boy or a girl  so i do nt say anything anymore and just live with the shame ',\n",
              " 'also people who do not have a disability may be drivingparking the car to pick up someone who does have one ',\n",
              " 'looks like i got it  1  internal to computer  modified constantly  2  backed up to google drive  modified constantly  3  in second hard drive in closet refreshed quarterly 4  in 3rd hdd on firesafe refreshed on my birthday',\n",
              " '33 year old recovering from crushed spine and 2 surgeries i look 100  normal constant pain in my back and side  and walking more than a few hundred feet feels like fire under my skin  i get constant looks from people  and i know they are thinking   this guy just took his grandma s tag to park close to the door   at first i was angered  then felt guilty  now i just keep on walking  but thank you for posting this to make others aware  maybe i ll get a few less looks now ',\n",
              " 'adding to this  i was at the mall with my grandmother  who is disabled she almost took a fall when we were walking around  and it scared me so i took the keys off her and told her to wait on the bench so i could pull the car around  she had parked in a disabled spot  and as i entered the driver s seat and started the car  by myself  i did get a lot of dirty looks i m not disabled  but i am with someone who is and she needed extra help that afternoon  never judge ',\n",
              " 'invisible disability here  i had a      at 34 look at me  ca nt see a      thing wrong  what s actually wrong  foot is numb  hip constantly aches  poor balance  i fall down once a week on average  arm is numb  ca nt use one hand  eyes       double vision  deaf in one ear   but you look fine   yeah  i look fine until you watch me try to walk more than 2 minutes',\n",
              " 'i ve a friend who has to use a wheelchair because if she stands for more than a few min at a time she ll pass out  some circulator issue i believe   so she ll wheel into shops  then stand to get things off shelves when need be the looks  comments she gets are quite something ',\n",
              " 'you should probably specify which country this is for ',\n",
              " 'my father has ms but still likes to ride his harley when he s feeling he has a good balance that day  a lady literally waited out in the parking lot next to it to talk to this  abuser  of the handicap system  he walks out and asks if he can help her  and she replies  i just wanted to know who had this bike in the handicap parking lot and how disabled they were  he replied  well  i guess it s none of your      business  she promptly      off ',\n",
              " 'should nt they be legally required to make their disability more obvious  for example by shooting out their achilles tendon or putting their elbow in a blender ',\n",
              " 'careful with dropbox accidentally delete your data off of it and it ll sync the deletion to all other devices note you ll have 30 days to recover deleted data  available through the web interface  before it fully vanishes ',\n",
              " 'ysk that some people are dicks and will talk      about anyone for the slightest reason ',\n",
              " 'it works with me also ',\n",
              " 'i make 67 and used hr block for free bro',\n",
              " 'my dad has a disabled plate and most people would think he gets around just fine what they do nt know is how bad he fractured his left ankle 5 years ago it still has nt healed right ',\n",
              " 'yeah it works really well i hold it under my head on my neck i think your head amplifies the signal somehow allowing it to go further it s a really cool lifehack ',\n",
              " 'i know 50  of reddit is from the us but that s still just a coin toss ',\n",
              " 'i often pick up my niece from school or take her out for fun days i once got out of the car  after having parked in the handicap spot  to get yelled at by this soccer mom saying i was horrible for using a disabled spot when there was obviously nothing wrong with me i ignored her went to the trunk to grab my nieces stuff and got my 4 year old niece out of the car set up her walker and helped her put on her prosthetic leg we then walked past the woman and i looked at her and said  you were saying   she did nt say a word i hope she felt like      ',\n",
              " 'i agree with you and personally do nt care either way because it s just a parking spot but there is a market in my city for the placards it s known that they are commonly for sale or easily accessible in various ways i did nt think about it much until i moved to this area and learned about it it s hard sometimes not to wonder about some people who use the placards now ',\n",
              " 'similarly  people tend to think those who use wheelchairs are paralyzed and if they stand up from their chair that they re just faking it sometimes people can stand for short periods of time and still need a wheelchair  come on ',\n",
              " 'my gf did her taxes on 122 and it says they sent the refund already i did mine on 123 and it still has nt been approved ',\n",
              " 'nice try  ted cruz ',\n",
              " 'will this shorten the longterm use of my flamethrower ',\n",
              " 'is this why you should mix a little oil with the gas for a weedeater  i ve never known why  i just know my dad told me to do that ',\n",
              " 'i ve never really thought about this before  but i did some quick research note  this is my understanding it s paraphrased and probably wrong on some points    pros     ethanol has fewer particulates when burned than gasoline   when you have a higher percentage of ethanol in your gasoline  water is less likely to separate in your tank    cons     ethanol attracts water  and thus when your fuel is added to your car it will likely have a higher concentration of water  miniscule amounts  than pure gasoline would if you re running your car  this water runs through the system with the gasoline and ethanol if you let your car sit for too long or in the wrong conditions  the water  ethanol  and gasoline will separate this is essentially why you put fuel stabilizer in the tank before storing theoretically  the shelflife of e10  90  gas  10  ethanol  is 3 months   alcohol may oxidize in your tank  creating deposits that may or may not clog andor damage your car s fuel system  engine  or exhaust   ethanol has a lower energy density than gasoline one gallon of ethanol will have 66  of the energy of a gallon of pure gasoline   ethanol in the us is primarily cornbased less corn for cows  more corn for cars  edit  formatting and sources  popular mechanics on increasing the percentage of ethanol in gasoline   http  wwwpopularmechanicscomcarshybridelectrica6244e15gasolinedamageengine   wikipedia link  of course   https  enwikipediaorgwikiethanol_fuel   road and track article   http  wwwroadandtrackcomcarculturea17240howdoesethanolimpactfuelefficiency ',\n",
              " 'for a second i read  flamethrower  instead of  snowthrower  and i was like  sigh  americans ',\n",
              " 'why are we still so dependent on the corn subsidize ',\n",
              " 'why does the government promote the use of ethanol  yesterday s big event has a lot to do with it ',\n",
              " 'star tron for the double win  1 stabilizes ethanol laced fuel 2 awesome product name  http  wwwstarbritecomenstartron',\n",
              " 'if you live near a small airport  stop by with a container most will let you use their self serve pump for aviation gasoline  referred to as av gas or 100 low lead  100ll   no ethanol my father uses this in our jetski and our lawnmower ',\n",
              " 'i m glad i uninstalled earlier this week  then  my issue was that it never let me reregister for a free year  always got the same error probably for the best ',\n",
              " 'in a nutshell  when a researcher claims their research is statistically significant  all this means is that the researcher has used mathematics to determine with confidence that the independent variable s effect on the dependent variable is  not due to chance   for example  a researcher could say   i think that cellphones cause cancer  they could then do a variety of expensive research methods to come up with the  hypothetical  conclusion that  yes  they re confident that  all variables accounted for  cell phones can cause cancer at a 001  higher rate in comparison to the population that does nt use them that does nt sound like much  does it  like maybe it s so low a percent that it does nt really even have practical use  well guess what  that result is still  statistically significant   simply because the researchers have determined that the result was not due to chance  now this is different from what statistically meaningfulpractically significant means in regards to research for something to be statistically meaningful or practically significant  the results not only must be not due to chance  but they must be large or meaningful enough that we should  actually care   unfortunately  this line can something be fairly arbitrary  take the previous example how much of an increased cancer risk from cell phone use are we willing to tolerate before it makes any difference at all in our behaviour  001   01   1   5   as high as 10   or higher  the line is not clear cut when do the results become statistically meaningful  unfortunately  this is probably going to be unique to each specific research topic  the real takeaway from this is  though  that just because you see research claiming its results are statistically significant  does nt automatically mean you should care the real question comes from whether or not the results are dramatic enough that they make a practical difference in how we interact with or interpret the topic of research and  that  is what being statistically meaningful is all about  clarification edit  so  looks like some people out there are upset at the term statistically meaningful this is the term that i was taught to use  so it s the term i think of when i m assessing research if you d like to use the term practically significant or substantial significance or effect size or clinical significance  if you re in medicine   great  or if you have another term  that s fine too  the main things i m trying to help people take away from this is that  1  the term statistically significant  might not mean what you think it does  especially if you re not familiar with reading and interpreting research  and  2  you should try to think critically about research and how it s advertised  as to the example i used  i know it s limited i purposefully tried not to include a bunch of statistical jargon so that it would be easier for the average person to understand i understand in doing that there is a tradeoff between ease of understanding and realismaccuracy there are  of course  a multitude of other elements to consider when assessing research  but i tried to use an example that narrowly illustrated the one i chose  also  thank you for gold  stranger that was very kind  ',\n",
              " 'i m in the 5th year of my phd  i know  i know  and i have never come across the term statistically meaningful  edit for clarity on op i think the point is that  if i have a large enough sample  any effect will be statistically significant  ie the study is overpowered   whether that group difference is  meanngful   eg the treatment had a meaningful effect on someone s health  is another story entirely ',\n",
              " 'who is upvoting this and giving it gold  you re clearly just pulling this      out of your       nobody says statistically meaningful in science ',\n",
              " 'what op means is  look at the effect size  some studies do nt publish effect size  which means you should be extra critical when evaluating that study ',\n",
              " 'never have i heard of the term  statistically meaningful  in any research  this seems like a      distinction op is trying to make so that you can look at research  see that it does nt conform to your worldview  then dismiss it because you think it is nt  statistically meaningful  ',\n",
              " 'one of the first things my statistics instructor told the class is whenever you see someone trying to prove anything by using statistics  they are lying he then went on to explain that statistics can indicate something  but should not be used as an absolute proof by itself and that when used in media  tvnewspaper etc  they are almost always using statistics wrong he then took the same set of statistical data and showed how it could be used to prove  both sides of an argument ',\n",
              " 'this is a particularly useful ysk thank you for helping fight misinformation have some gold ',\n",
              " 'so basically half of the stats used on reddit are junk essentially be use they use stastical significance i ve been trying to tell people this exact thing  but i m a horticulturist and not a statistician  and couldnt fully explain it thank you  this is well written ',\n",
              " 'https  supportgooglecomwebsearchanswer29508  hl  en gt  when you do a google search  you can filter your results to find images  videos  or text that you have permission to use to do this  you  ll use an advanced search filter called  usage rights  that lets you know when you can use  share  or modify something you find online  this might have been obvious to some but i did nt know that google had such a filter ',\n",
              " 'it sounds arbitrary to me to say something statistically significant should be ignored or disregarded as not enough if it s indicative of something happening beyond chance  then i think it begs further investigation because it would imply some kind of phenomenon is happening  even if it s very weak  edit  i guess it really depends on the study the redgreen apple example is a good point i was thinking more for when the study is trying to determine if a given phenomenon even exists or not at that point  anything significant is interesting to me because it s  is it even a real thing or not   but if we re comparing two things and there s technically a difference but it s so small no one would care  then i see the point ',\n",
              " 'yeah  this filter really ca nt be trusted i would not rely on it for any commercial work   how not to use google image usage rights filter   http  wwwstockphotosecretscomnewsphotobuyershownotusegoogleimageusagerightsfilterhtml ',\n",
              " 'helpful  i work in graphic design  typically we use a stock photo vendor so we know it s safe  but photos are often   20 for a high res copy being able to safely use google images would be really convenient i will check it out ',\n",
              " 'gt  when asked about your packaging  read your guidelines word for word  i used to work for an independent ups pack and ship store this is the single most important piece of advice  but make sure that you end your sentence with  per the packaging guidelines that you provide  i never had a claim denied ',\n",
              " '  general rule of thumb    you can not divide by 0',\n",
              " 'great post  lots of great info  thank you',\n",
              " 'you can as well install all your applications through  ninite   https  ninitecom   you can download a batch of applications for quick installation without going through each installer and unchecking everything ',\n",
              " 'cold sores       ',\n",
              " '  obvious    1  always  2  if the number is even  5  if the number ends in 5 or 0  10  if the numbers ends in 0    less obvious    3  add all of the digits in the number if the result is divisible by 3  then so is the original number  note that this rule can be repeated with the result if you still do nt know  4  if the last 2 digits of the number are divisible by 4  then so is the entire number if you do nt know then halve the last 2 digits twice if you still have a whole number then it is divisible by 4  6  if the number passes the 2  rule and the 3  rule  then  yes  8  if the last 3 digits of the number are divisible by 8  then so is the entire number if you do nt know then halve the last 3 digits three times if you still have a whole number then it is divisible by 8  9  add all of the digits in the number if the result is divisible by 9  then so is the original number  note that this rule can be repeated with the result if you still do nt know    obscure    7  remove the last digit from the number take the number formed by the remaining digits and subtract by 2x the removed digit if the result is divisible by 7  then so is the original number  example    889       88       9   x2   8818  70  gt  889 is divisible by 7 ',\n",
              " 'wowi ve had premium for a year now and did nt know that i assumed it automatically went to  high quality  when you got premium i just changed my settings to  extreme  quality',\n",
              " 'anybody else have success with carmex  i was surprised to not see it mentioned by anyone carmex was originally created as  actual medicine  for cold sores  and if i use it the same way abreva is used i get the exact same results ',\n",
              " 'wife gave me a bath with essential oils  eucalyptus  to soothe my muscle ache from lifting after 30 seconds into the bath  my balls felt like ice cubes never try this ',\n",
              " 'it s the same reason that you do nt get head after they have brushed their teeth  it s like getting      by an ice cube ',\n",
              " 'yeah the thing is i have nt showered yet and have 1 hour before the next thing on my schedule so i could take a bath instead and i m pretty sure i have some eucalyptus essential oils  so  brb',\n",
              " 'so many comments against using mint oil i use a mild mint oil to      with mix a few drops with lotion and go to town your shaft will tingle for balls  use less than a few drops very little now your balls got this warmtingle thing that feels similar to      as you are approaching climate it makes your      feel about 4 times bigger  and you ll      harder than you ever thought possible ',\n",
              " 'the heat of a bath can make you infertile for a time that s one reason your testicles are  outside  your body  to keep them cool ',\n",
              " 'honestly one of the most useful ysks this sub has ever had',\n",
              " 'ysk to never try jacking off with icy hot justtrust me on this one ',\n",
              " 'this is your phone number and you re lonely right ',\n",
              " 'i thought i was having issues with cold cores and i went untested but got prescription antivirals from my doc  which did nothing so i got tested instead i was suffering from bacterial infections which mimic d cold sores  i d get holes on the bottom of my tongue which are incredibly painful and my tongue swells up and i cant talk for about a week  anyways it was toothpaste  once i switched to a milder tooth paste i stopped having problems  this stuff has been a miracle  http  wwwsquiglecomsquiglehtml',\n",
              " 'it seems it s almost easier just to learn how to math in your head than to try to remember some of these ',\n",
              " 'i ve have one now  have been applying abreva frequently for four days  had pain and swelling yesterday and the lesion developed today i wonder whether i can buy lysine pills over the counter at the pharmacy  or maybe the health food store would carry them next time i see my doctor i will ask about acyclovir a tiny tube of abreva costs  23 plus tax here  but i still buy it because it seems to reduce the symptoms somewhat ',\n",
              " 'you should try using the red rembrandt toothpaste as well also works for canker sores i ve never had an outbreak while using it  ',\n",
              " 'as someone who gets cold sores pretty horribly  thought not as bad as my sister who would have them take over her entire upper lip  all the way into her nose  this      is      magic  tl  dr this      is      magic ',\n",
              " 'my otc regimen for a cold sore has been   wake up with one        brush my teeth really  really well  and use mouthwash  purposely dribbling a little bit of the mouthwash so it gets on the cold sore   wet the tip of a qtip in apple cider vinegar and place on cold sore  wince a little because it stings a bit   after a minute or two of the vinegar  to make sure it is nice and soaked in   i apply abreva  for me  this has cut the life of my cold sores drastically it used to take me a solid two weeks to get one all healed up completely now  it takes me about 45 days on average sometimes  if i do catch a cold sore at the  just a tingle  stage  this regimen will prevent it from even developing ',\n",
              " 'it s very interesting and educational but why we should know this ',\n",
              " 'is there a rule for 12 ',\n",
              " ' apple emit a gas that causes other fruits around them to ripen the riper the apple  the more gas released a single overripe apple can cause a chain reaction of overripening thus spoiling the whole bunch   http  mentalflosscomarticle31666doesonebadapplereallyspoilwholebunch  so pick out those bad apples from your bunches  especially if you are cellaring them for the winter in your cold  snow covered home state ',\n",
              " 'doing this ',\n",
              " 'you know what  i kind of hate      like this why not just learn how division works  then it s obvious for example  889  just say  700  140  840  889  840  49  49  7  7 so 889 is divisible by 7 all these rules just seem pointless to me  it should just be easy to work out from how math works ',\n",
              " 'just eucalyptus  how much oil  you know  so i can avoid it better ',\n",
              " ' use bengay   http  staticfjcdncomlargepictures069a069a03_132860jpg ',\n",
              " 'http  waterfordwhispersnewscom20160203leakedfilesclaimguantanamoinmateswereforcedtowashgenitalswithmintyshowergel',\n",
              " 'https  wwwyoutubecomwatch  v  d9cqjfwesv0',\n",
              " 'brb after reposting this to rapple',\n",
              " 'someone cross post this to the hilary wall street thread      apples',\n",
              " 'this sub pretty consistently provides me w      i already know ',\n",
              " 'i used to work for an importerexporter that focused on bananas he showed me a sea of 18 wheeler climate controlled trucks just sitting there full of green bananas for many many weeks when he wants to send them to market  he d open the door  throw in one ripe banana  and within 2 days  they were off to market ready to be consumed ',\n",
              " 'all fruit give off this gas ',\n",
              " 'checkmate  us law enforment ',\n",
              " 'just learned this by accidentally using the volume rock totally going to be useful in the future ',\n",
              " 'this happens with all fruit they release ethylene gas as they ripen  which causes anything nearby to ripen faster ',\n",
              " 'it s not a  bad apple  it s just a ripe apple ripening fruit does in fact release ethylene but it has literally nothing to do with the fruit quality at all all fruits emit it once they start to ripen  it just speeds up the process so buy your batch regardless but keep them away from each other ',\n",
              " 'k thanks',\n",
              " 'ysk ozone destroy ethylene gas helping keep fruitsveggies fresh longer having a small generator in your cellar or fridge is a great way to keep everything fresh a lot of fruit picked that would ripen quickly in transit is treated with ozone to essentially halt the ripening phase until it reaches its destination ',\n",
              " 'i ca nt find  hack   http  wwwmobygamescomgamedoshack ',\n",
              " 'apples do nt grow in bunches ',\n",
              " 'even f117a stealth fighter ',\n",
              " ' gateway   http  playdosgamesonlinecomgatewayhtml  and  gateway 2  homeworld   http  playdosgamesonlinecomgateway2homeworldhtml  are easily some of the best hard scifi adventure games i have ever played ',\n",
              " 'jill of the jungle  halloween harry  keen commander   thank you so much',\n",
              " 'cosmo s cosmic adventure here we come ',\n",
              " 'so  too many cooks could spoil the broth ',\n",
              " 'gt  the lost vikings       thank you have nt played this for almost 20 years ',\n",
              " 'brilliant  all the old classics ',\n",
              " 'awww yiss 3d dinosaur adventure ',\n",
              " 'i just did it it took less than a minute and i already have 2 extra gb   thanks for the post up44v9n',\n",
              " 'a little bummed that i could nt find shogun  never could beat that   but i found this for you  a reddit  http  playdosgamesonlinecomsafeopeningsimulatorhtml',\n",
              " 'they can also be used to make ethanol  bread tastes better  though ',\n",
              " 'good check up  i found one thing that surprised me  does anyone know what  chromeover  is and why it had full access to my google account  i just turned it off  googling  chromeover  shed no light ',\n",
              " 'why should i know this',\n",
              " 'the mood cast not nominated for comedy  or mysterious universe for anything else  pshh',\n",
              " 'because so many people resort to this for making money  it s only logical that prices are going to drop  see this in lots of different areas  app development gt  have a good idea  a simple good selling game  next day 10 clones almost same name',\n",
              " 'how much is a  part  ',\n",
              " '003 cents  or 3 cents   3 cents is 003 dollars assuming this is a typo ',\n",
              " 'i ve met a bunch of dudes who sell scrap they were all on or trying to get more            or coke ',\n",
              " 'i use a wheelchair  but can stand up and walk  apparently normally  between the car and the curb i still get yelled at on a daily basis ',\n",
              " 'thanks for the info ',\n",
              " '15gb freee gt  2gb last time i did this gt  2gb today  19gb total ',\n",
              " 'are nt all nutrients seeped out of the grains ',\n",
              " 'this is kind of misleading for many programs advanced or custom installations really should be limited to people who know what they re doing they may install in the wrong location or not install a component that they need  addons are often a separate piece that they will try to sneak in somewhere in between all those next buttons the most important thing is to read what you re actually doing ',\n",
              " 'in my experience  anything that you could  get for free  and make some use out of starts getting charged for really quick  usually by the time you hear about it ',\n",
              " 'i just ate at tommyknockers in co and they served tortilla chips made from their own spent grains meh at least it s a form of recycling i do nt eat wheat so i could eat them ',\n",
              " 'you should also know that hops are poisonous to dogs it s not typical to have hops in the mash but people do some odd stuff these days also black and bittering malts make the spent grain a little unpleasant once baked ',\n",
              " 'have spent grain dough rising as i type this ',\n",
              " 'yeah but i m lazy and using fluoride seems a lot easier ',\n",
              " 'are you really surprised  the state department has to do their best to prevent false marriages and immigration is very tightly controlled into the us  when you re doing it legitimately anyway   the price you re paying covers all the hours that will be spent investigating you and your wife to ensure you are nt just trying to scam the system ',\n",
              " 'i propose a slight revision to the original post  ysk to take advice from your dr andor conclusive  evidence based research  not business insider  to prevent the common cold',\n",
              " 'just picked up 800 lbs of we mash this evening  got a free beer for being the only farmer to show up and pick up sweet ',\n",
              " 'obligatory ib grad shoutout',\n",
              " 'lol proprietary software',\n",
              " 'for example  gap outlet is not a gap store went to one near me and all the denim pants had the labels and everything written in french after a few wears these pants are no where near the same as gap denim already worn  stretched out  and stitching coming undone terrible ',\n",
              " 'how come more people do nt know about this  is there a catch ',\n",
              " 'best advice ever i ve been using them for ages now never going back anywhere ',\n",
              " 'it s been a bit since i took ap tests  but in what situation would you know what questions you got right  but not your score  or is this just for minmaxing beforehand ',\n",
              " 'viewing symbolab on a web browser  nonmobile  will give you the same results for free as well ',\n",
              " 'so is this just h and r block ',\n",
              " 'i take my mum shopping every weekend since my dad passed away  i use the disabled spot since she has a blue badge the amount of people who have given me a dirty look for using the spot yes i am allowed in this spot  when picking up a disabled passenger i also had someone spit on my car once  they were a bit surprised when i jumped out and chased them down the street ',\n",
              " 'my sister has ehlers danlos it s a disability in which she is basically too flexible is how she describes it at any given time her hip will pop out and she ll fall over or a knee will dislocated some days she perfectly normal and can do anything i can  but other days she is literally falling apart she s gotten stares and rude comments while using her handicap plaque on her good days just remember folks  like op said  you never know ',\n",
              " 'my father is a disabled vet and only uses a cane except sometimes for trips to the gas station and such but he has had the same problem  but he is not nice about it  last time someone said something to him about it he responded by telling the person that  if they really wanted the placard he did nt have a problem crippling them ',\n",
              " 'my father has had this problem for years he has an autoimmune disease that has basically caused his heart to calcify itself  among other problems  including most of his lungs being removed  despite being an otherwise healthy looking and acting person he was told by his doctors that he would nt see my 12th birthday  and that was 20 years ago  so he s beat all the odds but  because he looks and acts relatively healthy  he s gotten      for using a handicapped plaque quite often  while it is true that handicapped plaques are often abused and misused  do nt always be so quick to judge ',\n",
              " 'same with disabled toilets  they re not made for just people in wheelchairs i ve been given dirty looks before for using a disabled toilet even though i do nt have a      bowel and go for a      upwards of 10 times a day ',\n",
              " 'another thing i ve was guilty of in the past was being irrationally angry when i see a huge lifted f150 or super nice corvette parked in a handicapped spot i would always complain under my breath  ca nt be that disabled if he s able to get in and out of that   but then one day i realized that just because you may be disabled somehow does nt mean you wo nt sacrifice some comfort in order to have a      truck or sports car  everyone s allowed to have nice      if they want it ',\n",
              " 'oh my  this so much my gf has an prosthetic limb  above knee amputee she is quite in form so she walks really nice  people usually do nt notice if she s not wearing a skirt or something the      looks we sometimes get she feels      guilty sometimes  which is just crazy like  people expect all handicaped people to be wheelchair bound',\n",
              " 'or they could be picking up someone that s disable  if they have a legal sign  assume they got it legally ',\n",
              " 'i do nt feel i need to know a person s condition it s not my place to judge  so long as they have the placard  it s no business of mine to comment and even if they do nt  it still is nt my job to pass judgment since i m not a cop or a parking inspector ',\n",
              " 'it s not the disability it s the assholes i went to a coffee shop  put up my card  put my chair together and good to go  not quite  an old lady  helping an older lady  started going over me about how rude i was too take the spot  her friend who had a placard needed it more because she was weak and could nt handle much walking  i was on a first date so i did nt want to make a nasty impression but she then said  just get him to push you if you ca nt do it yourself   i then decided to      it  i told her that actually  this is our first date  and unlike some people s narrow minded views you can be independent in a chair because they could nt walk away in shame fast  i told them how if i parked in a normal spot  i would either not be able to open my car door wide enough to put my wheelchair together myself or because i m in a normal  not extra wide spot i would not be able to get inside my car  i might have mentioned something about ignorant assumptions and that yah my legs do nt work but my mouth does so be careful who you belittle it has a fuzzy red tinge to the memory  tl  dr  i lost it on a little old lady who did nt know what she was getting into by berating me i have a smart      mouth ',\n",
              " 'recent surgery  brain tumor  cancer treatment patients  there are a lot out there  thank you for this post ',\n",
              " 'i run into this a lot even though i walk with my cane  people notice i m relatively young  late 20s  and assume i should nt be using handicapped tags it s ridiculous i ve had senior citizens tell me  ya know  people need those spaces   yeah  i knowi m one of those people ',\n",
              " 'i only talk      if people do nt have the tag then it s fair game ',\n",
              " ' you do nt look disabled  what s your disability    tourette s  now      ',\n",
              " 'after googling the issue   actually  a long time ago mythbusters tested out putting your keyless entry to underneath your chin to extend the range it works because your voice box is a natural amplifier i do nt think it give you more than 20 extra feet ',\n",
              " 'thank you for this  i m 32 and recovering from guillainbarre syndrome i ve gone through a wheelchair  a walker  and a cane and rarely use the cane any more  i guess it s not completely invisible because i still walk a little funky plus getting in and out of the car makes it a bit obvious as well  my gbs diagnosis was less than a year ago and i was always healthy and physically able until then it s definitely an eyeopening experience to see everyday things from that perspective ',\n",
              " 'ysk that if you have to audacity to complain about not getting to park 20 steps closer to the door to walmart because you suspect that someone might not be disabled  you might be a flaming      ',\n",
              " 'in other news the sky is blue are people actually this      that they need to be told that you dont have to be missing both arms and a leg to get a disability card ',\n",
              " 'i d give my number to somebody unknown because ',\n",
              " 'autotech here  i hold it against my chin  saves me so much time walking through a basement of over 100 cars ',\n",
              " 'you can also place it against the center of your chest  and facing towards your car when you press the button  basically making your chest cavity into a resonant antenna ',\n",
              " 'in addition  just because an overweight person is using a mobility scooter at a store  it does not mean they are  lazy  my aunt had polio as a child and cerebral palsy her whole life  and we get mean looks in the store because she is obese  even though she truly can not walk ',\n",
              " 'the problem with gas is the short shelf life when it degrades it creates a gummy mess that will clog up all the tiny holes of the carb and give you problems just empty out your tank if you are nt going to use the fuel in a month and do nt put bad gas that has been sitting around for months back in to fill it up  most 2stroke engines ask for 89 octane and a quality 2stroke oil will come with a fuel stabilizer that will extend the shelf life of the gas while running pure gas is great if you can get it  it s often hard to find and not 100  necessary  and you should never run a 2stroke engine dry what do you think is lubing that piston ',\n",
              " 'thanks  big oil  did nt know you had a reddit account how s the election going ',\n",
              " 'am i doing something wrong here with the  7 rule or is this the exception   number    126      12     6  2   0  126   7  18  i tried 7  18 randomly amp  worked my way backwards',\n",
              " 'this is an awesome ysk thank you ',\n",
              " 'you should also know that there are 8 bits to a byte so  25 divided by 8 is 3125 ',\n",
              " 'this seems like the place to ask  i watch an american podcast and i often hear them talking about how some of them have 300 or 400 down while others have gigabyte  what would that be that they re measuring in  for example i get between 35 and 45mbps down here in the uk  are they really getting at least 10x better speeds ',\n",
              " 'turning it off did nt work until the next reboot inserted itself into gmail using web client  uninstalled and said goodbye ',\n",
              " 'you should post this on sandersforpresident or politics',\n",
              " 'worth it        thank you very much for adding two more gigs to my storage ',\n",
              " 'thanks  op ',\n",
              " 'i ve actually started to trust nvidia drivers  i have nt seen junk demos attached for a couple years  but i still check because i get the feeling they are going to wait for me to get lazy and then throw them back in ',\n",
              " 'i work at a call center i assure you they know what theyre doing ',\n",
              " 'instead of spending hours in      support websites just search on http  justdeleteme edit  wauw this surely blew up glad i could help ',\n",
              " 'i wish lastpass would add these links as an option in their security challenge if i have nt visited that site in two years  i would rather just delete it ',\n",
              " '1 mm',\n",
              " 'animal crossing  you can check out any time you want but you can never leave ',\n",
              " ' does it work on aol    https  wwwyoutubecomwatch  v  edyhbmvuia0 ',\n",
              " 'sorry  what sample size was used  with which near 0 variance  in order to show a 001  increase in risk of cancer   i agree with your point  i think the word significance is confusing for those not in the sciences  and quite a few in the sciences   however i think the point is exaggerated here beyond the truth statistical significance does mean something  it just does nt mean what most people expect ',\n",
              " 'your therapist should have 2 sets of notes  a set of the session and insight  a set of billable services and tests and results  and the second set is normally sent to the insurance',\n",
              " 'this is a very good point just to add to it  what you refer to as  statistically meaningful  is usually called  substantial significance  in the methods literature i m familiar with i think it s a better way to differentiate the two  what is statistically significant may not necessarily be substantially significant ',\n",
              " 'eerily i was just thinking about this half an hour ago before i jumped on reddit and saw this by chance',\n",
              " 'gt  the researcher has used mathematics to determine with confidence that the independent variable s effect on the dependent variable is not due to chance  and gt  the researchers have determined that the result was not due to chance  this is incomplete at best  and misleading test for statistical significance are used with an associated significance level  in most case 5   what this  more or less  means is that there is still a 5  chance that your results are not statistically significant  you can of course use a smaller threshold  but in general people will try to use the smallest possible threshold  and are limited by the size of the effect andor the number of their samples  but you never  ever  determine for sure that your results are not due to chance  there are numerous cases  however  where you can use an absurdly small threshold for your test  which is as close as we get to being sure of something   tl  dr  a significance      meaningless without the associated significance level edit  two nice xkcds illustrating this  https  xkcdcom882 https  xkcdcom1478',\n",
              " 'so violations of doctorpatient confidentiality are legal there  great  never moving there  thanks ',\n",
              " 'okay since some of the comments in here are insanely arrogant and uammicha is really the only one who gets it  the point of this is really exploiting base 10 arithmetic remember the finger trick to keep track of multiples of 9  you can do the same thing with 5 base 6  all these rules come from writing the number out in base 10  applying modular arithmetic by whatever number you want to find a pattern for  set it equal to zero and see what you can figure out this is what that other user laid out nicely so i wont bother with it again it is covered in any first semester elementary number theory class  some fun stuff  the reason why 6 works by just testing 2 and 3 is because 2 and 3 are coprime meaning you cant test for divisibility by 12 by checking 2 and 6 since they are not coprime  1433 is prime  lets come up with a rule  some people also came up with some really inconvenient rules  but these are good in instances when factorization is tough and multiplication is easy  hint  always  http  arxivorgftpmathpapers00010001012pdf',\n",
              " 'awesome  thanks for reminding me of this heard it somewhere  but forgot ',\n",
              " 'that s pretty messed up even a summary would be pretty invasive ',\n",
              " 'you have to eventually sign it to redeem your billion dollars and signing it now alleviates potentially catastrophic madness if your winning ticket is lost or stolen ',\n",
              " 'also these are worth your time since you re now the proud owner of hundreds of millions of dollars   http  wwwforbescomsitesdeborahljacobs2012021110thingstodowhenyouwinthepowerball  rpersonalfinance  they have some great threads about financial windfalls including one last week that had excellent advice about your first few steps after a windfall   edit  please treat that forbes link carefully i highly recommend reading this before going to forbes http  wwwengadgetcom20160108yousayadvertisingisayblockthatmalware  utm_source  farkamp  utm_medium  websiteamp  utm_content  link  ',\n",
              " 'lpt  do nt bother  nobody reading this is going to win if every subscriber to this group purchased a ticket the odds of anyone winning would still be  017 ',\n",
              " ' worth watching for anyone who buys lottery tickets   https  youtube9pknetuhha ',\n",
              " 'general rule for numbers coprime to 10  find the first multiple with a 1 in the rightmost digit make a rule so that  some multiple of number without unit digit    some multiple of unit digit  equals zero  mod your number   that gives a test that always works for divisibility for example  17  3  51 so the check if any number is divisible by 17  multiply the units digit by 5 and subtract from the rest so with 578  57  8  5  17  so it is divisible  13  7  91 since 9  1  4  13  we can use the rule of multiplying units digit by 4  then adding to the rest so with 182  18  4  2  26  which is divisible by 13  so the original one was too ',\n",
              " 'my gf is a therapist she says this is a thing everywhere it s why a lot of therapists do nt accept insurance it is one of the biggest problems plaguing the industry ',\n",
              " 'awesome for the paranoids like meh',\n",
              " 'you can install this software too  http  uncheckycom it will automatically uncheck all potentially unwanted programs during installation it s a godsend for family computers that you defacto support ',\n",
              " 'there is also unrollme  which manages all the subscriptions that are linked to your email account you can easily go through and unsubscribe from junk mail',\n",
              " 'yesyou ve been able to do it for about five years now ',\n",
              " 'why is it that if you search for reddit  and expand the info about how to delete your account  all it says it says is  gt   increase your productivity by over 5 times with this one trick they do nt want you to know about   did that site just go full buzzfeed on me ',\n",
              " 'remindme',\n",
              " 'or just go here https  wwwaccountkillercomen',\n",
              " 'remindme  2 days ',\n",
              " ' warlizardforumcom   http  warlizardforumcom  is not on the list  ',\n",
              " 'i placed my email into the search box  came up with a bunch of      i do nt use any more and do nt even remember using i can delete them through here  that will take me a while ',\n",
              " 'wow this is the same concept ',\n",
              " 'holy      how do they know all the subscriptions that are linked to your email up for an account with different sites ',\n",
              " 'another math secret is multiplying 11s in the bigger numbers  add the first and second number then put the result in the middle  ex 11 x 72  7  2  792 edit  must ve haf a      while typing this ',\n",
              " 'ctrl  f  steam  oh thank     ',\n",
              " 'is there a site that goes through and checks if you have an account with different sites ',\n",
              " 'wow i almost forgot about this project i think i contributed a couple sites to it a few years back it s very easy to add sites iirc  just follow the json file s format  https  githubcomrmlewisukjustdeleteme',\n",
              " 'twitter is marked as easy  thought it s finally time to delete my unreasonably banned account unfortunately the information is incorrect  the provided link lands me on the usual trap page with no possibility to at least remove public information ',\n",
              " 'that s better then straight talk s 5gb  unlimited  plan ',\n",
              " 'why does this work ',\n",
              " 'what kind of insurance ',\n",
              " 'here s an addition to obscure  11  add every other digit in the number  and compare that to the sum for the remaining digits if they are equal or if there s a difference of 11  it s divisible by 11  eg 24678533 2683  19 4753  19 sums are equal  therefore divisible by 11  eg 6178447 6747  24 184  13 difference of 11  therefore divisible by 11  source  my obsession with numbers since the age of  i m not      you  11 ',\n",
              " ...]"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196,
          "referenced_widgets": [
            "718f00e7417e4f1da90767a562c605e2",
            "c2439ead24bc4d3e95b49e621b6c2b43",
            "dbaa582ff84149248120e95c36c344eb",
            "5f6593e5784942eaa431a8f15579be79",
            "1618eede9af54d10a3a23fd6444b563c",
            "820ed13a53e445ebb6b94d7c99567b8e",
            "ef049747a4e642dfaf5ab0811a44113a",
            "2e3bca59c32744408eddf50121def79a",
            "be51c4a94e0e42f687f351146b8626cf",
            "7557d831a2d543b6b41ab0ef09f5f0a4",
            "e7c00645df224d8993932c50fe17bab5"
          ]
        },
        "id": "nR1kxbH5h6yZ",
        "outputId": "12c55ad9-a4d7-4abe-d6ce-eff6e6f3ec18"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-31 23:25:22,761 - BERTopic - Embedding - Transforming documents to embeddings.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "718f00e7417e4f1da90767a562c605e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/49 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-31 23:26:10,978 - BERTopic - Embedding - Completed ✓\n",
            "2025-07-31 23:26:10,979 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
            "2025-07-31 23:26:15,497 - BERTopic - Dimensionality - Completed ✓\n",
            "2025-07-31 23:26:15,498 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
            "2025-07-31 23:26:15,736 - BERTopic - Cluster - Completed ✓\n",
            "2025-07-31 23:26:15,742 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
            "2025-07-31 23:26:15,862 - BERTopic - Representation - Completed ✓\n"
          ]
        }
      ],
      "source": [
        "topics, probs = topic_model.fit_transform(comments)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGiteQxiP4b7"
      },
      "source": [
        "## Model evaluation\n",
        "Checking topics coherence from the trained bert model\n",
        "- first use gensim evaluation metrics to compare , then use simplified pipeline to optimise hyperparams\n",
        "- second use this lib : https://github.com/MIND-Lab/OCTIS for hyper params optimisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBWUHMe5d58Y"
      },
      "outputs": [],
      "source": [
        "from gensim import corpora\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "#method found in : https://github.com/MaartenGr/BERTopic/issues/469\n",
        "def calculate_coherence_metrics(topic_model,comments,topics):\n",
        "  metrics={}\n",
        "  documents = pd.DataFrame({\"comment\": comments,\n",
        "                            \"id\": range(len(comments)),\n",
        "                            \"topic\": topics})\n",
        "  documents_per_topic = documents.groupby(['topic'], as_index=False).agg({'comment': ' '.join})\n",
        "  cleaned_docs = topic_model._preprocess_text(documents_per_topic.comment.values)\n",
        "  vectorizer = topic_model.vectorizer_model\n",
        "  analyzer = vectorizer.build_analyzer()\n",
        "  words = vectorizer.get_feature_names_out()\n",
        "  tokens = [analyzer(doc) for doc in cleaned_docs]\n",
        "  dictionary = corpora.Dictionary(tokens)\n",
        "  corpus = [dictionary.doc2bow(token) for token in tokens]\n",
        "  topic_words = [[words for words, _ in topic_model.get_topic(topic)] for topic in range(len(set(topics))-1)]\n",
        "\n",
        "\n",
        "  metrics[\"u_mass_coherence\"]=CoherenceModel(topics=topic_words,\n",
        "                                 texts=tokens,  corpus=corpus,\n",
        "                                 dictionary=dictionary, coherence='u_mass').get_coherence()\n",
        "  metrics[\"c_v_coherence\"]=CoherenceModel(topics=topic_words,\n",
        "                                 texts=tokens,  corpus=corpus,\n",
        "                                 dictionary=dictionary, coherence='c_v').get_coherence()\n",
        "  metrics[\"c_uci_coherence\"]=CoherenceModel(topics=topic_words,\n",
        "                                 texts=tokens,  corpus=corpus,\n",
        "                                 dictionary=dictionary, coherence='c_uci').get_coherence()\n",
        "  metrics[\"c_npmi_coherence\"]=CoherenceModel(topics=topic_words,\n",
        "                                 texts=tokens,  corpus=corpus,\n",
        "                                 dictionary=dictionary, coherence='c_npmi').get_coherence()\n",
        "  return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8M-7-G9M0gQ0"
      },
      "outputs": [],
      "source": [
        "coherence_metrics=calculate_coherence_metrics(topic_model,comments,topics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzsLNJuGmgSU",
        "outputId": "3d9e6437-94ca-4ddf-933d-2c578be661c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'u_mass_coherence': -0.5423984897540505,\n",
              " 'c_v_coherence': 0.6382336416341199,\n",
              " 'c_uci_coherence': -2.6242854186847,\n",
              " 'c_npmi_coherence': 0.007330986080360313}"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "coherence_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VXyi7RD0IMGV",
        "outputId": "5f567289-124a-455c-dbf0-db8a55cefc82"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"topic_model\",\n  \"rows\": 39,\n  \"fields\": [\n    {\n      \"column\": \"Topic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11,\n        \"min\": -1,\n        \"max\": 37,\n        \"num_unique_values\": 39,\n        \"samples\": [\n          32,\n          35,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 49,\n        \"min\": 11,\n        \"max\": 297,\n        \"num_unique_values\": 30,\n        \"samples\": [\n          14,\n          26,\n          18\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 39,\n        \"samples\": [\n          \"32_account_delete_site_email\",\n          \"35_subreddit_bot_ryoushouldknow_rbreadit\",\n          \"3_doctor_and_to_doctors\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Representation\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Representative_Docs\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-0edea23f-5899-452f-96b6-e1cef8903c4d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic</th>\n",
              "      <th>Count</th>\n",
              "      <th>Name</th>\n",
              "      <th>Representation</th>\n",
              "      <th>Representative_Docs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1</td>\n",
              "      <td>297</td>\n",
              "      <td>-1_you_to_it_this</td>\n",
              "      <td>[you, to, it, this, on, the, in, and, that, for]</td>\n",
              "      <td>[just to be clear  i m not a windows 10 hater ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>133</td>\n",
              "      <td>0_free_tax_taxes_you</td>\n",
              "      <td>[free, tax, taxes, you, for, file, and, pay, t...</td>\n",
              "      <td>[you can file your taxes for free anyway  but ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>94</td>\n",
              "      <td>1_this_thanks_thank_know</td>\n",
              "      <td>[this, thanks, thank, know, great, why, op, no...</td>\n",
              "      <td>[this is      , thanks so much for this , than...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>85</td>\n",
              "      <td>2_she_her_my_spot</td>\n",
              "      <td>[she, her, my, spot, disabled, he, and, parkin...</td>\n",
              "      <td>[i also get looks when i take my disabled moth...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>74</td>\n",
              "      <td>3_doctor_and_to_doctors</td>\n",
              "      <td>[doctor, and, to, doctors, nt, they, had, my, ...</td>\n",
              "      <td>[a 2nd opinion has saved or prolonged a life t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>4</td>\n",
              "      <td>65</td>\n",
              "      <td>4_library_text_card_it</td>\n",
              "      <td>[library, text, card, it, wikipedia, phone, th...</td>\n",
              "      <td>[ kiwix   http  wwwkiwixorgwikimain_page  is a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5</td>\n",
              "      <td>46</td>\n",
              "      <td>5_antivirus_bitdefender_avast_the</td>\n",
              "      <td>[antivirus, bitdefender, avast, the, free, of,...</td>\n",
              "      <td>[  tl  dr if you are running windows and need ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>6</td>\n",
              "      <td>43</td>\n",
              "      <td>6_ethanol_gas_fuel_water</td>\n",
              "      <td>[ethanol, gas, fuel, water, gasoline, engine, ...</td>\n",
              "      <td>[the problem with gas is the short shelf life ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>7</td>\n",
              "      <td>40</td>\n",
              "      <td>7_drive_backup_data_up</td>\n",
              "      <td>[drive, backup, data, up, back, files, my, to,...</td>\n",
              "      <td>[i make sure i have at least two copies of eve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>8</td>\n",
              "      <td>38</td>\n",
              "      <td>8_cold_sore_abreva_sores</td>\n",
              "      <td>[cold, sore, abreva, sores, mouth, the, lip, i...</td>\n",
              "      <td>[i have a sure remedy and guy s and      s it ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>9</td>\n",
              "      <td>37</td>\n",
              "      <td>9_by_number_divisible_11</td>\n",
              "      <td>[by, number, divisible, 11, digits, rule, is, ...</td>\n",
              "      <td>[explanation of the   7   rule  since it took ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>10</td>\n",
              "      <td>33</td>\n",
              "      <td>10_ups_item_box_the</td>\n",
              "      <td>[ups, item, box, the, package, you, it, they, ...</td>\n",
              "      <td>[  3 years working for a ups store   hey guys ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>11</td>\n",
              "      <td>31</td>\n",
              "      <td>11_insurance_notes_records_company</td>\n",
              "      <td>[insurance, notes, records, company, health, t...</td>\n",
              "      <td>[as others have said this is so the insurance ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>12_calls_debt_number_call</td>\n",
              "      <td>[calls, debt, number, call, collectors, phone,...</td>\n",
              "      <td>[i did not know this  when i first got my cell...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>13</td>\n",
              "      <td>27</td>\n",
              "      <td>13_warranty_2011_macbook_cable</td>\n",
              "      <td>[warranty, 2011, macbook, cable, cables, pro, ...</td>\n",
              "      <td>[wait  what    back in august of 2011  i broug...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>14</td>\n",
              "      <td>27</td>\n",
              "      <td>14_2gb_checkup_extra_storage</td>\n",
              "      <td>[2gb, checkup, extra, storage, gb, google, yea...</td>\n",
              "      <td>[remindme  one year  do 2017 google security c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>15</td>\n",
              "      <td>27</td>\n",
              "      <td>15_ysk_this_nonamerican_us</td>\n",
              "      <td>[ysk, this, nonamerican, us, reddit, grammar, ...</td>\n",
              "      <td>[ysk not all reddit users are from the us, if ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>16</td>\n",
              "      <td>27</td>\n",
              "      <td>16_balls_oil_bath_peppermint</td>\n",
              "      <td>[balls, oil, bath, peppermint, eucalyptus, dro...</td>\n",
              "      <td>[wife gave me a bath with essential oils  euca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>17</td>\n",
              "      <td>26</td>\n",
              "      <td>17_cat_cats_orange_collars</td>\n",
              "      <td>[cat, cats, orange, collars, collar, lost, we,...</td>\n",
              "      <td>[our 2 indoor cats have dog collars  we do nt ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>18</td>\n",
              "      <td>25</td>\n",
              "      <td>18_vote_caucus_voting_election</td>\n",
              "      <td>[vote, caucus, voting, election, primaries, po...</td>\n",
              "      <td>[wow  thank you for this  so instead of a  sec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>19</td>\n",
              "      <td>25</td>\n",
              "      <td>19_weather_noaa_http_listen</td>\n",
              "      <td>[weather, noaa, http, listen, for, forecast, i...</td>\n",
              "      <td>[they have an app  too not sure if it shows th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>20</td>\n",
              "      <td>24</td>\n",
              "      <td>20_game_play_played_adventure</td>\n",
              "      <td>[game, play, played, adventure, http, nt, find...</td>\n",
              "      <td>[i looked for dark sun  shattered lands  and i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>21</td>\n",
              "      <td>23</td>\n",
              "      <td>21_grains_spent_grain_dogs</td>\n",
              "      <td>[grains, spent, grain, dogs, chicken, them, fo...</td>\n",
              "      <td>[some breweries use the spent grains themselve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22_speeds_speed_data_deprioritized</td>\n",
              "      <td>[speeds, speed, data, deprioritized, tmobile, ...</td>\n",
              "      <td>[from their own website   to ensure that we  r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>23</td>\n",
              "      <td>21</td>\n",
              "      <td>23_car_seat_seats_correctly</td>\n",
              "      <td>[car, seat, seats, correctly, installed, fire,...</td>\n",
              "      <td>[when i was growing up there were no car seats...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>24</td>\n",
              "      <td>21</td>\n",
              "      <td>24_http_https_wwwyoutubecomwatch_warlizardforu...</td>\n",
              "      <td>[http, https, wwwyoutubecomwatch, warlizardfor...</td>\n",
              "      <td>[ does it work on aol    https  wwwyoutubecomw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>25</td>\n",
              "      <td>21</td>\n",
              "      <td>25_install_installation_custom_you</td>\n",
              "      <td>[install, installation, custom, you, software,...</td>\n",
              "      <td>[this is kind of misleading for many programs ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>26</td>\n",
              "      <td>20</td>\n",
              "      <td>26_usps_mail_rate_flat</td>\n",
              "      <td>[usps, mail, rate, flat, rises, firstclass, pr...</td>\n",
              "      <td>[daily usps shipper here maybe this was nt pub...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>27</td>\n",
              "      <td>20</td>\n",
              "      <td>27_ublock_element_block_origin</td>\n",
              "      <td>[ublock, element, block, origin, adblock, over...</td>\n",
              "      <td>[why would you have to spend time looking thro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>28</td>\n",
              "      <td>19</td>\n",
              "      <td>28_ticket_lottery_sign_windfall</td>\n",
              "      <td>[ticket, lottery, sign, windfall, tickets, win...</td>\n",
              "      <td>[or do nt sign it because that prevents you fr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>29</td>\n",
              "      <td>18</td>\n",
              "      <td>29_banana_apples_fruit_ripen</td>\n",
              "      <td>[banana, apples, fruit, ripen, half, ethylene,...</td>\n",
              "      <td>[ysk ozone destroy ethylene gas helping keep f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>30</td>\n",
              "      <td>17</td>\n",
              "      <td>30_statistically_significant_meaningful_effect</td>\n",
              "      <td>[statistically, significant, meaningful, effec...</td>\n",
              "      <td>[gt  the researcher has used mathematics to de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>31</td>\n",
              "      <td>16</td>\n",
              "      <td>31_math_language_subject_books</td>\n",
              "      <td>[math, language, subject, books, think, of, pr...</td>\n",
              "      <td>[the best part about it is the detail instead ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>32</td>\n",
              "      <td>15</td>\n",
              "      <td>32_account_delete_site_email</td>\n",
              "      <td>[account, delete, site, email, sites, website,...</td>\n",
              "      <td>[i m very      i ca nt delete my pinterest acc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>33</td>\n",
              "      <td>14</td>\n",
              "      <td>33_cmv_zika_microcephaly_was</td>\n",
              "      <td>[cmv, zika, microcephaly, was, pregnant, of, i...</td>\n",
              "      <td>[this would substantiate my claim  that it ca ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>34</td>\n",
              "      <td>14</td>\n",
              "      <td>34_bits_byte_bit_bytes</td>\n",
              "      <td>[bits, byte, bit, bytes, inclusive, mega, 255,...</td>\n",
              "      <td>[b is bit  b is byte those bits also include a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>35</td>\n",
              "      <td>13</td>\n",
              "      <td>35_subreddit_bot_ryoushouldknow_rbreadit</td>\n",
              "      <td>[subreddit, bot, ryoushouldknow, rbreadit, sub...</td>\n",
              "      <td>[i do nt understand this bot it s on every sub...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>36</td>\n",
              "      <td>13</td>\n",
              "      <td>36_snow_emergency_car_routes</td>\n",
              "      <td>[snow, emergency, car, routes, will, exhaust, ...</td>\n",
              "      <td>[i m from canada what s a snow emergency , wit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>37</td>\n",
              "      <td>11</td>\n",
              "      <td>37_snowthrower_snowblower_flamethrower_tiller</td>\n",
              "      <td>[snowthrower, snowblower, flamethrower, tiller...</td>\n",
              "      <td>[what the      is a snowthrower  is that a les...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0edea23f-5899-452f-96b6-e1cef8903c4d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0edea23f-5899-452f-96b6-e1cef8903c4d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0edea23f-5899-452f-96b6-e1cef8903c4d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f775a746-df93-404a-9c3c-249491ce5685\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f775a746-df93-404a-9c3c-249491ce5685')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f775a746-df93-404a-9c3c-249491ce5685 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "    Topic  Count                                               Name  \\\n",
              "0      -1    297                                  -1_you_to_it_this   \n",
              "1       0    133                               0_free_tax_taxes_you   \n",
              "2       1     94                           1_this_thanks_thank_know   \n",
              "3       2     85                                  2_she_her_my_spot   \n",
              "4       3     74                            3_doctor_and_to_doctors   \n",
              "5       4     65                             4_library_text_card_it   \n",
              "6       5     46                  5_antivirus_bitdefender_avast_the   \n",
              "7       6     43                           6_ethanol_gas_fuel_water   \n",
              "8       7     40                             7_drive_backup_data_up   \n",
              "9       8     38                           8_cold_sore_abreva_sores   \n",
              "10      9     37                           9_by_number_divisible_11   \n",
              "11     10     33                                10_ups_item_box_the   \n",
              "12     11     31                 11_insurance_notes_records_company   \n",
              "13     12     30                          12_calls_debt_number_call   \n",
              "14     13     27                     13_warranty_2011_macbook_cable   \n",
              "15     14     27                       14_2gb_checkup_extra_storage   \n",
              "16     15     27                         15_ysk_this_nonamerican_us   \n",
              "17     16     27                       16_balls_oil_bath_peppermint   \n",
              "18     17     26                         17_cat_cats_orange_collars   \n",
              "19     18     25                     18_vote_caucus_voting_election   \n",
              "20     19     25                        19_weather_noaa_http_listen   \n",
              "21     20     24                      20_game_play_played_adventure   \n",
              "22     21     23                         21_grains_spent_grain_dogs   \n",
              "23     22     22                 22_speeds_speed_data_deprioritized   \n",
              "24     23     21                        23_car_seat_seats_correctly   \n",
              "25     24     21  24_http_https_wwwyoutubecomwatch_warlizardforu...   \n",
              "26     25     21                 25_install_installation_custom_you   \n",
              "27     26     20                             26_usps_mail_rate_flat   \n",
              "28     27     20                     27_ublock_element_block_origin   \n",
              "29     28     19                    28_ticket_lottery_sign_windfall   \n",
              "30     29     18                       29_banana_apples_fruit_ripen   \n",
              "31     30     17     30_statistically_significant_meaningful_effect   \n",
              "32     31     16                     31_math_language_subject_books   \n",
              "33     32     15                       32_account_delete_site_email   \n",
              "34     33     14                       33_cmv_zika_microcephaly_was   \n",
              "35     34     14                             34_bits_byte_bit_bytes   \n",
              "36     35     13           35_subreddit_bot_ryoushouldknow_rbreadit   \n",
              "37     36     13                       36_snow_emergency_car_routes   \n",
              "38     37     11      37_snowthrower_snowblower_flamethrower_tiller   \n",
              "\n",
              "                                       Representation  \\\n",
              "0    [you, to, it, this, on, the, in, and, that, for]   \n",
              "1   [free, tax, taxes, you, for, file, and, pay, t...   \n",
              "2   [this, thanks, thank, know, great, why, op, no...   \n",
              "3   [she, her, my, spot, disabled, he, and, parkin...   \n",
              "4   [doctor, and, to, doctors, nt, they, had, my, ...   \n",
              "5   [library, text, card, it, wikipedia, phone, th...   \n",
              "6   [antivirus, bitdefender, avast, the, free, of,...   \n",
              "7   [ethanol, gas, fuel, water, gasoline, engine, ...   \n",
              "8   [drive, backup, data, up, back, files, my, to,...   \n",
              "9   [cold, sore, abreva, sores, mouth, the, lip, i...   \n",
              "10  [by, number, divisible, 11, digits, rule, is, ...   \n",
              "11  [ups, item, box, the, package, you, it, they, ...   \n",
              "12  [insurance, notes, records, company, health, t...   \n",
              "13  [calls, debt, number, call, collectors, phone,...   \n",
              "14  [warranty, 2011, macbook, cable, cables, pro, ...   \n",
              "15  [2gb, checkup, extra, storage, gb, google, yea...   \n",
              "16  [ysk, this, nonamerican, us, reddit, grammar, ...   \n",
              "17  [balls, oil, bath, peppermint, eucalyptus, dro...   \n",
              "18  [cat, cats, orange, collars, collar, lost, we,...   \n",
              "19  [vote, caucus, voting, election, primaries, po...   \n",
              "20  [weather, noaa, http, listen, for, forecast, i...   \n",
              "21  [game, play, played, adventure, http, nt, find...   \n",
              "22  [grains, spent, grain, dogs, chicken, them, fo...   \n",
              "23  [speeds, speed, data, deprioritized, tmobile, ...   \n",
              "24  [car, seat, seats, correctly, installed, fire,...   \n",
              "25  [http, https, wwwyoutubecomwatch, warlizardfor...   \n",
              "26  [install, installation, custom, you, software,...   \n",
              "27  [usps, mail, rate, flat, rises, firstclass, pr...   \n",
              "28  [ublock, element, block, origin, adblock, over...   \n",
              "29  [ticket, lottery, sign, windfall, tickets, win...   \n",
              "30  [banana, apples, fruit, ripen, half, ethylene,...   \n",
              "31  [statistically, significant, meaningful, effec...   \n",
              "32  [math, language, subject, books, think, of, pr...   \n",
              "33  [account, delete, site, email, sites, website,...   \n",
              "34  [cmv, zika, microcephaly, was, pregnant, of, i...   \n",
              "35  [bits, byte, bit, bytes, inclusive, mega, 255,...   \n",
              "36  [subreddit, bot, ryoushouldknow, rbreadit, sub...   \n",
              "37  [snow, emergency, car, routes, will, exhaust, ...   \n",
              "38  [snowthrower, snowblower, flamethrower, tiller...   \n",
              "\n",
              "                                  Representative_Docs  \n",
              "0   [just to be clear  i m not a windows 10 hater ...  \n",
              "1   [you can file your taxes for free anyway  but ...  \n",
              "2   [this is      , thanks so much for this , than...  \n",
              "3   [i also get looks when i take my disabled moth...  \n",
              "4   [a 2nd opinion has saved or prolonged a life t...  \n",
              "5   [ kiwix   http  wwwkiwixorgwikimain_page  is a...  \n",
              "6   [  tl  dr if you are running windows and need ...  \n",
              "7   [the problem with gas is the short shelf life ...  \n",
              "8   [i make sure i have at least two copies of eve...  \n",
              "9   [i have a sure remedy and guy s and      s it ...  \n",
              "10  [explanation of the   7   rule  since it took ...  \n",
              "11  [  3 years working for a ups store   hey guys ...  \n",
              "12  [as others have said this is so the insurance ...  \n",
              "13  [i did not know this  when i first got my cell...  \n",
              "14  [wait  what    back in august of 2011  i broug...  \n",
              "15  [remindme  one year  do 2017 google security c...  \n",
              "16  [ysk not all reddit users are from the us, if ...  \n",
              "17  [wife gave me a bath with essential oils  euca...  \n",
              "18  [our 2 indoor cats have dog collars  we do nt ...  \n",
              "19  [wow  thank you for this  so instead of a  sec...  \n",
              "20  [they have an app  too not sure if it shows th...  \n",
              "21  [i looked for dark sun  shattered lands  and i...  \n",
              "22  [some breweries use the spent grains themselve...  \n",
              "23  [from their own website   to ensure that we  r...  \n",
              "24  [when i was growing up there were no car seats...  \n",
              "25  [ does it work on aol    https  wwwyoutubecomw...  \n",
              "26  [this is kind of misleading for many programs ...  \n",
              "27  [daily usps shipper here maybe this was nt pub...  \n",
              "28  [why would you have to spend time looking thro...  \n",
              "29  [or do nt sign it because that prevents you fr...  \n",
              "30  [ysk ozone destroy ethylene gas helping keep f...  \n",
              "31  [gt  the researcher has used mathematics to de...  \n",
              "32  [the best part about it is the detail instead ...  \n",
              "33  [i m very      i ca nt delete my pinterest acc...  \n",
              "34  [this would substantiate my claim  that it ca ...  \n",
              "35  [b is bit  b is byte those bits also include a...  \n",
              "36  [i do nt understand this bot it s on every sub...  \n",
              "37  [i m from canada what s a snow emergency , wit...  \n",
              "38  [what the      is a snowthrower  is that a les...  "
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "topic_model.get_topic_info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DubUJHE-RQd-"
      },
      "source": [
        "As we can see, the model's default representation output might not be easy to interpret: many articles and non-representative keywords appear in the topics.  Fine-tuning a custom representation model is necessary for our use case to generate more meaningful topics. For this purpose, we use KeyBERT to diversify the topic keywords and enhance their interpretability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHQmBWgcROjX"
      },
      "outputs": [],
      "source": [
        "from bertopic.representation import KeyBERTInspired\n",
        "# Create your representation model\n",
        "representation_model = KeyBERTInspired()\n",
        "\n",
        "topic_model_improved = BERTopic( low_memory=True,calculate_probabilities=True,verbose=True,representation_model=representation_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196,
          "referenced_widgets": [
            "a078f22b38f3411998c5eec014ecc2f5",
            "1c0b1b9f4ca9437a95306bc2b56811da",
            "ab31b7fcc24e49c8b1ced738ad2f8b2a",
            "a640fbad053e4f0cb5aafbb805ed27f1",
            "df27605719fb4454b2e4dd38f179bdae",
            "36bc5b99bb6049c78b8fbb2cbd972aaf",
            "fa95fd2b0a7a485aa08a71dc600864d0",
            "8bab0975ced34ccd851f11b85f55ce7b",
            "57619b86608345c68c4848d7120ea87d",
            "7d0161c060464fb7911ac278d2e677ee",
            "4c86388ebc5a4c47a3f1f977d80561eb"
          ]
        },
        "id": "kMitTZmRf8VS",
        "outputId": "a8ca0e82-fa8e-4698-cf4e-9f1bd32ed528"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-31 23:28:31,874 - BERTopic - Embedding - Transforming documents to embeddings.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a078f22b38f3411998c5eec014ecc2f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/49 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-31 23:29:17,226 - BERTopic - Embedding - Completed ✓\n",
            "2025-07-31 23:29:17,228 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
            "2025-07-31 23:29:23,656 - BERTopic - Dimensionality - Completed ✓\n",
            "2025-07-31 23:29:23,657 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
            "2025-07-31 23:29:23,876 - BERTopic - Cluster - Completed ✓\n",
            "2025-07-31 23:29:23,882 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
            "2025-07-31 23:29:29,959 - BERTopic - Representation - Completed ✓\n"
          ]
        }
      ],
      "source": [
        "topics_n, probs_n=topic_model_improved.fit_transform(comments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRUQFodgv_cH",
        "outputId": "87e7ef5c-18f0-44bb-a3af-637e4a8249a7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{-1: [('how', 0.26784837),\n",
              "  ('using', 0.19766784),\n",
              "  ('use', 0.1852465),\n",
              "  ('have', 0.16990498),\n",
              "  ('know', 0.16589744),\n",
              "  ('that', 0.1637072),\n",
              "  ('do', 0.16245279),\n",
              "  ('just', 0.16064931),\n",
              "  ('never', 0.15858206),\n",
              "  ('by', 0.15425947)],\n",
              " 0: [('https', 0.23767155),\n",
              "  ('that', 0.22330272),\n",
              "  ('this', 0.21886063),\n",
              "  ('http', 0.21800631),\n",
              "  ('simple', 0.19971004),\n",
              "  ('easy', 0.19599748),\n",
              "  ('can', 0.1937404),\n",
              "  ('more', 0.19112921),\n",
              "  ('of', 0.18373194),\n",
              "  ('like', 0.18029198)],\n",
              " 1: [('have', 0.26218718),\n",
              "  ('isnt', 0.25237232),\n",
              "  ('found', 0.24102579),\n",
              "  ('for', 0.23222923),\n",
              "  ('tip', 0.22631001),\n",
              "  ('do', 0.22384085),\n",
              "  ('post', 0.22274187),\n",
              "  ('then', 0.22171307),\n",
              "  ('if', 0.21932694),\n",
              "  ('again', 0.21802372)],\n",
              " 2: [('not', 0.4960704),\n",
              "  ('for', 0.4922192),\n",
              "  ('nt', 0.49026814),\n",
              "  ('can', 0.4780009),\n",
              "  ('this', 0.46299338),\n",
              "  ('that', 0.46245056),\n",
              "  ('so', 0.4600702),\n",
              "  ('is', 0.45229894),\n",
              "  ('to', 0.45190647),\n",
              "  ('but', 0.45001042)],\n",
              " 3: [('as', 0.19155703),\n",
              "  ('if', 0.18652786),\n",
              "  ('take', 0.18428054),\n",
              "  ('for', 0.17688927),\n",
              "  ('did', 0.1703377),\n",
              "  ('with', 0.17030784),\n",
              "  ('do', 0.16886738),\n",
              "  ('nt', 0.166348),\n",
              "  ('just', 0.16309607),\n",
              "  ('when', 0.16291964)],\n",
              " 4: [('not', 0.39240575),\n",
              "  ('up', 0.38909498),\n",
              "  ('now', 0.38169146),\n",
              "  ('fine', 0.33451706),\n",
              "  ('you', 0.30792207),\n",
              "  ('should', 0.30537552),\n",
              "  ('will', 0.3051626),\n",
              "  ('just', 0.29634386),\n",
              "  ('better', 0.2931978),\n",
              "  ('your', 0.28950894)],\n",
              " 5: [('that', 0.3776307),\n",
              "  ('about', 0.36431217),\n",
              "  ('and', 0.36371335),\n",
              "  ('re', 0.36033863),\n",
              "  ('what', 0.35477704),\n",
              "  ('good', 0.3474689),\n",
              "  ('is', 0.34547696),\n",
              "  ('you', 0.34439993),\n",
              "  ('if', 0.3425988),\n",
              "  ('it', 0.34130275)],\n",
              " 6: [('that', 0.25270432),\n",
              "  ('works', 0.24984238),\n",
              "  ('on', 0.247958),\n",
              "  ('you', 0.24769542),\n",
              "  ('into', 0.24133795),\n",
              "  ('this', 0.24092805),\n",
              "  ('to', 0.23577583),\n",
              "  ('out', 0.23483765),\n",
              "  ('use', 0.22874509),\n",
              "  ('only', 0.2254183)],\n",
              " 7: [('restore', 0.36509967),\n",
              "  ('nt', 0.34679532),\n",
              "  ('321', 0.33637357),\n",
              "  ('version', 0.32655048),\n",
              "  ('got', 0.29778713),\n",
              "  ('not', 0.29560962),\n",
              "  ('use', 0.28941536),\n",
              "  ('folder', 0.28850284),\n",
              "  ('pc', 0.28502196),\n",
              "  ('computer', 0.28223115)],\n",
              " 8: [('and', 0.3083933),\n",
              "  ('it', 0.26653993),\n",
              "  ('add', 0.2658348),\n",
              "  ('10x', 0.25047082),\n",
              "  ('10', 0.23417094),\n",
              "  ('that', 0.22996297),\n",
              "  ('sum', 0.22955997),\n",
              "  ('same', 0.22331029),\n",
              "  ('then', 0.22232659),\n",
              "  ('multiply', 0.22186673)],\n",
              " 9: [('how', 0.27320793),\n",
              "  ('amp', 0.23623905),\n",
              "  ('customer', 0.22482738),\n",
              "  ('charge', 0.21833628),\n",
              "  ('on', 0.19831222),\n",
              "  ('important', 0.19536638),\n",
              "  ('step', 0.19448946),\n",
              "  ('about', 0.17985083),\n",
              "  ('this', 0.17310084),\n",
              "  ('store', 0.16747215)],\n",
              " 10: [('info', 0.27505457),\n",
              "  ('comp', 0.26242262),\n",
              "  ('information', 0.2377289),\n",
              "  ('this', 0.23375544),\n",
              "  ('provider', 0.19535947),\n",
              "  ('notes', 0.19500566),\n",
              "  ('it', 0.1921621),\n",
              "  ('gt', 0.18738535),\n",
              "  ('need', 0.18166244),\n",
              "  ('services', 0.17519118)],\n",
              " 11: [('google', 0.42258793),\n",
              "  ('phone', 0.39669347),\n",
              "  ('remove', 0.3950814),\n",
              "  ('on', 0.34316054),\n",
              "  ('did', 0.3060219),\n",
              "  ('only', 0.29897434),\n",
              "  ('for', 0.28910428),\n",
              "  ('got', 0.28263295),\n",
              "  ('spam', 0.28034508),\n",
              "  ('not', 0.2769305)],\n",
              " 12: [('broken', 0.28103733),\n",
              "  ('code', 0.26612356),\n",
              "  ('status', 0.26401904),\n",
              "  ('fixes', 0.2448959),\n",
              "  ('rebuilt', 0.23942311),\n",
              "  ('free', 0.23829857),\n",
              "  ('printed', 0.22869092),\n",
              "  ('feature', 0.22804502),\n",
              "  ('pdf', 0.22602883),\n",
              "  ('use', 0.22298911)],\n",
              " 13: [('http', 0.21005102),\n",
              "  ('infocomelsewhereorggalleryamfvamfvhtml', 0.2067088),\n",
              "  ('ca', 0.19356054),\n",
              "  ('https', 0.16133696),\n",
              "  ('years', 0.13724737),\n",
              "  ('sad', 0.12942609),\n",
              "  ('pc', 0.12746212),\n",
              "  ('implemented', 0.12542492),\n",
              "  ('website', 0.12474546),\n",
              "  ('yiss', 0.12154312)],\n",
              " 14: [('but', 0.34470618),\n",
              "  ('bot', 0.30825624),\n",
              "  ('trust', 0.29854444),\n",
              "  ('that', 0.28647974),\n",
              "  ('really', 0.28279316),\n",
              "  ('and', 0.2814527),\n",
              "  ('for', 0.2771784),\n",
              "  ('also', 0.27070612),\n",
              "  ('so', 0.27014238),\n",
              "  ('it', 0.2652266)],\n",
              " 15: [('ram', 0.39723426),\n",
              "  ('free', 0.31247556),\n",
              "  ('ssd', 0.30331856),\n",
              "  ('bought', 0.30099815),\n",
              "  ('mbp', 0.2956592),\n",
              "  ('gt', 0.2584312),\n",
              "  ('macbook', 0.2570791),\n",
              "  ('saved', 0.25565532),\n",
              "  ('had', 0.24062908),\n",
              "  ('new', 0.23499113)],\n",
              " 16: [('wwwgooglecomsettingsstorage', 0.3373788),\n",
              "  ('storage', 0.32639372),\n",
              "  ('19gb', 0.32384866),\n",
              "  ('15gb', 0.31389806),\n",
              "  ('googleblogblogspotcouk201502takesecuritycheckuponsaferinternethtml',\n",
              "   0.3012612),\n",
              "  ('2gb', 0.3006876),\n",
              "  ('accessing', 0.29285377),\n",
              "  ('1493gb', 0.28648883),\n",
              "  ('17gb', 0.28460637),\n",
              "  ('thanks', 0.2673406)],\n",
              " 17: [('need', 0.2197397),\n",
              "  ('got', 0.20287924),\n",
              "  ('found', 0.20183867),\n",
              "  ('on', 0.19989504),\n",
              "  ('for', 0.19694152),\n",
              "  ('how', 0.17641999),\n",
              "  ('stripe', 0.15957762),\n",
              "  ('to', 0.15925121),\n",
              "  ('bloomberg', 0.15790336),\n",
              "  ('and', 0.1576949)],\n",
              " 18: [('library', 0.31242865),\n",
              "  ('cool', 0.22600353),\n",
              "  ('awesome', 0.22427377),\n",
              "  ('all', 0.16781422),\n",
              "  ('thank', 0.16412604),\n",
              "  ('itt', 0.15584034),\n",
              "  ('for', 0.15073007),\n",
              "  ('on', 0.14771521),\n",
              "  ('posted', 0.14754382),\n",
              "  ('tip', 0.14084896)],\n",
              " 19: [('verbotten', 0.32517537),\n",
              "  ('put', 0.31390697),\n",
              "  ('for', 0.31023872),\n",
              "  ('of', 0.30234545),\n",
              "  ('like', 0.28736484),\n",
              "  ('it', 0.28730547),\n",
              "  ('using', 0.28487575),\n",
              "  ('vegetable', 0.28346044),\n",
              "  ('warmtingle', 0.28338382),\n",
              "  ('that', 0.27804965)],\n",
              " 20: [('votes', 0.27708942),\n",
              "  ('vote', 0.25381902),\n",
              "  ('game', 0.24857414),\n",
              "  ('such', 0.24616167),\n",
              "  ('voting', 0.24278218),\n",
              "  ('information', 0.2350131),\n",
              "  ('idea', 0.22299552),\n",
              "  ('this', 0.21855515),\n",
              "  ('support', 0.21815652),\n",
              "  ('summaries', 0.21806422)],\n",
              " 21: [('need', 0.23677516),\n",
              "  ('here', 0.2241464),\n",
              "  ('alerted', 0.20778069),\n",
              "  ('had', 0.20209372),\n",
              "  ('niceit', 0.1914327),\n",
              "  ('how', 0.18710642),\n",
              "  ('way', 0.18199137),\n",
              "  ('saving', 0.1816026),\n",
              "  ('nt', 0.18098861),\n",
              "  ('item', 0.1754833)],\n",
              " 22: [('nt', 0.24630237),\n",
              "  ('ie', 0.20549518),\n",
              "  ('manual', 0.19516526),\n",
              "  ('safe', 0.16351539),\n",
              "  ('local', 0.15974112),\n",
              "  ('mind', 0.15684992),\n",
              "  ('any', 0.15410283),\n",
              "  ('them', 0.1434115),\n",
              "  ('as', 0.13757266),\n",
              "  ('that', 0.13729194)],\n",
              " 23: [('download', 0.22652242),\n",
              "  ('this', 0.20988396),\n",
              "  ('mobile', 0.18500274),\n",
              "  ('that', 0.18418922),\n",
              "  ('other', 0.18312305),\n",
              "  ('use', 0.18150158),\n",
              "  ('gigs', 0.17959735),\n",
              "  ('second', 0.17674634),\n",
              "  ('you', 0.16920796),\n",
              "  ('should', 0.16411728)],\n",
              " 24: [('use', 0.29923117),\n",
              "  ('note', 0.28199136),\n",
              "  ('using', 0.2617992),\n",
              "  ('need', 0.25808895),\n",
              "  ('again', 0.25186306),\n",
              "  ('on', 0.2505857),\n",
              "  ('screen', 0.24803618),\n",
              "  ('remove', 0.24780703),\n",
              "  ('addon', 0.24264482),\n",
              "  ('nt', 0.23405844)],\n",
              " 25: [('those', 0.31578207),\n",
              "  ('directly', 0.29158843),\n",
              "  ('videos', 0.2779735),\n",
              "  ('code', 0.27582812),\n",
              "  ('here', 0.2747237),\n",
              "  ('sites', 0.27379882),\n",
              "  ('of', 0.27321744),\n",
              "  ('page', 0.27135232),\n",
              "  ('wwwwundergroundcom', 0.27009368),\n",
              "  ('currently', 0.2696816)],\n",
              " 26: [('until', 0.29238993),\n",
              "  ('video', 0.26782572),\n",
              "  ('does', 0.23566452),\n",
              "  ('for', 0.2335735),\n",
              "  ('about', 0.22743042),\n",
              "  ('youtubewr1yowesw8e', 0.22708268),\n",
              "  ('this', 0.22475563),\n",
              "  ('fixed', 0.22173052),\n",
              "  ('youtubeqpx6awroki8', 0.22145133),\n",
              "  ('not', 0.21974371)],\n",
              " 27: [('http', 0.382539),\n",
              "  ('need', 0.22137299),\n",
              "  ('titletext', 0.21200353),\n",
              "  ('mentalflosscomarticle31666doesonebadapplereallyspoilwholebunch',\n",
              "   0.19365868),\n",
              "  ('way', 0.18575895),\n",
              "  ('pubsacsorgdoiabs101021jf970877q', 0.16846287),\n",
              "  ('zone', 0.15295625),\n",
              "  ('indianapublicmediaorgamomentofsciencefiles200907banana940x633jpg',\n",
              "   0.15280649),\n",
              "  ('cellaring', 0.14941849),\n",
              "  ('articles', 0.14722928)],\n",
              " 28: [('practical', 0.26320308),\n",
              "  ('what', 0.26003182),\n",
              "  ('example', 0.25229222),\n",
              "  ('use', 0.25132322),\n",
              "  ('this', 0.2486337),\n",
              "  ('researcher', 0.24531326),\n",
              "  ('something', 0.24523824),\n",
              "  ('due', 0.24001184),\n",
              "  ('care', 0.23263246),\n",
              "  ('that', 0.22447687)],\n",
              " 29: [('software', 0.32354045),\n",
              "  ('applications', 0.3150895),\n",
              "  ('need', 0.28967702),\n",
              "  ('computers', 0.2556831),\n",
              "  ('computer', 0.25358048),\n",
              "  ('check', 0.24622017),\n",
              "  ('users', 0.24315068),\n",
              "  ('advanced', 0.24047022),\n",
              "  ('programs', 0.23923479),\n",
              "  ('unnecessary', 0.23089617)],\n",
              " 30: [('claim', 0.3275274),\n",
              "  ('ticket', 0.3109716),\n",
              "  ('confirming', 0.2896378),\n",
              "  ('win', 0.27697825),\n",
              "  ('wwwengadgetcom20160108yousayadvertisingisayblockthatmalware', 0.27212846),\n",
              "  ('com', 0.27141762),\n",
              "  ('that', 0.26929942),\n",
              "  ('wwwforbescomsitesdeborahljacobs2012021110thingstodowhenyouwinthepowerball',\n",
              "   0.26640928),\n",
              "  ('tax', 0.26227543),\n",
              "  ('law', 0.25835243)],\n",
              " 31: [('that', 0.35340166),\n",
              "  ('just', 0.3469483),\n",
              "  ('my', 0.33730286),\n",
              "  ('there', 0.3271367),\n",
              "  ('your', 0.31206805),\n",
              "  ('finds', 0.31135017),\n",
              "  ('justfuckmeupfam', 0.30929744),\n",
              "  ('linked', 0.30846736),\n",
              "  ('visited', 0.30256397),\n",
              "  ('unrollme', 0.3016617)],\n",
              " 32: [('ticketing', 0.35106224),\n",
              "  ('wwwnydailynewscomnewyorkbrooklynbrooklynmanmissingstormfounddeadcararticle12509309',\n",
              "   0.34281135),\n",
              "  ('is', 0.33261424),\n",
              "  ('not', 0.32036003),\n",
              "  ('to', 0.31283176),\n",
              "  ('roads', 0.3110438),\n",
              "  ('realy', 0.3100024),\n",
              "  ('violations', 0.30624944),\n",
              "  ('montreal', 0.3024929),\n",
              "  ('up', 0.30092138)],\n",
              " 33: [('megabit', 0.27707314),\n",
              "  ('mega', 0.25792027),\n",
              "  ('dsl', 0.20457526),\n",
              "  ('when', 0.18452427),\n",
              "  ('know', 0.18028963),\n",
              "  ('megabyte', 0.17858854),\n",
              "  ('kbitmbitgbit', 0.17626072),\n",
              "  ('may', 0.16836083),\n",
              "  ('respond', 0.16691928),\n",
              "  ('that', 0.16529128)],\n",
              " 34: [('tested', 0.38693026),\n",
              "  ('screened', 0.33784732),\n",
              "  ('found', 0.32036403),\n",
              "  ('cases', 0.3149347),\n",
              "  ('this', 0.28911138),\n",
              "  ('had', 0.2780831),\n",
              "  ('have', 0.26932868),\n",
              "  ('wwwnationalcmvorg', 0.26904416),\n",
              "  ('knew', 0.26240888),\n",
              "  ('for', 0.25524583)],\n",
              " 35: [('just', 0.24072821),\n",
              "  ('android', 0.2390014),\n",
              "  ('seems', 0.21247369),\n",
              "  ('not', 0.21116307),\n",
              "  ('it', 0.21013999),\n",
              "  ('like', 0.1976879),\n",
              "  ('ctrl', 0.1964242),\n",
              "  ('does', 0.19260144),\n",
              "  ('webpage', 0.19221118),\n",
              "  ('anymore', 0.19188778)],\n",
              " 36: [('nt', 0.25489163),\n",
              "  ('use', 0.22586083),\n",
              "  ('seems', 0.213476),\n",
              "  ('correct', 0.2062431),\n",
              "  ('trouble', 0.20477323),\n",
              "  ('go', 0.19465292),\n",
              "  ('for', 0.19297042),\n",
              "  ('less', 0.1893675),\n",
              "  ('can', 0.1890704),\n",
              "  ('just', 0.18652296)],\n",
              " 37: [('welcome', 0.29117414),\n",
              "  ('now', 0.26279694),\n",
              "  ('coming', 0.24178508),\n",
              "  ('info', 0.23696835),\n",
              "  ('started', 0.23242101),\n",
              "  ('about', 0.23073584),\n",
              "  ('site', 0.22839095),\n",
              "  ('wonderful', 0.22240204),\n",
              "  ('get', 0.22026476),\n",
              "  ('xyz', 0.21544507)],\n",
              " 38: [('searches', 0.17612876),\n",
              "  ('google', 0.17597727),\n",
              "  ('service', 0.17519653),\n",
              "  ('mobile', 0.17345926),\n",
              "  ('phone', 0.16995358),\n",
              "  ('entry', 0.15552214),\n",
              "  ('whatever', 0.15477815),\n",
              "  ('cheat', 0.15169783),\n",
              "  ('nt', 0.1422579),\n",
              "  ('ca', 0.13696776)]}"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "topic_model_improved.get_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZBNxAybu_v4",
        "outputId": "59d59867-4d79-4f2d-d748-4f741a2eda06"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'u_mass_coherence': -0.7269715747364682,\n",
              " 'c_v_coherence': 0.3551674363004895,\n",
              " 'c_uci_coherence': -6.294601969755953,\n",
              " 'c_npmi_coherence': -0.21081947605182805}"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "coherence_metrics=calculate_coherence_metrics(topic_model_improved,comments,topics_n)\n",
        "coherence_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOR8CyLPxOTQ"
      },
      "source": [
        "The lower coherence observed with the centroid-based representation is expected, as KeyBERT is designed to enhance topic quality in BERTopic by providing more meaningful and diverse keywords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lbaPHQzFhY8R",
        "outputId": "fae184b0-2174-4d62-f85d-0d8c9bb555d5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"topic_model_improved\",\n  \"rows\": 40,\n  \"fields\": [\n    {\n      \"column\": \"Topic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11,\n        \"min\": -1,\n        \"max\": 38,\n        \"num_unique_values\": 40,\n        \"samples\": [\n          18,\n          15,\n          14\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 54,\n        \"min\": 11,\n        \"max\": 337,\n        \"num_unique_values\": 26,\n        \"samples\": [\n          32,\n          22,\n          337\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 40,\n        \"samples\": [\n          \"18_library_cool_awesome_all\",\n          \"15_ram_free_ssd_bought\",\n          \"14_but_bot_trust_that\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Representation\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Representative_Docs\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-33e7611c-346c-4866-b6d9-720c335fd765\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic</th>\n",
              "      <th>Count</th>\n",
              "      <th>Name</th>\n",
              "      <th>Representation</th>\n",
              "      <th>Representative_Docs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1</td>\n",
              "      <td>337</td>\n",
              "      <td>-1_how_using_use_have</td>\n",
              "      <td>[how, using, use, have, know, that, do, just, ...</td>\n",
              "      <td>[why should i know this  i believe it s better...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>127</td>\n",
              "      <td>0_https_that_this_http</td>\n",
              "      <td>[https, that, this, http, simple, easy, can, m...</td>\n",
              "      <td>[you can file your taxes for free anyway  but ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>99</td>\n",
              "      <td>1_have_isnt_found_for</td>\n",
              "      <td>[have, isnt, found, for, tip, do, post, then, ...</td>\n",
              "      <td>[this is      , thanks so much for this , than...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>84</td>\n",
              "      <td>2_not_for_nt_can</td>\n",
              "      <td>[not, for, nt, can, this, that, so, is, to, but]</td>\n",
              "      <td>[i also get looks when i take my disabled moth...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>74</td>\n",
              "      <td>3_as_if_take_for</td>\n",
              "      <td>[as, if, take, for, did, with, do, nt, just, w...</td>\n",
              "      <td>[a 2nd opinion has saved or prolonged a life t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>4</td>\n",
              "      <td>40</td>\n",
              "      <td>4_not_up_now_fine</td>\n",
              "      <td>[not, up, now, fine, you, should, will, just, ...</td>\n",
              "      <td>[the problem with gas is the short shelf life ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5</td>\n",
              "      <td>40</td>\n",
              "      <td>5_that_about_and_re</td>\n",
              "      <td>[that, about, and, re, what, good, is, you, if...</td>\n",
              "      <td>[  tl  dr if you are running windows and need ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>6</td>\n",
              "      <td>38</td>\n",
              "      <td>6_that_works_on_you</td>\n",
              "      <td>[that, works, on, you, into, this, to, out, us...</td>\n",
              "      <td>[i have a sure remedy and guy s and      s it ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>7</td>\n",
              "      <td>38</td>\n",
              "      <td>7_restore_nt_321_version</td>\n",
              "      <td>[restore, nt, 321, version, got, not, use, fol...</td>\n",
              "      <td>[i make sure i have at least two copies of eve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>8</td>\n",
              "      <td>37</td>\n",
              "      <td>8_and_it_add_10x</td>\n",
              "      <td>[and, it, add, 10x, 10, that, sum, same, then,...</td>\n",
              "      <td>[explanation of the   7   rule  since it took ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>9</td>\n",
              "      <td>32</td>\n",
              "      <td>9_how_amp_customer_charge</td>\n",
              "      <td>[how, amp, customer, charge, on, important, st...</td>\n",
              "      <td>[  3 years working for a ups store   hey guys ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>10</td>\n",
              "      <td>31</td>\n",
              "      <td>10_info_comp_information_this</td>\n",
              "      <td>[info, comp, information, this, provider, note...</td>\n",
              "      <td>[as others have said this is so the insurance ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>11</td>\n",
              "      <td>30</td>\n",
              "      <td>11_google_phone_remove_on</td>\n",
              "      <td>[google, phone, remove, on, did, only, for, go...</td>\n",
              "      <td>[i did not know this  when i first got my cell...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>12_broken_code_status_fixes</td>\n",
              "      <td>[broken, code, status, fixes, rebuilt, free, p...</td>\n",
              "      <td>[most library systems offer ebooks  kindle boo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>13</td>\n",
              "      <td>29</td>\n",
              "      <td>13_http_infocomelsewhereorggalleryamfvamfvhtml...</td>\n",
              "      <td>[http, infocomelsewhereorggalleryamfvamfvhtml,...</td>\n",
              "      <td>[just found this brilliant website  https  www...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>14</td>\n",
              "      <td>29</td>\n",
              "      <td>14_but_bot_trust_that</td>\n",
              "      <td>[but, bot, trust, that, really, and, for, also...</td>\n",
              "      <td>[some breweries use the spent grains themselve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>15</td>\n",
              "      <td>28</td>\n",
              "      <td>15_ram_free_ssd_bought</td>\n",
              "      <td>[ram, free, ssd, bought, mbp, gt, macbook, sav...</td>\n",
              "      <td>[wait  what    back in august of 2011  i broug...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>16</td>\n",
              "      <td>27</td>\n",
              "      <td>16_wwwgooglecomsettingsstorage_storage_19gb_15gb</td>\n",
              "      <td>[wwwgooglecomsettingsstorage, storage, 19gb, 1...</td>\n",
              "      <td>[remindme  one year  do 2017 google security c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>17</td>\n",
              "      <td>26</td>\n",
              "      <td>17_need_got_found_on</td>\n",
              "      <td>[need, got, found, on, for, how, stripe, to, b...</td>\n",
              "      <td>[our 2 indoor cats have dog collars  we do nt ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>18</td>\n",
              "      <td>26</td>\n",
              "      <td>18_library_cool_awesome_all</td>\n",
              "      <td>[library, cool, awesome, all, thank, itt, for,...</td>\n",
              "      <td>[ysk not all reddit users are from the us, if ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>19</td>\n",
              "      <td>26</td>\n",
              "      <td>19_verbotten_put_for_of</td>\n",
              "      <td>[verbotten, put, for, of, like, it, using, veg...</td>\n",
              "      <td>[wife gave me a bath with essential oils  euca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>20</td>\n",
              "      <td>25</td>\n",
              "      <td>20_votes_vote_game_such</td>\n",
              "      <td>[votes, vote, game, such, voting, information,...</td>\n",
              "      <td>[wow  thank you for this  so instead of a  sec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>21</td>\n",
              "      <td>25</td>\n",
              "      <td>21_need_here_alerted_had</td>\n",
              "      <td>[need, here, alerted, had, niceit, how, way, s...</td>\n",
              "      <td>[the usps discontinues priority mail express f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22_nt_ie_manual_safe</td>\n",
              "      <td>[nt, ie, manual, safe, local, mind, any, them,...</td>\n",
              "      <td>[also  for children around two and older   a p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>23</td>\n",
              "      <td>22</td>\n",
              "      <td>23_download_this_mobile_that</td>\n",
              "      <td>[download, this, mobile, that, other, use, gig...</td>\n",
              "      <td>[from their own website   to ensure that we  r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>24</td>\n",
              "      <td>21</td>\n",
              "      <td>24_use_note_using_need</td>\n",
              "      <td>[use, note, using, need, again, on, screen, re...</td>\n",
              "      <td>[why would you have to spend time looking thro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>25</td>\n",
              "      <td>21</td>\n",
              "      <td>25_those_directly_videos_code</td>\n",
              "      <td>[those, directly, videos, code, here, sites, o...</td>\n",
              "      <td>[good advice  i have an app on my phone called...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>26</td>\n",
              "      <td>20</td>\n",
              "      <td>26_until_video_does_for</td>\n",
              "      <td>[until, video, does, for, about, youtubewr1yow...</td>\n",
              "      <td>[https  wwwyoutubecomwatch  v  d9cqjfwesv0,  d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>27</td>\n",
              "      <td>18</td>\n",
              "      <td>27_http_need_titletext_mentalflosscomarticle31...</td>\n",
              "      <td>[http, need, titletext, mentalflosscomarticle3...</td>\n",
              "      <td>[ysk ozone destroy ethylene gas helping keep f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>28</td>\n",
              "      <td>17</td>\n",
              "      <td>28_practical_what_example_use</td>\n",
              "      <td>[practical, what, example, use, this, research...</td>\n",
              "      <td>[gt  the researcher has used mathematics to de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>29</td>\n",
              "      <td>17</td>\n",
              "      <td>29_software_applications_need_computers</td>\n",
              "      <td>[software, applications, need, computers, comp...</td>\n",
              "      <td>[this is kind of misleading for many programs ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>30</td>\n",
              "      <td>16</td>\n",
              "      <td>30_claim_ticket_confirming_win</td>\n",
              "      <td>[claim, ticket, confirming, win, wwwengadgetco...</td>\n",
              "      <td>[or do nt sign it because that prevents you fr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>31</td>\n",
              "      <td>14</td>\n",
              "      <td>31_that_just_my_there</td>\n",
              "      <td>[that, just, my, there, your, finds, justfuckm...</td>\n",
              "      <td>[you should be website that signs your email u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>32</td>\n",
              "      <td>14</td>\n",
              "      <td>32_ticketing_wwwnydailynewscomnewyorkbrooklynb...</td>\n",
              "      <td>[ticketing, wwwnydailynewscomnewyorkbrooklynbr...</td>\n",
              "      <td>[i m from canada what s a snow emergency , wit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>33</td>\n",
              "      <td>14</td>\n",
              "      <td>33_megabit_mega_dsl_when</td>\n",
              "      <td>[megabit, mega, dsl, when, know, megabyte, kbi...</td>\n",
              "      <td>[b is bit  b is byte those bits also include a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>34</td>\n",
              "      <td>13</td>\n",
              "      <td>34_tested_screened_found_cases</td>\n",
              "      <td>[tested, screened, found, cases, this, had, ha...</td>\n",
              "      <td>[i am cmv negative when i was looking for a   ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>35</td>\n",
              "      <td>12</td>\n",
              "      <td>35_just_android_seems_not</td>\n",
              "      <td>[just, android, seems, not, it, like, ctrl, do...</td>\n",
              "      <td>[did nt work for me i searched  flan  , ahw ma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>36</td>\n",
              "      <td>11</td>\n",
              "      <td>36_nt_use_seems_correct</td>\n",
              "      <td>[nt, use, seems, correct, trouble, go, for, le...</td>\n",
              "      <td>[what the      is a snowthrower  is that a les...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>37</td>\n",
              "      <td>11</td>\n",
              "      <td>37_welcome_now_coming_info</td>\n",
              "      <td>[welcome, now, coming, info, started, about, s...</td>\n",
              "      <td>[also  in my spelling   practice n  practise v...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>38</td>\n",
              "      <td>11</td>\n",
              "      <td>38_searches_google_service_mobile</td>\n",
              "      <td>[searches, google, service, mobile, phone, ent...</td>\n",
              "      <td>[this reminds me of when i was a lot youngerge...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-33e7611c-346c-4866-b6d9-720c335fd765')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-33e7611c-346c-4866-b6d9-720c335fd765 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-33e7611c-346c-4866-b6d9-720c335fd765');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-da6f3d4e-b7fe-4103-9320-c20a24c201ee\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-da6f3d4e-b7fe-4103-9320-c20a24c201ee')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-da6f3d4e-b7fe-4103-9320-c20a24c201ee button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "    Topic  Count                                               Name  \\\n",
              "0      -1    337                              -1_how_using_use_have   \n",
              "1       0    127                             0_https_that_this_http   \n",
              "2       1     99                              1_have_isnt_found_for   \n",
              "3       2     84                                   2_not_for_nt_can   \n",
              "4       3     74                                   3_as_if_take_for   \n",
              "5       4     40                                  4_not_up_now_fine   \n",
              "6       5     40                                5_that_about_and_re   \n",
              "7       6     38                                6_that_works_on_you   \n",
              "8       7     38                           7_restore_nt_321_version   \n",
              "9       8     37                                   8_and_it_add_10x   \n",
              "10      9     32                          9_how_amp_customer_charge   \n",
              "11     10     31                      10_info_comp_information_this   \n",
              "12     11     30                          11_google_phone_remove_on   \n",
              "13     12     30                        12_broken_code_status_fixes   \n",
              "14     13     29  13_http_infocomelsewhereorggalleryamfvamfvhtml...   \n",
              "15     14     29                              14_but_bot_trust_that   \n",
              "16     15     28                             15_ram_free_ssd_bought   \n",
              "17     16     27   16_wwwgooglecomsettingsstorage_storage_19gb_15gb   \n",
              "18     17     26                               17_need_got_found_on   \n",
              "19     18     26                        18_library_cool_awesome_all   \n",
              "20     19     26                            19_verbotten_put_for_of   \n",
              "21     20     25                            20_votes_vote_game_such   \n",
              "22     21     25                           21_need_here_alerted_had   \n",
              "23     22     22                               22_nt_ie_manual_safe   \n",
              "24     23     22                       23_download_this_mobile_that   \n",
              "25     24     21                             24_use_note_using_need   \n",
              "26     25     21                      25_those_directly_videos_code   \n",
              "27     26     20                            26_until_video_does_for   \n",
              "28     27     18  27_http_need_titletext_mentalflosscomarticle31...   \n",
              "29     28     17                      28_practical_what_example_use   \n",
              "30     29     17            29_software_applications_need_computers   \n",
              "31     30     16                     30_claim_ticket_confirming_win   \n",
              "32     31     14                              31_that_just_my_there   \n",
              "33     32     14  32_ticketing_wwwnydailynewscomnewyorkbrooklynb...   \n",
              "34     33     14                           33_megabit_mega_dsl_when   \n",
              "35     34     13                     34_tested_screened_found_cases   \n",
              "36     35     12                          35_just_android_seems_not   \n",
              "37     36     11                            36_nt_use_seems_correct   \n",
              "38     37     11                         37_welcome_now_coming_info   \n",
              "39     38     11                  38_searches_google_service_mobile   \n",
              "\n",
              "                                       Representation  \\\n",
              "0   [how, using, use, have, know, that, do, just, ...   \n",
              "1   [https, that, this, http, simple, easy, can, m...   \n",
              "2   [have, isnt, found, for, tip, do, post, then, ...   \n",
              "3    [not, for, nt, can, this, that, so, is, to, but]   \n",
              "4   [as, if, take, for, did, with, do, nt, just, w...   \n",
              "5   [not, up, now, fine, you, should, will, just, ...   \n",
              "6   [that, about, and, re, what, good, is, you, if...   \n",
              "7   [that, works, on, you, into, this, to, out, us...   \n",
              "8   [restore, nt, 321, version, got, not, use, fol...   \n",
              "9   [and, it, add, 10x, 10, that, sum, same, then,...   \n",
              "10  [how, amp, customer, charge, on, important, st...   \n",
              "11  [info, comp, information, this, provider, note...   \n",
              "12  [google, phone, remove, on, did, only, for, go...   \n",
              "13  [broken, code, status, fixes, rebuilt, free, p...   \n",
              "14  [http, infocomelsewhereorggalleryamfvamfvhtml,...   \n",
              "15  [but, bot, trust, that, really, and, for, also...   \n",
              "16  [ram, free, ssd, bought, mbp, gt, macbook, sav...   \n",
              "17  [wwwgooglecomsettingsstorage, storage, 19gb, 1...   \n",
              "18  [need, got, found, on, for, how, stripe, to, b...   \n",
              "19  [library, cool, awesome, all, thank, itt, for,...   \n",
              "20  [verbotten, put, for, of, like, it, using, veg...   \n",
              "21  [votes, vote, game, such, voting, information,...   \n",
              "22  [need, here, alerted, had, niceit, how, way, s...   \n",
              "23  [nt, ie, manual, safe, local, mind, any, them,...   \n",
              "24  [download, this, mobile, that, other, use, gig...   \n",
              "25  [use, note, using, need, again, on, screen, re...   \n",
              "26  [those, directly, videos, code, here, sites, o...   \n",
              "27  [until, video, does, for, about, youtubewr1yow...   \n",
              "28  [http, need, titletext, mentalflosscomarticle3...   \n",
              "29  [practical, what, example, use, this, research...   \n",
              "30  [software, applications, need, computers, comp...   \n",
              "31  [claim, ticket, confirming, win, wwwengadgetco...   \n",
              "32  [that, just, my, there, your, finds, justfuckm...   \n",
              "33  [ticketing, wwwnydailynewscomnewyorkbrooklynbr...   \n",
              "34  [megabit, mega, dsl, when, know, megabyte, kbi...   \n",
              "35  [tested, screened, found, cases, this, had, ha...   \n",
              "36  [just, android, seems, not, it, like, ctrl, do...   \n",
              "37  [nt, use, seems, correct, trouble, go, for, le...   \n",
              "38  [welcome, now, coming, info, started, about, s...   \n",
              "39  [searches, google, service, mobile, phone, ent...   \n",
              "\n",
              "                                  Representative_Docs  \n",
              "0   [why should i know this  i believe it s better...  \n",
              "1   [you can file your taxes for free anyway  but ...  \n",
              "2   [this is      , thanks so much for this , than...  \n",
              "3   [i also get looks when i take my disabled moth...  \n",
              "4   [a 2nd opinion has saved or prolonged a life t...  \n",
              "5   [the problem with gas is the short shelf life ...  \n",
              "6   [  tl  dr if you are running windows and need ...  \n",
              "7   [i have a sure remedy and guy s and      s it ...  \n",
              "8   [i make sure i have at least two copies of eve...  \n",
              "9   [explanation of the   7   rule  since it took ...  \n",
              "10  [  3 years working for a ups store   hey guys ...  \n",
              "11  [as others have said this is so the insurance ...  \n",
              "12  [i did not know this  when i first got my cell...  \n",
              "13  [most library systems offer ebooks  kindle boo...  \n",
              "14  [just found this brilliant website  https  www...  \n",
              "15  [some breweries use the spent grains themselve...  \n",
              "16  [wait  what    back in august of 2011  i broug...  \n",
              "17  [remindme  one year  do 2017 google security c...  \n",
              "18  [our 2 indoor cats have dog collars  we do nt ...  \n",
              "19  [ysk not all reddit users are from the us, if ...  \n",
              "20  [wife gave me a bath with essential oils  euca...  \n",
              "21  [wow  thank you for this  so instead of a  sec...  \n",
              "22  [the usps discontinues priority mail express f...  \n",
              "23  [also  for children around two and older   a p...  \n",
              "24  [from their own website   to ensure that we  r...  \n",
              "25  [why would you have to spend time looking thro...  \n",
              "26  [good advice  i have an app on my phone called...  \n",
              "27  [https  wwwyoutubecomwatch  v  d9cqjfwesv0,  d...  \n",
              "28  [ysk ozone destroy ethylene gas helping keep f...  \n",
              "29  [gt  the researcher has used mathematics to de...  \n",
              "30  [this is kind of misleading for many programs ...  \n",
              "31  [or do nt sign it because that prevents you fr...  \n",
              "32  [you should be website that signs your email u...  \n",
              "33  [i m from canada what s a snow emergency , wit...  \n",
              "34  [b is bit  b is byte those bits also include a...  \n",
              "35  [i am cmv negative when i was looking for a   ...  \n",
              "36  [did nt work for me i searched  flan  , ahw ma...  \n",
              "37  [what the      is a snowthrower  is that a les...  \n",
              "38  [also  in my spelling   practice n  practise v...  \n",
              "39  [this reminds me of when i was a lot youngerge...  "
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "topic_model_improved.get_topic_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 808
        },
        "id": "VwxJVfh0hrcs",
        "outputId": "976c296b-ee6d-429e-9617-24a2e6d8482d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"topic_model_improved\",\n  \"rows\": 1552,\n  \"fields\": [\n    {\n      \"column\": \"Document\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1552,\n        \"samples\": [\n          \"excellent  excellent advice as patients  we feel awkward questioning or secondguessing a doctor  or even getting a second opinion while some doctors get annoyed  asking questions and getting secondopinions are essential to getting a good diagnosis  figuring out a good treatment  and getting good care  unfortunately  even with insurance  doing all of that can be hard making appointments is difficult  as the economics of healthcare encourages doctors to overbook and to spend as little time as possible with patients many doctors do nt take any insurance or have moved into extremely expensive vip practices  and others  who do take insurance  wo nt see patients who do nt have the particular insurance that the doctor accepts  moreover  finding reliable data and ratings on doctors is nearly impossible  people will say   just google the doctor   but that rarely  if ever  gives reliable results  even once you scroll down below the paidemptyform web sites that inevitably come up first in those searches  and doing all of this while sick is even harder  but completely agree with op s point \",\n          \"i do nt get step 3 before you ship how much  insurance  i get is the declared value  essentially  i m paying for my item again with ups \",\n          \"awesome  i ll remember this next time i m in the 1990s \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Topic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11,\n        \"min\": -1,\n        \"max\": 38,\n        \"num_unique_values\": 40,\n        \"samples\": [\n          2,\n          37,\n          18\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 40,\n        \"samples\": [\n          \"2_not_for_nt_can\",\n          \"37_welcome_now_coming_info\",\n          \"18_library_cool_awesome_all\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Representation\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Representative_Docs\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Top_n_words\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 40,\n        \"samples\": [\n          \"not - for - nt - can - this - that - so - is - to - but\",\n          \"welcome - now - coming - info - started - about - site - wonderful - get - xyz\",\n          \"library - cool - awesome - all - thank - itt - for - on - posted - tip\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Probability\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3483655715443441,\n        \"min\": 0.010836265897712147,\n        \"max\": 1.0,\n        \"num_unique_values\": 1127,\n        \"samples\": [\n          0.16215867347362992,\n          0.6326099006311487,\n          0.6460582793354889\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Representative_document\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-3210e3c3-32cc-4380-ad0c-573b6b3f1914\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Document</th>\n",
              "      <th>Topic</th>\n",
              "      <th>Name</th>\n",
              "      <th>Representation</th>\n",
              "      <th>Representative_Docs</th>\n",
              "      <th>Top_n_words</th>\n",
              "      <th>Probability</th>\n",
              "      <th>Representative_document</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>informative post and truly helpful it s easy t...</td>\n",
              "      <td>32</td>\n",
              "      <td>32_ticketing_wwwnydailynewscomnewyorkbrooklynb...</td>\n",
              "      <td>[ticketing, wwwnydailynewscomnewyorkbrooklynbr...</td>\n",
              "      <td>[i m from canada what s a snow emergency , wit...</td>\n",
              "      <td>ticketing - wwwnydailynewscomnewyorkbrooklynbr...</td>\n",
              "      <td>0.344923</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>actually that is such a good tip it s not what...</td>\n",
              "      <td>1</td>\n",
              "      <td>1_have_isnt_found_for</td>\n",
              "      <td>[have, isnt, found, for, tip, do, post, then, ...</td>\n",
              "      <td>[this is      , thanks so much for this , than...</td>\n",
              "      <td>have - isnt - found - for - tip - do - post - ...</td>\n",
              "      <td>0.597480</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>do nt put it in the fridge with the peel on   ...</td>\n",
              "      <td>27</td>\n",
              "      <td>27_http_need_titletext_mentalflosscomarticle31...</td>\n",
              "      <td>[http, need, titletext, mentalflosscomarticle3...</td>\n",
              "      <td>[ysk ozone destroy ethylene gas helping keep f...</td>\n",
              "      <td>http - need - titletext - mentalflosscomarticl...</td>\n",
              "      <td>0.137166</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>this probably wo nt come up that often  but i ...</td>\n",
              "      <td>27</td>\n",
              "      <td>27_http_need_titletext_mentalflosscomarticle31...</td>\n",
              "      <td>[http, need, titletext, mentalflosscomarticle3...</td>\n",
              "      <td>[ysk ozone destroy ethylene gas helping keep f...</td>\n",
              "      <td>http - need - titletext - mentalflosscomarticl...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>just in general with usbc be careful what you ...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1_how_using_use_have</td>\n",
              "      <td>[how, using, use, have, know, that, do, just, ...</td>\n",
              "      <td>[why should i know this  i believe it s better...</td>\n",
              "      <td>how - using - use - have - know - that - do - ...</td>\n",
              "      <td>0.701761</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1547</th>\n",
              "      <td>so if 73  are nt installed properly  is there ...</td>\n",
              "      <td>22</td>\n",
              "      <td>22_nt_ie_manual_safe</td>\n",
              "      <td>[nt, ie, manual, safe, local, mind, any, them,...</td>\n",
              "      <td>[also  for children around two and older   a p...</td>\n",
              "      <td>nt - ie - manual - safe - local - mind - any -...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1548</th>\n",
              "      <td>certified child passenger safety technician he...</td>\n",
              "      <td>22</td>\n",
              "      <td>22_nt_ie_manual_safe</td>\n",
              "      <td>[nt, ie, manual, safe, local, mind, any, them,...</td>\n",
              "      <td>[also  for children around two and older   a p...</td>\n",
              "      <td>nt - ie - manual - safe - local - mind - any -...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1549</th>\n",
              "      <td>or you can vent all your problems right here o...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1_how_using_use_have</td>\n",
              "      <td>[how, using, use, have, know, that, do, just, ...</td>\n",
              "      <td>[why should i know this  i believe it s better...</td>\n",
              "      <td>how - using - use - have - know - that - do - ...</td>\n",
              "      <td>0.757225</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1550</th>\n",
              "      <td>i think this is usa only  yes it s similar to ...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1_how_using_use_have</td>\n",
              "      <td>[how, using, use, have, know, that, do, just, ...</td>\n",
              "      <td>[why should i know this  i believe it s better...</td>\n",
              "      <td>how - using - use - have - know - that - do - ...</td>\n",
              "      <td>0.381640</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1551</th>\n",
              "      <td>http  wwwcrisistextlineorg designed for teens ...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1_how_using_use_have</td>\n",
              "      <td>[how, using, use, have, know, that, do, just, ...</td>\n",
              "      <td>[why should i know this  i believe it s better...</td>\n",
              "      <td>how - using - use - have - know - that - do - ...</td>\n",
              "      <td>0.695914</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1552 rows × 8 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3210e3c3-32cc-4380-ad0c-573b6b3f1914')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3210e3c3-32cc-4380-ad0c-573b6b3f1914 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3210e3c3-32cc-4380-ad0c-573b6b3f1914');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-618d5ea6-ccaa-44c4-91cf-40a0b0f9af6f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-618d5ea6-ccaa-44c4-91cf-40a0b0f9af6f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-618d5ea6-ccaa-44c4-91cf-40a0b0f9af6f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                               Document  Topic  \\\n",
              "0     informative post and truly helpful it s easy t...     32   \n",
              "1     actually that is such a good tip it s not what...      1   \n",
              "2     do nt put it in the fridge with the peel on   ...     27   \n",
              "3     this probably wo nt come up that often  but i ...     27   \n",
              "4     just in general with usbc be careful what you ...     -1   \n",
              "...                                                 ...    ...   \n",
              "1547  so if 73  are nt installed properly  is there ...     22   \n",
              "1548  certified child passenger safety technician he...     22   \n",
              "1549  or you can vent all your problems right here o...     -1   \n",
              "1550  i think this is usa only  yes it s similar to ...     -1   \n",
              "1551  http  wwwcrisistextlineorg designed for teens ...     -1   \n",
              "\n",
              "                                                   Name  \\\n",
              "0     32_ticketing_wwwnydailynewscomnewyorkbrooklynb...   \n",
              "1                                 1_have_isnt_found_for   \n",
              "2     27_http_need_titletext_mentalflosscomarticle31...   \n",
              "3     27_http_need_titletext_mentalflosscomarticle31...   \n",
              "4                                 -1_how_using_use_have   \n",
              "...                                                 ...   \n",
              "1547                               22_nt_ie_manual_safe   \n",
              "1548                               22_nt_ie_manual_safe   \n",
              "1549                              -1_how_using_use_have   \n",
              "1550                              -1_how_using_use_have   \n",
              "1551                              -1_how_using_use_have   \n",
              "\n",
              "                                         Representation  \\\n",
              "0     [ticketing, wwwnydailynewscomnewyorkbrooklynbr...   \n",
              "1     [have, isnt, found, for, tip, do, post, then, ...   \n",
              "2     [http, need, titletext, mentalflosscomarticle3...   \n",
              "3     [http, need, titletext, mentalflosscomarticle3...   \n",
              "4     [how, using, use, have, know, that, do, just, ...   \n",
              "...                                                 ...   \n",
              "1547  [nt, ie, manual, safe, local, mind, any, them,...   \n",
              "1548  [nt, ie, manual, safe, local, mind, any, them,...   \n",
              "1549  [how, using, use, have, know, that, do, just, ...   \n",
              "1550  [how, using, use, have, know, that, do, just, ...   \n",
              "1551  [how, using, use, have, know, that, do, just, ...   \n",
              "\n",
              "                                    Representative_Docs  \\\n",
              "0     [i m from canada what s a snow emergency , wit...   \n",
              "1     [this is      , thanks so much for this , than...   \n",
              "2     [ysk ozone destroy ethylene gas helping keep f...   \n",
              "3     [ysk ozone destroy ethylene gas helping keep f...   \n",
              "4     [why should i know this  i believe it s better...   \n",
              "...                                                 ...   \n",
              "1547  [also  for children around two and older   a p...   \n",
              "1548  [also  for children around two and older   a p...   \n",
              "1549  [why should i know this  i believe it s better...   \n",
              "1550  [why should i know this  i believe it s better...   \n",
              "1551  [why should i know this  i believe it s better...   \n",
              "\n",
              "                                            Top_n_words  Probability  \\\n",
              "0     ticketing - wwwnydailynewscomnewyorkbrooklynbr...     0.344923   \n",
              "1     have - isnt - found - for - tip - do - post - ...     0.597480   \n",
              "2     http - need - titletext - mentalflosscomarticl...     0.137166   \n",
              "3     http - need - titletext - mentalflosscomarticl...     1.000000   \n",
              "4     how - using - use - have - know - that - do - ...     0.701761   \n",
              "...                                                 ...          ...   \n",
              "1547  nt - ie - manual - safe - local - mind - any -...     1.000000   \n",
              "1548  nt - ie - manual - safe - local - mind - any -...     1.000000   \n",
              "1549  how - using - use - have - know - that - do - ...     0.757225   \n",
              "1550  how - using - use - have - know - that - do - ...     0.381640   \n",
              "1551  how - using - use - have - know - that - do - ...     0.695914   \n",
              "\n",
              "      Representative_document  \n",
              "0                       False  \n",
              "1                       False  \n",
              "2                       False  \n",
              "3                        True  \n",
              "4                       False  \n",
              "...                       ...  \n",
              "1547                    False  \n",
              "1548                    False  \n",
              "1549                    False  \n",
              "1550                    False  \n",
              "1551                    False  \n",
              "\n",
              "[1552 rows x 8 columns]"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "topic_model_improved.get_document_info(comments)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVZRhC4_A2uk"
      },
      "source": [
        "We now use lemmatisation to avoid repeating keywords in the topics having the same root.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIXWGn3bOub8",
        "outputId": "de41d1ac-01c0-4b7a-a89a-1733ab4e6074"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "class LemmaTokenizer:\n",
        "    def __init__(self):\n",
        "        self.wnl = WordNetLemmatizer()\n",
        "    def __call__(self, doc):\n",
        "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
        "vectorizer_model= CountVectorizer(tokenizer=LemmaTokenizer())\n",
        "\n",
        "lemma_topic_model = BERTopic(vectorizer_model=vectorizer_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I76ABgodcOgs"
      },
      "outputs": [],
      "source": [
        "topics_nl, probs_nl=lemma_topic_model.fit_transform(comments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cNTqcfFYjXdz",
        "outputId": "1226a0c4-0669-4826-fe58-da8c4c54de27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'u_mass_coherence': -0.5560704088811599, 'c_v_coherence': 0.6176426199262081, 'c_uci_coherence': -2.2208819351595697, 'c_npmi_coherence': 0.013289891356831302}\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"lemma_topic_model\",\n  \"rows\": 38,\n  \"fields\": [\n    {\n      \"column\": \"Topic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11,\n        \"min\": -1,\n        \"max\": 36,\n        \"num_unique_values\": 38,\n        \"samples\": [\n          32,\n          35,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 47,\n        \"min\": 12,\n        \"max\": 276,\n        \"num_unique_values\": 26,\n        \"samples\": [\n          36,\n          26,\n          276\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 38,\n        \"samples\": [\n          \"32_cmv_zika_disease_of\",\n          \"35_bit_byte_8_b\",\n          \"3_doctor_and_to_a\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Representation\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Representative_Docs\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-41fccf29-ab50-408c-804b-b21115ae571e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic</th>\n",
              "      <th>Count</th>\n",
              "      <th>Name</th>\n",
              "      <th>Representation</th>\n",
              "      <th>Representative_Docs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1</td>\n",
              "      <td>276</td>\n",
              "      <td>-1_you_to_it_a</td>\n",
              "      <td>[you, to, it, a, i, the, this, on, and, that]</td>\n",
              "      <td>[in any new relationship  people are infatuate...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>130</td>\n",
              "      <td>0_tax_free_you_for</td>\n",
              "      <td>[tax, free, you, for, and, year, pay, turbotax...</td>\n",
              "      <td>[many of us have been paying turbo tax or hamp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>115</td>\n",
              "      <td>1_this_thanks_thank_know</td>\n",
              "      <td>[this, thanks, thank, know, great, why, someth...</td>\n",
              "      <td>[this is      , thanks so much for this , than...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>82</td>\n",
              "      <td>2_she_her_i_a</td>\n",
              "      <td>[she, her, i, a, my, spot, disabled, and, he, to]</td>\n",
              "      <td>[when i broke my ankle  i got a temporary plac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>75</td>\n",
              "      <td>3_doctor_and_to_a</td>\n",
              "      <td>[doctor, and, to, a, i, nt, they, that, my, had]</td>\n",
              "      <td>[this reminds me of a story i read by richard ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>4</td>\n",
              "      <td>72</td>\n",
              "      <td>4_text_library_book_it</td>\n",
              "      <td>[text, library, book, it, i, card, this, phone...</td>\n",
              "      <td>[ kiwix   http  wwwkiwixorgwikimain_page  is a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5</td>\n",
              "      <td>42</td>\n",
              "      <td>5_gas_ethanol_engine_fuel</td>\n",
              "      <td>[gas, ethanol, engine, fuel, water, gasoline, ...</td>\n",
              "      <td>[i work at a lawn equipment shop and around 80...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>6</td>\n",
              "      <td>42</td>\n",
              "      <td>6_antivirus_bitdefender_avast_the</td>\n",
              "      <td>[antivirus, bitdefender, avast, the, of, is, a...</td>\n",
              "      <td>[  tl  dr if you are running windows and need ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>7</td>\n",
              "      <td>38</td>\n",
              "      <td>7_number_by_divisible_7</td>\n",
              "      <td>[number, by, divisible, 7, digit, 2, rule, 11,...</td>\n",
              "      <td>[gt  if you do nt know then halve the last 2 d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "      <td>8_sore_cold_abreva_mouth</td>\n",
              "      <td>[sore, cold, abreva, mouth, the, lip, it, i, a...</td>\n",
              "      <td>[i have a sure remedy and guy s and      s it ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>9</td>\n",
              "      <td>35</td>\n",
              "      <td>9_backup_data_drive_up</td>\n",
              "      <td>[backup, data, drive, up, back, my, i, to, ext...</td>\n",
              "      <td>[i make sure i have at least two copies of eve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>10</td>\n",
              "      <td>35</td>\n",
              "      <td>10_http_v_work_wwwyoutubecomwatch</td>\n",
              "      <td>[http, v, work, wwwyoutubecomwatch, warlizardf...</td>\n",
              "      <td>[https  wwwyoutubecomwatch  v  d9cqjfwesv0,   ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>11</td>\n",
              "      <td>33</td>\n",
              "      <td>11_ysk_this_country_u</td>\n",
              "      <td>[ysk, this, country, u, nonamerican, reddit, i...</td>\n",
              "      <td>[ysk not all reddit users are from the us, ysk...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>12</td>\n",
              "      <td>32</td>\n",
              "      <td>12_ups_item_box_package</td>\n",
              "      <td>[ups, item, box, package, claim, store, the, i...</td>\n",
              "      <td>[  3 years working for a ups store   hey guys ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>13</td>\n",
              "      <td>31</td>\n",
              "      <td>13_insurance_record_note_company</td>\n",
              "      <td>[insurance, record, note, company, health, pat...</td>\n",
              "      <td>[i m a health insurance employee in arkansas s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>14</td>\n",
              "      <td>30</td>\n",
              "      <td>14_2gb_extra_storage_checkup</td>\n",
              "      <td>[2gb, extra, storage, checkup, google, gb, sec...</td>\n",
              "      <td>[remindme  one year  do 2017 google security c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>15</td>\n",
              "      <td>29</td>\n",
              "      <td>15_call_debt_number_collector</td>\n",
              "      <td>[call, debt, number, collector, phone, i, you,...</td>\n",
              "      <td>[i did not know this  when i first got my cell...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>16</td>\n",
              "      <td>28</td>\n",
              "      <td>16_grain_spent_dog_taste</td>\n",
              "      <td>[grain, spent, dog, taste, chicken, filter, th...</td>\n",
              "      <td>[some breweries use the spent grains themselve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>17</td>\n",
              "      <td>28</td>\n",
              "      <td>17_ball_oil_bath_peppermint</td>\n",
              "      <td>[ball, oil, bath, peppermint, eucalyptus, feel...</td>\n",
              "      <td>[wife gave me a bath with essential oils  euca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>18</td>\n",
              "      <td>28</td>\n",
              "      <td>18_warranty_cable_2011_macbook</td>\n",
              "      <td>[warranty, cable, 2011, macbook, model, pro, t...</td>\n",
              "      <td>[wait  what    back in august of 2011  i broug...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>19</td>\n",
              "      <td>26</td>\n",
              "      <td>19_game_play_played_i</td>\n",
              "      <td>[game, play, played, i, adventure, http, nt, f...</td>\n",
              "      <td>[i looked for dark sun  shattered lands  and i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>20</td>\n",
              "      <td>26</td>\n",
              "      <td>20_cat_collar_orange_lost</td>\n",
              "      <td>[cat, collar, orange, lost, we, reflective, an...</td>\n",
              "      <td>[our 2 indoor cats have dog collars  we do nt ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>21</td>\n",
              "      <td>25</td>\n",
              "      <td>21_vote_caucus_voting_election</td>\n",
              "      <td>[vote, caucus, voting, election, primary, poli...</td>\n",
              "      <td>[it s one of the best ways to get moderately i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>22</td>\n",
              "      <td>24</td>\n",
              "      <td>22_speed_data_tmobile_deprioritized</td>\n",
              "      <td>[speed, data, tmobile, deprioritized, slower, ...</td>\n",
              "      <td>[from their own website   to ensure that we  r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>23</td>\n",
              "      <td>22</td>\n",
              "      <td>23_seat_car_correctly_installed</td>\n",
              "      <td>[seat, car, correctly, installed, kid, child, ...</td>\n",
              "      <td>[also  for children around two and older   a p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>24</td>\n",
              "      <td>21</td>\n",
              "      <td>24_usps_rate_package_mail</td>\n",
              "      <td>[usps, rate, package, mail, ship, priority, av...</td>\n",
              "      <td>[the usps discontinues priority mail express f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>25</td>\n",
              "      <td>21</td>\n",
              "      <td>25_noaa_weather_http_forecast</td>\n",
              "      <td>[noaa, weather, http, forecast, listen, he, si...</td>\n",
              "      <td>[they have an app  too not sure if it shows th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>26</td>\n",
              "      <td>19</td>\n",
              "      <td>26_element_ublock_overlay_origin</td>\n",
              "      <td>[element, ublock, overlay, origin, adblock, bl...</td>\n",
              "      <td>[google is making all users switch over to goo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>27</td>\n",
              "      <td>18</td>\n",
              "      <td>27_ticket_lottery_windfall_sign</td>\n",
              "      <td>[ticket, lottery, windfall, sign, winning, sig...</td>\n",
              "      <td>[i think where i live  quebec  it is law to ha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>28</td>\n",
              "      <td>18</td>\n",
              "      <td>28_account_email_site_delete</td>\n",
              "      <td>[account, email, site, delete, my, password, w...</td>\n",
              "      <td>[discovered this  https  wwwgooglecomsettingsa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>29</td>\n",
              "      <td>18</td>\n",
              "      <td>29_banana_apple_fruit_ripen</td>\n",
              "      <td>[banana, apple, fruit, ripen, half, ethylene, ...</td>\n",
              "      <td>[also  this makes apples useful with bananas i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>30</td>\n",
              "      <td>17</td>\n",
              "      <td>30_statistically_significant_meaningful_effect</td>\n",
              "      <td>[statistically, significant, meaningful, effec...</td>\n",
              "      <td>[gt  the researcher has used mathematics to de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>31</td>\n",
              "      <td>17</td>\n",
              "      <td>31_installation_install_software_custom</td>\n",
              "      <td>[installation, install, software, custom, you,...</td>\n",
              "      <td>[this is kind of misleading for many programs ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>32</td>\n",
              "      <td>15</td>\n",
              "      <td>32_cmv_zika_disease_of</td>\n",
              "      <td>[cmv, zika, disease, of, common, that, wa, mic...</td>\n",
              "      <td>[this would substantiate my claim  that it ca ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>33</td>\n",
              "      <td>12</td>\n",
              "      <td>33_book_subject_practise_teach</td>\n",
              "      <td>[book, subject, practise, teach, podcast, read...</td>\n",
              "      <td>[i m a teacher i teach english  language arts ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>34</td>\n",
              "      <td>12</td>\n",
              "      <td>34_snowthrower_snowblower_flamethrower_tiller</td>\n",
              "      <td>[snowthrower, snowblower, flamethrower, tiller...</td>\n",
              "      <td>[what the      is a snowthrower  is that a les...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>35</td>\n",
              "      <td>12</td>\n",
              "      <td>35_bit_byte_8_b</td>\n",
              "      <td>[bit, byte, 8, b, 1, there, inclusive, kbmbgb,...</td>\n",
              "      <td>[little b means bits  big b means bytes  also ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>36</td>\n",
              "      <td>12</td>\n",
              "      <td>36_snow_emergency_car_route</td>\n",
              "      <td>[snow, emergency, car, route, will, garage, vi...</td>\n",
              "      <td>[gt  it was devised to help clear out snow eme...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-41fccf29-ab50-408c-804b-b21115ae571e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-41fccf29-ab50-408c-804b-b21115ae571e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-41fccf29-ab50-408c-804b-b21115ae571e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-4ca15461-26d5-42e9-8ace-210ad6fc0c69\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4ca15461-26d5-42e9-8ace-210ad6fc0c69')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-4ca15461-26d5-42e9-8ace-210ad6fc0c69 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "    Topic  Count                                            Name  \\\n",
              "0      -1    276                                  -1_you_to_it_a   \n",
              "1       0    130                              0_tax_free_you_for   \n",
              "2       1    115                        1_this_thanks_thank_know   \n",
              "3       2     82                                   2_she_her_i_a   \n",
              "4       3     75                               3_doctor_and_to_a   \n",
              "5       4     72                          4_text_library_book_it   \n",
              "6       5     42                       5_gas_ethanol_engine_fuel   \n",
              "7       6     42               6_antivirus_bitdefender_avast_the   \n",
              "8       7     38                         7_number_by_divisible_7   \n",
              "9       8     36                        8_sore_cold_abreva_mouth   \n",
              "10      9     35                          9_backup_data_drive_up   \n",
              "11     10     35               10_http_v_work_wwwyoutubecomwatch   \n",
              "12     11     33                           11_ysk_this_country_u   \n",
              "13     12     32                         12_ups_item_box_package   \n",
              "14     13     31                13_insurance_record_note_company   \n",
              "15     14     30                    14_2gb_extra_storage_checkup   \n",
              "16     15     29                   15_call_debt_number_collector   \n",
              "17     16     28                        16_grain_spent_dog_taste   \n",
              "18     17     28                     17_ball_oil_bath_peppermint   \n",
              "19     18     28                  18_warranty_cable_2011_macbook   \n",
              "20     19     26                           19_game_play_played_i   \n",
              "21     20     26                       20_cat_collar_orange_lost   \n",
              "22     21     25                  21_vote_caucus_voting_election   \n",
              "23     22     24             22_speed_data_tmobile_deprioritized   \n",
              "24     23     22                 23_seat_car_correctly_installed   \n",
              "25     24     21                       24_usps_rate_package_mail   \n",
              "26     25     21                   25_noaa_weather_http_forecast   \n",
              "27     26     19                26_element_ublock_overlay_origin   \n",
              "28     27     18                 27_ticket_lottery_windfall_sign   \n",
              "29     28     18                    28_account_email_site_delete   \n",
              "30     29     18                     29_banana_apple_fruit_ripen   \n",
              "31     30     17  30_statistically_significant_meaningful_effect   \n",
              "32     31     17         31_installation_install_software_custom   \n",
              "33     32     15                          32_cmv_zika_disease_of   \n",
              "34     33     12                  33_book_subject_practise_teach   \n",
              "35     34     12   34_snowthrower_snowblower_flamethrower_tiller   \n",
              "36     35     12                                 35_bit_byte_8_b   \n",
              "37     36     12                     36_snow_emergency_car_route   \n",
              "\n",
              "                                       Representation  \\\n",
              "0       [you, to, it, a, i, the, this, on, and, that]   \n",
              "1   [tax, free, you, for, and, year, pay, turbotax...   \n",
              "2   [this, thanks, thank, know, great, why, someth...   \n",
              "3   [she, her, i, a, my, spot, disabled, and, he, to]   \n",
              "4    [doctor, and, to, a, i, nt, they, that, my, had]   \n",
              "5   [text, library, book, it, i, card, this, phone...   \n",
              "6   [gas, ethanol, engine, fuel, water, gasoline, ...   \n",
              "7   [antivirus, bitdefender, avast, the, of, is, a...   \n",
              "8   [number, by, divisible, 7, digit, 2, rule, 11,...   \n",
              "9   [sore, cold, abreva, mouth, the, lip, it, i, a...   \n",
              "10  [backup, data, drive, up, back, my, i, to, ext...   \n",
              "11  [http, v, work, wwwyoutubecomwatch, warlizardf...   \n",
              "12  [ysk, this, country, u, nonamerican, reddit, i...   \n",
              "13  [ups, item, box, package, claim, store, the, i...   \n",
              "14  [insurance, record, note, company, health, pat...   \n",
              "15  [2gb, extra, storage, checkup, google, gb, sec...   \n",
              "16  [call, debt, number, collector, phone, i, you,...   \n",
              "17  [grain, spent, dog, taste, chicken, filter, th...   \n",
              "18  [ball, oil, bath, peppermint, eucalyptus, feel...   \n",
              "19  [warranty, cable, 2011, macbook, model, pro, t...   \n",
              "20  [game, play, played, i, adventure, http, nt, f...   \n",
              "21  [cat, collar, orange, lost, we, reflective, an...   \n",
              "22  [vote, caucus, voting, election, primary, poli...   \n",
              "23  [speed, data, tmobile, deprioritized, slower, ...   \n",
              "24  [seat, car, correctly, installed, kid, child, ...   \n",
              "25  [usps, rate, package, mail, ship, priority, av...   \n",
              "26  [noaa, weather, http, forecast, listen, he, si...   \n",
              "27  [element, ublock, overlay, origin, adblock, bl...   \n",
              "28  [ticket, lottery, windfall, sign, winning, sig...   \n",
              "29  [account, email, site, delete, my, password, w...   \n",
              "30  [banana, apple, fruit, ripen, half, ethylene, ...   \n",
              "31  [statistically, significant, meaningful, effec...   \n",
              "32  [installation, install, software, custom, you,...   \n",
              "33  [cmv, zika, disease, of, common, that, wa, mic...   \n",
              "34  [book, subject, practise, teach, podcast, read...   \n",
              "35  [snowthrower, snowblower, flamethrower, tiller...   \n",
              "36  [bit, byte, 8, b, 1, there, inclusive, kbmbgb,...   \n",
              "37  [snow, emergency, car, route, will, garage, vi...   \n",
              "\n",
              "                                  Representative_Docs  \n",
              "0   [in any new relationship  people are infatuate...  \n",
              "1   [many of us have been paying turbo tax or hamp...  \n",
              "2   [this is      , thanks so much for this , than...  \n",
              "3   [when i broke my ankle  i got a temporary plac...  \n",
              "4   [this reminds me of a story i read by richard ...  \n",
              "5   [ kiwix   http  wwwkiwixorgwikimain_page  is a...  \n",
              "6   [i work at a lawn equipment shop and around 80...  \n",
              "7   [  tl  dr if you are running windows and need ...  \n",
              "8   [gt  if you do nt know then halve the last 2 d...  \n",
              "9   [i have a sure remedy and guy s and      s it ...  \n",
              "10  [i make sure i have at least two copies of eve...  \n",
              "11  [https  wwwyoutubecomwatch  v  d9cqjfwesv0,   ...  \n",
              "12  [ysk not all reddit users are from the us, ysk...  \n",
              "13  [  3 years working for a ups store   hey guys ...  \n",
              "14  [i m a health insurance employee in arkansas s...  \n",
              "15  [remindme  one year  do 2017 google security c...  \n",
              "16  [i did not know this  when i first got my cell...  \n",
              "17  [some breweries use the spent grains themselve...  \n",
              "18  [wife gave me a bath with essential oils  euca...  \n",
              "19  [wait  what    back in august of 2011  i broug...  \n",
              "20  [i looked for dark sun  shattered lands  and i...  \n",
              "21  [our 2 indoor cats have dog collars  we do nt ...  \n",
              "22  [it s one of the best ways to get moderately i...  \n",
              "23  [from their own website   to ensure that we  r...  \n",
              "24  [also  for children around two and older   a p...  \n",
              "25  [the usps discontinues priority mail express f...  \n",
              "26  [they have an app  too not sure if it shows th...  \n",
              "27  [google is making all users switch over to goo...  \n",
              "28  [i think where i live  quebec  it is law to ha...  \n",
              "29  [discovered this  https  wwwgooglecomsettingsa...  \n",
              "30  [also  this makes apples useful with bananas i...  \n",
              "31  [gt  the researcher has used mathematics to de...  \n",
              "32  [this is kind of misleading for many programs ...  \n",
              "33  [this would substantiate my claim  that it ca ...  \n",
              "34  [i m a teacher i teach english  language arts ...  \n",
              "35  [what the      is a snowthrower  is that a les...  \n",
              "36  [little b means bits  big b means bytes  also ...  \n",
              "37  [gt  it was devised to help clear out snow eme...  "
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "coherence_metrics=calculate_coherence_metrics(lemma_topic_model,comments,topics_nl)\n",
        "print(coherence_metrics)\n",
        "lemma_topic_model.get_topic_info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubut74ZzBKn7"
      },
      "source": [
        "The final step of this model training process is to select the best parameters for the different models. To do so, we need to compare the topic-building pipelines and identify the one that represents the optimal set of hyperparameters.\n",
        "\n",
        "In practice, for unsupervised models in general ,and topic inference in particular, the metrics are not 100% reliable, and human judgment is required. For that reason, we select the top 5 models and make a final decision after visualizing the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoJ2h1KVlYCE"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from umap import UMAP\n",
        "from sklearn.decomposition import PCA\n",
        "from hdbscan import HDBSCAN\n",
        "from sklearn.cluster import KMeans\n",
        "from bertopic.vectorizers import ClassTfidfTransformer\n",
        "from transformers.pipelines import pipeline\n",
        "import tensorflow_hub\n",
        "##hyperparameters tuning\n",
        "\n",
        "#pipeline 1\n",
        "representation_model = KeyBERTInspired()\n",
        "\n",
        "p1 = BERTopic( representation_model=representation_model\n",
        "              ,low_memory=True,calculate_probabilities=True,verbose=True)\n",
        "\n",
        "#pipeline 2\n",
        "vectorizer_model= CountVectorizer(tokenizer=LemmaTokenizer())\n",
        "\n",
        "p2 = BERTopic(vectorizer_model=vectorizer_model\n",
        "              ,low_memory=True,calculate_probabilities=True,verbose=True)\n",
        "\n",
        "#pipelines 3\n",
        "#---embedding extractors\n",
        "extractors=[ SentenceTransformer(\"all-MiniLM-L6-v2\"),\n",
        "            #pipeline(\"feature-extraction\", model=\"distilbert-base-cased\"),\n",
        "            tensorflow_hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")]\n",
        "#---dimentiality reduction\n",
        "dim_red=[ UMAP(n_neighbors=15, n_components=10, min_dist=0.0, metric='cosine'),\n",
        "         UMAP(n_neighbors=20, n_components=15, min_dist=0.0, metric='euclidean'),\n",
        "         PCA(n_components=5),\n",
        "         PCA(n_components=10)]\n",
        "#---clustering\n",
        "clus=[HDBSCAN(min_cluster_size=10, metric='euclidean', cluster_selection_method='eom', prediction_data=True),\n",
        "HDBSCAN(min_cluster_size=10, metric='manhattan', cluster_selection_method='eom', prediction_data=True),\n",
        "HDBSCAN(min_cluster_size=5, metric='euclidean', cluster_selection_method='eom', prediction_data=True),\n",
        "KMeans(n_clusters=20)\n",
        "]\n",
        "#---bag-of-words extraction\n",
        "vectorisers=[CountVectorizer(stop_words=\"english\"),\n",
        "             CountVectorizer(tokenizer=LemmaTokenizer())\n",
        "             ,CountVectorizer(tokenizer=LemmaTokenizer(),stop_words=\"english\")\n",
        "             ,CountVectorizer(tokenizer=LemmaTokenizer(),\n",
        "              ngram_range=(1, 2),stop_words=\"english\")\n",
        "             ,CountVectorizer(tokenizer=LemmaTokenizer(),\n",
        "              ngram_range=(2, 2),stop_words=\"english\")]\n",
        "#---topic representation\n",
        "reps=[\n",
        "    #ClassTfidfTransformer(),\n",
        "    KeyBERTInspired()]\n",
        "general_params =[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFC5zq2A5Agi"
      },
      "outputs": [],
      "source": [
        "meta=[]\n",
        "pipelines=[]\n",
        "for e in extractors:\n",
        "  for d in dim_red:\n",
        "    for c in clus:\n",
        "      for v in vectorisers:\n",
        "        for r in reps:\n",
        "          meta.append({\"Topic extraction\": str(e),\n",
        "                       \"Dimensionality Reduction\":str(d),\n",
        "                       \"Clustering\":str(c),\n",
        "                       \"Vectorization\":str(v),\n",
        "                       \"Representation\":str(r)})\n",
        "          pipelines.append(BERTopic(  embedding_model=e,\n",
        "                                      umap_model=d,\n",
        "                                      hdbscan_model=c,\n",
        "                                      vectorizer_model=v,\n",
        "                                      representation_model=r,\n",
        "                                      low_memory=True,\n",
        "                                      calculate_probabilities=True,\n",
        "                                      verbose=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVoNS9VQ7Wta",
        "outputId": "2580c92f-6504-47b4-86c8-b8da714fb3f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------RUN0-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': \"CountVectorizer(stop_words='english')\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[6, 0, 27, 27, -1, 0, 7, 5, 5, 5, -1, 22, 16, 22, 5, -1, 1, 6, 0, 0, -1, 16, 5, 11, 11, 21, -1, 0, 0, 0, 0, 0, 27, 26, 0, -1, 4, -1, -1, 0, 11, -1, 25, 11, 25, -1, 25, 25, 11, 0, 25, 11, 5, 5, 5, 14, 14, 0, 5, -1, 1, 0, -1, 5, -1, 5, 4, 1, -1, 4, 26, -1, 1, 1, 1, 4, -1, -1, 0, -1, 14, 14, 29, 14, 14, 21, 14, 1, 0, 1, 1, 5, 14, -1, -1, 5, 5, 1, -1, -1, 22, 15, 0, 15, 32, 26, -1, -1, -1, -1, -1, 16, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, 11, 15, 25, -1, 16, 0, 11, 16, 0, 19, 0, 16, 16, 6, 11, 0, 16, -1, 5, 5, 6, 31, 19, 5, 6, 0, 0, -1, 5, 5, 11, 5, -1, 5, 1, -1, -1, 2, 0, -1, 26, 2, -1, -1, 6, 15, -1, 0, 5, -1, -1, -1, 19, 5, 2, 34, 19, -1, 0, 0, 19, -1, 16, 5, 1, 0, 16, 24, -1, 19, 14, 0, 31, -1, 13, 13, 14, 0, 14, 0, 14, 14, 34, 14, 9, 14, 1, -1, 1, 29, -1, 14, 14, -1, -1, 1, 0, 1, 13, 1, 1, 1, -1, 13, -1, -1, -1, 31, 31, 31, 31, 13, 13, 31, 21, 31, 3, 13, 13, 4, 4, -1, 13, -1, -1, 15, 30, 0, 15, 13, 22, 6, -1, 7, 7, 7, 1, 1, -1, 15, -1, -1, 0, -1, 0, -1, -1, 4, 4, 4, 4, 1, 4, 32, 4, 4, 21, -1, 1, -1, 4, 7, 7, -1, 13, 2, 22, 2, 22, 0, 22, 22, -1, 13, 5, 32, 13, 4, 4, 4, 16, 4, 4, 4, 1, 4, 28, 4, 11, 4, 25, 0, 34, -1, -1, -1, 15, 6, 5, 7, 32, -1, -1, 6, -1, 6, -1, 6, 2, -1, 24, 6, 20, 6, -1, 19, 1, -1, 19, -1, 21, 21, 5, 34, 19, -1, 3, 19, -1, 33, 0, 0, 5, 24, 19, 24, 24, 6, 15, 16, 0, 16, 24, 4, 16, 1, 21, 16, 24, -1, 16, 19, 5, 16, 3, 5, 0, 3, 10, 10, -1, 5, 5, 5, 24, 5, 24, 21, 29, -1, 24, 10, 10, 10, 10, 10, 10, 10, -1, 10, 13, 0, 13, -1, 10, 1, 31, 31, 21, 24, -1, 15, 31, 21, 31, -1, 20, -1, -1, -1, -1, -1, 20, 20, 0, 20, 1, 1, 1, -1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 20, 6, 4, 34, 4, 1, 5, 26, 4, 4, 4, 20, 0, 4, 5, 1, 4, -1, 4, -1, 0, 13, 4, 4, 4, 4, 1, 1, 7, 0, 1, 1, 1, 4, 7, 1, 1, 1, 2, 1, -1, -1, 7, 2, 7, 32, 7, -1, 0, 0, 21, 2, 22, 1, -1, 8, 6, 8, 8, 6, 1, 34, 13, 13, 16, -1, 3, 6, 6, 8, 8, 8, 8, 8, 15, 24, 8, 4, 8, 33, 24, -1, 18, 33, 8, 6, 0, 8, -1, 33, -1, 1, -1, -1, 33, 7, 21, 7, 7, 1, -1, 4, 1, -1, 15, 7, 7, 0, 7, 7, 12, 12, 12, 12, 3, 7, 7, 0, 22, 20, 3, 3, 3, 3, 3, 3, 2, 12, 33, -1, 18, 18, 18, 33, 24, 1, 12, 12, 11, -1, 12, 33, 12, -1, 26, 4, 3, 3, 21, 24, -1, 5, 3, 11, 3, 3, 21, 3, 3, 15, 3, 3, 0, 10, 3, 21, 21, 26, -1, 9, 9, 9, 9, 9, 9, 12, 9, 9, 1, 1, -1, 0, -1, 19, 6, 15, 9, 9, -1, 9, 9, 9, 1, 9, 9, 1, 1, 1, 1, -1, 1, 8, 1, 1, 1, -1, 4, 1, -1, 8, 10, 0, 8, 31, 0, 8, 6, 8, 6, 8, 6, 8, 34, 8, 11, 5, 11, 0, -1, -1, 8, 8, 8, 0, 8, 6, 8, 18, 24, 8, 6, 15, 7, 7, 7, 7, -1, 7, 7, -1, -1, 7, 2, 16, 7, 7, 19, 27, 1, 16, -1, 12, 12, 15, 12, 0, -1, -1, 12, -1, 27, 3, 0, -1, -1, 12, 0, 2, 0, 3, 3, 8, 32, 8, 3, 3, 0, 3, 3, 32, 0, 3, -1, 18, 3, 22, 18, 22, 3, 10, 19, 3, 18, -1, -1, 18, 18, 18, 0, -1, 11, 0, 31, 2, 2, 2, 26, 32, 22, 1, -1, 18, 0, 10, 22, 10, 1, 1, 1, 32, -1, 2, 2, -1, -1, 5, 4, 4, 9, 9, 1, 9, 9, 1, 9, 9, 9, 9, 1, 1, 2, 2, 2, 2, 2, -1, -1, 2, 9, 2, 2, 2, 2, -1, 2, 2, 9, 15, 34, 1, 2, 2, 15, 2, 2, 2, 1, -1, 6, 8, 8, 6, 8, 8, 8, 8, -1, 28, 28, 28, 28, 28, 28, 15, 28, -1, 28, -1, -1, 4, 11, 0, 25, 10, 11, -1, 10, 17, -1, 17, 17, 17, 15, 15, 5, 10, 32, 10, 10, 10, 10, 0, 11, 27, 0, 11, 17, 26, 26, 26, 0, 20, 0, 27, 27, 1, 0, 27, 27, 0, 27, 19, 27, -1, 19, 0, -1, 27, 19, 0, 19, 14, 19, 8, -1, 0, -1, -1, -1, 11, 8, 2, 0, 14, 22, 25, -1, 22, 22, 22, 22, 1, 10, 22, -1, 25, -1, 0, 0, 32, 5, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, -1, 2, 2, 2, 2, 2, -1, 2, 2, 2, 8, -1, 11, 15, 33, 24, -1, 20, 14, 0, 25, 13, -1, 29, 33, 19, 26, 28, 12, 28, 0, 28, 12, 11, 0, -1, 30, 30, -1, 30, 11, 12, -1, 25, 29, -1, -1, -1, 26, -1, 26, 29, 0, 29, 11, -1, 29, -1, 29, 24, -1, 4, 11, 30, 10, 5, 3, 23, 23, 23, -1, 0, 5, -1, 16, 5, 10, -1, -1, 17, 6, 6, 32, 6, 10, 17, 17, 10, 0, 8, 17, 17, 17, 0, 17, 2, 17, 23, 11, 23, 23, -1, 0, 27, 27, 0, -1, 27, 0, 27, 6, 6, 6, 6, 6, 6, -1, 6, 6, -1, 4, 21, 0, 20, 20, 1, -1, -1, -1, 4, -1, 26, 16, 1, 1, 5, 5, 1, -1, -1, 1, 21, 8, -1, 6, 16, 13, 7, 7, 5, -1, 5, -1, -1, -1, 15, -1, 3, 7, 25, -1, -1, 7, 25, 1, 0, 20, 7, 24, 7, 15, 33, 33, -1, 18, 4, 2, -1, 30, 29, 11, 11, 25, 29, 11, 21, 4, -1, -1, -1, 30, 11, -1, 1, 24, -1, 1, 5, 4, -1, 1, -1, 5, -1, -1, 5, 19, 5, 4, 23, 23, 1, 23, 11, 23, 18, 30, 3, 12, 34, 6, -1, 14, -1, 5, 24, 6, 6, 6, 6, 3, 6, 9, 6, 6, 13, -1, 6, 6, -1, -1, 13, 16, 30, 30, 30, 30, 2, 14, 30, -1, -1, 3, 12, 3, -1, 0, 10, 3, -1, 23, 18, 10, 20, 20, 23, 0, 0, -1, 3, 23, 0, 5, -1, 23, -1, -1, 23, 6, 20, -1, 0, 2, 1, -1, 20, -1, 0, 7, -1, 1, 7, 7, 5, -1, 2, 11, 11, 12, 26, 1, 11, 7, 15, -1, 1, 12, 12, 2, 1, 1, -1, 12, 1, 12, -1, 1, 18, 2, -1, 19, -1, 4, 9, 19, 17, 2, -1, 9, 1, -1, 14, 0, -1, 0, 9, 5, 9, 5, 27, 2, 3, -1, 2, 20, 0, 17, 17, 3, 25, 25, 0, 17, 11, 11, 17, 17, -1, 0, -1, 4, 10, 17, 3, 3, 10, 2, 3, 0, 5, 13, 10, -1, -1, 0, 14, 3, 14, 0, -1, 3, 25, -1, 0, 3, -1, 22, 3, 4, 4, 4, 2, 1, 2, 2, 23, 7, 4, 2, 2, 1, 14, 7, 9, 23, 0, 1, 7, 2, 13, 20, 14, 1, 7, 20, 1, 0, 12, 12, -1, 13, 1, 18, 18, 1, 33, 0, 32, 1, 26, 1, 1, 0, 17, 12, -1, -1, 1, 1, 9, 2, 4, -1, 34, 2, 2, 1, -1, -1, -1, 18, 9, 22, 1, 3, -1, 18, 3, 18, -1, 26, 16, 17, 0, 0, 3, 28, 17, 21, -1, 3, 3, 3, 10, 2, 32, 3, 4, 0, 29, 21, 4, 0, -1, 9, 9, 4, 4, 4, 8, 16, 4, 0, 3, 2, 7, -1, 2, 2, 10, 7, -1, 2, 28, 28, 2, 1, -1, -1, -1, -1, -1, 4, 0, 7, 17, 29, -1, 4, 12, -1, 0, 0, -1, -1, 29, 4, 30, 30, 12, 8, 3, 3, -1, 13, 12, 1, 2, 10, 0, 13, 23, 0, -1, 10, 20, 1, 9, 0, -1, -1, 1, 1, -1, 0, 9, 20, 1, 28, 23, 18, -1, -1, 15, 1, 18, 3, 3, 3, 18, 1, 21, 2, -1, 2, 2, 29, 3, 0, 18, 3, -1, 3, 2, 3, 3, 3, 0, 3, 18, 20, 10, 1, 0, 2, 2, 3, 8, 6, 17, 17, 0, 0, 2, 2, 3, 3, 23, 9, 28, 23, 23, -1, -1, -1]\n",
            "-------RUN1-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': 'CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7df3b00f5c90>)', 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[17, 30, 27, 27, -1, 34, -1, 4, 4, 4, -1, 22, 20, 22, 4, -1, 0, 17, -1, -1, -1, 20, 4, 9, 9, 21, -1, -1, 25, -1, 34, -1, 27, 11, -1, -1, 3, -1, -1, -1, 9, -1, 28, 9, 28, -1, 28, 28, 9, 34, 28, 9, 4, 4, 4, 14, 14, 30, 4, -1, 0, -1, -1, 4, -1, 4, 3, 0, -1, 3, 11, -1, 0, 0, 0, 3, -1, -1, -1, -1, 14, 14, -1, 14, 14, 21, 14, 0, 25, 0, 0, 4, 14, -1, -1, 4, 4, 0, -1, -1, 22, 15, 34, 15, 36, 11, -1, -1, -1, -1, -1, 20, -1, -1, -1, -1, -1, 21, 34, 4, 5, -1, 9, 15, 28, -1, 20, 25, 9, 20, 30, 18, -1, 20, 20, 17, 9, -1, 20, -1, 4, 4, 23, 32, 18, 4, 17, -1, 30, -1, 4, 4, 9, 4, -1, 4, 0, -1, -1, 1, -1, 1, 11, 1, -1, -1, 23, 15, 11, -1, 4, -1, -1, -1, 18, 4, 1, 11, 18, 0, -1, -1, 18, -1, 20, 4, 0, 25, 20, 26, 26, 18, 14, 25, 32, -1, 12, 12, 14, -1, 14, 30, 14, 14, -1, 14, 7, 14, 0, 22, 0, 21, -1, 14, 14, -1, -1, 0, 25, 0, 12, 0, 0, 0, -1, 12, -1, -1, -1, 32, 32, 32, 32, 12, 12, 32, 21, 32, 2, 12, 12, 3, 3, -1, 12, -1, -1, 15, 31, -1, -1, 12, 22, -1, -1, 5, 5, 5, 0, 0, -1, 15, 12, -1, 30, -1, -1, -1, -1, 3, 3, 3, 3, 0, 3, 36, 3, 3, 21, -1, 0, -1, 3, 5, -1, -1, 12, 1, 22, 1, 22, -1, 22, 22, -1, 12, 4, 36, 12, 3, 3, 3, 20, 3, -1, 3, 0, 3, 29, 3, 9, 3, 28, -1, 11, -1, -1, -1, 15, -1, 4, 5, 36, -1, -1, 17, -1, 17, -1, 17, 1, 15, 26, 17, 19, 23, -1, 18, 0, -1, 18, -1, 21, 21, 4, 11, 18, 18, 2, 18, -1, 33, -1, -1, 4, -1, 18, 26, 33, -1, 15, 20, 34, 20, 26, 3, 20, 0, 21, -1, 26, 2, 20, 18, 4, 20, 2, 4, -1, 2, 8, 8, -1, 4, 4, 4, 26, 4, 26, 21, 35, -1, 26, 8, 8, 8, 8, 8, 8, 8, -1, 8, 12, 30, 12, -1, 8, 0, 32, 32, 21, 26, -1, 15, 32, 21, 32, -1, 19, -1, -1, -1, -1, 4, 19, 19, -1, 19, 0, 0, 0, -1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 19, 17, 3, -1, 3, 0, 4, 11, 3, 3, 3, 19, 30, 3, 4, 0, 3, -1, 3, -1, 30, 12, 3, 3, 3, 3, 0, 0, 5, 25, 0, 0, 0, 3, 5, 0, 0, 0, 1, 0, -1, -1, 5, 1, 5, 36, 5, -1, -1, -1, 21, 1, -1, 0, -1, 6, 17, 6, 6, 17, 0, 11, 12, 12, 20, -1, 2, 17, 17, 6, 6, 6, 6, 6, 15, 26, 6, 3, 6, 33, 26, -1, 16, 33, 6, 17, -1, 6, -1, 33, -1, 0, -1, -1, 33, 5, -1, 5, 5, 0, 0, 3, 0, -1, 15, 5, 5, 25, 5, 5, 10, 10, 10, 10, 2, 5, 5, 25, 22, 19, 2, 2, 2, 2, 2, 2, 1, 10, 33, -1, 16, 16, 16, 33, 26, 0, 10, 10, 9, -1, 10, 33, 10, 26, 11, 3, 2, 2, 21, -1, -1, 4, 2, 9, 2, 2, 21, 2, 2, 15, 2, 2, -1, 8, 2, 21, 21, 11, -1, 7, 7, 7, 7, 7, 7, 10, 7, 7, 0, 0, -1, 30, -1, 18, 17, 15, 7, 7, 7, 7, 7, 7, 0, 7, 7, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, -1, 3, 0, 0, 6, 8, -1, 6, 32, -1, 6, 17, 6, 17, 6, 17, 6, 11, 6, 9, 4, 9, -1, -1, 11, 6, 6, 6, -1, 6, 17, 6, 16, 26, 6, 17, 15, 5, 5, 5, 5, 4, 5, 5, -1, 0, 5, 1, 20, -1, -1, 18, 27, 0, 20, -1, 10, 10, 15, 10, -1, -1, -1, 10, -1, 27, 2, -1, -1, -1, 10, 34, 1, -1, 2, 2, 6, -1, 6, 2, 2, -1, 2, 2, 36, -1, 2, -1, 16, 2, 22, 16, 22, 2, 8, 18, 2, 16, 4, -1, 16, 16, 16, 25, -1, 9, 34, 32, 1, 1, 1, 11, -1, 22, 0, -1, 16, 34, 8, 22, 8, 0, -1, 0, 36, -1, 1, 1, 0, -1, 4, 3, 3, 7, 7, 0, 7, 7, 0, 7, 7, 7, 7, 0, 0, 1, 1, 1, 1, 1, -1, -1, 1, 7, 1, 1, 1, 1, -1, 1, 1, 7, 15, 11, 0, 1, 1, 15, 1, 1, 1, 0, 19, 17, 6, 6, 17, 6, 6, 6, 6, -1, 29, 29, 29, 29, 29, 29, 15, 29, -1, 29, -1, -1, 3, 9, 25, 28, 8, 9, -1, 8, 13, -1, 13, 13, 13, 15, 15, 4, 8, 36, 8, 8, 8, 8, -1, 9, 27, -1, 9, 13, 11, 11, 11, -1, 19, -1, 27, 27, 0, -1, 27, 27, 25, 27, 18, 27, -1, 18, -1, -1, 27, 18, -1, 18, 14, 18, 6, 21, 34, -1, 0, -1, 9, 6, 1, 25, 14, 22, 28, -1, 22, 22, 22, 22, 0, 8, 22, -1, 28, -1, 34, -1, -1, 4, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 6, 19, 9, 15, 33, 26, -1, 19, 14, 25, 28, 12, 11, 35, 33, 18, 11, 29, 10, 29, 30, 29, 10, 9, 30, -1, 31, 31, -1, 31, 9, 10, -1, 28, 35, -1, -1, -1, 11, -1, 11, 35, 30, 35, 9, 11, 35, -1, 35, 26, -1, 3, 9, 31, 8, 4, 2, 24, 24, 24, -1, -1, 4, -1, 20, 4, 8, -1, 4, 13, 23, -1, 36, 23, 8, 13, 13, 8, -1, 6, 13, 13, 13, 34, 13, 1, 13, 24, 9, 24, 24, -1, -1, 27, 27, -1, -1, 27, -1, 27, 23, 23, 17, 17, 23, 23, -1, 23, 23, -1, 3, 21, -1, 19, 19, 0, -1, -1, -1, 3, -1, 11, 20, 0, 0, 4, 4, 0, -1, 4, 0, 21, 6, -1, 17, 20, 12, 5, 5, 4, -1, 4, -1, -1, -1, 15, -1, 2, 5, 28, -1, -1, 5, 28, 0, -1, 19, 5, 26, 5, 15, 33, 33, -1, 16, 3, 1, -1, 31, 35, 9, 9, -1, 35, 9, 21, 3, -1, -1, -1, 31, 9, 15, 0, 26, -1, 0, 4, 3, -1, 0, -1, 4, 18, -1, 4, 18, 4, 3, 24, 24, 0, 24, 9, 24, 16, 31, 2, 10, 11, 23, -1, 14, 7, -1, 26, 23, 23, 23, 23, 2, 23, 7, 23, 23, 12, 7, 23, 23, -1, -1, 12, 20, 31, 31, 31, 31, 1, 14, 31, 6, -1, 2, 10, 2, -1, 30, 8, 2, -1, 24, 16, 8, 19, 19, 24, -1, -1, -1, 2, 24, 30, 4, -1, 24, -1, -1, 24, 17, 19, -1, -1, 1, 0, -1, 19, -1, -1, 5, -1, 0, 5, 5, 4, -1, 1, 9, 9, 10, 11, 0, 9, 5, 15, -1, 0, 10, 10, 1, 0, 0, -1, 10, 0, 10, -1, 0, 16, 1, -1, -1, -1, 3, 7, 18, 13, 1, -1, 7, 0, -1, 14, 25, -1, -1, 7, 4, 7, 4, 27, 1, 2, -1, 1, 19, 30, 13, 13, 2, 28, 28, -1, 13, 9, 9, 13, 13, -1, 25, -1, 3, 8, 13, 2, 2, 8, 1, 2, -1, 4, 12, 8, -1, -1, -1, 14, 2, 14, -1, -1, 2, -1, -1, -1, 2, -1, 22, 2, 3, 3, 3, 1, 0, 1, 1, 24, 5, 3, 1, 1, 0, 14, 5, 7, 24, 25, 0, 5, 1, 12, 19, 14, 0, 5, 19, 0, 25, 10, 10, -1, 12, 0, 16, 16, 0, 33, -1, 36, 0, 11, 0, 0, 34, 13, 10, 0, -1, 0, 0, 7, 1, 3, -1, 11, 1, 1, 0, 13, -1, 31, 16, 7, 22, 0, 2, 28, 16, 2, 16, -1, 11, 20, 13, -1, -1, 2, 29, 13, 21, -1, 2, 2, 2, 8, 1, 36, 2, 3, 25, 35, -1, 3, -1, -1, 7, 7, 3, 3, 3, 6, 20, 3, -1, 2, 1, 5, -1, 1, 1, 8, 5, -1, 1, 29, 29, 1, 0, -1, -1, -1, -1, -1, 3, 25, 5, 13, 35, -1, 3, 10, -1, -1, -1, -1, -1, 35, 3, 31, 31, 10, 6, 2, 2, 4, 12, 10, 0, 1, 8, 30, 12, 24, -1, 18, 8, 19, 0, 7, -1, 0, -1, 0, 0, -1, -1, 7, 19, 0, 29, 24, 16, 4, -1, 15, 0, 16, 2, 2, 2, 16, 0, 21, 1, -1, 1, 1, 35, 2, 34, 16, 2, -1, 2, 1, 2, 2, 2, -1, 2, 16, 19, 8, 0, -1, 1, 1, 2, 6, 23, 13, 13, -1, 25, 1, 1, 2, 2, 24, 7, 29, 24, 24, -1, -1, -1]\n",
            "-------RUN2-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': \"CountVectorizer(stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b02d2c50>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[18, 1, 27, 27, -1, 1, -1, 5, 5, 5, -1, 25, 21, 25, 5, -1, 0, 18, -1, -1, -1, 21, 5, 10, 10, 22, -1, 1, 1, -1, 1, -1, 27, 12, -1, -1, 4, -1, -1, -1, 10, -1, 32, 10, 32, -1, 32, 32, 10, 1, 32, 10, 5, 5, 5, 16, 16, 1, 5, -1, 0, 1, -1, 5, -1, 5, 4, 0, -1, 4, 12, -1, 0, 0, 0, 4, 33, -1, -1, -1, 16, 16, -1, 16, 16, 22, 16, 0, 1, 0, 0, 5, 16, -1, -1, 5, 5, 0, -1, -1, 25, 15, 1, 15, 33, 12, -1, -1, -1, -1, -1, 21, -1, -1, 1, -1, -1, 22, 1, 5, -1, -1, 10, 15, 32, 31, 21, 1, 10, 21, 1, 20, -1, 21, 21, 18, 10, 1, 21, -1, 5, 5, 26, 31, 20, 5, 18, 1, 1, -1, 5, 5, 10, 5, -1, 5, 0, -1, -1, 2, 1, -1, 12, 2, -1, -1, -1, 15, 30, 1, 5, -1, -1, -1, 20, 5, 2, -1, 20, 0, 1, 1, 20, -1, 21, 5, 0, 1, 21, 24, 24, 20, 16, 1, 31, -1, 13, 13, 16, 1, 16, 1, 16, 16, -1, 16, 8, 16, 0, 6, 0, 30, -1, 16, 16, -1, -1, 0, 1, -1, 13, 0, 0, 0, -1, 13, -1, -1, -1, 31, 31, 31, 31, 13, 13, 31, 22, 31, 3, 13, 13, 4, 4, -1, 13, -1, 1, 15, 28, -1, 15, 13, 25, -1, -1, 7, 7, 7, 0, 0, -1, 15, 13, -1, 1, -1, -1, -1, -1, 4, 4, 4, 4, 0, 4, 33, 4, 4, 22, -1, 0, -1, 4, 7, -1, -1, 13, 2, 25, 2, 25, -1, 25, 25, -1, 13, 5, 33, 13, 4, 4, 4, 21, 4, -1, 4, 0, 4, 29, 4, 10, 4, 32, -1, 12, -1, -1, -1, 15, 18, 5, 31, 33, -1, -1, 18, -1, 18, -1, 18, 2, -1, 24, 18, 19, 26, -1, 20, 0, -1, 20, -1, 22, 22, -1, 12, 20, 20, 3, 20, -1, -1, 1, 1, 5, 24, 20, 24, 24, 18, 15, 21, 1, 21, 24, 4, 21, 0, 22, -1, 24, 3, 21, 20, 5, 21, 3, 5, -1, 3, 9, 9, -1, 5, 5, 5, 24, 5, 24, 22, 30, -1, 24, 9, 9, 9, 9, 9, 9, 9, -1, 9, 13, 1, 13, -1, 9, 0, 31, 31, 22, -1, -1, 15, 31, 22, 31, -1, 19, -1, -1, -1, -1, 5, 19, 19, 1, 19, 0, 0, 0, -1, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 19, 18, 4, 12, 4, 0, 5, 12, 4, 4, 4, 19, 1, 4, 5, 0, 4, -1, 4, -1, 1, 13, 4, 4, 4, 4, 0, 0, 7, 1, 0, 0, 0, 4, 7, 0, 0, 0, 2, 0, -1, -1, -1, 2, 7, 33, 7, -1, 1, -1, 22, 2, 14, 0, -1, 6, 18, 6, 6, 18, 0, 12, 13, 13, 21, -1, 3, -1, 18, 6, 6, 6, 6, 6, -1, 24, 6, 4, 6, 34, 24, -1, 17, 34, 6, 18, 1, 6, -1, 34, -1, 0, -1, -1, 34, 7, 22, 7, 7, 0, 0, 4, 0, -1, 15, 7, 7, 1, 7, 7, 11, 11, 11, 11, 3, 7, 7, 1, 25, 19, 3, 3, 3, 3, 3, 3, 35, 11, 34, 34, 17, 17, 17, 34, 24, 0, 11, 11, 10, -1, 11, 34, 11, 24, 12, 4, 3, 3, 22, -1, -1, 5, 3, 10, 3, 3, 22, 3, 3, 15, 3, 3, 1, 9, 3, 22, 22, 12, -1, 8, 8, 8, 8, 8, 8, 11, 8, 8, 0, 0, -1, 1, 27, 20, 18, 15, 8, 8, -1, 8, 8, 8, 0, 8, 8, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, -1, 4, 0, -1, 6, 9, 1, 6, 31, -1, 6, 18, 6, 18, 6, 18, 6, 12, 6, 10, 5, 10, -1, -1, 12, 6, 6, 6, 1, 6, 18, 6, 17, 24, 6, 18, 15, 7, 7, 7, 7, 5, 7, 7, -1, 0, 7, 2, 21, -1, -1, 20, 27, 0, 21, -1, 11, 11, 15, 11, 1, -1, -1, 11, -1, 27, 3, 1, -1, -1, 11, 1, 35, -1, 3, 3, 6, -1, 6, 3, 3, 1, 3, 3, 33, -1, 3, 17, 17, 3, 25, 17, 25, 3, 9, 20, 3, 17, 5, 0, 17, 17, 17, 1, -1, 10, 1, 31, 2, 2, 2, 12, -1, 25, 0, -1, 17, 1, 9, 25, 9, 0, 0, 0, 33, -1, 35, 35, -1, -1, 5, 4, 4, 8, 8, 0, 8, 8, 0, 8, 8, 8, 8, 0, -1, 2, 2, 2, 2, 2, -1, -1, 2, 8, 2, 2, 2, 2, 15, 2, 2, 8, 15, 12, 0, 2, 35, 15, 2, 2, 2, 0, 19, 18, 6, 6, 18, 6, 6, 6, 6, -1, 29, 29, 29, 29, 29, 29, 15, 29, -1, 29, -1, -1, 4, 10, 1, 32, 9, 10, -1, 9, 14, -1, 14, 14, 14, 15, 15, 5, 9, 33, 9, 9, 9, 9, 1, 10, 27, 1, 10, 14, 12, 12, 12, 1, 19, 1, 27, 27, 0, 1, 27, 27, 1, 27, 20, 27, -1, 20, 1, -1, 27, 20, 1, 20, 16, 20, 6, 22, 1, -1, 0, 0, 10, 6, 2, 1, 16, 25, 32, -1, 25, 25, 25, 25, 0, 9, 25, -1, 32, -1, 1, 1, 33, 5, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, -1, 2, 35, 2, 2, 2, 5, 35, 35, 2, 6, 19, 10, 15, 34, 24, -1, 19, 16, 1, 21, 13, 12, 30, 34, 20, 12, 29, 11, 29, 1, 29, 11, 10, 1, -1, 28, 28, 28, 28, 10, 11, -1, 32, 30, -1, -1, -1, 12, -1, 12, 30, 1, 30, 10, -1, 30, -1, 30, 24, -1, 4, 10, 28, 9, 5, 3, 23, 23, 23, -1, 1, 5, -1, -1, 5, 9, -1, -1, 14, 26, -1, 33, 26, 9, 14, 14, 9, 1, 6, 14, 14, 14, 1, 14, 2, 14, 23, 10, 23, 23, -1, 1, 27, 27, -1, -1, 27, 1, 27, 26, 26, 18, -1, 26, 26, -1, 26, 26, -1, 4, 22, -1, 19, 19, 0, -1, 30, -1, 4, -1, 12, 21, 0, 0, 5, 5, 0, -1, 5, 0, 22, 6, -1, 18, 21, 13, 7, 7, 5, 1, 5, -1, -1, -1, 15, -1, 3, 7, 32, -1, -1, 7, 32, 0, 1, 19, 7, 24, 7, 15, 34, 34, -1, 17, 4, 2, -1, -1, 30, 10, 10, -1, 30, 10, -1, 4, -1, -1, 28, 28, 10, 28, 0, 24, -1, 0, 5, 4, -1, 0, -1, 5, -1, -1, -1, 20, 5, 4, 23, 23, -1, 23, 10, 23, 17, 28, 3, 11, 12, 26, -1, 16, -1, 5, 24, 26, 26, 26, 26, 3, 26, 8, 26, 26, 13, -1, 26, 26, -1, -1, 13, 21, 28, -1, 28, 28, 2, 16, 28, -1, -1, 3, 11, 3, -1, 1, 9, 3, 0, 23, 17, 9, 19, 19, 23, -1, 1, 15, 3, 23, 1, 5, 6, 23, -1, 15, 23, 18, 19, 1, 1, 2, 0, -1, 19, -1, 1, 7, -1, 0, 7, 7, 5, -1, 2, 10, 10, 11, 12, 0, 10, 7, 15, -1, 0, 11, 11, 2, 0, 0, -1, 11, 0, 11, -1, 0, 17, -1, 3, 20, -1, 4, 8, 20, 14, 2, -1, 8, 0, -1, 16, 1, -1, -1, 8, 5, 8, 5, 27, 2, 3, -1, 2, 19, 1, 14, 14, 3, 32, 32, 1, 14, 10, 10, 14, 14, -1, 1, -1, 4, 9, 14, 3, 3, 9, 2, 3, -1, 5, 13, 9, -1, -1, 1, 16, 3, 16, 1, -1, 3, -1, -1, 1, 3, -1, 25, 3, 4, 4, 4, 2, 0, 2, 2, 23, 7, 4, 2, 2, 0, 16, 7, -1, 23, 1, 0, 7, 2, 13, 19, 16, 0, 7, 19, 0, 1, 11, 11, -1, 13, 0, 17, 17, 0, 34, 1, 33, 0, 12, 0, 0, 1, 14, 11, -1, -1, 0, 0, 8, 2, 4, -1, -1, 2, 2, 0, 14, -1, 28, 17, 8, 25, 0, 3, -1, 17, 3, 17, -1, 12, 21, 14, 1, -1, 3, 29, 14, 22, -1, 3, 3, 3, 9, 35, 33, 3, 4, 1, 30, 22, 4, 1, -1, 8, 8, 4, 4, 4, 6, 21, 4, 1, 3, 2, 7, -1, 2, 2, 9, 7, -1, 2, 29, 29, 2, 0, -1, -1, -1, -1, -1, 4, 1, 7, 14, 30, -1, 4, 11, 12, 1, 1, -1, -1, 30, 4, 28, 28, 11, 6, 3, 3, 5, 13, 11, 0, 35, 9, 1, 13, 23, 1, 12, 9, 19, 0, 8, 1, -1, -1, 0, 0, -1, -1, 8, 19, 0, 29, 23, 17, 5, -1, 15, 0, 17, 3, 3, 3, 17, 0, 22, 2, -1, 2, 35, 30, 3, 1, 17, 3, -1, 3, 2, 3, 3, 3, -1, 3, 17, 19, 9, 0, 1, 2, 2, 3, 6, 26, 14, 14, -1, 1, 2, 2, 3, 3, 23, 8, 29, 23, 23, -1, 5, -1]\n",
            "-------RUN3-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': \"CountVectorizer(ngram_range=(1, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b891dd10>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[13, 20, 31, 31, 30, 20, -1, 2, 2, 2, -1, 15, 14, 15, 2, -1, -1, 13, -1, -1, -1, 14, 2, 8, 8, 23, -1, -1, 21, -1, 20, -1, 31, 28, -1, -1, -1, -1, -1, -1, 8, -1, 30, 8, 30, -1, 30, 30, 8, -1, 30, 8, 2, 2, 2, 12, 12, 20, 2, -1, -1, -1, -1, 2, -1, 2, 9, -1, 14, -1, 28, 14, 3, 3, -1, -1, -1, -1, -1, -1, 12, 12, -1, 12, 12, 23, 12, 3, 21, -1, 3, 2, 12, -1, -1, 2, 2, 3, -1, -1, 15, 17, 20, 17, -1, 28, -1, -1, -1, -1, -1, 14, -1, -1, -1, -1, -1, 23, -1, -1, -1, -1, 8, 17, 30, -1, 14, 21, 8, 14, 21, 18, -1, 14, 14, 13, 8, -1, 14, -1, 2, 2, 26, 35, 18, 2, 13, -1, 20, -1, 2, 2, 8, 2, -1, 2, 29, -1, -1, 0, -1, -1, 28, 0, -1, -1, 26, 17, 34, -1, 2, -1, -1, -1, 18, 2, 37, -1, 18, -1, -1, -1, 18, -1, 14, 2, -1, 21, 14, 24, 24, 18, 12, 21, 35, 12, 11, 11, 12, -1, 12, 20, 12, 12, -1, 12, 5, 12, 3, 15, 29, -1, -1, 12, 12, -1, -1, 3, 21, -1, 11, -1, 29, 29, -1, 11, -1, -1, -1, 35, 35, 35, 35, 11, 11, 35, 23, 35, 1, 11, 11, 9, 9, -1, 11, -1, -1, 17, 33, -1, 17, 11, 15, -1, -1, 4, 4, 4, 29, 29, -1, 17, 11, -1, 20, -1, -1, -1, -1, 9, 9, 9, -1, 29, 9, -1, 9, -1, 23, -1, 3, -1, 9, 4, -1, -1, 11, 0, -1, 0, 15, -1, 15, 15, -1, 11, 2, -1, 11, 25, 25, -1, 14, 25, -1, 25, -1, 25, 32, -1, 8, 25, 30, -1, -1, -1, -1, -1, 17, -1, 2, 4, -1, -1, -1, 13, -1, 13, -1, 13, 0, -1, 24, 13, 22, 26, 2, 18, -1, -1, 18, -1, 23, 23, 2, -1, 18, 18, 1, 18, -1, -1, -1, 20, 2, 24, 18, 24, 24, 13, 17, 14, 20, 14, 24, 9, 14, -1, 23, -1, 24, -1, 14, 18, 2, 14, 1, 2, 21, 1, 6, 6, -1, 2, 2, 2, 24, 2, 24, 23, 34, -1, 24, 6, 6, 6, 6, 6, 6, 6, -1, 6, 11, 20, 11, -1, 6, -1, 35, 35, 23, 24, -1, 17, 35, 23, 35, -1, 22, -1, -1, -1, -1, 2, 22, 22, -1, 22, 29, 3, 29, -1, 3, 25, 3, 3, 3, -1, 3, 3, 3, -1, 3, 22, 13, 9, -1, 9, 29, 2, 28, 9, 9, 9, 22, 20, 9, -1, -1, 9, -1, 9, -1, 20, 11, 9, 9, 9, 9, -1, 3, 4, 21, 3, -1, -1, 9, 4, -1, 3, -1, 0, 3, -1, -1, 4, 0, 4, -1, 4, -1, -1, -1, 23, 0, -1, -1, -1, 7, 13, 7, 7, 13, -1, -1, 11, 11, 14, 15, 1, 13, 13, 7, 7, 7, 7, 7, 17, 24, 7, 25, 7, 36, 24, 15, 19, 36, 7, 13, 20, 7, 15, 36, -1, 3, -1, -1, 36, 4, -1, 4, 4, -1, -1, 25, 3, 23, 17, 4, 4, 21, 4, 4, 10, 10, 10, 10, 1, 4, 4, 21, 15, 22, 1, 1, 1, 1, 1, 1, -1, 10, 36, 24, 19, 19, 19, 36, 24, -1, 10, 10, 8, -1, 10, 36, 10, 24, 28, -1, 1, 1, 23, 24, -1, 2, 1, 8, 1, 1, 23, 1, 1, 17, 1, 1, -1, 6, 1, 23, 23, 28, -1, 5, 5, 5, 5, 5, 5, 10, 5, 5, 3, 3, -1, 20, -1, 18, 13, 17, 5, 5, 5, 5, 5, 5, 3, 5, 5, -1, 3, 3, 3, -1, 3, 7, 3, 3, -1, 28, 9, -1, -1, 7, 6, -1, 7, 35, -1, 7, 13, 7, 13, 7, 13, 7, -1, 7, 8, 2, 8, 21, -1, -1, 7, 7, 7, -1, 7, 13, 7, 19, 24, 7, 13, 17, 4, 4, 4, 4, -1, 4, 4, -1, -1, 4, 0, 14, -1, -1, 18, 31, 3, 14, -1, 10, 10, 17, 10, -1, 15, -1, 10, -1, 31, 1, -1, -1, -1, 10, -1, 37, -1, 1, 1, -1, -1, -1, 1, 1, -1, 1, 1, -1, -1, 1, -1, 19, 1, 15, 19, 15, 1, 6, 18, 1, 19, 2, -1, 19, 19, 19, 21, -1, 8, -1, 35, 0, 0, 0, 28, -1, 15, 29, -1, 19, 20, 6, 15, 6, 29, -1, 29, -1, -1, 37, 37, -1, -1, 2, 9, 9, 5, 5, -1, 5, 5, 3, 5, 5, 5, 5, -1, -1, 0, 0, 0, 0, 0, -1, -1, 0, 5, 0, 0, 0, 0, -1, 0, 0, 5, 17, -1, -1, 0, 37, 17, 0, 0, 0, 29, 22, 13, 7, 7, 13, 7, 7, 7, 7, -1, 32, 32, 32, 32, 32, 32, 17, 32, -1, 32, -1, -1, 9, 8, 21, 30, 6, 8, -1, 6, 16, -1, 16, 16, 16, 17, 17, 2, 6, -1, 6, 6, 6, 6, -1, 8, 31, -1, 8, 16, 28, 28, 28, -1, 22, -1, 31, 31, -1, -1, 31, 31, 21, 31, 18, 31, -1, 18, -1, -1, 31, 18, 20, 18, 12, 18, 15, 23, 20, -1, -1, -1, 8, -1, 0, 21, 12, 15, 30, -1, 15, 15, 15, 15, -1, 6, 15, -1, 30, -1, -1, -1, -1, 2, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0, 37, 0, 0, 0, -1, 37, 37, 0, 7, 22, 8, 17, 36, 24, -1, 22, 12, 21, 30, 11, 28, 34, 36, 18, 28, 32, 10, 32, 20, 32, 10, 8, -1, -1, 33, 33, -1, 33, 8, 10, -1, 30, 34, -1, -1, -1, 28, -1, 28, 34, 20, 34, 8, -1, 34, -1, 34, 24, -1, 9, 8, 33, 6, 2, 1, 27, 27, 27, -1, 20, 2, -1, 14, 2, 6, -1, -1, 16, 26, -1, -1, 26, 6, 16, 16, 6, -1, 7, 16, 16, 16, 20, 16, 0, 16, 27, 8, 27, 27, -1, -1, 31, 31, -1, -1, 31, -1, 31, 26, 26, 13, 13, 26, 26, -1, 26, 26, -1, 25, 23, -1, 22, 22, 3, -1, -1, -1, 9, -1, 28, 14, -1, -1, 2, 2, -1, -1, 2, 3, 23, 7, -1, 13, 14, 11, 4, 4, 2, -1, 2, -1, -1, -1, 17, -1, 1, 4, 30, -1, -1, 4, 30, 3, 21, 22, 4, 24, 4, 17, 36, 36, -1, 19, 25, 0, -1, 33, 34, 8, 8, -1, 34, 8, 23, 9, -1, -1, -1, 33, 8, -1, 3, 24, -1, 3, 2, -1, -1, 3, -1, 2, -1, -1, -1, 18, 2, 25, 27, 0, -1, 27, 8, 27, 19, 33, 1, 10, -1, 26, -1, 12, -1, 2, 24, 26, 26, 26, 26, 1, 26, 5, 26, 26, 11, -1, 26, 26, -1, -1, 11, 14, 33, 33, 33, 33, 0, 12, 33, 15, -1, 1, 10, 1, -1, 20, 6, 1, -1, 27, 19, 6, 22, 22, 27, -1, -1, -1, 1, 27, 20, 2, -1, 27, 10, -1, 27, 13, 22, -1, -1, 0, -1, -1, 22, 10, -1, 4, -1, -1, 4, 4, 2, -1, 0, 8, 8, 10, 28, 29, 8, 4, 17, -1, -1, 10, 10, 0, 3, -1, -1, 10, 29, 10, -1, 3, 19, 0, -1, 18, -1, 9, 5, 18, 16, 0, -1, 5, -1, -1, 12, 21, -1, -1, 5, 2, 5, 2, 31, 0, 1, -1, 0, 22, -1, 16, 16, 1, 30, 30, -1, 16, 8, 8, 16, 16, -1, 21, -1, 25, 6, 16, 1, 1, 6, 0, 1, -1, 2, 11, 6, -1, -1, -1, 12, 1, 12, -1, -1, 1, -1, -1, -1, 1, -1, 15, 1, 25, 25, 25, 0, -1, 0, 0, 27, 4, 25, 0, 0, -1, 12, 4, 5, 27, 21, -1, 4, 0, 11, 22, 12, 3, 4, 22, -1, 21, 10, 10, -1, 11, 3, 19, 19, -1, 36, -1, -1, 3, 28, 29, 3, -1, 16, 10, -1, -1, 29, 3, 5, 0, 9, -1, -1, 0, 0, 3, 16, -1, -1, 19, 5, 15, -1, 1, 30, 19, 1, 19, -1, 28, 14, 16, -1, -1, 1, 32, 16, 23, -1, 1, 1, 1, 6, 37, -1, 1, 25, 21, 34, 23, 25, -1, -1, 5, 5, 25, 9, 25, 7, 14, 25, -1, 1, 0, 4, -1, 0, 0, 6, 4, -1, 0, 32, 32, 0, 3, -1, -1, -1, -1, -1, 9, 21, 4, 16, 34, -1, 25, 10, -1, 20, -1, -1, -1, 34, 25, 33, -1, 10, 7, 1, 1, 2, 11, 10, -1, 37, 6, 21, 11, 27, -1, 18, 6, 22, 3, 5, -1, -1, -1, -1, -1, -1, -1, 5, 22, -1, 32, 27, 19, 2, -1, 17, -1, 19, 1, 1, 1, 19, -1, 23, 0, 2, 0, 37, 34, 1, 20, 19, 1, -1, 1, 0, 1, 1, 1, -1, 1, 19, 22, 6, 29, -1, 0, 0, 1, 7, 26, 16, 16, -1, 21, 0, 0, 1, 1, 27, 5, 32, 27, 27, -1, -1, -1]\n",
            "-------RUN4-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': \"CountVectorizer(ngram_range=(2, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df4a9008410>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[15, 0, 31, 31, -1, 0, 4, 3, 3, 3, -1, 13, 16, 13, 3, -1, -1, 15, -1, 0, -1, 16, 3, 8, 8, 20, -1, -1, 0, -1, 0, -1, 31, 26, -1, -1, -1, -1, -1, -1, 8, -1, 30, 8, 30, -1, 30, 30, 8, 0, 30, 8, 3, 3, 3, 12, 12, 0, 3, 3, -1, -1, -1, 3, -1, 3, 11, -1, 16, -1, 26, 16, 5, 5, -1, -1, -1, -1, -1, -1, 12, 12, -1, 12, 12, 20, 12, 5, 0, -1, 5, 3, 12, -1, 3, 3, 3, 5, -1, 26, 13, 22, 0, 22, 34, 26, -1, -1, -1, -1, -1, 16, -1, -1, 0, -1, -1, 20, 0, 3, 4, -1, 8, 22, 30, 33, 16, 0, 8, 16, 0, 19, -1, 16, 16, 15, 8, 0, 16, -1, 3, 3, 27, 33, 19, 3, 15, -1, 0, -1, 3, 3, 8, 3, -1, 3, -1, -1, -1, 1, 0, -1, 26, 1, -1, -1, 15, 22, -1, 0, 3, -1, -1, -1, 19, 3, 1, -1, 19, -1, 0, 0, 19, -1, 16, 3, -1, 0, 16, 25, -1, 19, 12, 0, 33, -1, 14, 14, 12, 0, 12, 0, 12, 12, -1, 12, 7, 12, 5, 13, 29, -1, -1, 12, 12, -1, -1, 5, 0, -1, 14, -1, 29, 29, -1, 14, -1, -1, -1, 33, 33, 33, 33, 14, 14, 33, 20, 33, 2, 14, 14, 11, 11, -1, 14, -1, -1, 22, 28, -1, 22, 14, 13, -1, -1, 4, 4, 4, 29, 29, -1, 22, 28, -1, 0, -1, 0, -1, -1, 11, 11, 11, -1, 29, 11, 34, 11, -1, 20, -1, 5, -1, -1, 4, 4, -1, 14, 1, 13, 1, 13, -1, 13, 13, -1, 14, 3, 34, 14, 23, -1, -1, 16, 23, -1, 23, -1, -1, 32, -1, 8, 23, 30, -1, -1, -1, -1, -1, 22, -1, 3, 4, 34, -1, -1, 15, -1, 15, -1, 15, -1, -1, 25, 15, 21, 27, -1, 19, -1, -1, 19, -1, 20, 20, -1, -1, 19, 19, 2, 19, -1, -1, 0, 0, 3, 25, 19, 25, 25, 15, 22, 16, 0, 16, 25, 11, 16, -1, 20, -1, 25, 2, 16, 19, 3, 16, 2, 3, 0, 2, 9, 9, -1, 3, 3, 3, 25, 3, 25, 20, 35, -1, 25, 9, 9, 9, 9, 9, 9, 9, -1, 9, 14, 0, 14, -1, 9, -1, 33, 33, 20, 25, -1, -1, 33, 20, 33, -1, 21, -1, -1, -1, -1, 3, 21, 21, 0, 21, 29, 5, 29, -1, 5, 23, 5, 5, 5, -1, 5, 5, 5, -1, 5, 21, 15, 11, -1, 11, 29, 3, 26, 11, 11, 11, 21, 0, 11, 3, -1, 11, 28, 11, -1, 0, 14, 11, -1, 11, 11, -1, 5, 4, 0, 5, -1, -1, 11, 4, -1, 5, -1, 1, 5, -1, -1, 4, 1, 4, 34, 4, -1, -1, 0, 20, 1, -1, -1, -1, 6, 15, 6, 6, 15, -1, -1, 14, 14, 16, 13, 2, -1, 15, 6, 6, 6, 6, 6, 22, 25, 6, 23, 6, 36, 25, 13, 18, 36, 6, 15, 0, 6, 13, 36, -1, 5, -1, -1, 36, 4, 20, 4, 4, -1, -1, 23, 5, 20, 22, 4, 4, 0, 4, 4, 10, 10, 10, 10, 2, 4, 4, 0, 13, 21, 2, 2, 2, 2, 2, 2, 37, 10, 36, -1, 18, 18, 18, 36, 25, -1, 10, 10, 8, -1, 10, 36, 10, 25, 26, -1, 2, 2, 20, -1, -1, 3, 2, 8, 2, 2, 20, 2, 2, 22, 2, 2, 0, 9, 2, 20, 20, 26, -1, 7, 7, 7, 7, -1, 7, 10, 7, 7, 5, 5, -1, 0, -1, 19, 15, 22, 7, 7, -1, 7, 7, 7, 5, 7, 7, -1, 5, 5, 5, -1, 5, 6, 5, -1, -1, -1, -1, -1, -1, 6, 9, 0, 6, 33, -1, 6, 15, 6, 15, 6, 15, 6, -1, 6, 8, 3, 8, 0, -1, -1, 6, 6, 6, 0, 6, 15, 6, 18, 25, 6, 15, 22, 4, 4, 4, 4, 3, 4, 4, 34, -1, 4, 1, 16, -1, 4, 19, 31, 5, 16, -1, 10, 10, 22, 10, -1, 13, -1, 10, -1, 31, 2, -1, -1, -1, 10, 0, 37, -1, 2, 2, 6, 34, 6, 2, 2, 0, 2, 2, 34, -1, 2, -1, 18, 2, 13, 18, 13, 2, 9, 19, 2, 18, 3, -1, 18, 18, 18, 0, -1, 8, 0, 33, 1, 1, 1, 26, -1, 13, 29, -1, 18, 0, 9, 13, 9, 29, -1, 29, 34, -1, 37, 37, -1, -1, 3, 11, 11, 7, 7, -1, 7, 7, 5, 7, 7, 7, 7, -1, -1, 1, 1, 1, 1, 1, -1, -1, 1, 7, 1, 1, 1, 1, -1, 1, 1, 7, 22, -1, -1, 1, 37, -1, 1, 1, 1, 29, 21, 15, 6, 6, 15, 6, 6, 6, 6, 30, 32, 32, 32, 32, 32, 32, 22, 32, -1, 32, -1, -1, 11, 8, 0, 30, 9, 8, -1, 9, 17, -1, 17, 17, 17, 22, 22, 3, 9, 34, 9, 9, 9, 9, 0, 8, 31, 0, 8, 17, 26, 26, 26, -1, 21, -1, 31, 31, -1, 0, 31, 31, 0, 31, 19, 31, -1, 19, 0, -1, 31, 19, 0, 19, 12, 19, 13, -1, 0, -1, -1, -1, 8, 6, 1, 0, 12, 13, 30, -1, 13, 13, 13, 13, -1, 9, 13, -1, 30, -1, 0, -1, 34, 3, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, -1, 1, 37, 1, 1, 1, 3, 37, 37, 1, 6, 21, 8, 22, 36, 25, -1, 21, 12, 0, 30, 14, 26, 35, 36, -1, 26, 32, 10, 32, 0, 32, 10, 8, 0, -1, 28, 28, -1, 28, 8, 10, -1, 30, 35, -1, -1, -1, 26, -1, 26, 35, 0, 35, 8, -1, 35, -1, -1, 25, -1, 11, 8, 28, 9, 3, 2, 24, 24, 24, -1, 0, 3, -1, 16, 3, 9, -1, -1, 17, 27, -1, 34, 27, 9, 17, 17, 9, -1, 6, 17, 17, 17, 0, 17, 1, 17, 24, 8, 24, 24, -1, 0, 31, 31, -1, -1, 31, 0, 31, 27, 27, 15, 15, 27, 27, -1, 27, 27, -1, 23, 20, -1, 21, 21, 5, -1, -1, -1, 11, -1, 26, 16, 29, -1, 3, 3, -1, -1, 3, 5, 20, 6, -1, 15, 16, 14, 4, 4, 3, 0, 3, -1, -1, -1, 22, -1, 2, 4, 30, 12, 4, 4, 30, 5, -1, 21, 4, 25, 4, 22, 36, 36, -1, 18, 23, 1, -1, 28, 35, 8, 8, -1, 35, 8, 20, 11, -1, -1, -1, 28, 8, -1, 5, 25, -1, 5, -1, -1, -1, 5, -1, 3, -1, -1, 3, 19, 3, 23, 24, 24, -1, 24, 8, 24, 18, 28, 2, 10, -1, 27, -1, 12, -1, 3, 25, 27, 27, 27, 27, 2, 27, 7, 27, 27, 14, -1, 27, 27, 3, 3, 14, 16, 28, 28, 28, 28, 1, 12, 28, 13, -1, 2, 10, 2, -1, 0, 9, 2, -1, 24, 18, 9, 21, 21, 24, -1, 0, -1, 2, 24, 0, 3, -1, 24, 2, -1, 24, 15, 21, -1, 0, 1, -1, -1, 21, -1, 0, 4, -1, -1, 4, 4, 3, -1, 1, 8, 8, 10, 26, -1, 8, 4, -1, -1, -1, 10, 10, 1, 5, -1, -1, 10, 29, 10, -1, 5, 18, -1, -1, 19, -1, 11, 7, 19, 17, 1, -1, 7, -1, -1, 12, 0, -1, -1, 7, 3, 7, 3, 31, 1, 2, -1, 1, 21, 0, 17, 17, 2, 30, 30, 0, 17, 8, 8, 17, 17, -1, 0, -1, 23, 9, 17, 2, 2, 9, 1, 2, 0, 3, 14, 9, -1, -1, 0, 12, 2, 12, -1, -1, 2, -1, -1, 0, 2, -1, 13, 2, 23, 23, 23, 1, -1, 1, 1, 24, 4, 23, 1, 1, -1, 12, 4, 7, 24, 0, -1, 4, 1, 14, 21, 12, 5, 4, 21, -1, 0, 10, 10, -1, 14, 5, 18, 18, -1, 36, 0, 34, -1, 26, 29, 5, 0, 17, 10, -1, -1, 29, -1, 7, 1, 11, -1, 26, 1, 1, 5, 17, -1, 28, 18, 7, 13, -1, 2, 30, 18, 2, 18, -1, 26, 16, 17, 0, -1, 2, 32, 17, 20, -1, 2, 2, 2, 9, 37, 34, 2, 23, 0, 35, 20, 23, -1, -1, 7, 7, 23, 11, 23, 6, 16, 23, 0, 2, 1, 4, -1, 1, 1, 9, 4, -1, 1, 32, 32, 1, 5, 20, -1, -1, -1, -1, 11, 0, 4, 17, 35, -1, 23, 10, -1, 0, 0, -1, -1, 35, 23, 28, 28, 10, 6, 2, 2, 3, 14, 10, -1, 37, 9, 0, 14, 24, 0, 19, 9, 21, 5, 7, 0, -1, -1, -1, -1, -1, -1, 7, 21, -1, 32, 24, 18, -1, -1, 22, -1, 18, 2, 2, 2, 18, -1, 20, 1, -1, 1, 37, 35, 2, 0, 18, 2, -1, 2, 1, 2, 2, 2, 0, 2, 18, 21, -1, 29, 0, 1, 1, 2, 6, -1, 17, 17, -1, 0, 1, 1, 2, 2, 24, 7, 32, 24, 24, -1, -1, -1]\n",
            "-------RUN5-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': \"CountVectorizer(stop_words='english')\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[22, 1, 27, 27, -1, 1, -1, 5, 5, 5, -1, 23, 19, 23, 5, -1, 0, 22, -1, 1, -1, 19, 5, 10, 10, 25, -1, 1, 1, -1, 1, -1, 27, 26, 1, -1, 4, -1, -1, -1, 10, -1, 29, 10, 29, -1, 29, 29, 10, 1, 29, 10, 5, 5, -1, 13, 13, 1, 5, -1, 0, 1, -1, 5, -1, 5, 4, 0, -1, 4, 26, -1, 0, 0, 0, 4, -1, -1, 1, -1, 13, 13, -1, 13, 13, 25, 13, 0, 1, 0, 0, 5, 13, -1, -1, 5, 5, 0, -1, -1, 23, 17, 1, 17, 32, 26, -1, -1, -1, -1, -1, 19, -1, -1, 1, -1, -1, 25, 1, 5, -1, -1, 10, 17, 29, -1, 19, 1, 10, 19, 1, 18, -1, 19, 19, 22, 10, 1, 19, -1, 5, 5, 21, 31, 18, 5, 22, -1, 1, -1, 5, 5, 10, 5, -1, 5, 0, -1, -1, 2, -1, -1, 26, 2, -1, -1, 21, 17, -1, 1, 5, 34, -1, -1, 18, 5, 2, 34, 18, -1, 1, 1, 18, -1, 19, 5, -1, 1, 19, 11, 11, 18, 13, 1, 31, 8, 15, 15, 13, 1, 13, 1, 13, 13, -1, 13, 8, 13, 0, 23, 0, 30, 34, 13, 13, -1, -1, 0, 1, 0, 15, 0, 0, 0, -1, 15, -1, -1, -1, 31, 31, 31, 31, 15, 15, 31, 25, 31, 3, 15, -1, 4, 4, -1, 15, -1, 1, 17, 33, 1, 17, 15, 23, -1, -1, 6, 6, 6, 0, 0, -1, 17, 15, -1, 1, -1, 1, -1, -1, 4, 4, 4, 4, 0, 4, 32, 4, 4, 25, -1, 0, -1, 4, 6, -1, -1, 15, 2, 23, 2, 23, -1, 23, 23, -1, 15, 5, 32, 15, 4, 4, 4, 19, 4, -1, 4, 0, 4, 28, 4, 10, 4, 29, 1, 34, -1, -1, -1, 17, -1, 5, 6, 32, -1, -1, 22, -1, 22, -1, 22, 2, -1, 11, 22, 20, 21, 5, 18, 0, -1, 18, -1, 25, -1, -1, 34, 18, 18, 3, 18, -1, 11, 1, 1, 5, 11, 18, 11, 11, 21, 17, 19, 1, 19, 11, 4, 19, 0, 25, 0, 11, 12, 19, 18, 5, 19, 3, 5, 1, 3, 9, 9, -1, 5, 5, 5, 11, 5, 11, 25, 30, -1, 11, 9, 9, 9, 9, 9, 9, 9, -1, 9, 15, 1, 15, -1, 9, 0, 31, 31, 25, 11, -1, 17, 31, 25, 31, -1, 20, -1, -1, -1, -1, -1, 20, 20, 1, 20, 0, 0, 0, -1, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 20, 22, 4, 34, 4, 0, 5, 26, 4, 4, 4, 20, 1, 4, -1, 0, 4, -1, 4, -1, 1, 15, 4, 4, 4, 4, 0, 0, 6, 1, 0, 0, 0, 4, 6, 0, 0, 0, 2, 0, -1, -1, 6, 2, 6, 32, 6, -1, 1, 1, 25, 2, -1, 0, -1, 7, 22, 7, 7, 22, 0, 34, 15, 15, 19, -1, 3, 22, 22, 7, 7, 7, 7, 7, 17, 11, 7, 4, 7, 11, 11, -1, 16, 11, 7, 22, 1, 7, -1, 11, -1, 0, -1, -1, 11, 6, -1, 6, 6, 0, 0, 4, 0, -1, 17, 6, 6, 1, 6, 6, 12, 12, 12, 12, 3, 6, 6, 1, 23, 20, 3, 3, 3, 3, 3, 3, 2, 12, 11, -1, 16, 16, 16, 11, 11, 0, 12, 12, 10, -1, 12, 11, 12, 11, 26, 4, 3, 3, 25, 11, -1, 5, 3, 10, 3, 3, 25, 3, 3, 17, 3, 3, 1, 9, 3, 25, 25, 26, -1, 8, 8, 8, 8, 8, 8, 12, 8, 8, 0, 0, -1, 1, -1, 18, 22, 17, 8, 8, 8, 8, 8, 8, 0, 8, 8, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 26, 4, 0, -1, 7, 9, 1, 7, 31, -1, 7, 22, 7, 22, 7, 22, 7, 34, 7, 10, 5, 10, 1, -1, 34, 7, 7, 7, 1, 7, 22, 7, 16, 11, 7, 22, 17, 6, 6, 6, 6, -1, 6, 6, 32, 0, 6, 2, 19, 29, -1, 18, 27, 0, 19, -1, 12, 12, 17, 12, 1, -1, -1, 12, -1, 27, 3, 1, -1, -1, 12, 1, 2, -1, 3, 3, 7, -1, 7, 3, 3, 1, 3, 3, 32, -1, 3, -1, 16, 3, 23, 16, 23, 3, 9, 18, 3, 16, -1, 0, 16, 16, 16, 1, -1, 10, 1, 31, 2, 2, 2, 26, 32, 23, 0, -1, 16, 1, 9, 23, 9, 0, 0, 0, 32, -1, 2, 2, -1, -1, 5, 4, 4, 8, 8, 0, 8, 8, 0, 8, 8, 8, 8, 0, 0, 2, 2, 2, 2, 2, -1, -1, 2, 8, 2, 2, 2, 2, -1, 2, 2, 8, 17, 34, 0, 2, 2, 17, 2, 2, 2, 0, 20, 22, 7, 7, 22, 7, 7, 7, 7, -1, 28, 28, 28, 28, 28, 28, 17, 28, -1, 28, -1, -1, 4, 10, 1, 29, 9, 10, -1, 9, 14, -1, 14, 14, 14, 17, 17, 5, 9, 32, 9, 9, 9, 9, 1, 10, 27, 1, 10, 14, 26, 26, 26, 1, 20, 1, 27, 27, 0, 1, 27, 27, 1, 27, 18, 27, -1, 18, 1, -1, 27, 18, 1, 18, 13, 18, 7, 30, 1, -1, 0, -1, 10, 7, 2, 1, 13, 23, 29, -1, 23, 23, 23, 23, 0, 9, 23, 1, 29, -1, 1, 1, -1, 5, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, -1, 2, 2, 2, 2, 2, 5, 2, 2, 2, 7, 20, 10, 17, 11, 11, -1, 20, 13, 1, 29, 15, -1, 30, 11, -1, 26, 28, 12, 28, 1, 28, 12, 10, 1, -1, 33, 33, -1, 33, 10, 12, -1, 29, 30, -1, -1, -1, 26, -1, 26, 30, 1, 30, 10, 34, 30, -1, 30, 11, -1, 4, 10, 33, 9, 5, 3, 24, 24, 24, -1, 1, 5, 32, 19, 5, 9, -1, -1, 14, 21, 21, 32, 21, 9, 14, 14, 9, 1, 7, 14, 14, 14, 1, 14, 2, 14, 24, 10, 24, 24, -1, 1, 27, 27, -1, -1, 27, 1, 27, 21, 21, -1, -1, 21, 21, -1, 21, 21, -1, 4, 25, 1, 20, 20, 0, -1, -1, -1, 4, -1, 26, 19, 0, 0, 5, 5, 0, -1, 5, 0, 25, 7, -1, 22, 19, 15, 6, 6, 5, -1, 5, -1, -1, -1, 17, -1, 3, 6, 29, -1, -1, 6, 29, 0, 1, 20, 6, 11, 6, 17, 11, 11, -1, 16, 4, 2, -1, -1, 30, 10, 10, -1, 30, 10, 25, 4, -1, -1, -1, 33, 10, -1, 0, 11, -1, 0, -1, 4, -1, 0, -1, 5, 18, -1, -1, 18, 5, 4, 24, 24, 0, 24, 10, 24, 16, 33, 3, 12, 34, 21, -1, 13, -1, -1, 11, 21, 21, 21, 21, 3, 21, 8, 21, 21, 15, -1, 21, 21, -1, -1, 15, 19, 33, -1, 33, 33, 2, 13, 33, 23, -1, 3, 12, 3, -1, 1, 9, 3, 0, 24, 16, 9, 20, 20, 24, 1, -1, -1, 3, 24, 1, 5, -1, 24, 12, -1, 24, 22, 20, -1, 1, 2, 0, -1, 20, 12, 1, 6, -1, 0, 6, 6, 5, -1, 2, 10, 10, 12, 26, 0, 10, 6, 17, -1, 0, 12, 12, 2, 0, 0, -1, 12, 0, 12, -1, 0, 16, 2, -1, 18, -1, 4, 8, 18, 14, 2, -1, 8, 0, -1, 13, 1, -1, 1, 8, 5, 8, 5, 27, 2, 3, -1, 2, 20, 1, 14, 14, 3, 29, 29, 1, 14, 10, 10, 14, 14, -1, 1, -1, 4, 9, 14, 3, 3, 9, 2, 3, 1, 5, 15, 9, -1, -1, 1, 13, 3, 13, 1, -1, 3, -1, -1, 1, 3, 8, 23, 3, 4, 4, 4, 2, 0, 2, 2, 24, 6, 4, 2, 2, 0, 13, 6, 8, 24, 1, 0, 6, 2, 15, 20, 13, 0, 6, 20, 0, 1, 12, 12, -1, 15, 0, 16, 16, 0, 11, -1, 32, 0, 26, 0, 0, 1, 14, 12, -1, -1, 0, 0, 8, 2, 4, -1, -1, 2, 2, 0, 14, -1, -1, 16, 8, 23, 0, 3, -1, 16, 3, 16, -1, 26, 19, 14, 1, 1, 3, 28, 14, 25, -1, 3, 3, 3, 9, 2, 32, 3, 4, 1, 30, 25, 4, -1, -1, 8, 8, 4, 4, 4, 7, 19, 4, 1, 3, 2, 6, -1, 2, 2, 9, 6, -1, 2, 28, 28, 2, 0, -1, -1, -1, -1, -1, 4, 1, 6, 14, 30, -1, 4, 12, -1, 1, -1, 18, -1, 30, 4, 33, 33, 12, 7, 3, 3, 5, 15, 12, 0, 2, 9, 1, 15, 24, 1, 26, 9, 20, 0, 8, 1, 0, -1, 0, 0, -1, -1, 8, 20, 0, 28, 24, 16, 5, -1, 17, 0, 16, 3, 3, 3, 16, 0, 25, 2, -1, 2, 2, 30, 3, 1, 16, 3, -1, 3, 2, 3, 3, 3, 1, 3, 16, 20, 31, 0, 1, 2, 2, 3, 7, 21, 14, 14, -1, 1, 2, 2, 3, 3, 24, 8, 28, 24, 24, -1, -1, -1]\n",
            "-------RUN6-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': 'CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7df3b00f5c90>)', 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[32, 1, 27, 27, 25, 1, -1, 4, 4, 4, -1, 18, 14, 18, 4, -1, 0, 32, -1, -1, -1, 14, 4, 8, 8, 23, -1, 1, 1, -1, 1, -1, 27, 26, -1, -1, -1, -1, -1, -1, 8, -1, 25, 8, 25, -1, 25, 25, 8, 1, 25, 8, 4, 4, -1, 17, 17, 1, 4, -1, 0, -1, 4, 4, -1, 4, 10, 0, 14, -1, 26, 14, 0, 0, 0, -1, -1, -1, -1, -1, 17, 17, 30, 17, 17, 23, 17, 0, 1, 0, 0, 4, 17, -1, -1, 4, 4, 0, -1, -1, 18, 13, 1, 13, 37, 26, -1, -1, -1, -1, -1, 14, -1, -1, -1, -1, -1, -1, 1, -1, 25, -1, 8, 13, 25, -1, 14, 1, 8, 14, 1, 20, -1, 14, 14, 32, 8, 1, 14, -1, 4, 4, 22, -1, 20, 4, 32, -1, 1, -1, 4, 4, 8, 4, -1, 4, 0, -1, -1, 2, -1, -1, 26, 2, -1, -1, 22, 13, -1, 1, 4, 38, -1, -1, 20, 4, 2, 38, 20, 14, 1, 1, 20, -1, 14, 4, -1, 1, 14, 28, -1, 20, 17, 1, 31, -1, 12, 12, 17, 1, 17, 1, 17, 17, -1, 17, 6, 17, 0, 18, 0, 30, 38, 17, -1, 30, -1, 0, 1, 0, 12, 0, 0, 0, -1, 12, -1, -1, 9, 31, 31, 31, 31, 12, 12, 31, 23, 31, 3, 12, 12, 10, 10, -1, 12, -1, -1, 13, 35, -1, 13, 12, 18, -1, -1, 5, 5, 5, 0, 0, -1, 13, 12, -1, 1, -1, -1, -1, -1, 10, 10, 10, -1, 0, 10, 37, 10, 10, 23, -1, 0, -1, -1, 5, 5, -1, 12, 2, 18, 2, 18, -1, 18, 18, -1, 12, 4, 37, 12, 21, 21, -1, 14, 21, -1, 21, 0, 21, 29, -1, 8, 21, 25, -1, 38, -1, -1, -1, 13, -1, 4, 5, 37, -1, -1, 32, -1, 32, -1, 32, 2, -1, 28, 32, 19, 22, -1, 20, 0, -1, 20, -1, 23, 23, 4, 38, 20, 20, 3, 20, -1, 33, 1, 1, 4, 28, 20, 28, -1, -1, 13, 14, 1, -1, 28, -1, 14, 0, 23, -1, 28, -1, 14, 20, 4, 14, 3, 4, 1, 3, 9, 9, -1, 4, 4, 4, -1, 4, 28, 23, 30, -1, 28, 9, 9, 9, 9, 9, 9, 9, -1, 9, 12, 1, 12, -1, 9, -1, 31, 31, 23, -1, -1, 13, 31, 23, 31, -1, 19, -1, -1, -1, -1, 4, 19, 19, 1, 19, 0, 0, 0, -1, 0, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 19, 34, 10, 38, 10, 0, 4, 26, 10, 10, 10, 19, 1, 10, 4, 0, 10, -1, 10, -1, 1, 12, 10, 21, 10, 10, 0, 0, 5, 1, 0, 0, 0, 10, 5, -1, 0, 0, 2, 0, -1, -1, -1, 2, 5, 37, 5, -1, 1, -1, 23, 2, -1, 0, -1, 7, 34, 7, 7, 34, 0, 38, 12, 12, 14, 18, 3, -1, 34, 7, 7, 7, 7, 7, 13, 28, 7, 21, 7, 33, 28, 18, 16, 33, 7, -1, 1, 7, -1, 33, -1, 0, 4, -1, 33, 5, 23, 5, 5, 0, 0, 21, 0, 23, 13, 5, 5, 1, 5, 5, 11, 11, 11, 11, 3, 5, 5, 1, 18, 19, 3, 3, 3, 3, 3, 3, 36, 11, 33, -1, 16, 16, 16, 33, 28, 0, 11, 11, 8, -1, 11, 33, 11, -1, 26, 28, 3, 3, 23, -1, -1, 4, 3, 8, 3, 3, 23, 3, 3, 13, 3, 3, 1, 9, 3, 23, 23, 26, -1, 6, 6, 6, 6, 6, 6, 11, 6, 6, 0, 0, -1, 1, -1, 20, 34, 13, 6, 6, -1, 6, 6, 6, 0, 6, 6, 0, 0, 0, 0, -1, 0, 7, 0, 0, 0, -1, 10, 0, 0, 7, 9, 1, 7, 31, -1, 7, 34, 7, 34, 7, 34, 7, 38, 7, 8, 4, 8, 1, -1, -1, 7, 7, 7, -1, 7, 34, 7, 16, 28, 7, 32, 13, 5, 5, 5, 5, 4, 5, 5, 1, 0, 5, 2, 14, -1, 5, 20, 27, 0, 14, -1, 11, 11, 13, 11, 1, -1, -1, 11, -1, 27, 3, 1, -1, -1, 11, 1, 36, -1, 3, 3, 7, -1, 7, 3, 3, 1, 3, 3, 37, -1, 3, -1, 16, 3, 18, 16, 18, 3, 9, 20, 3, 16, -1, 0, 16, 16, 16, 1, -1, 8, 1, 31, 2, 2, 2, 26, -1, 18, 0, -1, 16, 1, 9, 18, 9, 0, 0, 0, 37, -1, 36, 36, 0, -1, 4, 10, 10, 6, 6, 0, 6, 6, 0, 6, 6, 6, 6, 0, 0, 2, 2, 2, 2, 2, -1, -1, 2, 6, 2, 2, 2, 2, 13, 2, 2, 6, 13, 38, 0, 2, 36, 13, 2, 2, 2, 0, 19, 34, 7, 7, 34, 7, 7, 7, 7, 25, 29, 29, 29, 29, 29, 29, 13, 29, -1, 29, -1, -1, 10, 8, 1, 25, 9, 8, -1, 9, 15, -1, 15, 15, 15, 13, 13, 4, 9, 37, 9, 9, 9, 9, 1, 8, 27, 1, 8, 15, 26, 26, 26, -1, 19, 1, 27, 27, 0, 1, 27, 27, 1, 27, 20, 27, -1, 20, 1, -1, 27, 20, 1, 20, 17, 20, 18, -1, 1, -1, 0, 0, 8, 7, 2, 1, 17, 18, 25, -1, 18, 18, 18, 18, 0, 31, 18, -1, 25, -1, 1, -1, -1, 4, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, -1, 2, 36, 2, 2, 2, 4, 36, 36, 2, 7, 19, 8, 13, 33, 28, -1, 19, 17, 1, 25, 12, 26, 30, 33, 20, 26, 29, 11, 29, 1, 29, 11, 8, 1, -1, 35, 35, -1, -1, 8, 11, -1, 25, 30, -1, -1, -1, 26, -1, 26, 30, 1, 30, 8, 38, 30, -1, 30, 28, -1, -1, 8, 35, 9, 4, 3, 24, 24, 24, -1, 1, 4, -1, 14, 4, 9, -1, 4, 15, 22, -1, 37, 22, 9, 15, 15, 9, 1, 7, 15, 15, 15, 1, 15, 2, 15, 24, 8, 24, 24, -1, 1, 27, 27, -1, -1, 27, 1, 27, 22, 22, 32, 32, 22, 22, -1, 22, 22, -1, 21, 23, -1, 19, 19, 0, -1, -1, -1, 10, -1, 26, 14, 0, 0, 4, 4, 0, -1, 4, 0, 23, 7, -1, 32, 14, 12, 5, 5, 4, -1, 4, -1, -1, -1, 13, -1, 3, 5, 25, -1, -1, 5, 25, 0, -1, 19, 5, 28, 5, 13, 33, 33, -1, 16, 21, 2, -1, 35, 30, 8, 8, 25, 30, 8, -1, 10, -1, -1, -1, -1, 8, 13, 0, -1, -1, 0, 4, -1, -1, 0, -1, 4, -1, -1, 4, 20, 4, 21, 24, 24, 0, 24, 8, 24, 16, 35, 3, 11, 38, 22, -1, 17, 6, 4, 28, 22, 22, 22, 22, 3, 22, 6, 22, 22, 12, -1, 22, 22, -1, -1, 12, 14, 35, -1, 35, 35, 2, 17, 35, 18, -1, 3, 11, 3, -1, 1, 9, 3, 0, 24, 16, 9, 19, 19, 24, -1, 1, -1, 3, 24, 1, 4, -1, 24, -1, -1, 24, 32, 19, -1, 1, 2, 0, -1, 19, -1, 1, 5, -1, 0, 5, 5, 4, -1, 2, 8, 8, 11, 26, 0, 8, 5, 13, -1, 0, 11, 11, 2, 0, 0, -1, 11, 0, 11, -1, 0, 16, 2, -1, 26, -1, 10, 6, 20, 15, 2, -1, 6, 0, -1, 17, 1, 1, -1, 6, 4, 6, 4, 27, 2, 3, -1, 2, 19, 1, 15, 15, 3, 25, 25, 1, 15, 8, 8, 15, 15, -1, 1, -1, 21, 9, 15, 3, 3, 9, 2, 3, -1, 4, 12, 9, -1, -1, 1, 17, 3, 17, -1, 0, 3, 25, -1, 1, 3, 6, 18, 3, 21, 21, 21, 2, 0, 2, 2, 24, 5, 21, 2, 2, 0, 17, 5, 6, 24, 1, 0, 5, 2, 12, 19, 17, 0, 5, 19, 0, 1, 11, 11, -1, 12, 0, 16, 16, 0, 33, 1, 37, 0, 26, 0, 0, 1, 15, 11, 0, -1, 0, 0, 6, 2, 10, -1, -1, 2, 2, 0, -1, -1, -1, 16, 6, 18, 0, 3, -1, 16, 3, 16, -1, 26, 14, 15, 1, -1, 3, 29, 15, 23, -1, 3, 3, 3, 9, 36, 37, 3, -1, 1, 30, 23, 21, -1, -1, 6, 6, 21, 10, 21, 7, 14, -1, 1, 3, 2, 5, -1, 2, 2, 9, 5, -1, 2, 29, 29, 2, 0, -1, -1, -1, -1, -1, 10, 1, 5, 15, 30, -1, 21, 11, -1, 1, 1, -1, -1, 30, 21, 35, -1, 11, 7, 3, 3, 4, 12, 11, 0, 36, 9, 1, 12, 24, -1, -1, 9, 19, 0, 6, 1, -1, -1, 0, 0, -1, -1, 6, 19, 0, 29, 24, 16, 4, -1, 13, 0, 16, 3, 3, 3, 16, 0, 23, 2, -1, 2, 36, 30, 3, 1, 16, 3, -1, 3, 2, 3, 3, 3, 1, 3, 16, 19, -1, 0, 1, 2, 2, 3, 7, 22, 15, 15, -1, 1, 2, 2, 3, 3, 24, 6, 29, 24, 24, -1, 4, -1]\n",
            "-------RUN7-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': \"CountVectorizer(stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b02d2c50>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[18, 1, 28, 28, -1, 1, 6, 5, 5, 5, -1, 22, 16, 22, 5, -1, 0, 18, -1, 1, -1, 16, 5, 10, 10, 21, -1, 1, 1, 1, 1, 1, 28, 25, 1, -1, 4, -1, -1, -1, 10, -1, 27, 10, 27, -1, 27, 27, 10, 1, 27, 10, -1, 5, -1, 14, 14, 1, -1, -1, 0, 1, -1, 5, -1, 5, 4, 0, 16, 4, 25, 16, 0, 0, 0, 4, 1, -1, 1, -1, 14, 14, -1, 14, 14, 21, 14, 0, 1, 0, 0, 5, 14, -1, -1, 5, 5, 0, -1, -1, 22, 11, 1, 11, 31, 25, -1, 13, -1, 13, -1, 16, -1, -1, 1, -1, -1, 21, 1, -1, 27, -1, 10, 11, 27, -1, 16, 1, 10, 16, 1, 13, -1, 16, 16, 18, 10, 1, 16, -1, 5, 5, 24, 32, 13, 5, 18, -1, 1, -1, 5, 5, 10, 5, -1, 5, 0, -1, -1, 2, 1, -1, 25, 2, -1, -1, -1, 11, 33, 1, 5, -1, -1, -1, 13, 5, 35, 36, 13, 0, 1, 1, 13, -1, 16, 5, 0, 1, 16, 26, -1, 13, 14, 1, 32, 14, 15, 15, 14, 1, 14, 1, 14, 14, -1, 14, 7, 14, 0, -1, 0, -1, 36, 14, 14, -1, -1, 0, 1, -1, 15, 0, 0, 0, -1, 15, -1, -1, -1, 32, 32, 32, 32, 15, 15, 32, 21, 32, 3, 15, 15, 4, 4, -1, 15, -1, 1, 11, 30, 1, 11, 15, 22, -1, -1, 6, 6, 6, 0, 0, -1, 11, -1, -1, 1, -1, -1, -1, -1, 4, 4, 4, 4, 0, 4, 31, 4, 4, 21, -1, 0, -1, 4, 6, -1, -1, 15, -1, 22, 2, 22, -1, 22, 22, -1, 15, 5, 31, 15, 4, 4, 4, 16, 4, -1, 4, 0, 4, 29, 4, 10, 4, 27, -1, 36, -1, -1, -1, 11, 18, -1, -1, 31, -1, -1, 18, -1, 18, -1, 18, 2, -1, 26, 18, 20, 24, -1, 13, 0, -1, 13, -1, 21, -1, -1, 36, 13, 13, 3, 13, -1, 34, 1, 1, 5, 26, 13, 26, 26, 18, 11, 16, 1, 16, 26, 4, 16, 0, 21, 0, 26, 12, 16, 13, 5, 16, 3, 5, 1, 3, 9, 9, 1, 5, 5, 5, 26, 5, 26, 21, 33, -1, 26, 9, 9, 9, 9, 9, 9, 9, -1, 9, 15, 1, 15, -1, 9, 0, 32, 32, 21, 26, -1, 11, 32, 21, 32, -1, 20, -1, -1, -1, -1, -1, 20, 20, 1, 20, 0, 0, 0, -1, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 20, 18, 4, -1, 4, 0, 5, 25, 4, 4, 4, 20, 1, 4, -1, 0, 4, -1, 4, -1, 1, 15, 4, 4, 4, 4, 0, 0, 6, 1, 0, 0, 0, 4, 6, 0, 0, 0, 2, 0, -1, -1, 6, 2, 6, 31, 6, -1, 1, -1, 21, 2, -1, 0, 1, 8, 18, 8, 8, 18, 0, 36, 15, 15, 16, -1, 3, -1, 18, 8, 8, 8, 8, 8, 11, 26, 8, 4, 8, 34, 26, -1, 19, 34, 8, 18, 1, 8, -1, 34, 15, 0, 5, -1, 34, 6, 21, 6, 6, 0, 0, 4, 0, 21, 11, 6, 6, 1, 6, 6, 12, 12, 12, 12, 3, 6, 6, 1, 22, 20, 3, 3, 3, 3, 3, 3, 35, 12, 34, -1, 19, 19, 19, 34, 26, 0, 12, 12, 10, -1, 12, 34, 12, 26, 25, 4, 3, 3, 21, -1, -1, 5, 3, 10, 3, 3, 21, 3, 3, 11, 3, 3, 1, 9, 3, 21, 21, 25, -1, 7, 7, 7, 7, 7, 7, 12, 7, 7, 0, 0, -1, 1, -1, 13, 18, 11, 7, 7, 7, 7, 7, 7, 0, 7, 7, 0, 0, 0, 0, -1, 0, 8, 0, 0, 0, -1, 4, 0, 0, 8, 9, 1, 8, 32, -1, 8, 18, 8, 18, 8, 18, 8, 36, 8, 10, 5, 10, 1, -1, -1, 8, 8, 8, 1, 8, 18, 8, 19, 26, 8, 18, 11, 6, 6, 6, 6, -1, 6, 6, 1, 0, 6, 2, 16, -1, -1, 13, 28, 0, 16, -1, 12, 12, 11, 12, 1, -1, -1, 12, -1, 28, 3, 1, -1, -1, 12, 1, 35, -1, 3, 3, 8, 31, 8, 3, 3, 1, 3, 3, 31, -1, 3, -1, 19, 3, 22, 19, 22, 3, 9, 13, 3, 19, -1, 0, 19, 19, 19, 1, -1, 10, 1, 32, 2, 2, 2, 25, -1, 22, 0, -1, 19, 1, 9, 22, 9, 0, 0, 0, 31, -1, 35, 35, -1, -1, 5, 4, 4, 7, 7, 0, 7, 7, 0, 7, 7, 7, 7, 0, -1, 2, 2, 2, 2, 2, -1, -1, 2, 7, 2, 2, 2, 2, 11, 2, 2, 7, 11, 36, 0, 2, 35, 11, 2, 2, 2, 0, 20, 18, 8, 8, 18, 8, 8, 8, 8, -1, 29, 29, 29, 29, 29, 29, 11, 29, -1, 29, -1, -1, 4, 10, 1, 27, 9, 10, -1, 9, 17, -1, 17, 17, 17, 11, 11, 5, 9, 31, 9, 9, 9, 9, 1, 10, 28, 1, 10, 17, 25, 25, 25, 1, 20, 1, 28, 28, 0, 1, 28, 28, 1, 28, 13, 28, -1, 13, 1, 13, 28, 13, 1, 13, 14, 13, 8, 21, 1, -1, 0, -1, 10, -1, 2, 1, 14, 22, 27, -1, 22, 22, 22, 22, 0, 9, 22, -1, 27, -1, 1, 1, 31, 5, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, -1, 2, 35, 2, 2, 2, -1, 35, 35, 2, 8, 20, 10, 11, 34, 26, -1, 20, 14, 1, 27, 15, 25, 33, 34, 13, 25, 29, 12, 29, 1, 29, 12, 10, 1, -1, 30, 30, -1, 30, 10, 12, -1, 27, 33, -1, -1, -1, 25, -1, 25, 33, 1, 33, 10, 36, 33, -1, -1, 26, -1, 4, 10, 30, 9, 5, 3, 23, 23, 23, -1, 1, 5, 31, 16, 5, 9, -1, -1, 17, 24, -1, 31, 24, 9, 17, 17, 9, 1, 8, 17, 17, 17, 1, 17, 2, 17, 23, 10, 23, 23, -1, -1, 28, 28, -1, -1, 28, 1, 28, 24, 24, 18, 18, 24, 24, -1, 24, 24, -1, 4, 21, 1, 20, 20, 0, -1, 25, -1, 4, -1, 25, 16, 0, 0, 5, 5, 0, -1, 5, 0, 21, 8, -1, 18, 16, 15, 6, 6, 5, -1, 5, -1, -1, 11, 11, 11, 3, 6, 27, 14, -1, 6, 27, 0, 1, 20, 6, 26, 6, 11, 34, 34, -1, 19, 4, 2, -1, 30, 33, 10, 10, -1, 33, 10, 21, 4, -1, -1, -1, 30, 10, -1, 0, -1, -1, 0, 5, 4, -1, 0, -1, 5, 13, -1, -1, 13, 5, 4, 23, 2, 0, 23, 10, 23, 19, 30, 3, 12, 36, 24, -1, 14, -1, 5, 26, 24, 24, 24, 24, 3, 24, 7, 24, 24, 15, 7, 24, 24, -1, -1, 15, 16, 30, 30, 30, 30, 2, 14, 30, 22, -1, 3, 12, 3, -1, 1, 9, 3, 0, 23, 19, 9, 20, 20, 23, 1, 1, 11, 3, 23, 1, 5, -1, 23, 12, 11, 23, 18, 20, 1, 1, 2, 0, -1, 20, 12, 1, 6, -1, 0, 6, 6, 5, -1, 2, 10, 10, 12, 25, 0, 10, 6, 11, -1, 0, 12, 12, 2, 0, 0, -1, 12, 0, 12, -1, 0, 19, 2, 3, 13, -1, 4, 7, 13, 17, 2, -1, 7, 0, -1, 14, 1, 1, -1, 7, 5, 7, 5, 28, -1, 3, 11, 2, 20, 1, 17, 17, 3, 27, 27, 1, 17, 10, 10, 17, 17, -1, 1, -1, 4, 9, 17, 3, 3, 9, 2, 3, 11, 5, 15, 9, -1, -1, 1, 14, 3, 14, 1, 0, 3, 27, -1, 1, 3, 7, 22, 3, 4, 4, 4, 2, 0, 2, 2, 23, 6, 4, 2, 2, 0, 14, 6, 7, 23, 1, 0, 6, 2, 15, 20, 14, 0, 6, 20, 0, 1, 12, 12, -1, 15, 0, 19, 19, 0, 34, -1, 31, 0, 25, 0, 0, 1, 17, 12, -1, -1, 0, 0, 7, 2, 4, 1, 36, 2, 2, 0, 17, -1, -1, 19, 7, 22, 0, 3, -1, 19, 3, 19, -1, 25, 16, 17, 1, -1, 3, 29, 17, 21, -1, 3, 3, 3, 9, 35, 31, 3, 4, 1, 33, 21, 4, 1, -1, 7, 7, 4, 4, 4, 8, 16, 4, 1, 3, 2, 6, -1, 2, 2, 9, 6, -1, 2, 29, 29, 2, 0, -1, -1, -1, -1, -1, 4, 1, 6, 17, 33, -1, 4, 12, 25, 1, 1, 13, -1, 33, 4, 30, 30, 12, 8, 3, 3, 5, 15, 12, 0, 35, 9, 1, 15, 23, 1, -1, 9, 20, 0, 7, 1, -1, -1, 0, 0, -1, -1, 7, 20, 0, 29, 23, 19, 5, 11, 11, 0, 19, 3, 3, 3, 19, 0, 21, 2, -1, 2, 35, 33, 3, 1, 19, 3, -1, 3, 2, 3, 3, 3, -1, 3, 19, 20, 9, 0, 1, 2, 2, 3, 8, 24, 17, 17, -1, 1, 2, 2, 3, 3, 23, 7, 29, 23, 23, -1, -1, -1]\n",
            "-------RUN8-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': \"CountVectorizer(ngram_range=(1, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b891dd10>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[16, 0, 28, 28, -1, 0, -1, 5, 5, 5, -1, 25, 18, 25, 5, -1, 33, 16, -1, -1, -1, 18, 5, 9, 9, 23, -1, 0, 0, 0, 0, -1, 28, 27, 0, -1, -1, -1, -1, -1, 9, -1, 26, 9, 26, -1, 26, 26, 9, 0, 26, 9, 5, 5, 5, 12, 12, 0, 5, -1, 33, 0, -1, 5, -1, 5, 4, -1, -1, 4, 27, -1, 1, 1, -1, 4, -1, -1, 0, -1, 12, 12, 30, 12, 12, 23, 12, 1, 0, 1, 1, 5, 12, -1, -1, 5, 5, 1, -1, -1, 25, 24, 0, 24, 32, 27, -1, -1, -1, -1, -1, 18, -1, -1, 0, -1, -1, 23, 0, -1, -1, -1, 9, 24, 26, 31, 18, 0, 9, 18, 0, 20, 0, 18, 18, 16, 9, 0, 18, -1, 5, 5, 22, 31, 20, 5, 16, -1, 0, -1, 5, 5, 9, 5, -1, 5, 1, -1, -1, 2, 0, -1, 27, 2, -1, -1, 22, 24, -1, 0, 5, -1, -1, -1, 20, 5, 2, 37, 20, -1, 0, 0, -1, -1, 18, 5, -1, 0, 18, 19, 19, 20, 12, 0, 31, 12, 13, 13, 12, 0, 12, 0, 12, 12, -1, 12, 8, 12, 1, -1, 1, 30, 37, 12, 12, 30, 30, 1, 0, -1, 13, 1, 1, 1, -1, 13, -1, -1, -1, 31, 31, 31, 31, 13, 13, 31, 23, 31, 3, 13, -1, 4, 4, -1, 13, -1, 0, 24, 35, -1, -1, 13, 25, -1, -1, 6, 6, 6, 1, 1, -1, 24, 13, -1, 0, -1, -1, -1, -1, 4, 4, 4, 4, 1, 4, 32, 4, 4, 23, -1, 1, -1, 4, 26, 6, -1, 13, 2, 25, 2, 25, 0, 25, 25, -1, 13, 5, 32, 13, 4, 4, 4, 18, 4, -1, 4, 33, 4, 29, -1, 9, 4, 26, -1, 37, -1, -1, -1, 24, -1, 5, 31, 32, -1, -1, 16, -1, 16, -1, 16, 2, -1, 19, 16, 17, 22, 5, 20, 1, -1, 20, -1, 23, 23, -1, 37, 20, 20, 3, 20, -1, 34, 0, 0, 5, 19, 20, 19, 19, 16, 24, 18, 0, 18, 19, 4, 18, -1, 23, -1, 19, 11, 18, 20, 5, 18, 3, 5, 0, 3, 10, 10, 0, 5, 5, 5, 19, 5, 19, 23, 30, -1, 19, 10, 10, 10, 10, 10, 10, 10, -1, 10, 13, 0, 13, -1, 10, -1, 31, 31, 23, 19, -1, -1, 31, 23, 31, -1, 17, -1, -1, -1, -1, -1, 17, 17, 0, 17, 1, 1, 1, -1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 33, 1, 17, 16, 4, 37, 4, 1, 5, 27, 4, 4, 4, 17, 0, 4, -1, -1, 4, -1, 4, -1, 0, 13, 4, 4, 4, 4, 1, 1, 6, 0, 1, -1, 1, 4, 6, -1, 1, 33, 2, 1, -1, -1, 6, 2, 6, 32, 6, -1, 0, -1, 23, 2, -1, 33, -1, 7, 16, 7, 7, 16, -1, 37, 13, 13, 18, -1, 3, -1, 16, 7, 7, 7, 7, 7, 24, 19, 7, 4, 7, 34, 19, -1, 15, 34, 7, 16, 0, 7, -1, 34, -1, 1, -1, -1, 34, 6, -1, 6, 6, 33, -1, 4, 1, -1, 24, 6, 6, 0, 6, 6, 11, 11, 11, 11, 3, 6, 6, 0, 25, 17, 3, 3, 3, 3, 3, 3, -1, 11, 34, -1, 15, 15, 15, 34, 19, -1, 11, 11, 9, -1, 11, 34, 11, 19, 27, 4, 3, 3, 23, -1, -1, 5, 3, 9, 3, 3, 23, 3, 3, 24, 3, 3, 0, 10, 3, 23, 23, 27, -1, 8, 8, 8, 8, 8, 8, 11, 8, 8, 1, 1, -1, 0, -1, 20, 16, 24, 8, 8, -1, 8, 8, 8, 1, 8, 8, 1, 1, 1, 1, -1, 1, 7, 1, 1, 33, -1, 4, -1, -1, 7, 10, 0, 7, 31, -1, 7, 16, 7, 16, 7, 16, 7, 37, 7, 9, 5, 9, -1, -1, -1, 7, 7, 7, 0, 7, 16, 7, 15, 19, 7, 16, 24, 6, 6, 6, 6, -1, 6, 6, 32, -1, 6, 2, 18, 26, -1, 20, 28, 1, 18, -1, 11, 11, 24, 11, -1, -1, -1, 11, -1, 28, 3, 0, -1, -1, 11, 0, 36, -1, 3, 3, 7, -1, 7, 3, 3, 0, 3, 3, 32, -1, 3, -1, 15, 3, 25, 15, 25, 3, 10, 20, 3, 15, 5, -1, 15, 15, 15, 0, -1, 9, 0, 31, 2, 2, 2, 27, -1, 25, 1, -1, 15, 0, 10, 25, 10, 1, -1, 1, 32, -1, 36, 36, -1, -1, 5, 4, 4, 8, 8, 1, 8, 8, 1, 8, 8, 8, 8, 1, -1, 2, 2, 2, 2, 2, -1, -1, 2, 8, 2, 2, 2, 2, -1, 2, 2, 8, 24, 37, 1, 2, 36, -1, 2, 2, 2, 1, 17, 16, 7, 7, 16, 7, 7, 7, 7, 26, 29, 29, 29, 29, 29, 29, 24, 29, -1, 29, -1, -1, 4, 9, 0, 26, 10, 9, -1, 10, 14, -1, 14, 14, 14, 24, 24, 5, 10, 32, 10, 10, 10, 10, 0, 9, 28, 0, 9, 14, 27, 27, 27, 0, 17, -1, 28, 28, -1, -1, 28, 28, 0, 28, 20, 28, -1, 20, 0, -1, 28, 20, 0, 20, 12, 20, 7, 23, 0, -1, -1, -1, 9, 7, 2, 0, 12, 25, 26, -1, 25, 25, 25, 25, -1, 10, 25, -1, 26, -1, 0, -1, 32, 5, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, -1, 2, 36, 2, 2, 2, -1, 36, 36, 2, 7, 17, 9, 24, 34, 19, -1, 17, 12, 0, 26, 13, -1, 30, 34, -1, 27, 29, 11, 29, 0, 29, 11, 9, 0, -1, 35, 35, 35, -1, 9, 11, -1, 26, 30, -1, -1, -1, 27, -1, 27, 30, 0, 30, 9, 37, 30, -1, -1, 19, -1, 4, 9, 35, 10, 5, 3, 21, 21, 21, -1, 0, 5, 32, -1, 5, 10, -1, -1, 14, 22, -1, 32, 22, 10, 14, 14, 10, 0, 7, 14, 14, 14, 0, 14, 2, 14, 21, 9, 21, 21, -1, 0, 28, 28, -1, -1, 28, 0, 28, 22, 22, 16, -1, 22, 22, -1, 22, 22, -1, 4, 23, -1, 17, 17, 1, -1, -1, -1, 4, -1, -1, 18, 1, 1, 5, 5, 33, -1, -1, 1, 23, 7, -1, 16, 18, 13, 6, 6, 5, -1, 5, -1, -1, -1, -1, -1, 3, 6, 26, 12, -1, 6, 26, 1, 0, 17, 6, 19, 6, 24, 34, 34, -1, 15, 4, 2, -1, -1, 30, 9, 9, -1, 30, 9, -1, 4, -1, -1, -1, -1, 9, -1, 1, 19, -1, 1, -1, 4, -1, 1, -1, 5, -1, -1, -1, 20, 5, 4, 21, 21, -1, 21, 9, 21, 15, 35, 3, 11, 37, 22, -1, 12, -1, 5, 19, 22, 22, 22, 22, 3, 22, 8, 22, 22, 13, -1, 22, 22, -1, -1, 13, 18, 35, -1, 35, 35, 2, 12, 35, -1, -1, 3, 11, 3, -1, 0, 10, 3, -1, 21, 15, 10, 17, 17, 21, 0, -1, -1, 3, 21, 0, 5, -1, 21, 11, -1, 21, 16, 17, -1, 0, 2, 1, -1, 17, 11, 0, 6, -1, -1, 6, 6, 5, -1, 2, 9, 9, 11, -1, 1, 9, 6, -1, -1, 33, 11, 11, 2, 1, 33, -1, 11, 1, 11, -1, 1, 15, -1, -1, 27, -1, 4, 8, 20, 14, 2, -1, 8, 1, -1, 12, 0, -1, 0, 8, 5, 8, 5, 28, 2, 3, -1, 2, 17, 0, 14, 14, 3, 26, 26, 0, 14, 9, 9, 14, 14, -1, 0, -1, 4, 10, 14, 3, 3, 10, 2, 3, 24, 5, 13, 10, -1, -1, 0, 12, 3, 12, -1, -1, 3, -1, -1, 0, 3, 8, 25, 3, 4, 4, 4, 2, -1, 2, 2, 21, 6, 4, 2, 2, 1, 12, 6, 8, 21, 0, -1, 6, 2, 13, 17, 12, 1, 6, 17, 1, 0, 11, 11, -1, 13, 1, 15, 15, -1, 34, 0, 32, 1, 27, 1, 1, 0, 14, 11, -1, -1, 1, 1, 8, 2, 4, -1, -1, 2, 2, 1, 14, -1, 35, 15, 8, 25, -1, 3, -1, 15, 3, 15, -1, 27, 18, 14, 0, 0, 3, 29, 14, 23, -1, 3, 3, 3, 10, 36, 32, 3, 4, 0, 30, 23, 4, -1, -1, 8, 8, 4, 4, 4, 7, 18, 4, 0, 3, 2, 6, -1, 2, 2, 10, 6, -1, 2, 29, 29, 2, 1, -1, -1, -1, -1, -1, 4, 0, 6, 14, 30, -1, 4, 11, 27, 0, -1, -1, -1, 30, 4, 35, -1, 11, 7, 3, 3, 5, 13, 11, 1, 36, 10, 0, 13, 21, 0, -1, 10, 17, 1, 8, 0, -1, -1, 33, 1, -1, -1, 8, 17, 33, 29, 21, 15, 5, -1, 24, 33, 15, 3, 3, 3, 15, -1, 23, 2, -1, 2, 36, 30, 3, 0, 15, 3, -1, 3, 2, 3, 3, 3, -1, 3, 15, 17, -1, 1, 0, 2, 2, 3, 7, 22, 14, 14, -1, 0, 2, 2, 3, 3, 21, 8, 29, 21, 21, -1, 5, -1]\n",
            "-------RUN9-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': \"CountVectorizer(ngram_range=(2, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df4a9008410>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[5, 2, 28, 28, -1, 2, 7, 6, 6, 6, -1, 14, 19, 14, 6, -1, 0, 5, -1, -1, -1, 19, -1, 11, 11, 22, -1, -1, 2, -1, 2, -1, 28, 13, -1, -1, 4, -1, -1, -1, 11, -1, 27, 11, 27, -1, 27, 27, 11, 2, 27, 11, 6, 6, -1, 18, 18, 2, -1, -1, 0, -1, -1, 6, -1, 6, 4, 0, 19, 4, 13, 19, 0, 0, 0, 4, 33, -1, -1, -1, 18, 18, -1, 18, 18, 22, 18, 0, 2, 0, 0, 6, 18, -1, -1, 6, 6, 0, -1, -1, 14, 25, 2, 25, 33, 13, -1, -1, -1, -1, -1, 19, -1, 5, 2, -1, -1, 22, 2, -1, -1, -1, 11, 25, 27, -1, 19, 2, 11, 19, 2, 17, -1, 19, 19, 5, 11, 2, 19, -1, 6, -1, 5, 30, 17, 6, 5, -1, 2, -1, 6, 6, 11, 6, -1, 6, 0, -1, -1, 1, 2, 1, 13, 1, -1, 5, 5, 25, 31, 2, 6, -1, -1, -1, 17, 6, 1, 13, 17, -1, -1, 2, 17, -1, 19, 6, -1, 2, 19, -1, 24, 17, 18, 2, 30, -1, 16, 16, 18, 2, 18, 2, 18, 18, -1, 18, 9, 18, 0, 14, 0, -1, 13, 18, 18, -1, -1, 0, 2, -1, 16, 0, 0, 0, -1, 16, -1, -1, -1, 30, 30, 30, 30, 16, 16, 30, 22, 30, 3, 16, 16, 4, 4, -1, 16, -1, 2, 25, 26, -1, -1, 16, 14, 5, -1, 7, 7, 7, 0, 0, -1, 25, 26, -1, 2, -1, 2, -1, -1, 4, 4, 4, 4, 0, 4, 33, 4, 4, 22, -1, 0, -1, 4, 7, -1, -1, 16, 1, 14, 1, 14, -1, 14, 14, -1, 16, 6, 33, 16, 4, 4, 4, 19, 4, -1, 4, 0, 4, 29, 4, 11, 4, 27, -1, 13, -1, -1, -1, 25, 5, 6, 7, 33, -1, -1, 5, -1, 5, -1, 5, 1, -1, 24, 5, 21, 5, -1, 17, 0, -1, 17, -1, 22, 22, -1, -1, 17, 17, 3, 17, -1, 24, 2, 2, 6, 24, 17, 24, 24, 5, 25, 19, 2, -1, 24, 4, 19, 0, 22, 19, 24, -1, 19, 17, 6, 19, 3, -1, 2, 3, 10, 10, -1, 6, 6, 6, 24, 6, 24, 22, 31, -1, 24, 10, 10, 10, 10, 10, 10, 10, -1, 10, 16, 2, 16, -1, 10, -1, 30, 30, 22, 24, -1, -1, 30, 22, 30, -1, 21, -1, -1, -1, -1, -1, 21, 21, 2, 21, 0, 0, 0, -1, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 21, 5, 4, -1, 4, 0, 6, 13, 4, 4, 4, 21, 2, 4, -1, -1, 4, 26, 4, -1, 2, 16, 4, 4, 4, 4, 0, 0, 7, 2, 0, 0, 0, 4, 7, -1, 0, 0, 1, 0, -1, 15, 7, 1, 7, 33, 7, -1, 2, -1, 22, 1, -1, 0, -1, 8, 5, 8, 8, 5, 0, 13, 16, 16, 19, 14, 3, 5, 5, 8, 8, 8, 8, 8, 25, 24, 8, 4, 8, 32, 24, 14, 20, 32, 8, 5, 2, 8, 14, 32, -1, 0, -1, 5, 32, 7, 22, 7, 7, 0, 0, 4, 0, 22, 25, 7, 7, 2, 7, 7, 12, 12, 12, 12, 3, 7, 7, 2, 14, 21, 3, 3, 3, 3, 3, 3, 1, 12, 32, -1, 20, 20, 20, 32, 24, 0, 12, 12, 11, -1, 12, 32, 12, 24, 13, 4, 3, 3, 22, -1, -1, 6, 3, 11, 3, 3, 22, 3, 3, 25, 3, 3, 2, 10, 3, 22, 22, 13, -1, 9, 9, 9, 9, -1, 9, 12, 9, 9, 0, 0, -1, 2, -1, 17, 5, -1, 9, 9, -1, 9, 9, 9, 0, 9, 9, 0, 0, 0, 0, -1, 0, 8, 0, 0, 0, -1, 4, 0, -1, 8, 10, 2, 8, 30, -1, 8, 5, 8, 5, 8, 5, 8, 13, 8, 11, 6, 11, 2, -1, 13, 8, 8, 8, 2, 8, 5, 8, 20, 24, 8, 5, 25, 7, 7, 7, 7, -1, 7, 7, -1, 0, 7, 1, 19, -1, -1, 17, 28, 0, 19, -1, 12, 12, -1, 12, -1, 14, -1, 12, -1, 28, 3, -1, -1, -1, 12, 2, 1, -1, 3, 3, -1, -1, 8, 3, 3, 2, 3, 3, 33, -1, 3, -1, 20, 3, 14, 20, 14, 3, 10, 17, 3, 20, -1, -1, 20, 20, 20, 2, -1, 11, 2, 30, 1, 1, 1, 13, -1, 14, 0, 5, 20, 2, 10, 14, 10, 0, -1, 0, -1, -1, 1, 1, -1, -1, 6, 4, 4, 9, 9, 0, 9, 9, 0, 9, 9, 9, 9, 0, -1, 1, 1, 1, 1, 1, -1, -1, 1, 9, 1, 1, 1, 1, -1, 1, 1, 9, 25, 13, 0, 1, 1, -1, 1, 1, 1, 0, 21, 5, 8, 8, 5, 8, 8, 8, 8, 27, 29, 29, 29, 29, 29, 29, 25, 29, -1, 29, -1, -1, 4, 11, 2, 27, 10, 11, -1, 10, 15, -1, 15, 15, 15, 25, 25, 6, 10, 33, 10, 10, 10, 10, 2, 11, 28, 2, 11, 15, 13, 13, 13, 2, 21, -1, 28, 28, 0, -1, 28, 28, 2, 28, 17, 28, -1, 17, 2, -1, 28, 17, 2, 17, 18, 17, 14, -1, 2, -1, -1, -1, 11, 8, 1, 2, 18, 14, 27, -1, 14, 14, 14, 14, 0, 10, 14, -1, 27, -1, 2, -1, -1, 6, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 8, 21, 11, 25, 32, 24, -1, 21, 18, 2, 27, 16, 13, 31, 32, 17, 13, 29, 12, 29, 2, 29, 12, 11, 2, -1, 26, 26, 26, 26, 11, 12, -1, 27, 31, -1, -1, -1, 13, -1, 13, 31, 2, 31, 11, 13, 31, -1, -1, 24, -1, 4, 11, 26, 10, 6, 3, 23, 23, 23, -1, 2, 6, -1, 19, 6, 10, -1, -1, 15, 5, 5, -1, 5, 10, 15, 15, 10, -1, 8, 15, 15, 15, 2, 15, 1, 15, 23, 11, 23, 23, -1, 2, 28, 28, -1, -1, 28, 2, 28, 5, 5, 5, 5, 5, 5, -1, 5, 5, -1, 4, 22, -1, 21, 21, 0, -1, -1, -1, 4, -1, 13, 19, 0, 0, 6, 6, 0, -1, 6, 0, 22, 8, -1, 5, 19, 16, 7, 7, 6, 2, 6, -1, -1, -1, -1, -1, 3, 7, 27, -1, -1, 7, 27, 0, -1, 21, 7, 24, 7, 25, 32, 32, -1, 20, 4, 1, -1, 26, 31, 11, 11, -1, 31, 11, -1, 4, -1, -1, -1, 26, 11, -1, 0, -1, -1, 0, 6, 4, -1, 0, -1, 6, 17, -1, -1, 17, 6, 4, 23, 23, 0, 23, 11, 23, 20, 26, 3, 12, 13, 5, 5, 18, -1, 6, 24, 5, 5, 5, 5, 3, 5, 9, 5, 5, 16, 9, 5, 5, -1, -1, 16, 19, 26, 26, 26, 26, 1, 18, 26, 14, -1, 3, 12, 3, -1, 2, 10, 3, 0, 23, 20, 10, 21, 21, 23, -1, -1, -1, 3, 23, 2, 6, -1, 23, 12, -1, 23, 5, 21, -1, -1, 1, 0, -1, 21, 12, 2, 7, -1, 0, 7, 7, 6, -1, 1, 11, 11, 12, 13, 0, 11, 7, -1, -1, 0, 12, 12, 1, 0, 0, -1, 12, 0, 12, -1, 0, 20, 1, -1, 17, -1, 4, 9, 17, 15, 1, -1, 9, 0, -1, 18, 2, -1, -1, 9, 6, 9, 6, 28, 1, 3, -1, 1, 21, 2, 15, 15, 3, 27, 27, -1, 15, 11, 11, 15, 15, -1, 2, -1, 4, 10, 15, 3, 3, 10, 1, 3, 25, 6, 16, 10, -1, -1, 2, 18, 3, 18, -1, -1, 3, -1, -1, 2, 3, 9, 14, 3, 4, 4, 4, 1, 0, 1, 1, 23, 7, 4, 1, 1, 0, 18, 7, 9, 23, 2, 0, 7, 1, 16, 21, 18, 0, 7, 21, 0, 2, 12, 12, -1, 16, 0, 20, 20, 0, 32, -1, 33, 0, 13, 0, 0, 2, 15, 12, -1, -1, 0, 0, 9, 1, 4, -1, 13, 1, 1, 0, -1, -1, 26, 20, 9, 14, 0, 3, 27, 20, 3, 20, -1, 13, 19, 15, 2, -1, 3, 29, 15, 22, -1, 3, 3, 3, 10, 1, 33, 3, 4, 2, 31, 22, 4, -1, -1, 9, 9, 4, 4, 4, 8, 19, 4, 2, 3, 1, 7, -1, 1, 1, 10, 7, -1, 1, 29, 29, 1, 0, -1, -1, -1, -1, -1, 4, 2, 7, 15, 31, -1, 4, 12, -1, 2, -1, -1, 13, 31, 4, 26, 26, 12, 8, 3, 3, -1, 16, 12, 0, 1, 10, 2, 16, 23, -1, 17, 10, 21, 0, 9, 2, -1, -1, 0, 0, -1, -1, 9, 21, -1, 29, 23, 20, 6, -1, -1, 0, 20, 3, 3, 3, 20, 0, 22, 1, -1, 1, 1, 31, 3, 2, 20, 3, -1, 3, 1, 3, 3, 3, -1, 3, 20, 21, -1, 0, 2, 1, 1, 3, 8, 5, 15, 15, -1, 2, 1, 1, 3, 3, 23, 9, 29, 23, 23, -1, -1, -1]\n",
            "-------RUN10-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': \"CountVectorizer(stop_words='english')\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[38, 13, 26, 26, -1, 39, -1, 21, 21, 21, -1, -1, 14, -1, 21, -1, 46, 42, -1, -1, -1, 14, -1, 8, 8, 25, 48, -1, 19, -1, 13, -1, 26, 9, 57, -1, 0, -1, -1, -1, 8, -1, 30, 8, 30, 56, 30, 30, 8, -1, 30, 8, 31, -1, -1, 11, 11, -1, 31, 31, 41, 54, 31, 21, -1, 21, 0, 41, 14, 0, 9, 14, 3, 3, -1, 0, -1, -1, -1, -1, 11, 11, -1, 11, 11, 25, 11, 3, 19, -1, 3, 21, 11, 49, 59, 21, 21, 3, -1, 9, 27, 35, 39, 43, 33, 9, -1, -1, -1, -1, -1, 14, -1, -1, 54, 56, 56, -1, -1, -1, 4, 56, 8, 35, 30, -1, 14, 19, 8, 14, 19, 12, 57, 14, 14, 42, 8, 52, 14, -1, 21, -1, 22, 32, 12, 31, 42, -1, 13, -1, 31, 31, 8, 21, -1, -1, 24, -1, -1, 34, -1, -1, 9, 34, -1, -1, -1, 35, -1, 39, 21, -1, -1, -1, 12, 31, -1, 9, 12, -1, 54, 52, 12, -1, 14, -1, -1, 19, 14, 20, 20, 12, 11, 19, 32, 11, 15, 15, 11, -1, 11, -1, 11, 11, -1, 11, 6, 11, 3, 55, 24, -1, 9, 11, 11, -1, -1, 3, 19, -1, 15, -1, 24, 24, 59, 15, 48, 48, -1, 32, 32, 32, 32, 15, 15, 32, 25, 32, 1, 15, 15, 0, 0, -1, 15, -1, -1, 35, 28, -1, -1, 15, 27, -1, 51, 4, 4, 4, 24, 24, -1, 35, -1, -1, 13, -1, -1, 48, 49, 0, 0, 0, 0, 24, 0, 33, 0, 0, -1, 48, 3, 51, 0, 4, 4, -1, 15, 34, -1, 34, 27, 57, 27, 27, 56, 15, 21, 33, 15, 0, 0, 0, 14, 0, 0, 0, 41, 0, 29, 0, 8, 0, 30, -1, 9, -1, 51, -1, 35, 22, 44, -1, 33, 14, -1, 42, 45, 42, -1, 42, 34, -1, 20, 42, 18, 22, -1, 12, -1, -1, 12, -1, 25, 25, -1, 9, 12, 12, 50, 12, 58, 20, 13, 13, 47, 20, 12, 20, 20, -1, 35, 14, 39, 14, 20, 0, 14, 46, 25, -1, 20, 10, 14, 12, 47, 14, 50, 47, 19, 50, 7, 7, 57, 47, 47, 47, 20, 47, 20, 25, 36, 59, 20, 7, 7, 7, 7, 7, 7, 7, 49, 7, 15, 13, 15, 53, 7, -1, 32, 32, 25, 20, -1, 43, 32, 25, 32, -1, 18, -1, -1, -1, 49, -1, 18, 18, -1, 18, 24, 3, 24, -1, 3, 0, 3, 3, 3, -1, 3, 3, 3, 46, -1, 18, -1, 0, 9, 0, 24, 21, 9, 0, 0, 0, 18, 13, 0, -1, -1, 0, -1, 0, -1, 13, 15, 0, 0, 0, 0, -1, 3, 4, 19, 3, -1, -1, 0, 4, -1, 3, 46, 34, 3, -1, -1, 4, 34, 4, 33, 4, -1, -1, -1, 25, 34, -1, 41, -1, 5, 38, 5, 5, 38, -1, 9, 15, 15, 14, 55, -1, -1, 38, 5, 5, 5, 5, 5, -1, 20, 5, 0, 5, 37, 20, 55, 17, 37, 5, 38, 39, 5, 55, 37, -1, 3, -1, -1, 37, 4, -1, 4, 4, -1, -1, 0, 3, -1, 35, 4, 4, 19, 4, 4, 10, 10, 10, 10, -1, 4, 4, 19, 27, 18, -1, 1, 1, 1, 1, 1, -1, 10, 37, -1, 17, 17, 17, 37, 20, -1, 10, 10, 8, 49, 10, 37, 10, 20, 9, 0, 50, 50, 25, 20, 56, 21, 1, 8, 1, 1, 25, 1, 1, 35, 1, 1, -1, 7, 1, 25, 25, 9, 49, 6, 6, 6, 6, 6, 6, 10, 6, 6, 3, 3, -1, 13, -1, 12, 38, 43, 6, 6, -1, 6, 6, 6, 3, 6, 6, -1, 3, 3, 3, -1, 3, 5, 3, 3, -1, -1, 0, -1, -1, 5, 7, -1, 5, 32, -1, 5, 38, 5, 38, 5, 38, 5, 9, 5, 8, 21, 8, 19, 59, 9, 5, 5, 5, 13, 5, -1, 5, 17, 20, 5, -1, 35, 4, 4, 4, 4, 44, 4, 4, -1, 41, 4, -1, 14, -1, 4, 12, 26, 3, 14, -1, 10, 10, 43, 10, -1, 55, -1, 10, -1, 26, -1, -1, -1, -1, 10, 52, 40, -1, 1, 1, 5, 33, 5, 1, 1, 54, 1, 1, 33, -1, 1, 58, 17, 1, 27, 17, 27, 1, 7, 12, 1, 17, -1, -1, 17, 17, 17, 19, -1, 8, -1, 32, 2, 2, -1, 9, -1, 27, 24, -1, 17, 39, 7, 27, 7, 24, -1, -1, 33, -1, 40, 40, -1, -1, 31, 0, 0, 6, 6, 3, 6, 6, 3, 6, 6, 6, 6, -1, -1, 2, 2, 2, 2, 2, 58, -1, 2, 6, 2, 2, 2, 2, 45, 2, 2, 6, -1, 9, -1, 2, 40, 43, 2, 2, 2, 24, 18, 38, 5, 5, 38, 5, 5, 5, 5, -1, 29, 29, 29, 29, 29, 29, 35, 29, -1, 29, -1, -1, 0, 8, 19, 30, 7, 8, -1, 7, 16, -1, 16, 16, 16, 35, 35, -1, 7, 33, 7, 7, 7, 7, 52, 8, 26, -1, 8, 16, 9, 9, 9, -1, 18, -1, 26, 26, -1, 54, 26, 26, 19, 26, 12, 26, 58, 12, 19, 12, 26, 12, 13, 12, 11, 12, -1, -1, 13, -1, 41, -1, 8, 5, 2, 19, 11, 27, 30, -1, 27, 27, 27, 27, -1, 7, 27, -1, 30, -1, 52, 13, 33, 21, -1, 2, 2, 2, 2, 2, 2, 2, -1, 2, 2, 1, 2, -1, 2, 40, 2, 2, 2, -1, 40, 40, 2, 5, 18, 8, 35, 37, 20, -1, 18, 11, 19, 30, 15, 9, 36, 37, -1, 9, 29, 10, 29, 13, 29, 10, 8, 13, -1, 28, 28, 28, 28, 8, 10, -1, 30, 36, -1, -1, 53, 9, 53, 9, 36, 13, 36, 8, 9, 36, -1, 36, 20, -1, 0, 8, 28, 7, 21, 1, 23, 23, 23, -1, 13, 21, 33, 14, 21, 7, -1, -1, 16, 22, -1, 33, 22, 7, 16, 16, 7, -1, 5, 16, 16, 16, 39, 16, 2, 16, 23, 8, 23, 23, 53, 39, 26, 26, -1, -1, 26, 13, 26, 22, 22, -1, 42, 22, 22, -1, 22, 22, -1, 0, 25, 57, 18, 18, 3, 53, 9, -1, 0, -1, 9, 14, 3, -1, 31, 44, 41, -1, 44, 3, 25, 5, -1, -1, 14, 15, 4, 4, 44, -1, 44, -1, -1, 45, 43, 45, -1, 4, 30, 11, -1, 4, 30, 3, -1, 18, 4, 20, 4, 43, 37, 37, -1, 17, 0, -1, 53, 28, 36, 8, 8, -1, 36, 8, -1, 0, 51, -1, 51, 28, 8, 51, 3, 20, -1, 3, -1, 0, 51, 3, -1, 21, 12, -1, 31, 12, 21, 0, 23, 23, -1, 23, 8, 23, 17, 28, 1, 10, 9, 22, -1, 11, -1, -1, 20, 22, 22, 22, 22, 50, 22, 6, 22, 22, 15, -1, 22, 22, 31, 31, 15, 14, 28, 28, 28, 28, -1, 11, 28, 55, -1, 1, 10, 1, -1, 13, 7, 1, -1, 23, 17, 7, 18, 18, 23, -1, -1, 45, 1, 23, 13, -1, 5, 23, 10, 45, 23, 42, 18, -1, 13, 2, -1, -1, 18, 10, 54, 4, -1, -1, 4, 4, 21, -1, 2, 8, 8, 10, 9, 24, 8, 4, 43, -1, 46, 10, 10, 2, 3, 46, -1, 10, 24, 10, -1, 3, 17, 34, -1, 12, 48, 0, 6, 12, 16, 34, 31, 6, -1, -1, 11, 19, -1, -1, 6, -1, 6, 47, 26, 34, 1, 45, 34, 18, 13, 16, 16, 1, 30, 30, -1, 16, 8, 8, 16, 16, 12, 19, -1, 0, 7, 16, 1, 1, 7, 2, 1, -1, 31, 15, 7, -1, -1, 13, 11, 50, 11, -1, -1, -1, -1, -1, -1, 1, 6, -1, 1, 0, 0, 0, 2, 41, 2, 2, 23, 4, 0, 2, 2, 24, 11, 4, 6, 23, 19, 46, 4, 34, 15, 18, 11, 3, 4, 18, -1, 19, 10, 10, 58, 15, 3, 17, 17, -1, 37, -1, 33, 3, 9, 24, 3, 13, 16, 10, -1, -1, 24, 3, 6, 2, 0, -1, 9, 2, 2, 3, 16, -1, 28, 17, 6, 27, -1, 1, -1, 17, 1, 17, -1, 9, 14, 16, 13, 57, 1, 29, 16, 25, 59, 1, 1, 1, 7, 40, 33, 1, 0, 19, 36, 25, 0, -1, -1, 6, 6, 0, 0, 0, 5, 14, 0, 52, 1, 2, 4, -1, 2, 2, 7, 4, -1, 2, 29, 29, 2, 3, 9, 48, 49, 49, 48, 0, 19, 4, 16, 36, -1, 0, 10, 9, 39, 39, 12, -1, 36, 0, 28, 28, 10, 5, 1, 1, 44, 15, 10, -1, 40, 7, -1, 15, 23, -1, 9, 7, 18, 3, 6, -1, -1, -1, 46, -1, -1, -1, 6, 18, 41, 29, 23, 17, 44, 45, 43, 41, 17, 1, 1, 1, 17, -1, 25, 2, -1, 2, 40, 36, 1, 39, 17, 1, -1, -1, 34, 1, 1, 1, 13, 1, 17, 18, 7, 24, 13, 2, 2, 1, 5, 22, 16, 16, -1, 19, 2, 2, 1, -1, 23, 6, 29, 23, 23, -1, 44, -1]\n",
            "Skipping coherence calculation due to error: unable to interpret topic as either a list of tokens or a list of ids\n",
            "-------RUN11-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': 'CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7df3b00f5c90>)', 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[34, 43, 27, 27, -1, -1, -1, 9, 9, 9, -1, 24, 12, 24, 9, -1, -1, 34, -1, -1, 52, 12, 9, 7, 7, 22, 48, -1, 23, -1, 36, -1, 27, 25, 41, -1, -1, 46, -1, -1, 7, -1, 30, 7, 30, -1, 30, 30, 7, 36, 30, 7, -1, 9, -1, 13, 13, -1, -1, 56, 47, 53, 56, 9, -1, 9, 10, 47, 12, -1, 25, 12, 1, 1, -1, -1, -1, -1, 41, -1, 13, 13, 29, 13, 13, 22, 13, 1, 23, -1, 1, 9, 13, 46, 52, 9, 9, 1, -1, -1, 24, 37, 36, 45, 26, 25, -1, -1, -1, -1, -1, 12, -1, -1, 53, 58, 58, 22, 55, -1, 30, 58, 7, 37, 30, 32, 12, 23, 7, 12, 23, 15, 41, 12, 12, 34, 7, -1, 12, -1, 9, -1, 20, 32, 15, 9, 34, -1, 43, -1, 9, 9, 7, 9, 26, -1, 1, -1, -1, 54, -1, -1, 25, 54, -1, -1, 20, 37, 29, -1, 9, -1, -1, -1, 15, 9, -1, 38, 15, -1, 53, 55, 15, -1, 12, -1, -1, 23, 12, 19, 19, 15, 13, 23, 32, 4, 11, 11, 13, 53, 13, -1, 13, 13, -1, 13, 4, 13, 1, 57, 31, 29, 38, 13, 13, -1, -1, 1, 23, -1, 11, 1, 31, 31, 52, 11, 48, 48, -1, 32, 32, 32, 32, 11, 11, 32, 22, 32, 0, 11, 11, 10, 10, -1, 11, 26, -1, -1, 33, -1, -1, 11, 24, -1, 44, 3, 3, 3, 31, 31, -1, 37, 11, 26, -1, -1, -1, 48, 46, 10, 10, 10, 18, 1, 10, 26, 10, -1, 22, 48, 1, 44, 10, 3, -1, -1, 11, 50, 24, 50, 24, 41, 24, 24, 58, 11, 9, 26, 11, 18, 18, -1, 12, 18, -1, 18, 47, -1, 28, -1, 7, 18, 30, -1, 38, -1, 44, -1, 37, 20, 51, 3, 26, -1, -1, 34, 49, 34, -1, 34, 54, 44, 19, 34, 17, 20, -1, 15, -1, -1, 15, -1, 22, 22, 40, 38, 15, 15, 0, 15, -1, 35, 61, 43, 40, 19, 15, 19, 19, 34, 37, 12, 36, 12, 19, 10, 12, -1, 22, -1, 19, 8, 12, 15, 40, 12, 0, 40, -1, 0, 6, 6, 41, 40, 40, 40, 19, 40, 19, 22, 29, 52, 19, 6, 6, 6, 6, 6, 6, 6, 46, 6, 11, 43, 11, 59, 6, -1, 32, 32, 22, 19, -1, 45, 32, 22, 32, -1, 17, -1, -1, -1, 46, -1, 17, 17, -1, 17, 31, 1, 31, -1, 1, 18, 1, 1, 1, -1, 1, 1, 1, -1, 1, 17, -1, 10, 38, 10, 31, 9, 25, 10, 10, 10, 17, 43, 10, -1, 60, 10, -1, 10, -1, -1, 11, 10, 18, 10, 10, -1, 1, 3, 23, 1, -1, 1, 10, 3, -1, 1, -1, 50, 1, 48, -1, -1, 50, 3, 26, 3, -1, -1, -1, 22, 50, -1, -1, 41, 5, 42, 5, 5, 42, -1, 38, 11, 11, 12, 57, 0, -1, 42, 5, 5, 5, 5, 5, -1, 19, 5, 18, 5, 35, 19, 57, 16, 35, 5, -1, 36, 5, 57, 35, -1, 1, 40, -1, 35, 3, -1, 3, 3, -1, -1, 18, 1, -1, 37, 3, 3, 23, 3, 3, 8, 8, 8, 8, 0, 3, 3, 23, 24, 17, 0, 0, 0, 0, 0, 0, 39, 8, 35, 35, 16, 16, 16, 35, 19, -1, 8, 8, 7, 46, 8, 35, 8, 19, 25, -1, 0, 0, 22, 19, 58, 9, 0, 7, 0, 0, 22, 0, 0, 37, 0, 0, -1, 6, 0, 22, 22, 25, 46, 4, 4, 4, 4, 4, 4, 8, 4, 4, 1, 1, -1, 43, -1, 15, 42, 45, 4, 4, -1, 4, 4, 4, 1, 4, 4, 1, 1, 1, 1, -1, 1, 5, 1, 1, -1, -1, 10, -1, -1, 5, 6, -1, 5, 32, -1, 5, 42, 5, 42, 5, 42, 5, 38, 5, 7, 9, 7, -1, 52, 38, 5, 5, 5, 61, 5, 42, 5, 16, 19, 5, 34, 37, 3, 3, 3, 3, -1, 3, 3, 26, 47, 3, -1, 12, -1, -1, 15, 27, 1, 12, -1, 8, 8, 45, 8, -1, 57, -1, 8, -1, 27, 0, -1, -1, -1, 8, 55, 39, -1, 0, 0, 5, 26, 5, 0, 0, 53, 0, 0, 26, -1, 0, -1, 16, 0, 24, 16, 24, 0, 6, 15, 0, 16, -1, 60, 16, 16, 16, 23, -1, 7, 36, 32, 2, 2, 2, 25, 26, 24, 31, -1, 16, 36, 6, 24, 6, 31, 60, 31, 26, -1, 39, 39, -1, -1, 9, 10, 10, 4, 4, 1, 4, 4, 1, 4, 4, 4, 4, 1, -1, 2, 2, 2, 2, 2, -1, -1, 2, 4, 2, 2, 2, 2, 49, 2, 2, 4, -1, 38, 1, 2, 39, 45, 2, 2, 2, 31, 17, 42, 5, 5, 42, 5, 5, 5, 5, -1, 28, 28, 28, 28, 28, 28, 37, 28, 58, 28, -1, -1, 10, 7, 23, 30, 6, 7, -1, 6, 14, -1, 14, 14, 14, 37, 37, 51, 6, 26, 6, 6, 6, 6, 55, 7, 27, -1, 7, 14, 25, 25, 25, 61, 17, -1, 27, 27, -1, 53, 27, 27, 23, 27, 15, 27, -1, 15, -1, -1, 27, 15, 43, 15, 13, 15, -1, -1, 36, -1, 47, -1, 7, 5, 2, 23, 13, 24, 30, -1, 24, 24, 24, -1, 60, 6, 24, -1, 30, -1, 55, -1, 26, 9, -1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, -1, 2, 39, 2, 54, 2, -1, 39, 39, 2, 5, 17, 7, 37, 35, 19, -1, 17, 13, 23, -1, 11, -1, 29, 35, -1, 25, 28, 8, 28, 36, 28, 8, 7, -1, -1, 33, 33, 44, 33, 7, 8, -1, 30, 29, -1, -1, 59, 25, 59, 25, 29, 43, 29, 7, 38, 29, -1, 29, 19, -1, -1, 7, 33, 6, 9, 0, 21, 21, 21, -1, 36, 9, 26, 12, 9, 6, 41, -1, 14, 20, -1, 26, 20, 6, 14, 14, 6, -1, 5, 14, 14, 14, -1, 14, 2, 14, 21, 7, 21, 21, 59, -1, 27, 27, -1, -1, 27, 61, 27, 20, 20, 34, 34, 20, 20, -1, 20, 20, -1, 18, 22, 41, 17, 17, 1, 59, 29, -1, 10, -1, 25, 12, 1, -1, 9, -1, 47, -1, -1, 1, 22, 5, 41, 34, 12, 11, 3, 3, 51, -1, 51, -1, -1, 49, 45, 49, 0, 3, 30, -1, -1, 3, 30, 1, -1, 17, 3, 19, 3, 45, 35, 35, -1, 16, 18, 2, 59, 33, 29, 7, 7, -1, 29, 7, -1, 10, 44, -1, 44, 33, 7, 44, 1, 19, -1, 1, 40, -1, 44, 1, -1, 9, 15, -1, 56, 15, 9, 18, 21, 21, -1, 21, 7, 21, 16, 33, 0, 8, 38, 20, -1, 13, -1, 52, 19, 20, 20, 20, 20, 0, 20, 4, 20, 20, 11, -1, 20, 20, 56, 56, 11, 12, 33, 33, 33, 33, -1, 13, 33, 57, -1, 0, 8, 0, -1, 43, 6, 0, 60, 21, 16, 6, 17, 17, 21, -1, -1, 49, 0, 21, 43, 51, -1, 21, 8, 49, 21, 34, 17, -1, -1, 2, -1, -1, 17, -1, 53, 3, -1, -1, 3, 3, 9, -1, 2, 7, 7, 8, 25, 31, 7, 3, 45, -1, -1, 8, 8, 2, 1, -1, -1, 8, 31, 8, -1, 1, 16, 54, 0, 15, 48, -1, 4, 15, 14, 50, 56, 4, -1, -1, 13, 23, -1, -1, 4, -1, 4, 40, 27, 50, 0, 49, 50, 17, 36, 14, 14, 0, 30, 30, -1, 14, 7, 7, 14, 14, -1, 23, -1, 18, 6, 14, 0, 0, 6, 2, 0, -1, 9, 11, 6, -1, -1, 61, 13, 0, 13, -1, -1, 0, -1, -1, -1, 0, 4, 24, 0, 18, 18, 18, 2, 47, 2, 2, 21, 3, 18, 2, 2, -1, 13, 3, 4, 21, 23, -1, 3, 54, 11, 17, 13, 1, 3, 17, -1, 23, 8, 8, -1, 11, 1, 16, 16, -1, 35, -1, 26, 1, 25, -1, 1, -1, 14, 8, -1, -1, 31, 1, 4, 2, 10, 41, -1, 2, 2, 1, 14, -1, 44, 16, 4, 24, 1, 0, -1, 16, 0, 16, -1, 25, 12, 14, -1, 41, 0, 28, 14, 22, 52, 0, 0, 0, 6, 39, 26, 0, -1, 23, 29, 22, 18, -1, -1, 4, 4, 18, 10, 18, 5, 12, -1, 55, 0, 2, 3, -1, 2, 2, 6, 3, -1, 2, 28, 28, 2, 1, 22, 48, 46, 46, 48, 10, 23, 3, 14, 29, -1, 18, 8, 25, 36, 36, 15, -1, 29, 18, 33, 33, 8, 5, 0, 0, 51, 11, 8, 1, 39, 6, -1, 11, 21, -1, 15, 6, 17, 1, 4, -1, -1, -1, -1, -1, -1, -1, 4, 17, 47, 28, 21, 16, 51, 49, 45, 47, 16, 0, 0, 0, 16, -1, 22, 2, -1, 2, 39, 29, 0, -1, 16, 0, -1, 0, 54, 0, 0, 0, -1, 0, 16, 17, 6, 31, -1, 2, 2, 0, 5, 20, 14, 14, -1, 23, 2, 2, 0, 0, 21, 4, 28, 21, 21, -1, -1, -1]\n",
            "-------RUN12-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': \"CountVectorizer(stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b02d2c50>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[36, -1, 23, 23, -1, -1, -1, 10, 10, 10, -1, 28, 12, 28, -1, -1, 41, 36, -1, -1, 50, 12, -1, 7, 7, 19, 40, -1, 24, -1, 27, -1, 23, 21, 58, 50, 0, -1, -1, -1, 7, -1, 25, 7, 25, -1, 25, 25, 7, -1, 25, 7, -1, -1, -1, 13, 13, 27, -1, 57, 48, 60, 57, 10, -1, 10, 0, -1, 12, 0, 21, 12, 3, 3, -1, 0, -1, -1, -1, -1, 13, 13, -1, 13, 13, 19, 13, 3, 24, 3, 3, 10, 13, 47, 50, 10, 10, 3, -1, -1, 28, 34, 59, 43, 26, 21, -1, -1, -1, -1, 12, 12, 40, -1, 60, -1, -1, 19, 49, -1, 25, -1, 7, 34, 25, -1, 12, 24, 7, 12, -1, 11, 58, 12, 12, 36, 7, 49, 12, 40, 10, -1, 20, 30, 11, -1, 36, -1, -1, -1, 10, 10, 7, 10, 26, -1, 22, -1, -1, 37, -1, -1, 21, -1, 11, -1, 20, 34, 33, -1, 10, 39, -1, -1, 11, 10, -1, 39, 11, -1, 60, 49, 11, -1, 12, -1, -1, 24, 12, 35, 51, 11, 13, 24, 30, 13, 14, 14, 13, 60, 13, -1, 13, 13, -1, 13, 4, 13, 3, 52, 22, 33, 39, 13, 13, -1, -1, 3, 24, -1, 14, -1, 22, 22, 50, 14, 40, 40, 23, 30, 30, 30, 30, 14, 14, 30, 19, 30, 1, 14, 14, 0, 0, -1, 14, 26, -1, 34, 31, -1, -1, 14, 28, -1, 45, 5, 5, 5, 22, 22, -1, 34, -1, 26, -1, -1, -1, 40, 47, 0, 0, 0, 0, 22, 0, 26, 0, 0, 19, 40, 3, 45, 0, 25, -1, -1, 14, 37, -1, 37, 28, 58, 28, 28, -1, 14, 10, 26, 14, 0, 0, 0, 12, 0, -1, 0, 48, 0, 29, 0, 7, 0, 25, 58, 39, -1, 45, -1, 34, -1, 10, 5, 26, 12, -1, 36, 46, 36, -1, 36, -1, -1, 35, 36, 17, 20, -1, 11, -1, -1, 11, -1, 19, 19, -1, 39, 11, 11, 56, 11, 61, 32, 53, 27, 44, 35, 11, 35, 32, 36, 34, 12, 59, 12, 35, 0, 12, -1, 19, 12, 35, -1, 12, 11, 44, 12, 56, 44, 54, 56, 8, 8, -1, 10, 44, 44, 51, 44, 51, 19, 33, 50, 51, 8, 8, 8, 8, 8, 8, 8, 47, 8, 14, 27, 14, 55, 8, -1, 30, 30, 19, 51, -1, 43, 30, 19, 30, -1, 17, -1, -1, -1, 47, -1, 17, 17, -1, 17, 22, 3, 22, 63, 3, 0, 3, 3, 3, -1, 3, 3, 3, 41, -1, 17, 38, 0, 39, 0, 22, 10, 21, 0, 0, 0, 17, 27, 0, -1, -1, 0, -1, 0, -1, 27, 14, 0, 0, 0, 0, -1, 3, 5, 24, 3, -1, -1, 0, 5, -1, 3, 41, 37, 3, -1, -1, -1, 37, 5, 26, 5, -1, -1, -1, 19, 37, -1, -1, -1, 6, 38, 6, 6, 38, -1, 39, 14, 14, 12, 52, -1, -1, 38, 6, 6, 6, 6, 6, -1, 35, 6, 0, 6, 32, 35, 52, 16, 32, 6, -1, 59, 6, 52, 32, -1, 3, -1, -1, 32, 5, -1, 5, 5, 41, 41, 0, 3, -1, 34, 5, 5, 24, 5, 5, 9, 9, 9, 9, -1, 5, 5, 24, 28, 17, -1, 1, 1, 1, 1, 1, -1, 9, 32, -1, 16, 16, 16, 32, 35, -1, 9, 9, 7, 47, 9, 32, 9, -1, 21, 0, 56, 56, 19, -1, -1, 10, 1, 7, 1, 1, 19, 1, 1, 34, 1, 1, -1, 8, 1, 19, 19, 21, 47, 4, 4, 4, 4, 4, 4, 9, 4, 4, 3, 3, -1, 27, -1, 11, 38, 43, 4, 4, -1, 4, 4, 4, 3, 4, 4, 3, 3, 3, 3, -1, 3, 6, 3, 3, -1, -1, 0, -1, -1, 6, 8, -1, 6, 30, -1, 6, 38, 6, 38, 6, 38, 6, 39, 6, 7, 10, 7, 54, 50, 21, 6, 6, 6, 53, 6, 38, 6, 16, 35, 6, 36, -1, 5, 5, 5, 5, 62, 5, 5, -1, 48, 5, -1, 12, -1, -1, 11, 23, 3, 12, -1, 9, 9, 43, 9, -1, 52, -1, 9, -1, 23, -1, -1, 11, -1, 9, 49, 42, -1, 1, 1, 6, 26, 6, 1, 1, -1, 1, 1, 26, 63, 1, 61, 16, 1, 28, 16, 28, 1, 8, 11, 1, 16, -1, -1, 16, 16, 16, -1, -1, 7, -1, 30, 2, 2, 2, 21, 26, 52, 22, -1, 16, 59, 8, 28, 8, 22, -1, 22, 26, 63, -1, 42, -1, -1, 10, 0, 0, 4, 4, -1, 4, 4, 3, 4, 4, 4, 4, 3, -1, 2, 2, 2, 2, 2, 61, -1, 2, 4, 2, 2, 2, 2, 46, 2, 2, 4, -1, 39, 3, 2, 42, 43, 2, 2, 2, 22, 17, 38, 6, 6, 38, 6, 6, 6, 6, -1, 29, 29, 29, 29, 29, 29, 34, 29, -1, 29, -1, -1, 0, 7, 24, 25, 8, 7, -1, 8, 15, -1, 15, 15, 15, 34, 34, 10, 8, 26, 8, 8, 8, 8, 49, 7, 23, -1, 7, 15, 21, 21, 21, 53, 17, 53, 23, 23, -1, 60, 23, 23, 24, 23, 11, 23, 61, 11, 24, -1, 23, 11, 27, 11, 13, 11, -1, 19, 27, -1, 48, -1, 7, -1, 2, 24, 13, 28, 25, -1, 28, 28, 28, -1, -1, 8, 28, -1, 25, -1, 49, -1, 26, 10, -1, 2, 2, 2, 2, 2, 2, 2, -1, 2, 2, 1, 2, -1, 2, 42, 2, -1, 2, -1, 42, 42, 2, 6, 17, 7, 34, 32, 35, -1, 17, 13, 54, 25, 14, 21, 33, 32, 11, 21, 29, 9, 29, 27, 29, 9, 7, 27, -1, 31, 31, 45, -1, 7, 9, 61, 25, 33, -1, -1, 55, 21, 55, 21, 33, 27, 33, 7, 39, 33, -1, -1, 51, 63, 0, 7, 31, 8, 10, 1, 18, 18, 18, -1, 27, 10, 26, 12, 10, 8, -1, -1, 15, 20, -1, 26, 20, 8, 15, 15, 8, -1, -1, 15, 15, 15, 59, 15, 2, 15, 18, 7, 18, 18, 55, -1, 23, 23, -1, -1, 23, 53, 23, -1, 20, 36, -1, 20, 20, -1, 20, 20, -1, 0, 19, 58, 17, 17, 3, 55, -1, -1, 0, -1, 21, -1, 3, -1, 10, -1, 48, -1, 62, 3, 19, 6, -1, -1, 12, 14, 5, 5, 62, -1, -1, -1, -1, 46, 43, 46, -1, 5, 25, 13, -1, 5, 25, 3, 24, 17, 5, 35, 5, 43, 32, 32, -1, 16, 0, -1, 55, 31, 33, 7, 7, -1, 33, 7, -1, 0, 45, -1, 45, 31, 7, 45, 3, 51, -1, 3, 44, 0, 45, 3, -1, 10, 11, -1, 57, 11, 10, 0, 18, 18, -1, 18, 7, 18, 16, 31, 1, 9, 39, 20, -1, 13, 4, -1, 35, 20, 20, 20, 20, -1, 20, 4, 20, 20, 14, -1, 20, 20, 57, 57, 14, 12, 31, 31, 31, 31, -1, 13, 31, 52, -1, 1, 9, 1, -1, 27, 8, 1, -1, 18, 16, 8, 17, 17, 18, -1, -1, 46, 1, 18, 27, -1, -1, 18, -1, 46, 18, 36, 17, -1, -1, 2, 3, -1, 17, -1, 60, 5, -1, -1, 5, 5, 10, -1, 2, 7, 7, 9, 21, 22, 7, 5, 43, -1, 41, 9, 9, 2, 3, 41, -1, 9, 22, 9, -1, 3, 16, 37, -1, 11, 40, 0, 4, 11, 15, 37, 57, 4, -1, -1, 13, 54, -1, 53, 4, -1, 4, 44, 23, 37, 1, 46, 37, 17, 27, 15, 15, 1, 25, 25, -1, 15, 7, 7, 15, 15, -1, 54, -1, 0, 8, 15, 1, 1, 8, 2, 1, -1, -1, 14, 8, -1, -1, -1, 13, 56, 13, -1, -1, -1, -1, -1, -1, 1, 4, -1, 1, 0, 0, 0, 2, 48, 2, 2, 18, 5, 0, 2, 2, 22, 13, 5, 4, 18, 24, 41, 5, 37, 14, 17, 13, 3, 5, 17, -1, 24, 9, 9, 61, 14, 3, 16, 16, -1, 32, 63, 26, 3, 21, 22, 3, -1, 15, 9, -1, -1, 22, 3, 4, 2, 0, -1, 21, 2, -1, 3, 15, -1, -1, 16, 4, 28, -1, 1, -1, 16, 1, 16, -1, 21, 12, 15, 53, 58, 1, 29, 15, 19, 50, 1, 1, 1, 8, 42, 26, 1, 0, 54, 33, 19, 0, -1, -1, 4, 4, 0, 0, 0, 6, 12, 0, 49, 1, 2, 5, -1, 2, 2, 8, 5, -1, 2, 29, 29, 2, 3, -1, 40, 47, -1, 40, 0, 24, 5, 15, 33, -1, 0, 9, -1, -1, 59, 11, -1, 33, 0, 31, 31, 9, 6, 1, 1, 62, 14, 9, 3, 42, 8, 27, 14, 18, -1, -1, 8, 17, 3, 4, -1, -1, -1, 41, -1, -1, -1, 4, 17, -1, 29, 18, 16, 62, 46, 43, 48, 16, 1, 1, 1, 16, -1, 19, 2, -1, 2, 42, 33, 1, -1, 16, 1, -1, 1, 37, 1, 1, 1, -1, 1, 16, 17, 30, 22, 27, 2, 2, 1, 6, 20, 15, 15, -1, 24, 2, 2, 1, -1, 18, 4, 29, 18, 18, -1, -1, -1]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------RUN13-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': \"CountVectorizer(ngram_range=(1, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b891dd10>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[42, -1, 24, 24, -1, 39, -1, 15, 15, 15, -1, 27, 12, -1, 15, -1, 51, 38, -1, -1, -1, 12, 37, 6, 6, 23, 40, -1, 31, -1, 9, -1, 24, 21, 64, -1, -1, -1, -1, -1, 6, -1, 34, 6, 34, -1, 34, 34, 6, 39, 34, 6, 37, 37, -1, 13, 13, 9, -1, 62, 45, 55, 62, 15, -1, 15, 8, 45, 12, 8, 21, 12, 2, 2, -1, 8, -1, -1, -1, -1, 13, 13, 25, 13, 13, 23, 13, 2, 31, 2, 2, 15, 13, 57, 15, 15, 15, 2, -1, -1, 27, 33, 39, 48, 35, 21, -1, -1, -1, 11, -1, 12, 40, 61, 55, 53, 53, -1, 9, -1, 3, 53, 6, 33, 34, 30, 12, 31, 6, 12, 31, 11, 64, 12, 12, 38, 6, 9, 12, 53, 15, 37, 19, 30, 11, 37, 38, -1, 9, -1, 37, 37, 6, 15, -1, -1, -1, -1, -1, 29, -1, -1, 21, 29, 11, 61, 19, 33, 25, 39, 15, 47, -1, -1, 11, 37, -1, 47, 11, 12, 55, 9, 11, -1, 12, -1, -1, 31, 12, -1, 46, 11, 13, 31, 30, 13, 14, 14, 13, 55, 13, 9, 13, 13, -1, 13, 5, 13, 2, 56, 28, 25, 47, 13, 13, -1, -1, 2, 31, -1, 14, -1, 28, 28, -1, 14, 40, 40, -1, 30, 30, 30, 30, 14, 14, 30, 23, 30, 0, 14, 14, 8, 8, -1, 14, -1, -1, 33, 32, -1, -1, 14, 27, 61, 44, 3, 3, 3, 28, 28, -1, 33, 14, -1, 9, -1, -1, 40, 57, 8, 8, 8, -1, 28, 8, 35, 8, 8, 23, 40, 2, 44, -1, 3, -1, -1, 14, 29, -1, 29, 27, 64, 27, 27, 53, 14, 15, 35, 14, 22, -1, -1, 12, 22, -1, 22, 45, -1, 26, -1, 6, 22, 34, -1, 47, -1, 44, -1, 33, -1, 37, 3, 35, 12, -1, 38, 50, 38, -1, 38, 29, -1, 41, 38, 18, 19, -1, 11, -1, 54, 11, -1, 23, 23, -1, -1, 11, 11, 59, 11, 54, 36, -1, 9, 49, -1, 11, 41, 36, 38, 33, 12, 39, 12, 46, 8, 12, 52, 23, -1, 41, -1, 12, 11, 49, 12, 59, 49, 58, 59, 7, 7, -1, 49, 49, 49, 46, 49, 46, 23, 25, 15, 46, 7, 7, 7, 7, 7, 7, 7, 57, 7, 14, -1, 14, 60, 7, -1, 30, 30, 23, 46, -1, 48, 30, 23, 30, -1, 18, -1, -1, -1, 57, -1, 18, 18, -1, 18, 28, 2, 28, -1, 2, 22, 2, 2, 2, -1, 2, 2, 2, 51, 2, 18, -1, 8, 21, 8, 28, 15, 21, 8, 8, 8, 18, 9, 8, -1, 52, 8, 14, 8, -1, 9, 14, 8, 22, 8, 8, -1, 2, 3, 31, 2, 51, -1, 8, 3, -1, 2, 51, 29, 2, 40, -1, -1, 29, 3, 35, 3, -1, 9, -1, 23, 29, -1, -1, -1, 4, 42, 4, 4, 42, -1, 47, 14, 14, 12, 56, -1, -1, 42, 4, 4, 4, 4, 4, -1, 41, 4, 22, 4, 36, 41, 56, 17, 36, 4, -1, 39, 4, 56, 36, -1, 2, -1, 61, 36, 3, -1, 3, 3, -1, -1, 22, 2, -1, 33, 3, 3, 31, 3, 3, 10, 10, 10, 10, -1, 3, 3, 31, 27, 18, 0, 0, 0, 0, 0, 0, -1, 10, 36, -1, 17, 17, 17, 36, 41, -1, 10, 10, 6, 57, 10, 36, 10, -1, 21, -1, 59, 59, 23, 46, 53, 15, 0, 6, 0, 0, 23, 0, 0, 33, 0, 0, -1, 7, 0, 23, 23, 21, 57, 5, 5, 5, 5, 5, 5, 10, 5, 5, 2, 2, -1, 9, -1, 11, 42, 48, 5, 5, -1, 5, 5, 5, 2, 5, 5, -1, 2, 2, 2, -1, -1, 4, 2, 2, -1, -1, 8, -1, -1, 4, 7, -1, 4, 30, -1, 4, 42, 4, 42, 4, 42, 4, 47, 4, 6, 15, 6, 58, 15, 21, 4, 4, 4, -1, 4, 42, 4, 17, 41, 4, 38, 33, 3, 3, 3, 3, 63, 3, 3, -1, 45, 3, -1, 12, -1, -1, 11, 24, 2, 12, -1, 10, 10, 48, 10, -1, 56, -1, 10, -1, 24, -1, -1, -1, -1, 10, 9, 43, -1, 0, 0, 4, 35, 4, 0, 0, 55, 0, 0, 35, -1, 0, 54, 17, 0, 27, 17, 27, 0, 7, 11, 0, 17, 37, 52, 17, 17, 17, -1, -1, 6, 9, 30, 1, 1, 1, 21, -1, 56, 28, -1, 17, 39, 7, 27, 7, 28, 52, 28, 35, -1, 43, 43, -1, -1, 37, 8, 8, 5, 5, 2, 5, 5, 2, 5, 5, 5, 5, -1, -1, 1, 1, 1, 1, 1, 54, -1, 1, 5, 1, 1, 1, 1, 50, 1, 1, 5, 33, 47, -1, 1, 43, 48, 1, 1, 1, 28, 18, -1, 4, 4, 42, 4, 4, 4, 4, -1, 26, 26, 26, 26, 26, 26, 33, 26, 53, 26, -1, -1, 8, 6, -1, 34, 7, 6, -1, 7, 16, -1, 16, 16, 16, 33, 33, -1, 7, 35, 7, 7, 7, 7, 9, 6, 24, -1, 6, 16, 21, 21, 21, -1, 18, 9, 24, 24, -1, 55, 24, 24, 31, 24, 11, 24, 54, 11, -1, 11, 24, 11, 9, 11, 13, 11, 4, 25, 9, -1, 45, -1, 6, -1, 1, 31, 13, 27, 34, -1, 27, 27, 27, -1, 52, 7, 27, -1, -1, -1, 9, -1, 35, 15, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, -1, 1, 43, 1, 29, 1, -1, 43, 43, 1, 4, 18, 6, 33, 36, 41, -1, 18, 13, 58, 34, 14, 21, 25, 36, -1, 21, 26, 10, 26, 9, 26, 10, 6, 9, -1, 32, 32, 44, 32, 6, 10, 54, 34, 25, -1, -1, 60, 21, 60, 21, 25, 9, 25, 6, 47, 25, -1, 25, 46, -1, 8, 6, 32, 7, 15, 0, 20, 20, 20, -1, 9, 15, 35, 12, 15, 7, -1, -1, 16, 19, -1, 35, 19, 7, 16, 16, 7, -1, 4, 16, 16, 16, 39, 16, 1, 16, 20, 6, 20, 20, 60, -1, 24, 24, -1, -1, 24, -1, 24, 19, 19, 38, 38, 19, 19, -1, 19, 19, -1, 22, 23, 64, 18, 18, 2, 60, -1, -1, 8, -1, -1, 12, 2, -1, 37, 63, 45, -1, 63, 2, 23, 4, -1, -1, 12, 14, 3, 3, 63, -1, -1, -1, 61, 50, 48, 50, 0, 3, 34, 13, -1, 3, 34, 2, -1, 18, 3, 41, 3, 48, 36, 36, -1, 17, 22, 1, 60, 32, 25, 6, 6, -1, 25, 6, -1, 8, 44, -1, 44, 32, 6, 44, 2, 46, -1, 2, 49, 8, 44, 2, -1, 15, 11, -1, 62, 11, 15, 22, 20, 20, -1, 20, 6, 20, 17, 32, 0, 10, 47, 19, 61, 13, -1, 15, 41, 19, 19, 19, 19, -1, 19, 5, 19, 19, 14, -1, 19, 19, 62, 62, 14, 12, 32, 32, 32, 32, 29, 13, 32, 56, -1, 0, 10, 0, -1, 9, 7, 0, 52, 20, 17, 7, 18, 18, 20, -1, -1, 50, 0, 20, 9, -1, -1, 20, -1, 50, 20, 38, 18, -1, -1, 1, -1, -1, 18, -1, 55, 3, -1, -1, 3, 3, 15, -1, 1, 6, 6, 10, 21, -1, 6, 3, 48, -1, 51, 10, 10, 1, 2, 51, -1, 10, 28, 10, -1, 2, 17, 29, -1, 11, 40, -1, 5, 11, 16, 29, 62, 5, -1, -1, 13, 58, -1, -1, 5, 37, 5, 49, 24, 29, 0, 50, 29, 18, 9, 16, 16, 0, 34, 34, -1, 16, 6, 6, 16, 16, -1, 58, -1, 22, 7, 16, 0, 0, 7, 1, 0, -1, 37, 14, 7, -1, -1, 9, 13, 59, 13, -1, 52, 0, -1, -1, -1, 0, 5, 27, 0, 22, 22, 22, 1, 45, 1, 1, 20, 3, 22, 1, 1, -1, 13, 3, 5, 20, 31, 51, 3, 29, 14, 18, 13, 2, 3, 18, -1, 31, 10, 10, 54, 14, 2, 17, 17, -1, 36, -1, 35, 2, 21, 2, 2, 9, 16, 10, -1, -1, 28, 2, 5, 1, 8, -1, 21, 1, -1, 2, 16, -1, 44, 17, 5, 27, -1, 0, -1, 17, 0, 17, -1, 21, 12, 16, -1, 64, 0, 26, 16, 23, 15, 0, 0, 0, 7, 43, 35, 0, 8, 58, 25, 23, 22, -1, -1, 5, 5, 22, 8, 22, 4, 12, -1, 9, 0, 1, 3, -1, 1, 1, 7, 3, -1, 1, 26, 26, 1, 2, -1, 40, 57, -1, 40, 8, 31, 3, 16, 25, -1, 22, 10, 21, 39, 39, 11, -1, 25, 22, 32, 32, 10, 4, 0, 0, -1, 14, 10, -1, 43, 7, 9, 14, 20, -1, 11, 7, 18, 2, 5, -1, -1, -1, 51, -1, -1, -1, 5, 18, 45, 26, 20, 17, 63, 50, 48, 45, 17, 0, 0, 0, 17, -1, 23, 1, -1, 1, 43, 25, 0, 39, 17, 0, -1, 0, 29, 0, 0, 0, -1, 0, 17, 18, 30, 28, 9, 1, 1, 0, 4, 19, 16, 16, -1, 31, 1, 1, 0, -1, 20, 5, 26, 20, 20, -1, -1, -1]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------RUN14-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': \"CountVectorizer(ngram_range=(2, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df4a9008410>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[38, 3, 28, 28, -1, 3, -1, 22, 22, 22, -1, 26, 10, -1, 22, -1, 42, 38, -1, -1, 51, 10, 11, 8, 8, 24, 44, -1, 27, -1, 3, -1, 28, 23, 57, -1, -1, -1, -1, -1, 8, -1, 29, 8, 29, -1, 29, 29, 8, 3, 29, 8, 11, 11, 11, 14, 14, 3, 11, 11, 49, -1, 11, 22, -1, 22, 13, -1, 10, -1, 23, 10, 2, 2, -1, -1, -1, -1, -1, -1, 14, 14, -1, 14, 14, 24, 14, -1, 27, -1, 2, 22, 14, 53, 51, 22, 22, 2, -1, -1, 26, 33, 3, 41, 34, 23, -1, 12, -1, 12, 10, 10, -1, -1, 58, -1, -1, 24, 3, -1, 29, -1, 8, 33, 29, -1, 10, 27, 8, 10, 27, 12, 57, 10, 10, 38, 8, 3, 10, -1, 22, 11, 20, -1, 12, 11, 38, -1, 3, -1, 11, 11, 8, 22, -1, -1, -1, -1, -1, -1, -1, -1, 23, -1, 12, -1, 20, 33, -1, 3, -1, -1, -1, -1, 12, 11, -1, 40, 12, 10, 58, 3, 12, -1, 10, -1, -1, 27, 10, 18, 18, 12, 14, 27, 37, 14, 17, 17, 14, 58, 14, 3, 14, 14, -1, 14, 4, 14, 2, 52, 32, -1, 40, 14, 14, -1, -1, 2, 27, -1, 17, -1, 32, 32, 51, 17, 44, 44, -1, 37, 37, 37, 37, 17, 17, 37, 24, 37, 1, 17, 17, 13, 13, -1, 17, -1, -1, 33, 31, -1, -1, 17, 26, -1, 47, 6, 6, 6, 32, 32, 44, 33, 31, -1, 3, -1, -1, 44, 53, 13, 13, 13, -1, -1, 13, 34, 13, 13, 24, 44, 2, 47, -1, 29, -1, -1, 17, 45, 26, 45, 26, 57, 26, 26, -1, 17, 22, 34, 17, 25, -1, -1, 10, 25, 10, 25, 49, -1, 30, -1, 8, 25, 29, -1, 40, -1, 47, -1, 33, 20, 11, -1, 34, 10, -1, 38, 48, 38, -1, 38, -1, 47, 18, 38, 19, 20, -1, 12, -1, -1, 12, -1, 24, 24, -1, 40, 12, 12, 56, 12, -1, 36, -1, 3, 46, 18, 12, 18, 18, -1, 33, 10, 3, 10, 18, -1, 10, 42, 24, 10, 18, 9, 10, 12, 46, 10, 56, 46, 54, 56, 7, 7, -1, 46, 46, 46, 18, 46, 18, 24, 35, 51, 18, 7, 7, 7, 7, 7, 7, 7, 53, 7, 17, 3, 17, 55, 7, -1, 37, 37, 24, 18, -1, 41, 37, 24, 37, -1, 19, -1, -1, -1, 53, -1, 19, 19, -1, 19, 32, 2, 32, -1, 2, 25, 2, 2, 2, 50, 2, 2, 2, 42, 2, 19, -1, 13, 40, 13, 32, 22, 23, 13, 13, 13, 19, 3, 13, -1, -1, 13, 31, -1, -1, 3, 17, 13, 25, 13, 13, -1, 2, 6, 27, 2, -1, 50, 13, 6, -1, 2, 42, 45, 2, -1, -1, -1, 45, 6, 34, 6, -1, -1, -1, 24, 45, -1, -1, -1, 5, 39, 5, 5, 39, 2, 40, 17, 17, 10, 52, -1, -1, 39, 5, 5, 5, 5, 5, -1, 18, 5, 25, 5, 36, 18, 52, 15, 36, 5, 39, 3, 5, 52, 36, -1, 2, -1, -1, 36, 6, -1, 6, 6, -1, -1, 25, 2, -1, 33, 6, 6, 27, 6, 6, 9, 9, 9, 9, -1, 6, 6, 27, 26, 19, -1, 1, 1, 1, 1, 1, -1, 9, 36, 18, 15, 15, 15, 36, 18, -1, 9, 9, 8, 53, 9, 36, 9, 18, 23, -1, 56, 56, 24, 18, -1, 22, 1, 8, 1, 1, 24, 1, 1, 33, 1, 1, -1, 7, 1, 24, 24, 23, 53, 4, 4, 4, 4, 4, 4, 9, 4, 4, 2, 2, -1, 3, -1, 12, 39, 41, 4, 4, -1, 4, 4, 4, 2, 4, 4, 2, 2, 2, 2, -1, 2, 5, 2, 2, -1, -1, 13, -1, 50, 5, 7, -1, 5, 37, -1, 5, 39, 5, 39, 5, 39, 5, 40, 5, 8, 22, 8, 54, 51, 23, 5, 5, 5, 3, 5, 39, 5, 15, 18, 5, 38, 33, 6, 6, 6, 6, -1, 6, 6, 34, 49, 6, -1, 10, -1, -1, 12, 28, 2, 10, -1, 9, 9, 41, 9, -1, 52, -1, 9, -1, 28, -1, -1, -1, -1, 9, 3, 43, -1, 1, 1, 5, 34, 5, 1, 1, -1, 1, 1, 34, -1, 1, 15, 15, 1, 26, 15, 26, 1, 7, 12, 1, 15, 11, -1, 15, 15, 15, 27, -1, 8, 3, 37, 0, 0, 0, 23, 34, 52, 32, -1, 15, 3, 7, 26, 7, 32, -1, 32, 34, -1, 43, 43, -1, -1, 11, 13, 13, 4, 4, -1, 4, 4, 2, 4, 4, 4, 4, -1, -1, 0, 0, 0, 0, 0, -1, -1, 0, 4, 0, 0, 0, 0, 48, 0, 0, 4, -1, 40, -1, 0, 43, 41, 0, 0, 0, 32, 19, 39, 5, 5, 39, 5, 5, 5, 5, -1, 30, 30, 30, 30, 30, 30, 33, 30, -1, 30, -1, 4, 13, 8, 27, 29, 7, 8, -1, 7, 16, -1, 16, 16, 16, 33, 33, 11, 7, 34, 7, 7, 7, 7, 3, 8, 28, -1, 8, 16, 23, 23, 23, -1, 19, -1, 28, 28, -1, 58, 28, 28, 27, 28, 12, 28, -1, 12, -1, 12, 28, 12, 3, 12, 14, 12, -1, -1, 3, 12, 49, 10, 8, 5, 0, 27, 14, 26, 29, -1, 26, 26, 26, 26, -1, 7, 26, -1, 29, -1, 3, -1, -1, 22, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0, 43, 0, 0, 0, 11, 43, 43, 0, 5, 19, 8, 33, 36, 18, -1, 19, 14, 54, 29, 17, 23, 35, 36, 12, 23, 30, 9, 30, 3, 30, 9, 8, 3, -1, 31, 31, -1, 31, 8, 9, -1, 29, 35, -1, -1, 55, 23, 55, 23, 35, 3, 35, 8, 40, 35, -1, 35, 18, -1, 13, 8, 31, 7, 22, 1, 21, 21, 21, -1, 3, 22, -1, 10, 22, 7, -1, -1, 16, 20, -1, 34, 20, 7, 16, 16, 7, -1, 5, 16, 16, 16, 3, 16, 0, 16, 21, 8, 21, 21, 55, 3, 28, 28, -1, -1, 28, 3, 28, -1, 20, 38, -1, 20, 20, -1, 20, 20, -1, 25, 24, 57, 19, 19, 2, 55, -1, -1, 13, -1, 23, 10, 2, 2, 11, 11, 49, -1, 11, 2, 24, 5, -1, 38, 10, 17, 6, 6, 11, -1, 11, -1, -1, 48, 41, 48, -1, 6, -1, 14, -1, 6, 29, 2, -1, 19, 6, 18, 6, 41, 36, 36, -1, 15, 25, 0, 55, 31, 35, 8, 8, -1, 35, 8, -1, 13, 47, -1, 47, 31, 8, 47, 2, 18, -1, 2, 46, -1, 47, 2, -1, 22, 12, -1, 11, 12, 22, 25, 21, 21, -1, 21, 8, 21, 15, 31, 1, 9, 40, 20, -1, 14, 4, 51, 18, 20, 20, 20, 20, -1, 20, 4, 20, 20, 17, -1, 20, 20, 11, 11, 17, 10, 31, 31, 31, 31, -1, 14, 31, 52, -1, 1, 9, 1, -1, 3, 7, 1, -1, 21, 15, 7, 19, 19, 21, -1, -1, 48, 1, 21, 3, 11, -1, 21, 9, 48, 21, 38, 19, -1, -1, 0, -1, 18, 19, 9, 58, 6, -1, 50, 6, 6, 22, -1, 0, 8, 8, 9, 23, 32, 8, 6, 41, -1, 42, 9, 9, 0, 2, 42, -1, 9, 32, 9, -1, 2, 15, -1, -1, 12, 44, 13, 4, 12, 16, 45, 11, 4, 50, -1, 14, 54, -1, -1, 4, 11, 4, 46, 28, 45, 1, 48, 45, 19, 3, 16, 16, 1, 29, 29, 3, 16, 8, 8, 16, 16, -1, 54, -1, 25, 7, 16, 1, 1, 7, 0, 1, -1, 11, 17, 7, -1, -1, 3, 14, 56, 14, -1, -1, 1, -1, -1, -1, 1, 4, 26, 1, 25, 25, 25, 0, 49, 0, 0, 21, 6, 25, 0, 0, -1, 14, 6, 4, 21, 27, 42, 6, 45, 17, 19, 14, 2, 6, 19, -1, 27, 9, 9, -1, 17, 2, 15, 15, -1, 36, -1, 34, 2, 23, 2, 2, -1, 16, 9, 50, -1, 32, 2, 4, 0, 13, -1, 23, 0, 0, 2, 16, -1, -1, 15, 4, 26, -1, 1, -1, 15, 1, 15, -1, 23, 10, 16, -1, 57, 1, 30, 16, 24, 51, 1, 1, 1, 7, 43, 34, 1, 13, 54, 35, 24, 25, -1, -1, 4, 4, -1, 13, 25, 5, 10, -1, -1, 1, 0, 6, -1, 0, 0, 7, 6, -1, 0, 30, 30, 0, 2, -1, 44, 53, -1, 44, 13, 27, 6, 16, 35, -1, 25, 9, -1, 3, 3, 12, -1, 35, 25, 31, 31, 9, 5, 1, 1, 11, 17, 9, 2, -1, 7, 3, 17, 21, -1, -1, 7, 19, 2, 4, 3, -1, -1, 42, 50, -1, -1, 4, 19, 42, 30, 21, 15, 11, 48, 41, 49, 15, 1, 1, 1, 15, -1, 24, 0, -1, 0, 43, 35, 1, 3, 15, 1, -1, 1, -1, 1, 1, 1, -1, 1, 15, 19, 7, 32, 3, 0, 0, 1, 5, 20, 16, 16, -1, 27, 0, 0, 1, -1, 21, 4, 30, 21, 21, -1, 11, -1]\n",
            "Skipping coherence calculation due to error: unable to interpret topic as either a list of tokens or a list of ids\n",
            "-------RUN15-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': \"CountVectorizer(stop_words='english')\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[9, 0, 2, 2, 4, 0, 4, 5, 5, 5, 5, 2, 4, 2, 5, 0, 11, 9, 5, 0, 5, 4, 5, 17, 17, 10, 4, 0, 0, 0, 0, 0, 2, 13, 0, 5, 7, 10, 0, 0, 17, 0, 4, 17, 4, 10, 4, 4, 17, 0, 4, 17, 5, 5, 5, 1, 1, 0, 5, 5, 11, 0, 5, 5, 0, 5, 7, 7, 7, 7, 13, 7, 12, 12, 11, 7, 6, 6, 0, 0, 1, 1, 10, 1, 1, 10, 1, 12, 0, 12, 12, 5, 1, 1, 5, 5, 5, 12, 13, 10, 2, 6, 6, 6, 0, 13, 5, 13, 13, 13, 7, 4, 4, 9, 0, 10, 10, 10, 0, 5, 4, 10, 17, 6, 4, 14, 4, 0, 17, 4, 0, 13, 0, 4, 4, 9, 17, 6, 4, 4, 5, 5, 9, 14, 13, 5, 9, 6, 0, 6, 5, 5, 17, 5, 6, 5, 11, 9, 6, 8, 6, 8, 13, 8, 13, 9, 9, 6, 10, 6, 5, 10, 0, 6, 13, 5, 8, 10, 13, 7, 0, 6, 13, 6, 4, 5, 11, 0, 4, 1, 1, 13, 1, 0, 14, 1, 16, 16, 1, 0, 1, 0, 1, 1, 10, 1, 1, 1, 12, 15, 11, 10, 13, 1, 1, 10, 10, 12, 0, 12, 16, 11, 11, 11, 5, 16, 1, 1, 14, 14, 14, 14, 14, 16, 16, 14, 10, 14, 3, 16, 16, 7, 7, 6, 16, 6, 0, 6, 16, 0, 6, 16, 2, 9, 6, 4, 4, 4, 11, 11, 4, 6, 16, 5, 0, 0, 0, 1, 1, 7, 7, 7, 7, 11, 7, 0, 7, 7, 10, 1, 12, 6, 7, 4, 4, 0, 16, 8, 2, 8, 2, 0, 2, 2, 10, 16, 5, 0, 16, 7, 7, 7, 7, 7, 7, 7, 11, 7, 2, 7, 17, 7, 4, 0, 10, 4, 6, 6, 6, 9, 5, 14, 0, 7, 6, 9, 6, 9, 0, 9, 8, 6, 1, 9, 18, 9, 5, 13, 11, 9, 13, 4, 10, 10, 5, 10, 13, 13, 3, 13, 9, 1, 0, 0, 5, 1, 13, 1, 1, 9, 6, 4, 0, 4, 1, 7, 4, 11, 10, 7, 1, 3, 4, 13, 5, 4, 3, 5, 0, 3, 14, 14, 0, 5, 5, 5, 1, 5, 1, 10, 10, 5, 1, 14, 14, 14, 14, 14, 14, 14, 1, 14, 16, 0, 16, 0, 14, 11, 14, 14, 10, 1, 0, 6, 14, 10, 14, 3, 18, 6, 6, 10, 1, 5, 18, 18, 6, 18, 11, 12, 11, 5, 12, 7, 12, 12, 12, 12, 12, 12, 12, 11, 12, 18, 9, 7, 10, 7, 11, 5, 13, 7, 7, 7, 18, 0, 7, 5, 11, 7, 16, 7, 6, 0, 16, 7, 7, 7, 7, 11, 12, 4, 0, 12, 11, 12, 7, 4, 7, 12, 11, 8, 12, 1, 2, 4, 8, 4, 0, 4, 2, 6, 0, 10, 8, 2, 11, 0, 15, 9, 15, 15, 9, 12, 10, 16, 16, 4, 15, 3, 9, 9, 15, 15, 15, 15, 15, 6, 1, 15, 7, 15, 1, 1, 15, 2, 1, 15, 9, 6, 15, 15, 1, 16, 12, 5, 9, 1, 4, 10, 4, 4, 11, 11, 7, 12, 10, 6, 4, 4, 0, 4, 4, 3, 3, 3, 3, 3, 4, 4, 0, 2, 18, 3, 3, 3, 3, 3, 3, 9, 3, 1, 1, 2, 2, 2, 1, 1, 11, 3, 3, 17, 1, 3, 1, 3, 1, 13, 7, 3, 3, 10, 1, 10, 5, 3, 17, 3, 3, 10, 3, 3, 6, 3, 3, 0, 14, 3, 10, 10, 13, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 12, 12, 2, 0, 2, 13, 9, 6, 1, 1, 1, 1, 1, 1, 12, 1, 1, 12, 12, 12, 12, 11, 12, 15, 12, 12, 11, 13, 7, 11, 12, 15, 14, 0, 15, 14, 6, 15, 9, 15, 9, 15, 9, 15, 10, 15, 17, 5, 17, 0, 5, 10, 15, 15, 15, 0, 15, 9, 15, 2, 1, 15, 9, 6, 4, 4, 4, 4, 5, 4, 4, 6, 11, 4, 8, 4, 4, 4, 13, 2, 12, 4, 10, 3, 3, 6, 3, 0, 15, 0, 3, 13, 2, 3, 6, 13, 4, 3, 6, 9, 0, 3, 3, 15, 0, 15, 3, 3, 0, 3, 3, 0, 0, 3, 2, 2, 3, 2, 2, 2, 3, 14, 13, 3, 2, 5, 11, 2, 2, 2, 0, 5, 17, 0, 14, 8, 8, 8, 13, 0, 15, 11, 9, 2, 0, 14, 2, 14, 11, 11, 11, 0, 5, 9, 9, 12, 11, 5, 7, 7, 1, 1, 11, 1, 1, 12, 1, 1, 1, 1, 11, 12, 8, 8, 8, 8, 8, 2, 6, 8, 1, 8, 8, 8, 8, 6, 8, 8, 1, 6, 10, 11, 8, 9, 6, 8, 8, 8, 11, 18, 9, 15, 15, 9, 15, 15, 15, 15, 4, 2, 2, 2, 2, 2, 2, 6, 2, 1, 2, 1, 1, 7, 17, 0, 4, 14, 17, 4, 14, 2, 9, 2, 2, 2, 6, 6, 5, 14, 0, 14, 14, 14, 14, 6, 17, 2, 0, 17, 2, 10, 13, 13, 0, 18, 0, 2, 2, 11, 0, 2, 2, 0, 2, 13, 2, 9, 13, 0, 13, 2, 13, 0, 13, 1, 13, 15, 10, 0, 0, 11, 7, 17, 15, 8, 0, 1, 2, 4, 12, 2, 2, 2, 2, 11, 14, 2, 0, 4, 7, 6, 0, 0, 5, 11, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 3, 8, 6, 8, 9, 8, 8, 8, 5, 9, 9, 8, 15, 18, 17, 6, 1, 1, 10, 18, 1, 0, 4, 16, 10, 10, 1, 13, 13, 2, 3, 2, 0, 2, 3, 17, 0, 5, 16, 16, 6, 16, 17, 3, 2, 4, 10, 11, 10, 0, 13, 0, 10, 10, 0, 10, 17, 13, 10, 10, 10, 1, 5, 7, 17, 16, 14, 5, 3, 19, 19, 19, 0, 0, 5, 0, 7, 5, 14, 0, 5, 2, 9, 9, 0, 9, 14, 2, 2, 14, 0, 15, 2, 2, 2, 0, 2, 8, 2, 19, 17, 19, 19, 0, 6, 2, 2, 6, 6, 2, 0, 2, 9, 9, 9, 9, 9, 9, 0, 9, 9, 6, 7, 10, 0, 18, 18, 12, 0, 10, 5, 7, 7, 13, 4, 12, 11, 5, 5, 11, 5, 5, 12, 10, 15, 0, 9, 4, 16, 4, 4, 5, 0, 5, 0, 9, 6, 6, 6, 3, 4, 4, 1, 4, 4, 4, 12, 0, 18, 4, 1, 4, 6, 1, 1, 5, 2, 7, 8, 0, 16, 10, 17, 17, 4, 10, 17, 10, 7, 6, 10, 6, 16, 17, 6, 12, 1, 3, 12, 5, 7, 6, 12, 12, 5, 13, 2, 5, 13, 5, 7, 19, 19, 11, 19, 17, 19, 2, 16, 3, 3, 10, 9, 9, 1, 1, 5, 1, 9, 9, 9, 9, 3, 9, 1, 9, 9, 16, 1, 9, 9, 5, 5, 16, 4, 16, 16, 16, 16, 8, 1, 16, 15, 9, 3, 3, 3, 13, 0, 14, 3, 11, 19, 2, 14, 18, 18, 19, 0, 0, 6, 3, 19, 0, 5, 15, 19, 3, 6, 19, 9, 18, 6, 0, 8, 12, 1, 18, 3, 0, 4, 12, 12, 4, 4, 5, 4, 8, 17, 17, 3, 10, 11, 17, 4, 6, 13, 11, 3, 3, 8, 12, 11, 0, 3, 11, 3, 0, 12, 2, 8, 3, 13, 1, 7, 1, 13, 2, 8, 5, 1, 12, 4, 1, 0, 6, 0, 1, 5, 1, 5, 2, 8, 3, 6, 8, 18, 0, 2, 2, 3, 4, 4, 0, 2, 17, 17, 2, 2, 0, 0, 13, 7, 14, 2, 3, 3, 14, 8, 3, 6, 5, 16, 14, 0, 8, 0, 1, 3, 1, 0, 11, 3, 4, 2, 0, 3, 1, 2, 3, 7, 7, 7, 8, 7, 8, 8, 19, 4, 7, 8, 8, 11, 1, 4, 1, 19, 0, 11, 4, 8, 16, 18, 1, 12, 4, 18, 11, 0, 3, 3, 2, 16, 12, 2, 2, 7, 1, 0, 0, 11, 13, 11, 12, 0, 2, 3, 12, 6, 11, 12, 1, 8, 7, 0, 10, 8, 8, 12, 2, 13, 6, 2, 1, 2, 11, 3, 4, 2, 3, 2, 0, 13, 4, 2, 0, 0, 3, 2, 2, 10, 5, 3, 3, 3, 14, 9, 0, 3, 7, 0, 10, 10, 7, 0, 13, 1, 1, 7, 7, 7, 15, 4, 7, 6, 3, 8, 4, 6, 8, 8, 14, 4, 0, 8, 2, 2, 8, 12, 10, 1, 1, 4, 1, 7, 0, 4, 2, 10, 9, 7, 3, 13, 6, 6, 13, 13, 10, 7, 16, 16, 3, 15, 3, 3, 5, 16, 3, 12, 9, 14, 0, 16, 19, 6, 13, 14, 18, 12, 1, 0, 11, 13, 11, 12, 6, 0, 1, 18, 11, 2, 19, 2, 5, 6, 6, 11, 2, 3, 3, 3, 2, 11, 10, 8, 10, 8, 9, 10, 3, 6, 2, 3, 0, 3, 8, 3, 3, 3, 0, 3, 2, 18, 14, 11, 0, 8, 8, 3, 15, 9, 2, 2, 13, 0, 8, 8, 3, 3, 19, 1, 2, 19, 19, 0, 5, 13]\n",
            "-------RUN16-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': 'CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7df3b00f5c90>)', 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[8, 0, 10, 10, 5, 0, 13, 6, 6, 6, 3, 10, 5, 10, 6, 3, 1, 8, 6, 0, 6, 5, 6, 15, 15, 5, 5, 0, 0, 0, 0, 0, 10, 3, 0, 6, 7, 5, 0, 0, 15, 0, 5, 15, 5, 5, 5, 5, 15, 0, 5, 15, 6, 6, 6, 2, 2, 0, 6, 6, 7, 0, 6, 6, 0, 6, 7, 7, 7, 7, 3, 7, 1, 1, 1, 7, 0, 17, 0, 0, 2, 2, 3, 2, 2, 5, 2, 1, 0, 1, 1, 6, 2, 5, 6, 6, 6, 1, 3, 3, 10, 0, 0, 0, 0, 3, 3, 3, 3, 3, 7, 5, 5, 8, 0, 5, 5, 5, 0, 6, 13, 5, 15, 0, 5, 11, 5, 0, 15, 5, 0, 3, 0, 5, 5, 8, 15, 0, 5, 5, 6, 6, 8, 11, 3, 6, 8, 0, 0, 0, 6, 6, 15, 6, 0, 6, 1, 8, 5, 4, 0, 4, 3, 4, 3, 8, 8, 0, 3, 0, 6, 6, 0, 17, 3, 6, 4, 3, 3, 7, 0, 0, 3, 17, 5, 6, 1, 0, 5, 2, 2, 3, 2, 0, 11, 2, 14, 14, 2, 0, 2, 0, 2, 2, 3, 2, 2, 2, 1, 12, 1, 3, 3, 2, 2, 3, 3, 1, 0, 6, 14, 1, 1, 1, 6, 14, 2, 2, 11, 11, 11, 11, 11, 14, 14, 11, 5, 11, 9, 14, 14, 7, 7, 0, 14, 0, 0, 0, 14, 0, 0, 14, 10, 8, 0, 13, 13, 13, 1, 1, 5, 0, 14, 0, 0, 0, 3, 2, 5, 7, 7, 7, 7, 1, 7, 0, 7, 7, 3, 2, 1, 0, 7, 13, 13, 0, 14, 4, 10, 4, 10, 0, 10, 10, 5, 14, 6, 0, 14, 7, 7, 7, 5, 7, 7, 7, 1, 7, 17, 7, 15, 7, 5, 0, 3, 5, 0, 0, 0, 8, 6, 13, 0, 7, 0, 8, 8, 8, 0, 8, 4, 0, 2, 8, 19, 8, 6, 3, 1, 8, 3, 5, 5, 5, 6, 3, 3, 3, 9, 3, 8, 2, 0, 0, 6, 2, 3, 2, 2, 8, 0, 5, 0, 5, 2, 7, 5, 1, 5, 7, 2, 9, 5, 3, 6, 5, 9, 6, 0, 9, 11, 11, 0, 6, 6, 6, 2, 6, 2, 5, 3, 6, 2, 11, 11, 11, 11, 11, 11, 11, 5, 11, 14, 0, 14, 0, 11, 1, 11, 11, 5, 2, 0, 0, 11, 5, 11, 17, 19, 0, 0, 5, 5, 6, 19, 19, 0, 19, 1, 1, 1, 3, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 19, 8, 7, 3, 7, 1, 6, 3, 7, 7, 7, 19, 0, 7, 6, 1, 7, 14, 7, 0, 0, 14, 7, 7, 7, 7, 1, 1, 13, 0, 1, 1, 1, 7, 13, 7, 1, 1, 4, 1, 5, 10, 13, 4, 13, 0, 13, 10, 0, 0, 5, 4, 10, 1, 0, 12, 8, 12, 12, 8, 1, 3, 14, 14, 5, 12, 9, 8, 8, 12, 12, 12, 12, 12, 0, 2, 12, 7, 12, 2, 2, 12, 18, 2, 12, 8, 0, 12, 12, 2, 14, 1, 6, 8, 2, 13, 5, 13, 13, 1, 1, 7, 1, 5, 0, 13, 13, 0, 13, 13, 16, 16, 16, 16, 9, 13, 13, 0, 10, 19, 9, 9, 9, 9, 9, 9, 8, 16, 2, 2, 18, 18, 18, 2, 2, 1, 16, 16, 15, 5, 16, 2, 16, 2, 3, 2, 9, 9, 5, 2, 5, 6, 9, 15, 9, 9, 5, 9, 9, 0, 9, 9, 0, 11, 9, 5, 5, 3, 5, 2, 2, 2, 2, 2, 2, 16, 2, 2, 1, 1, 10, 0, 10, 3, 8, 0, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 12, 1, 1, 1, 3, 7, 1, 1, 12, 11, 0, 12, 11, 0, 12, 8, 12, 8, 12, 8, 12, 3, 12, 15, 6, 15, 0, 6, 3, 12, 12, 12, 0, 12, 8, 12, 18, 2, 12, 8, 0, 13, 13, 13, 13, 6, 13, 13, 0, 7, 13, 4, 5, 13, 13, 3, 10, 1, 5, 5, 16, 16, 0, 16, 0, 12, 6, 16, 3, 10, 9, 0, 3, 5, 16, 0, 8, 0, 9, 9, 12, 0, 12, 9, 9, 0, 9, 9, 0, 3, 9, 8, 18, 9, 10, 18, 10, 9, 11, 3, 9, 18, 6, 1, 18, 18, 18, 0, 6, 15, 0, 11, 4, 4, 4, 3, 0, 12, 1, 8, 18, 0, 11, 10, 11, 1, 1, 1, 0, 3, 8, 8, 1, 1, 6, 7, 7, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 6, 4, 4, 4, 4, 4, 17, 17, 4, 2, 4, 4, 4, 4, 0, 4, 4, 2, 0, 3, 1, 4, 8, 0, 4, 4, 4, 1, 19, 8, 12, 12, 8, 12, 12, 12, 12, 5, 17, 17, 17, 17, 17, 17, 0, 17, 2, 17, 2, 2, 7, 15, 0, 5, 11, 15, 5, 11, 10, 8, 10, 10, 10, 0, 0, 6, 11, 0, 11, 11, 11, 11, 0, 15, 10, 0, 15, 10, 3, 3, 3, 0, 19, 0, 10, 10, 1, 0, 10, 10, 0, 10, 3, 10, 8, 3, 0, 3, 10, 3, 0, 3, 2, 3, 12, 3, 0, 0, 7, 7, 15, 12, 4, 0, 2, 10, 5, 1, 10, 10, 10, 10, 1, 11, 10, 0, 5, 7, 0, 0, 0, 6, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 9, 4, 0, 4, 8, 4, 4, 4, 6, 8, 8, 4, 12, 19, 15, 0, 2, 2, 3, 19, 2, 0, 5, 14, 3, 3, 2, 3, 3, 17, 16, 17, 0, 17, 16, 15, 0, 0, 14, 14, 14, 14, 15, 16, 0, 5, 3, 1, 3, 0, 3, 0, 3, 3, 0, 3, 15, 3, 3, 3, 3, 2, 3, 7, 15, 14, 11, 6, 9, 4, 4, 4, 0, 0, 6, 0, 7, 6, 11, 0, 6, 10, 8, 8, 0, 8, 11, 10, 10, 11, 0, 12, 10, 10, 10, 0, 10, 4, 10, 4, 15, 4, 4, 0, 0, 10, 10, 0, 0, 10, 0, 10, 8, 8, 8, 8, 8, 8, 0, 8, 8, 0, 7, 5, 0, 19, 19, 1, 0, 3, 6, 7, 7, 3, 5, 1, 1, 6, 6, 7, 6, 6, 1, 5, 12, 6, 8, 5, 14, 13, 13, 6, 0, 6, 0, 8, 0, 0, 0, 9, 13, 5, 2, 13, 13, 5, 1, 0, 19, 13, 2, 13, 0, 2, 2, 3, 18, 7, 4, 0, 14, 3, 15, 15, 5, 3, 15, 3, 7, 0, 3, 0, 14, 15, 0, 1, 2, 17, 1, 6, 7, 0, 1, 1, 6, 3, 10, 6, 3, 6, 7, 4, 4, 1, 4, 15, 4, 18, 14, 9, 16, 3, 8, 8, 2, 2, 6, 2, 8, 8, 8, 8, 9, 8, 2, 8, 8, 14, 2, 8, 8, 6, 6, 14, 5, 14, 14, 14, 14, 4, 2, 14, 12, 8, 9, 16, 9, 3, 0, 11, 9, 1, 4, 18, 11, 19, 19, 4, 0, 0, 0, 9, 4, 0, 6, 12, 4, 16, 0, 4, 8, 19, 0, 0, 4, 1, 2, 19, 16, 0, 13, 1, 1, 13, 13, 6, 5, 4, 15, 15, 16, 3, 1, 15, 13, 0, 3, 1, 16, 16, 4, 1, 1, 0, 16, 1, 16, 0, 1, 18, 4, 17, 3, 2, 7, 2, 3, 10, 4, 6, 2, 1, 5, 2, 0, 0, 0, 2, 6, 2, 6, 10, 4, 9, 0, 4, 19, 0, 10, 10, 9, 5, 5, 0, 10, 15, 15, 10, 10, 3, 0, 3, 7, 11, 10, 9, 9, 11, 4, 9, 0, 6, 14, 11, 3, 4, 0, 2, 9, 2, 0, 1, 9, 5, 17, 0, 9, 2, 10, 9, 7, 7, 7, 4, 7, 4, 4, 4, 13, 7, 4, 4, 1, 2, 13, 2, 4, 0, 1, 13, 4, 14, 19, 2, 1, 13, 19, 1, 0, 16, 16, 17, 14, 1, 18, 18, 7, 2, 3, 0, 1, 3, 1, 1, 0, 10, 16, 1, 0, 1, 1, 2, 4, 7, 0, 3, 4, 4, 1, 10, 3, 1, 18, 2, 10, 1, 9, 5, 18, 9, 18, 0, 3, 5, 10, 0, 0, 9, 17, 10, 5, 6, 9, 9, 9, 11, 8, 0, 9, 7, 0, 3, 3, 7, 0, 3, 2, 2, 7, 7, 7, 12, 5, 7, 0, 9, 4, 13, 0, 4, 4, 11, 13, 0, 4, 17, 17, 4, 1, 3, 2, 5, 5, 2, 7, 0, 13, 10, 3, 8, 7, 16, 3, 0, 0, 3, 3, 3, 7, 14, 14, 16, 12, 9, 9, 6, 14, 16, 1, 8, 11, 0, 14, 4, 0, 3, 11, 19, 1, 2, 0, 1, 3, 1, 1, 0, 0, 2, 19, 1, 17, 4, 18, 6, 0, 0, 7, 18, 9, 9, 9, 18, 1, 5, 4, 6, 4, 8, 3, 9, 0, 18, 9, 0, 9, 4, 9, 9, 9, 0, 9, 18, 19, 11, 1, 0, 4, 4, 9, 12, 8, 10, 10, 3, 0, 4, 4, 9, 9, 4, 2, 17, 4, 4, 0, 6, 3]\n",
            "-------RUN17-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': \"CountVectorizer(stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b02d2c50>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[8, 0, 2, 2, 4, 0, 4, 7, 7, 7, 0, 2, 3, 2, 7, 0, 1, 8, 0, 0, 7, 3, 7, 15, 15, 4, 4, 0, 0, 0, 0, 0, 2, 5, 0, 7, 3, 7, 0, 0, 15, 0, 4, 15, 4, 4, 4, 4, 15, 0, 4, 15, 7, 7, 7, 10, 10, 0, 7, 7, 3, 0, 7, 7, 0, 7, 3, 3, 3, 3, 5, 3, 1, 1, 1, 3, 0, 19, 0, 0, 10, 10, 5, 10, 10, 4, 10, 1, 0, 1, 1, 7, 10, 11, 7, 7, 7, 1, 5, 5, 2, 12, 0, 12, 0, 5, 5, 5, 5, 5, 3, 3, 4, 8, 0, 11, 11, 4, 0, 7, 4, 11, 15, 12, 4, 13, 3, 0, 15, 3, 0, 5, 0, 3, 3, 8, 15, 0, 3, 4, 7, 7, 8, 13, 5, 7, 8, 0, 0, 12, 7, 7, 15, 7, 12, 7, 1, 8, 12, 6, 0, 6, 5, 6, 5, 8, 8, 12, 5, 0, 7, 7, 0, 19, 5, 7, 6, 5, 5, 3, 0, 0, 5, 19, 3, 7, 3, 0, 3, 10, 10, 5, 10, 0, 13, 11, 14, 14, 10, 0, 10, 0, 10, 10, 5, 10, 11, 10, 1, 2, 1, 5, 5, 10, 10, 5, 5, 1, 0, 1, 14, 1, 1, 1, 7, 14, 10, 4, 13, 13, 13, 13, 13, 14, 14, 13, 4, 13, 9, 14, 14, 3, 3, 0, 14, 0, 0, 12, 14, 0, 12, 14, 2, 8, 12, 4, 4, 4, 1, 1, 4, 12, 14, 0, 0, 0, 5, 10, 11, 3, 3, 3, 3, 1, 3, 0, 3, 3, 4, 4, 1, 12, 3, 4, 4, 0, 14, 6, 2, 6, 2, 0, 2, 2, 11, 14, 7, 0, 14, 3, 3, 3, 3, 3, 3, 3, 3, 3, 19, 3, 15, 3, 4, 0, 5, 4, 12, 12, 12, 8, 7, 4, 0, 3, 0, 8, 12, 8, 0, 8, 6, 12, 10, 8, 18, 8, 7, 5, 1, 8, 5, 4, 4, 4, 7, 5, 5, 5, 9, 5, 8, 10, 0, 0, 7, 10, 5, 10, 10, 8, 12, 3, 0, 4, 10, 3, 3, 1, 4, 3, 10, 9, 3, 5, 7, 3, 9, 7, 0, 9, 13, 13, 0, 7, 7, 7, 10, 7, 10, 4, 5, 7, 10, 13, 13, 13, 13, 13, 13, 13, 11, 13, 14, 0, 14, 0, 13, 3, 13, 13, 4, 10, 0, 12, 13, 4, 13, 19, 18, 12, 12, 1, 11, 7, 18, 18, 12, 18, 1, 1, 1, 5, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 18, 8, 3, 5, 3, 1, 7, 5, 3, 3, 3, 18, 0, 3, 7, 1, 3, 14, 3, 12, 0, 14, 3, 3, 3, 3, 1, 1, 4, 0, 1, 1, 1, 3, 4, 3, 1, 1, 6, 1, 10, 2, 4, 6, 4, 0, 4, 2, 12, 0, 4, 6, 2, 1, 0, 2, 8, 2, 2, 8, 1, 5, 14, 14, 3, 2, 9, 8, 8, 2, 2, 2, 2, 2, 12, 10, 2, 3, 2, 10, 10, 2, 17, 10, 2, 8, 0, 2, 2, 10, 14, 1, 7, 8, 10, 4, 4, 4, 4, 1, 1, 3, 1, 4, 12, 4, 4, 0, 4, 4, 16, 16, 16, 16, 9, 4, 4, 0, 2, 18, 9, 9, 9, 9, 9, 9, 8, 16, 10, 10, 17, 17, 17, 10, 10, 1, 16, 16, 15, 11, 16, 10, 16, 10, 5, 3, 9, 9, 4, 10, 4, 7, 9, 15, 9, 9, 4, 9, 9, 12, 9, 9, 0, 13, 9, 4, 4, 5, 11, 11, 11, 11, 11, 11, 11, 16, 11, 11, 1, 1, 2, 0, 2, 5, 8, 12, 11, 11, 11, 11, 11, 11, 1, 11, 11, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 5, 3, 1, 1, 2, 13, 0, 2, 13, 0, 2, 8, 2, 8, 2, 8, 2, 5, 2, 15, 7, 15, 0, 7, 5, 2, 2, 2, 0, 2, 8, 2, 17, 10, 2, 8, 12, 4, 4, 4, 4, 7, 4, 4, 0, 3, 4, 6, 3, 4, 4, 5, 2, 1, 3, 7, 16, 16, 12, 16, 0, 2, 0, 16, 5, 2, 9, 0, 0, 4, 16, 0, 8, 0, 9, 9, 2, 0, 2, 9, 9, 0, 9, 9, 0, 0, 9, 8, 17, 9, 2, 17, 2, 9, 13, 5, 9, 17, 7, 1, 17, 17, 17, 0, 0, 15, 0, 13, 6, 6, 6, 5, 0, 2, 1, 8, 17, 0, 13, 2, 13, 1, 1, 1, 0, 7, 8, 8, 1, 1, 7, 3, 3, 11, 11, 1, 11, 11, 1, 11, 11, 11, 11, 1, 1, 6, 6, 6, 6, 6, 8, 19, 6, 11, 6, 6, 6, 6, 12, 6, 6, 11, 12, 5, 1, 6, 8, 12, 6, 6, 6, 1, 18, 8, 2, 2, 8, 2, 2, 2, 2, 4, 19, 19, 19, 19, 19, 19, 12, 19, 11, 19, 11, 11, 3, 15, 0, 4, 13, 15, 4, 13, 2, 8, 2, 2, 2, 12, 12, 7, 13, 0, 13, 13, 13, 13, 0, 15, 2, 0, 15, 2, 5, 5, 5, 0, 18, 0, 2, 2, 1, 0, 2, 2, 0, 2, 5, 2, 8, 5, 0, 5, 2, 5, 0, 5, 10, 5, 2, 5, 0, 0, 3, 3, 15, 2, 6, 0, 10, 2, 4, 1, 2, 2, 2, 2, 1, 13, 2, 0, 4, 3, 0, 0, 0, 7, 1, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 9, 6, 0, 6, 8, 6, 6, 6, 7, 8, 8, 6, 2, 18, 15, 12, 10, 10, 4, 18, 10, 0, 4, 14, 5, 5, 10, 5, 5, 19, 16, 19, 0, 19, 16, 15, 0, 0, 14, 14, 12, 14, 15, 16, 17, 4, 5, 1, 5, 0, 5, 0, 5, 5, 0, 5, 15, 5, 5, 5, 5, 10, 5, 3, 15, 14, 13, 7, 9, 6, 6, 6, 0, 0, 7, 0, 3, 7, 13, 0, 7, 2, 8, 8, 0, 8, 13, 2, 2, 13, 0, 2, 2, 2, 2, 0, 2, 6, 2, 6, 15, 6, 6, 0, 0, 2, 2, 0, 12, 2, 0, 2, 8, 8, 8, 8, 8, 8, 0, 8, 8, 0, 3, 4, 0, 18, 18, 1, 0, 5, 0, 3, 3, 5, 3, 1, 1, 7, 7, 3, 7, 7, 1, 4, 2, 0, 8, 3, 14, 4, 4, 7, 0, 7, 0, 8, 12, 12, 12, 9, 4, 4, 10, 4, 4, 4, 1, 0, 18, 4, 10, 4, 12, 10, 10, 5, 17, 3, 6, 0, 14, 5, 15, 15, 4, 5, 15, 5, 3, 12, 3, 12, 14, 15, 12, 1, 10, 19, 1, 7, 3, 12, 1, 1, 7, 5, 2, 7, 5, 7, 3, 6, 6, 1, 6, 15, 6, 17, 14, 9, 16, 5, 8, 8, 10, 11, 7, 10, 8, 8, 8, 8, 9, 8, 11, 8, 8, 14, 11, 8, 8, 7, 7, 14, 3, 14, 14, 14, 14, 6, 10, 14, 2, 8, 9, 16, 9, 5, 0, 13, 9, 1, 6, 17, 13, 18, 18, 6, 0, 0, 12, 9, 6, 0, 7, 2, 6, 16, 12, 6, 8, 18, 12, 0, 6, 1, 10, 18, 16, 0, 4, 1, 1, 4, 4, 7, 4, 6, 15, 15, 16, 5, 1, 15, 4, 12, 5, 1, 16, 16, 6, 1, 1, 0, 16, 1, 16, 0, 1, 17, 6, 9, 5, 10, 3, 11, 5, 2, 6, 7, 11, 1, 4, 10, 0, 0, 0, 11, 7, 11, 7, 2, 6, 9, 12, 6, 18, 0, 2, 2, 9, 4, 4, 0, 2, 15, 15, 2, 2, 0, 0, 5, 3, 13, 2, 9, 9, 13, 6, 9, 12, 7, 14, 13, 5, 6, 0, 10, 9, 10, 0, 1, 9, 4, 19, 0, 9, 11, 2, 9, 3, 3, 3, 6, 3, 6, 6, 6, 4, 3, 6, 6, 1, 10, 4, 11, 6, 0, 1, 4, 6, 14, 18, 10, 1, 4, 18, 1, 0, 16, 16, 8, 14, 1, 17, 17, 3, 10, 5, 0, 1, 5, 1, 1, 0, 2, 16, 1, 12, 1, 1, 11, 6, 3, 0, 5, 6, 6, 1, 2, 0, 12, 17, 11, 2, 1, 9, 4, 17, 9, 17, 0, 5, 3, 2, 0, 0, 9, 19, 2, 4, 7, 9, 9, 9, 13, 8, 0, 9, 3, 0, 5, 4, 3, 0, 5, 11, 11, 3, 3, 3, 2, 3, 3, 0, 9, 6, 4, 0, 6, 6, 13, 4, 0, 6, 19, 19, 6, 1, 5, 10, 11, 4, 10, 3, 0, 4, 2, 5, 8, 3, 16, 5, 0, 0, 5, 5, 5, 3, 14, 14, 16, 2, 9, 9, 7, 14, 16, 1, 8, 13, 0, 14, 6, 0, 5, 13, 18, 1, 11, 0, 3, 5, 1, 1, 12, 0, 11, 18, 1, 19, 6, 17, 7, 12, 12, 3, 17, 9, 9, 9, 17, 1, 4, 6, 7, 6, 8, 5, 9, 0, 17, 9, 0, 9, 6, 9, 9, 9, 0, 9, 17, 18, 13, 1, 0, 6, 6, 9, 2, 8, 2, 2, 5, 0, 6, 6, 9, 9, 6, 11, 19, 6, 6, 0, 7, 5]\n",
            "-------RUN18-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': \"CountVectorizer(ngram_range=(1, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b891dd10>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[10, 0, 11, 11, 9, 0, 14, 7, 7, 7, 0, 11, 9, 11, 7, 0, 2, 10, 0, 0, 7, 9, 7, 16, 16, 9, 1, 0, 0, 0, 0, 0, 11, 4, 0, 7, 8, 9, 0, 0, 16, 0, 9, 16, 9, 9, 9, 9, 16, 0, 9, 16, 7, 7, 7, 1, 1, 0, 7, 7, 8, 0, 7, 7, 0, 7, 8, 8, 8, 8, 4, 8, 2, 2, 2, 8, 6, 6, 0, 0, 1, 1, 4, 1, 1, 9, 1, 2, 0, 2, 2, 7, 1, 1, 7, 7, 7, 2, 4, 4, 11, 6, 6, 6, 0, 4, 15, 4, 4, 4, 8, 9, 9, 10, 0, 1, 1, 9, 6, 7, 14, 1, 16, 6, 9, 12, 9, 0, 16, 9, 0, 4, 0, 9, 9, 10, 16, 6, 9, 9, 7, 7, 10, 12, 4, 7, 10, 6, 0, 6, 7, 7, 16, 7, 6, 7, 2, 10, 6, 5, 6, 5, 4, 5, 4, 10, 10, 6, 4, 6, 7, 7, 0, 6, 4, 7, 5, 4, 4, 8, 0, 6, 4, 6, 9, 7, 8, 0, 9, 1, 1, 4, 1, 0, 12, 1, 15, 15, 1, 0, 1, 0, 1, 1, 4, 1, 1, 1, 2, 13, 2, 4, 4, 1, 1, 4, 4, 2, 0, 7, 15, 2, 2, 2, 7, 15, 1, 1, 12, 12, 12, 12, 12, 15, 15, 12, 9, 12, 3, 15, 15, 8, 8, 6, 15, 6, 0, 6, 15, 0, 6, 15, 11, 10, 6, 14, 14, 14, 2, 2, 9, 6, 15, 6, 0, 0, 0, 1, 1, 8, 8, 8, 8, 2, 8, 0, 8, 8, 9, 1, 2, 6, 8, 14, 14, 0, 15, 5, 11, 5, 11, 0, 11, 11, 1, 15, 7, 0, 15, 8, 8, 8, 9, 8, 8, 8, 2, 8, 19, 8, 16, 8, 9, 0, 4, 9, 6, 6, 6, 10, 7, 12, 0, 8, 6, 10, 6, 10, 0, 10, 5, 6, 1, 10, 18, 10, 7, 4, 2, 10, 4, 9, 9, 9, 7, 4, 4, 4, 3, 4, 10, 1, 0, 0, 7, 1, 4, 1, 1, 10, 6, 9, 6, 9, 1, 8, 9, 2, 9, 8, 1, 3, 9, 4, 7, 9, 3, 7, 0, 3, 12, 12, 0, 7, 7, 7, 1, 7, 1, 9, 4, 7, 1, 12, 12, 12, 12, 12, 12, 12, 1, 12, 15, 0, 15, 0, 12, 2, 12, 12, 9, 1, 0, 6, 12, 9, 12, 19, 18, 6, 6, 9, 1, 7, 18, 18, 6, 18, 2, 2, 2, 0, 2, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 18, 10, 8, 4, 8, 2, 7, 4, 8, 8, 8, 18, 0, 8, 7, 2, 8, 15, 8, 6, 0, 15, 8, 8, 8, 8, 2, 2, 14, 0, 2, 2, 2, 8, 14, 8, 2, 2, 5, 2, 1, 11, 14, 5, 14, 0, 14, 11, 6, 0, 9, 5, 11, 2, 0, 13, 10, 13, 13, 10, 2, 4, 15, 15, 9, 13, 3, 10, 10, 13, 13, 13, 13, 13, 6, 1, 13, 8, 13, 1, 1, 13, 17, 1, 13, 10, 6, 13, 13, 1, 15, 2, 7, 10, 1, 14, 9, 14, 14, 2, 2, 8, 2, 9, 6, 14, 14, 0, 14, 14, 3, 3, 3, 3, 3, 14, 14, 0, 11, 18, 3, 3, 3, 3, 3, 3, 10, 3, 1, 1, 17, 17, 17, 1, 1, 2, 3, 3, 16, 1, 3, 1, 3, 1, 4, 1, 3, 3, 9, 1, 9, 7, 3, 16, 3, 3, 9, 3, 3, 6, 3, 3, 0, 12, 3, 9, 9, 4, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 2, 2, 11, 0, 11, 4, 10, 6, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 13, 2, 2, 2, 4, 8, 2, 2, 13, 12, 0, 13, 12, 6, 13, 10, 13, 10, 13, 10, 13, 4, 13, 16, 7, 16, 0, 7, 4, 13, 13, 13, 0, 13, 10, 13, 17, 1, 13, 10, 6, 14, 14, 14, 14, 7, 14, 14, 6, 2, 14, 5, 9, 9, 14, 4, 11, 2, 9, 9, 3, 3, 6, 3, 0, 13, 0, 3, 4, 11, 3, 6, 4, 14, 3, 6, 10, 0, 3, 3, 13, 0, 13, 3, 3, 0, 3, 3, 0, 0, 3, 10, 17, 3, 11, 17, 11, 3, 12, 4, 3, 17, 7, 2, 17, 17, 17, 0, 0, 16, 0, 12, 5, 5, 5, 4, 0, 13, 2, 10, 17, 0, 12, 11, 12, 2, 2, 2, 0, 7, 10, 10, 2, 2, 7, 8, 8, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 4, 5, 5, 5, 5, 5, 10, 6, 5, 1, 5, 5, 5, 5, 6, 5, 5, 1, 6, 4, 2, 5, 10, 6, 5, 5, 5, 2, 18, 10, 13, 13, 10, 13, 13, 13, 13, 9, 19, 19, 19, 19, 19, 19, 6, 19, 1, 19, 1, 1, 8, 16, 0, 9, 12, 16, 9, 12, 11, 10, 11, 11, 11, 6, 6, 7, 12, 0, 12, 12, 12, 12, 6, 16, 11, 0, 16, 11, 4, 4, 4, 0, 18, 0, 11, 11, 2, 0, 11, 11, 0, 11, 4, 11, 10, 4, 0, 4, 11, 4, 0, 4, 1, 4, 13, 4, 0, 0, 8, 8, 16, 13, 5, 0, 1, 11, 9, 2, 11, 11, 11, 11, 2, 12, 11, 0, 9, 8, 6, 0, 0, 7, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 5, 0, 5, 10, 5, 5, 5, 7, 10, 10, 5, 13, 18, 16, 6, 1, 1, 9, 18, 1, 0, 9, 15, 4, 4, 1, 4, 4, 19, 3, 19, 0, 19, 3, 16, 0, 6, 15, 15, 6, 15, 16, 3, 10, 9, 4, 2, 4, 0, 4, 0, 4, 4, 0, 4, 16, 4, 4, 4, 4, 1, 0, 8, 16, 15, 12, 7, 3, 5, 5, 5, 0, 0, 7, 0, 8, 7, 12, 0, 7, 11, 10, 10, 0, 10, 12, 11, 11, 12, 0, 13, 11, 11, 11, 6, 11, 5, 11, 5, 16, 5, 5, 0, 6, 11, 11, 6, 6, 11, 0, 11, 10, 10, 10, 10, 10, 10, 0, 10, 10, 6, 8, 9, 0, 18, 18, 2, 0, 4, 7, 8, 8, 4, 9, 2, 2, 7, 7, 2, 7, 7, 2, 9, 13, 0, 10, 9, 15, 14, 14, 7, 0, 7, 0, 10, 6, 6, 6, 3, 14, 9, 1, 14, 14, 9, 2, 0, 18, 14, 1, 14, 6, 1, 1, 4, 17, 8, 5, 0, 15, 4, 16, 16, 9, 4, 16, 4, 8, 6, 8, 6, 15, 16, 6, 2, 1, 19, 2, 7, 8, 6, 2, 2, 7, 4, 11, 7, 4, 7, 8, 5, 5, 2, 5, 16, 5, 17, 15, 3, 3, 4, 10, 10, 1, 1, 7, 1, 10, 10, 10, 10, 3, 10, 1, 10, 10, 15, 1, 10, 10, 7, 7, 15, 9, 15, 15, 15, 15, 5, 1, 15, 13, 10, 3, 3, 3, 4, 0, 12, 3, 2, 5, 17, 12, 18, 18, 5, 0, 0, 6, 3, 5, 0, 7, 13, 5, 3, 6, 5, 10, 18, 6, 0, 5, 2, 1, 18, 3, 0, 14, 2, 2, 14, 14, 7, 9, 5, 16, 16, 3, 4, 2, 16, 14, 6, 4, 2, 3, 3, 5, 2, 2, 0, 3, 2, 3, 0, 2, 17, 5, 3, 4, 1, 8, 1, 4, 11, 5, 7, 1, 2, 9, 1, 0, 6, 0, 1, 7, 1, 7, 11, 5, 3, 6, 5, 8, 0, 11, 11, 3, 9, 9, 0, 11, 16, 16, 11, 11, 4, 0, 4, 8, 12, 11, 3, 3, 12, 5, 3, 6, 7, 15, 12, 4, 5, 0, 1, 3, 1, 0, 2, 3, 9, 19, 0, 3, 1, 11, 3, 8, 8, 8, 5, 8, 5, 5, 5, 14, 8, 5, 5, 2, 1, 14, 1, 5, 0, 2, 14, 5, 15, 18, 1, 2, 14, 18, 2, 0, 3, 3, 10, 15, 2, 17, 17, 8, 1, 0, 0, 2, 4, 2, 2, 0, 11, 3, 2, 6, 2, 2, 1, 5, 8, 0, 4, 5, 5, 2, 6, 4, 2, 17, 1, 11, 2, 3, 9, 17, 3, 17, 0, 4, 9, 11, 0, 0, 3, 19, 11, 9, 7, 3, 3, 3, 12, 10, 0, 3, 8, 0, 4, 4, 8, 0, 4, 1, 1, 8, 8, 8, 13, 9, 8, 6, 3, 5, 14, 0, 5, 5, 12, 14, 0, 5, 19, 19, 5, 2, 4, 1, 1, 9, 1, 8, 0, 14, 11, 4, 10, 8, 3, 4, 6, 6, 4, 4, 4, 8, 15, 15, 3, 13, 3, 3, 7, 15, 3, 2, 10, 12, 0, 15, 5, 6, 4, 12, 18, 2, 1, 0, 2, 4, 2, 2, 6, 0, 1, 18, 2, 19, 5, 17, 7, 6, 6, 8, 17, 3, 3, 3, 17, 2, 9, 5, 7, 5, 10, 4, 3, 6, 17, 3, 6, 3, 5, 3, 3, 3, 0, 3, 17, 18, 12, 2, 0, 5, 5, 3, 13, 10, 11, 11, 4, 0, 5, 5, 3, 3, 5, 1, 19, 5, 5, 0, 7, 4]\n",
            "-------RUN19-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': \"CountVectorizer(ngram_range=(2, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df4a9008410>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[9, 0, 11, 11, 8, 1, 8, 6, 6, 6, 6, 11, 3, 11, 6, 0, 2, 9, 6, 0, 6, 3, 6, 16, 16, 7, 8, 1, 0, 0, 1, 0, 11, 7, 0, 6, 3, 7, 0, 0, 16, 0, 8, 16, 8, 7, 8, 8, 16, 1, 8, 16, 6, 6, 6, 10, 10, 0, 6, 6, 3, 0, 6, 6, 0, 6, 3, 3, 3, 3, 7, 3, 2, 2, 2, 3, 1, 1, 0, 1, 10, 10, 7, 10, 10, 7, 10, 2, 0, 2, 2, 6, 10, 12, 6, 6, 6, 2, 0, 7, 11, 1, 1, 1, 1, 7, 6, 0, 0, 0, 3, 3, 8, 9, 0, 7, 12, 7, 1, 6, 8, 7, 16, 1, 8, 13, 3, 0, 16, 3, 0, 0, 0, 3, 3, 9, 16, 1, 3, 8, 6, 6, 9, 13, 0, 6, 9, 1, 0, 1, 6, 6, 16, 6, 1, 6, 2, 9, 7, 5, 1, 5, 7, 5, 0, 9, 9, 1, 7, 1, 6, 6, 1, 19, 0, 6, 9, 7, 0, 3, 0, 1, 0, 19, 3, 6, 2, 0, 3, 10, 10, 0, 10, 0, 13, 10, 15, 15, 10, 0, 10, 0, 10, 10, 7, 10, 12, 10, 2, 14, 2, 7, 7, 10, 10, 7, 7, 2, 0, 2, 15, 2, 2, 2, 6, 15, 10, 10, 13, 13, 13, 13, 13, 15, 15, 13, 7, 13, 4, 15, 15, 3, 3, 1, 15, 1, 1, 1, 15, 1, 1, 15, 11, 9, 1, 8, 8, 8, 2, 2, 8, 1, 15, 1, 0, 1, 0, 10, 12, 3, 3, 3, 3, 2, 3, 1, 3, 3, 7, 10, 2, 1, 3, 8, 8, 0, 15, 5, 11, 5, 11, 0, 11, 11, 12, 15, 6, 1, 15, 3, 3, 3, 3, 3, 3, 3, 3, 3, 19, 3, 16, 3, 8, 0, 7, 8, 1, 1, 1, 9, 6, 8, 1, 3, 1, 9, 1, 9, 0, 9, 5, 1, 10, 9, 18, 9, 6, 0, 2, 9, 0, 8, 7, 7, 6, 7, 0, 0, 4, 0, 1, 10, 0, 0, 6, 10, 0, 10, 10, 9, 1, 3, 1, 8, 10, 3, 3, 2, 8, 3, 10, 4, 3, 0, 6, 3, 4, 6, 0, 4, 13, 13, 0, 6, 6, 6, 10, 6, 10, 7, 7, 6, 10, 13, 13, 13, 13, 13, 13, 13, 12, 13, 15, 0, 15, 1, 13, 3, 13, 13, 7, 10, 0, 1, 13, 8, 13, 19, 18, 1, 1, 7, 12, 6, 18, 18, 1, 18, 2, 2, 2, 6, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 18, 9, 3, 7, 3, 2, 6, 7, 3, 3, 3, 18, 0, 3, 6, 2, 3, 15, 3, 1, 0, 15, 3, 3, 3, 3, 2, 2, 8, 0, 2, 2, 2, 3, 8, 3, 2, 2, 5, 2, 10, 11, 8, 5, 8, 1, 8, 11, 1, 0, 7, 5, 11, 2, 0, 14, 9, 14, 14, 9, 2, 7, 15, 15, 3, 14, 4, 9, 9, 14, 14, 14, 14, 14, 1, 10, 14, 3, 14, 10, 10, 14, 17, 10, 14, 9, 1, 14, 14, 10, 15, 2, 6, 9, 10, 8, 7, 8, 8, 2, 2, 3, 2, 7, 1, 8, 8, 0, 8, 8, 4, 4, 4, 4, 4, 8, 8, 0, 11, 2, 4, 4, 4, 4, 4, 4, 9, 4, 10, 10, 17, 17, 17, 10, 10, 2, 4, 4, 16, 12, 4, 10, 4, 10, 7, 3, 4, 4, 7, 10, 7, 6, 4, 16, 4, 4, 7, 4, 4, 1, 4, 4, 0, 13, 4, 7, 7, 7, 12, 12, 12, 12, 12, 12, 12, 4, 12, 12, 2, 2, 11, 0, 11, 0, 9, 1, 12, 12, 12, 12, 12, 12, 2, 12, 12, 2, 2, 2, 2, 2, 2, 14, 2, 2, 2, 7, 3, 2, 2, 14, 13, 0, 14, 13, 1, 14, 9, 14, 9, 14, 9, 14, 7, 14, 16, 6, 16, 0, 6, 7, 14, 14, 14, 0, 14, 9, 14, 17, 10, 14, 9, 1, 8, 8, 8, 8, 6, 8, 8, 1, 3, 8, 5, 3, 8, 8, 0, 11, 2, 3, 7, 4, 4, 1, 4, 1, 14, 0, 4, 7, 11, 4, 1, 0, 8, 4, 1, 9, 0, 4, 4, 14, 1, 14, 4, 4, 0, 4, 4, 1, 0, 4, 1, 17, 4, 11, 17, 11, 4, 13, 0, 4, 17, 6, 2, 17, 17, 17, 0, 0, 16, 1, 13, 5, 5, 5, 7, 1, 14, 2, 9, 17, 1, 13, 11, 13, 2, 2, 2, 1, 6, 9, 5, 2, 6, 6, 3, 3, 12, 12, 2, 12, 12, 2, 12, 12, 12, 12, 2, 2, 5, 5, 5, 5, 5, 1, 19, 5, 12, 5, 5, 5, 5, 1, 5, 5, 12, 1, 7, 2, 5, 9, 1, 5, 5, 5, 2, 18, 9, 14, 14, 9, 14, 14, 14, 14, 8, 19, 19, 19, 19, 19, 19, 1, 19, 12, 19, 12, 12, 3, 16, 0, 8, 13, 16, 8, 13, 11, 9, 11, 11, 11, 1, 1, 6, 13, 1, 13, 13, 13, 13, 1, 16, 11, 0, 16, 11, 7, 7, 7, 0, 18, 1, 11, 11, 2, 0, 11, 11, 0, 11, 0, 11, 1, 0, 0, 0, 11, 0, 0, 0, 10, 0, 14, 7, 1, 0, 3, 3, 16, 14, 5, 0, 10, 11, 8, 2, 11, 11, 11, 11, 2, 13, 11, 0, 8, 3, 1, 0, 0, 6, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 1, 5, 9, 5, 5, 5, 6, 9, 9, 5, 14, 18, 16, 1, 10, 10, 7, 18, 10, 0, 8, 15, 7, 7, 10, 0, 7, 19, 4, 19, 0, 19, 4, 16, 0, 1, 15, 15, 15, 15, 16, 4, 1, 8, 7, 6, 7, 1, 7, 1, 7, 7, 0, 7, 16, 7, 7, 7, 7, 10, 6, 3, 16, 15, 13, 6, 4, 5, 5, 5, 0, 0, 6, 1, 3, 6, 13, 0, 6, 11, 9, 9, 1, 9, 13, 11, 11, 13, 0, 14, 11, 11, 11, 1, 11, 5, 11, 5, 16, 5, 5, 1, 1, 11, 11, 1, 1, 11, 0, 11, 9, 9, 9, 9, 9, 9, 1, 9, 9, 1, 3, 7, 0, 18, 18, 2, 1, 7, 6, 3, 3, 7, 3, 2, 2, 6, 6, 3, 6, 6, 2, 7, 14, 0, 9, 3, 15, 8, 8, 6, 0, 6, 0, 9, 1, 1, 1, 4, 8, 8, 10, 8, 8, 8, 2, 0, 18, 8, 10, 8, 1, 10, 10, 6, 17, 3, 5, 1, 15, 7, 16, 16, 8, 7, 16, 7, 3, 1, 3, 1, 15, 16, 1, 2, 10, 19, 2, 6, 3, 1, 2, 2, 6, 0, 11, 6, 0, 6, 3, 5, 5, 2, 5, 16, 5, 17, 15, 4, 4, 7, 9, 9, 10, 12, 6, 10, 9, 9, 9, 9, 4, 9, 12, 9, 9, 15, 12, 9, 9, 6, 6, 15, 3, 15, 15, 15, 15, 5, 10, 15, 14, 9, 4, 4, 4, 7, 0, 13, 4, 2, 5, 17, 13, 18, 18, 5, 0, 0, 1, 4, 5, 0, 6, 14, 5, 6, 1, 5, 9, 18, 1, 0, 5, 2, 10, 18, 6, 0, 8, 2, 2, 8, 8, 6, 8, 5, 16, 16, 4, 7, 2, 16, 8, 1, 7, 2, 4, 4, 5, 2, 2, 0, 4, 2, 4, 0, 2, 17, 9, 4, 7, 10, 3, 12, 0, 11, 5, 6, 12, 2, 8, 10, 0, 1, 0, 12, 6, 12, 6, 11, 5, 4, 1, 5, 18, 0, 11, 11, 4, 8, 8, 0, 11, 16, 16, 11, 11, 1, 0, 7, 3, 13, 11, 4, 4, 13, 5, 4, 1, 6, 15, 13, 0, 5, 0, 10, 4, 10, 0, 2, 4, 8, 19, 1, 4, 12, 11, 4, 3, 3, 3, 5, 3, 5, 5, 5, 8, 3, 5, 5, 2, 10, 8, 12, 5, 0, 2, 8, 5, 15, 18, 10, 2, 8, 18, 2, 0, 4, 4, 1, 15, 2, 17, 17, 3, 10, 0, 1, 2, 7, 2, 2, 1, 11, 4, 2, 1, 2, 2, 12, 5, 3, 0, 7, 5, 5, 2, 11, 0, 15, 17, 12, 11, 2, 4, 8, 17, 4, 17, 0, 7, 3, 11, 0, 0, 4, 19, 11, 8, 6, 4, 4, 4, 13, 9, 1, 4, 3, 0, 7, 7, 3, 0, 0, 12, 12, 3, 3, 3, 14, 3, 3, 1, 4, 5, 8, 1, 5, 5, 13, 8, 0, 5, 19, 19, 5, 2, 7, 10, 12, 8, 10, 3, 0, 8, 11, 7, 9, 3, 4, 7, 1, 1, 0, 7, 7, 3, 15, 15, 4, 14, 4, 4, 6, 15, 4, 2, 9, 13, 0, 15, 5, 1, 7, 13, 18, 2, 12, 0, 3, 0, 2, 2, 1, 0, 12, 18, 2, 19, 5, 17, 6, 1, 1, 3, 17, 4, 4, 4, 17, 2, 7, 5, 6, 5, 9, 7, 4, 1, 17, 4, 1, 4, 5, 4, 4, 4, 0, 4, 17, 18, 13, 2, 0, 5, 5, 4, 14, 9, 11, 11, 6, 0, 5, 5, 4, 4, 5, 12, 19, 5, 5, 1, 6, 7]\n",
            "-------RUN20-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': \"CountVectorizer(stop_words='english')\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[5, 0, 26, 26, -1, 0, -1, 6, 6, 6, -1, 23, 22, 23, 6, -1, 1, 5, -1, 0, -1, 22, 6, 11, 11, 19, -1, 0, 0, 0, 0, 0, 26, 21, 0, 6, -1, -1, -1, 0, 11, -1, 28, 11, 28, -1, 28, 28, 11, 0, 28, 11, 6, 6, -1, 14, 14, 0, 6, 6, 1, 0, -1, 6, -1, 6, 4, 1, -1, 4, 21, -1, 1, 1, 1, 4, 0, -1, 0, -1, 14, 14, -1, 14, 14, 19, 14, 1, 0, 1, 1, 6, 14, -1, -1, 6, 6, 1, -1, -1, 23, 18, 0, 18, 0, 21, -1, -1, -1, -1, -1, 22, -1, 5, 0, -1, -1, 19, 0, -1, 28, -1, 11, 18, 28, 29, 22, 0, 11, 22, 0, 16, -1, 22, 22, 5, 11, 0, 22, -1, 6, 6, 5, 29, 16, 6, 5, 0, 0, -1, 6, 6, 11, 6, -1, 6, 1, -1, -1, 2, 0, -1, 21, 2, -1, 5, 5, 18, -1, 0, 6, -1, 0, -1, 16, 6, 2, -1, 16, 1, 0, 0, -1, -1, 22, 6, -1, 0, 22, 25, 25, 16, 14, 0, 29, -1, 13, 13, 14, 0, 14, 0, 14, 14, -1, 14, 8, 14, 1, 23, 1, -1, -1, 14, 14, -1, -1, 1, 0, -1, 13, 1, 1, 1, -1, 13, -1, -1, -1, 29, 29, 29, 29, 13, 13, 29, 19, 29, 3, 13, 13, 4, 4, -1, 13, -1, 0, 18, 30, 0, -1, 13, 23, 5, -1, 7, 7, 7, 1, 1, -1, 18, 30, -1, 0, -1, 0, -1, -1, 4, 4, 4, 4, 1, 4, 0, 4, 4, 19, -1, 1, -1, 4, 7, 7, -1, 13, 2, -1, 2, 23, 0, 23, 23, -1, 13, 6, -1, 13, 4, 4, 4, 22, 4, 1, 4, 1, 4, 27, 4, 11, 4, 28, -1, -1, -1, -1, -1, 18, 5, 6, 29, 0, -1, -1, 5, -1, 5, 0, 5, 2, -1, 25, 5, 20, 5, -1, 16, 1, -1, 16, -1, 19, 19, -1, -1, 16, 16, 3, 16, -1, -1, 0, 0, 6, 25, 16, 25, 25, 5, 18, 22, 0, -1, 25, 4, 22, 1, 19, 1, 25, 12, 22, 16, 6, 22, 3, -1, 0, 3, 10, 10, -1, 6, 6, -1, 25, 6, 25, 19, 31, -1, 25, 10, 10, 10, 10, 10, 10, 10, -1, 10, 13, 0, 13, 0, 10, -1, 29, 29, 19, 25, -1, 18, 29, 19, 29, -1, 20, -1, -1, -1, -1, -1, 20, 20, 0, 20, 1, 1, 1, -1, 1, 4, 1, 1, 1, -1, 1, 1, 1, 1, 1, 20, 5, 4, 21, 4, 1, 6, 21, 4, 4, 4, 20, 0, 4, 6, 1, 4, 30, 4, -1, 0, 13, 4, 4, 4, 4, 1, 1, 7, 0, 1, 1, 1, 4, 7, 1, 1, 1, 2, 1, -1, -1, -1, 2, 7, -1, 7, -1, 0, 0, 19, 2, -1, 1, -1, 9, 5, 9, 9, 5, 1, -1, 13, 13, 22, -1, 3, 5, 5, 9, 9, 9, 9, 9, 18, 25, 9, 4, 9, 32, 25, -1, 15, 32, 9, 5, 0, 9, -1, 32, -1, 1, -1, 5, 32, 7, 19, 7, 7, 1, 1, 4, 1, -1, 18, 7, 7, 0, 7, 7, 12, 12, 12, 12, 3, 7, 7, 0, 23, 20, 3, 3, 3, 3, 3, 3, 2, 12, 32, -1, 15, 15, 15, 32, 25, 1, 12, 12, 11, -1, 12, 32, 12, 25, 21, 4, 3, 3, 19, -1, -1, 6, 3, 11, 3, 3, 19, 3, 3, 18, 3, 3, 0, 10, 3, 19, 19, 21, -1, 8, 8, 8, 8, 8, 8, 12, 8, 8, 1, 1, -1, 0, -1, 16, 5, 18, 8, 8, -1, 8, 8, 8, 1, 8, 8, 1, 1, 1, 1, 1, 1, 9, 1, 1, 1, -1, 4, 1, -1, 9, 10, 0, 9, 29, 0, 9, 5, 9, 5, 9, 5, 9, 21, 9, 11, 6, 11, 0, -1, 21, 9, 9, 9, 0, 9, 5, 9, 15, 25, 9, 5, 18, 7, 7, 7, 7, -1, 7, 7, 0, 1, 7, 2, 22, -1, 7, 16, 26, 1, 22, -1, 12, 12, 18, 12, 0, -1, -1, 12, -1, 26, 3, 0, -1, -1, 12, 0, 2, 0, 3, 3, -1, 0, -1, 3, 3, 0, 3, 3, 0, -1, 3, -1, 15, 3, 23, 15, 23, 3, 10, 16, 3, 15, 6, -1, 15, 15, 15, 0, -1, 11, 0, 29, 2, 2, 2, 21, -1, 23, 1, -1, 15, 0, 10, 23, 10, 1, -1, 1, -1, -1, -1, 2, -1, -1, 6, 4, 4, 8, 8, 1, 8, 8, 1, 8, 8, 8, 8, 1, -1, 2, 2, 2, 2, 2, -1, -1, 2, 8, 2, 2, 2, 2, -1, 2, 2, 8, 18, -1, 1, 2, 2, 18, 2, 2, 2, 1, 20, 5, 9, 9, 5, 9, 9, 9, 9, -1, 27, 27, 27, 27, 27, 27, 18, 27, -1, 27, -1, 8, 4, 11, 0, 28, 10, 11, -1, 10, 17, -1, 17, 17, 17, 18, 18, 13, 10, 0, 10, 10, 10, 10, 0, 11, 26, 0, 11, 17, 21, 21, 21, 0, 20, 0, 26, 26, -1, 0, 26, 26, 0, 26, 16, 26, -1, 16, 0, -1, 26, 16, 0, 16, 14, 16, 23, 19, 0, -1, 1, -1, 11, -1, 2, 0, 14, 23, 28, -1, 23, 23, 23, 23, -1, 10, 23, -1, 28, -1, 0, -1, -1, 6, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, -1, 2, 2, 2, 2, 2, -1, 2, 2, 2, 9, -1, 11, 18, 32, 25, -1, 20, 14, 0, 28, 13, 21, 31, 32, 16, 21, 27, 12, 27, 0, 27, 12, 11, 0, -1, 30, 30, -1, 30, 11, 12, -1, 28, 31, -1, -1, -1, 21, -1, 21, 31, 0, 31, 11, -1, 31, -1, 31, -1, -1, 4, 11, 30, 10, 6, 3, 24, 24, 24, -1, 0, 6, -1, -1, 6, 10, 0, 6, 17, 5, 5, 0, 5, 10, 17, 17, 10, 0, 9, 17, 17, 17, 0, 17, 2, 17, 24, 11, 24, 24, -1, 0, 26, 26, 0, -1, 26, 0, 26, 5, 5, 5, 5, 5, 5, -1, 5, 5, -1, 4, 19, 0, 20, 20, 1, -1, -1, -1, 4, -1, 21, -1, 1, 1, 6, -1, 1, -1, -1, 1, 19, 9, 0, 5, 22, 13, 7, 7, 6, 0, 6, -1, 5, -1, 18, -1, 3, 7, 28, -1, -1, 7, 28, 1, 0, 20, 7, 25, 7, 18, 32, 32, -1, 15, 4, 2, -1, 30, 31, 11, 11, -1, 31, 11, 19, 4, -1, -1, -1, -1, 11, -1, 1, 25, -1, 1, 6, 4, -1, 1, -1, 6, 16, -1, 6, 16, 6, 4, 24, 24, -1, 24, 11, 24, 15, 30, 3, 12, 21, 5, 5, 14, -1, 6, 25, 5, 5, 5, 5, 3, 5, 8, 5, 5, 13, -1, 5, 5, -1, -1, 13, 22, 30, -1, 30, 30, 2, 14, 30, -1, 2, 3, 12, 3, -1, 0, 10, 3, -1, 24, 15, 10, 20, 20, 24, 0, 0, -1, 3, 24, 0, 6, -1, 24, 12, -1, 24, 5, 20, -1, 0, 2, 1, -1, 20, 12, 0, 7, -1, 1, 7, 7, 6, -1, 2, 11, 11, 12, 21, 1, 11, 7, 18, -1, 1, 12, 12, 2, 1, 1, -1, 12, 1, 12, -1, 1, 15, 2, -1, 16, -1, 4, 8, 16, 17, 2, -1, 8, 1, -1, 14, 0, 0, 0, 8, 6, 8, 6, 26, 2, 3, -1, 2, 20, 0, -1, 17, 3, 28, 28, -1, 17, 11, 11, 17, 17, -1, 0, -1, 4, 10, 17, 3, 3, 10, 2, 3, 0, 6, 13, 10, -1, -1, 0, 14, 3, 14, 0, -1, 3, -1, -1, 0, 3, -1, 23, 3, 4, 4, 4, 2, 1, 2, 2, 24, 7, 4, 2, 2, 1, 14, 7, 8, 24, 0, 1, 7, 2, 13, 20, 14, 1, 7, 20, 1, 0, 12, 12, -1, 13, 1, 15, 15, 1, 32, 0, 0, 1, 21, 1, 1, 0, -1, 12, -1, -1, 1, 1, 8, 2, 4, -1, -1, 2, 2, 1, 17, -1, -1, 15, 8, 23, 1, 3, -1, 15, 3, 15, -1, 21, 22, 17, 0, 0, 3, 27, 17, 19, -1, 3, 3, 3, 10, 2, 0, 3, 4, 0, 31, 19, 4, 0, -1, 8, 8, 4, 4, 4, 9, 22, 4, 0, 3, 2, 7, -1, 2, 2, 10, 7, -1, 2, 27, 27, 2, 1, -1, -1, -1, -1, -1, 4, 0, 7, 17, 31, -1, 4, 12, 16, 0, 0, -1, -1, 31, 4, 30, -1, 12, 9, 3, 3, -1, 13, 12, 1, 2, 10, 0, 13, 24, 0, -1, 10, 20, 1, 8, 0, -1, -1, 1, -1, -1, -1, 8, 20, 1, 27, 24, 15, -1, -1, 18, 1, 15, 3, 3, 3, 15, 1, 19, 2, -1, 2, 2, 31, 3, 0, 15, 3, -1, 3, 2, 3, 3, 3, 0, 3, 15, 20, 10, 1, 0, 2, 2, 3, 9, 5, 17, 17, -1, 0, 2, 2, 3, 3, 24, 8, 27, 24, 24, -1, -1, -1]\n",
            "-------RUN21-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': 'CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7df3b00f5c90>)', 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[5, 0, 26, 26, -1, 0, -1, 6, 6, 6, -1, 14, 23, 14, 6, -1, 1, 5, -1, 0, -1, 23, 6, 11, 11, 22, -1, 0, 0, 0, 0, 0, 26, 21, 0, -1, -1, -1, -1, 0, 11, -1, 29, 11, 29, -1, 29, 29, 11, 0, 29, 11, -1, -1, -1, 15, 15, 0, 6, -1, 1, 0, -1, 6, -1, 6, 3, 1, -1, 3, 21, -1, 1, 1, 1, 3, 0, -1, 0, 0, 15, 15, -1, 15, 15, 22, 15, 1, 0, 1, 1, 6, 15, -1, -1, 6, 6, 1, -1, -1, 14, 18, 0, 18, 0, 21, -1, -1, -1, -1, -1, 23, -1, 5, 0, -1, -1, -1, 0, -1, 7, -1, 11, 18, 29, 30, 23, 0, 11, 23, 0, 20, 0, 23, 23, 5, 11, 0, 23, -1, 6, -1, 5, 30, 20, 6, 5, 0, 0, -1, 6, 6, 11, 6, -1, 6, 1, -1, -1, -1, 0, -1, 21, -1, 0, 5, 5, 18, 31, 0, 6, -1, 0, -1, 20, 6, -1, -1, 20, 1, 0, 0, 20, -1, 23, -1, -1, 0, 23, 12, 12, 20, 15, 0, 30, 15, 17, 17, 15, 0, 15, 0, 15, 15, -1, 15, 10, 15, 1, 14, 1, -1, -1, 15, 15, -1, -1, 1, 0, -1, 17, 1, 1, 1, -1, 17, -1, -1, -1, 30, 30, 30, 30, 17, 17, 30, 22, 30, 2, 17, -1, 3, 3, 0, 17, -1, 0, 18, 27, 0, -1, 17, 14, 5, -1, 7, 7, 7, 1, 1, -1, 18, 27, 0, 0, -1, 0, -1, -1, 3, 3, 3, 3, 1, 3, 0, 3, 3, 22, -1, 1, -1, 3, 7, -1, -1, 17, 32, 14, 32, 14, 0, 14, 14, -1, 17, 6, 0, 17, 3, 3, 3, 23, 3, -1, 3, 1, 3, 28, 3, 11, 3, 29, 0, -1, -1, -1, -1, 18, 5, -1, 7, 0, -1, -1, 5, -1, 5, 0, 5, -1, -1, 12, 5, 24, 5, 6, 20, 1, -1, 20, -1, 22, 22, -1, -1, 20, 20, 2, 20, -1, 12, 0, 0, 6, 12, 20, 12, 12, 5, 18, 23, 0, 23, 12, 3, 23, 1, 22, 1, 12, 13, 23, 20, -1, 23, 2, 6, 0, 2, 8, 8, 0, 6, 6, 6, 12, 6, 12, 22, 31, -1, 12, 8, 8, 8, 8, 8, 8, 8, -1, 8, 17, 0, 17, 0, 8, 1, 30, 30, 22, 12, -1, 18, 30, 22, 30, -1, 24, -1, -1, -1, -1, -1, 24, 24, 0, 24, 1, 1, 1, 0, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 24, 5, 3, 21, 3, 1, 6, 21, 3, 3, 3, 24, 0, 3, -1, 1, 3, 27, 3, -1, 0, 17, 3, 3, 3, 3, 1, 1, 7, 0, 1, 1, 1, 3, 7, -1, 1, 1, 32, 1, -1, -1, -1, 32, 7, 0, 7, 14, 0, 0, 22, 32, 14, 1, -1, 9, 5, 9, 9, 5, 1, -1, 17, 17, 23, 14, 2, 5, 5, 9, 9, 9, 9, 9, 18, 12, 9, 3, 9, 12, 12, 14, 16, 12, 9, 5, 0, 9, 14, 12, -1, 1, -1, 5, 12, 7, 22, 7, 7, 1, 1, 3, 1, 22, 18, 7, 7, 0, 7, 7, 13, 13, 13, 13, 2, 7, 7, 0, 14, 24, 2, 2, 2, 2, 2, 2, -1, 13, 12, 12, 16, 16, 16, 12, 12, 1, 13, 13, 11, -1, 13, 12, 13, 12, 21, 3, 2, 2, 22, -1, -1, 6, 2, 11, 2, 2, 22, 2, 2, 18, 2, 2, 0, 8, 2, 22, 22, 21, -1, 10, 10, 10, 10, 10, 10, 13, 10, 10, 1, 1, -1, 0, -1, 20, 5, 18, -1, 10, -1, 10, 10, 10, 1, 10, 10, 1, 1, 1, 1, 1, 1, 9, 1, 1, 1, -1, 3, 1, 1, 9, 8, 0, 9, 30, 0, 9, 5, 9, 5, 9, 5, 9, -1, 9, 11, 6, 11, 0, -1, 21, 9, 9, 9, 0, 9, 5, 9, 16, 12, 9, 5, 18, 7, 7, 7, 7, -1, 7, 7, 0, 1, 7, -1, 23, -1, -1, 20, 26, 1, 23, -1, 13, 13, 18, 13, 0, 14, -1, -1, -1, 26, 2, 0, -1, -1, 13, 0, -1, 0, 2, 2, -1, 0, -1, 2, 2, 0, 2, 2, 0, 0, 2, -1, 16, 2, 14, 16, 14, 2, 8, 20, 2, 16, -1, 1, 16, 16, 16, 0, -1, 11, 0, 30, 4, 4, 4, 21, -1, 14, 1, -1, 16, 0, 8, 14, 8, 1, 1, 1, 0, -1, -1, -1, -1, -1, 6, 3, 3, 10, 10, 1, 10, 10, 1, 10, 10, 10, 10, 1, -1, 4, 4, 4, 4, 4, 16, -1, 4, 10, -1, 4, 4, 4, -1, 4, 4, 10, 18, -1, 1, 4, -1, 18, 4, 4, 4, 1, -1, 5, 9, 9, 5, 9, 9, 9, 9, -1, 28, 28, 28, 28, 28, 28, 18, 28, -1, 28, -1, -1, 3, 11, 0, 29, 8, 11, -1, 8, 19, -1, 19, 19, 19, 18, 18, -1, 8, 0, 8, 8, 8, 8, 0, 11, 26, 0, 11, 19, 21, 21, 21, 0, -1, 0, 26, 26, 1, 0, 26, 26, 0, 26, 20, 26, -1, 20, 0, -1, 26, 20, 0, 20, 15, -1, 14, 22, 0, -1, 1, -1, 11, -1, 4, 0, 15, 14, 29, -1, 14, 14, 14, 14, 1, 8, 14, -1, 29, -1, 0, 0, -1, 6, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, -1, 4, -1, 4, 32, 4, -1, -1, -1, 4, 9, -1, 11, 18, 12, 12, -1, 24, 15, 0, -1, 17, 21, 31, 12, -1, 21, 28, 13, 28, 0, 28, 13, 11, 0, 0, 27, 27, -1, 27, 11, 13, -1, 29, 31, -1, -1, 0, 21, 0, 21, 31, 0, 31, 11, -1, 31, -1, 31, 12, 0, 3, 11, 27, 8, 6, 2, 25, 25, 25, -1, 0, 6, -1, -1, 6, 8, 0, -1, 19, 5, 5, 0, 5, 8, 19, 19, 8, 0, 9, 19, 19, 19, 0, 19, -1, 19, 25, 11, 25, 25, 0, 0, 26, 26, 0, 0, 26, 0, 26, 5, 5, 5, 5, 5, 5, -1, 5, 5, -1, 3, 22, 0, 24, 24, 1, 0, 21, -1, 3, -1, 21, -1, 1, 1, 6, -1, 1, -1, -1, 1, 22, 9, 0, 5, 23, 17, 7, 7, 6, 0, 6, -1, -1, -1, 18, -1, 2, 7, 29, -1, -1, 7, 29, 1, 0, 24, 7, 12, 7, 18, 12, 12, -1, 16, 3, 4, 0, 27, 31, 11, 11, -1, 31, 11, -1, 3, -1, -1, -1, 27, 11, -1, 1, 12, -1, 1, 6, 3, -1, 1, -1, 6, 20, 14, 6, 20, 6, 3, 25, 25, -1, 25, 11, 25, 16, 27, 2, 13, -1, 5, 5, 15, -1, 6, 12, 5, 5, 5, 5, 2, 5, 10, 5, 5, 17, -1, 5, 5, -1, -1, 17, 23, 27, 27, 27, 27, 32, 15, 27, 14, -1, 2, 13, 2, -1, 0, 8, 2, 1, 25, 16, 8, 24, 24, 25, 0, 0, -1, 2, 25, 0, 6, -1, 25, 13, -1, 25, 5, 24, 0, 0, 4, 1, -1, 24, 13, 0, 7, -1, 1, 7, 7, 6, -1, 4, 11, 11, 13, 21, 1, 11, 7, 18, -1, 1, 13, 13, 4, 1, 1, -1, 13, 1, 13, -1, 1, 16, -1, -1, -1, -1, 3, 10, 20, 19, 32, -1, 10, 1, -1, 15, 0, 0, 0, 10, 6, 10, 6, 26, -1, 2, -1, 32, 24, 0, -1, 19, 2, 29, 29, 0, 19, 11, 11, 19, 19, -1, 0, -1, 3, 8, 19, 2, 2, 8, 4, 2, -1, 6, 17, 8, -1, -1, 0, 15, 2, 15, 0, -1, 2, -1, -1, 0, 2, -1, 14, 2, 3, 3, 3, 4, 1, 4, 4, 25, 7, 3, 4, 4, 1, 15, 7, -1, 25, 0, 1, 7, 32, 17, 24, 15, 1, 7, 24, 1, 0, 13, 13, -1, 17, 1, 16, 16, 1, 12, 0, 0, 1, 21, 1, 1, 0, -1, 13, -1, -1, 1, 1, 10, 4, 3, 0, 21, 4, 4, 1, 19, -1, -1, 16, 10, 14, 1, 2, 29, 16, 2, 16, -1, 21, 23, 19, 0, 0, 2, 28, 19, 22, -1, 2, 2, 2, 8, -1, -1, 2, 3, 0, 31, 22, 3, 0, -1, 10, 10, 3, 3, 3, 9, 23, 3, 0, 2, 4, 7, -1, 4, 4, 8, 7, -1, 4, 28, 28, 4, 1, -1, -1, -1, -1, -1, 3, 0, 7, 19, 31, -1, 3, 13, -1, 0, 0, -1, -1, 31, 3, 27, 27, 13, 9, 2, 2, -1, 17, 13, 1, -1, 8, 0, 17, 25, 0, 20, 8, 24, 1, 10, 0, -1, -1, 1, 1, -1, 0, 10, 24, 1, 28, 25, 16, -1, -1, 18, 1, 16, 2, 2, 2, 16, 1, 22, 4, -1, 4, -1, 31, 2, 0, 16, 2, -1, 2, 32, 2, 2, 2, 0, 2, 16, 24, 8, 1, 0, 4, 4, 2, 9, 5, 19, 19, -1, 0, 4, 4, 2, 2, 25, 10, 28, 25, 25, 0, -1, -1]\n",
            "-------RUN22-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': \"CountVectorizer(stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b02d2c50>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[5, 0, 27, 27, -1, 0, 6, 7, 7, 7, -1, 20, 19, 20, 7, -1, 1, 5, -1, 0, -1, 19, 7, 11, 11, 24, -1, 0, 0, 0, 0, 0, 27, 25, 0, -1, -1, -1, -1, 0, 11, -1, 29, 11, 29, -1, 29, 29, 11, 0, 29, 11, 7, -1, -1, 13, 13, 0, 7, -1, 1, 0, -1, 7, -1, 7, 4, 1, -1, 4, 25, -1, 1, 1, 1, 4, 0, -1, 0, 0, 13, 13, -1, 13, 13, 24, 13, 1, 0, 1, 1, 7, 13, -1, -1, 7, 7, 1, -1, -1, 20, 16, 0, 16, 0, 25, -1, -1, -1, -1, -1, 19, -1, -1, 0, -1, -1, -1, 0, -1, 6, -1, 11, 16, 29, 28, 19, 0, 11, 19, 0, 22, 0, 19, 19, 5, 11, 0, 19, -1, 7, 7, 5, 28, 22, 7, 5, 0, 0, -1, 7, 7, 11, 7, -1, 7, 1, -1, -1, 2, 0, -1, 25, 2, -1, -1, 5, 16, -1, 0, 7, -1, 0, -1, 22, 7, 2, -1, 22, 1, 0, 0, -1, -1, 19, -1, -1, 0, 19, 23, -1, 22, 13, 0, 28, 13, 15, 15, 13, 0, 13, 0, 13, 13, -1, 13, 8, 13, 1, 20, 1, -1, -1, 13, 13, -1, -1, 1, 0, -1, 15, 1, 1, 1, -1, 15, -1, -1, -1, 28, 28, 28, 28, 15, 15, 28, 24, 28, 3, 15, -1, 4, 4, -1, 15, -1, 0, 16, 30, 0, -1, 15, 20, 5, -1, 6, 6, 6, 1, 1, -1, 16, 30, -1, 0, -1, 0, -1, -1, 4, 4, 4, 4, 1, 4, 0, 4, 4, -1, -1, 1, -1, 4, 6, 6, -1, 15, 2, -1, 2, 20, 0, 20, 20, -1, 15, 7, 0, 15, 4, 4, 4, 19, 4, 1, 4, 1, 4, 26, 4, 11, 4, 29, 0, -1, -1, -1, 0, 16, 5, -1, 28, 0, -1, -1, 5, -1, 5, 0, 5, 2, -1, 23, 5, 18, 5, -1, 22, 1, -1, 22, -1, 24, -1, -1, -1, 22, 22, 3, 22, -1, -1, 0, 0, 7, 23, 22, 23, 31, 5, 16, 19, 0, 19, 23, 4, 19, 1, 24, 1, 23, 12, 19, 22, 7, 19, 3, 7, 0, 3, 9, 9, 0, 7, 7, -1, 23, 7, 23, 24, 32, -1, 23, 9, 9, 9, 9, 9, 9, 9, -1, 9, 15, 0, 15, 0, 9, -1, 28, 28, 24, 23, -1, 16, 28, 24, 28, -1, 18, -1, -1, -1, -1, -1, 18, 18, 0, 18, 1, 1, 1, -1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 18, 5, 4, -1, 4, 1, 7, 25, 4, 4, 4, 18, 0, 4, -1, -1, 4, 30, 4, -1, 0, 15, 4, 4, 4, 4, 1, 1, 6, 0, 1, 1, 1, 4, 6, -1, 1, 1, 2, 1, -1, -1, -1, 2, 6, 0, 6, -1, 0, 0, 24, 2, -1, 1, 0, 10, 5, 10, 10, 5, 1, -1, 15, 15, 19, 20, -1, 5, 5, 10, 10, 10, 10, 10, 16, 23, 10, 4, 10, 31, 23, -1, 14, 31, 10, 5, 0, 10, -1, 31, -1, 1, -1, -1, 31, 6, -1, 6, 6, 1, -1, 4, 1, -1, 16, 6, 6, 0, 6, 6, 12, 12, 12, 12, 3, 6, 6, 0, 20, 18, 3, 3, 3, 3, 3, 3, -1, 12, 31, -1, 14, 14, 14, 31, 23, -1, 12, 12, 11, -1, 12, 31, 12, -1, 25, 4, 3, 3, 24, 23, -1, 7, 3, 11, 3, 3, 24, 3, 3, 16, 3, 3, 0, 9, 3, 24, 24, 25, -1, 8, 8, 8, 8, 8, 8, 12, 8, 8, 1, 1, -1, 0, -1, -1, 5, 16, 8, 8, -1, 8, 8, 8, 1, 8, 8, 1, 1, 1, 1, 1, 1, 10, 1, 1, 1, -1, 4, 1, -1, 10, 9, 0, 10, 28, 0, 10, 5, 10, 5, 10, 5, 10, -1, 10, 11, 7, 11, 0, -1, -1, 10, 10, 10, 0, 10, 5, 10, 14, 23, 10, 5, 16, 6, 6, 6, 6, -1, 6, 6, 0, 1, 6, 2, 19, 29, 6, 22, 27, 1, 19, -1, 12, 12, 16, 12, 0, -1, 0, 12, -1, 27, 3, 0, -1, -1, 12, 0, -1, 0, 3, 3, -1, 0, -1, 3, 3, 0, 3, 3, 0, 0, 3, -1, 14, 3, 20, 14, 20, 3, 9, 22, 3, 14, -1, -1, 14, 14, 14, 0, -1, 11, 0, 28, 2, 2, 2, 25, -1, 20, 1, -1, 14, 0, 9, 20, 9, 1, -1, 1, -1, -1, -1, 2, -1, -1, 7, 4, 4, 8, 8, 1, 8, 8, 1, 8, 8, 8, 8, 1, 1, 2, 2, 2, 2, 2, -1, -1, 2, 8, 2, 2, 2, 2, -1, 2, 2, 8, 16, -1, 1, 2, 2, 16, 2, 2, 2, 1, 18, 5, 10, 10, 5, 10, 10, 10, 10, -1, 26, 26, 26, 26, 26, 26, 16, 26, -1, 26, -1, -1, 4, 11, 0, 29, 9, 11, -1, 9, 17, -1, 17, 17, 17, 16, 16, -1, 9, 0, 9, 9, 9, 9, 0, 11, 27, 0, 11, 17, 25, 25, 25, 0, 18, 0, 27, 27, -1, 0, 27, 27, 0, 27, 22, 27, -1, 22, 0, -1, 20, -1, 0, 22, 13, 22, -1, -1, 0, -1, 1, 1, 11, -1, 2, 0, 13, 20, 29, -1, 20, 20, 20, 20, -1, 9, 20, -1, 29, -1, 0, -1, -1, 7, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, -1, 2, -1, 2, 2, 2, -1, 2, 2, 2, 10, -1, 11, 16, 31, 23, -1, 18, 13, 0, -1, 15, -1, 32, 31, -1, 25, 26, 12, 26, 0, 26, 12, 11, 0, 0, 30, 30, -1, -1, 11, 12, -1, 29, 32, -1, -1, -1, 25, -1, 25, 32, 0, 32, 11, -1, 32, -1, 32, 23, 0, 4, 11, 30, 9, 7, 3, 21, 21, 21, -1, 0, 7, -1, -1, 7, 9, 0, -1, 17, 5, 5, 0, 5, 9, 17, 17, 9, 0, 10, 17, 17, 17, 0, 17, 2, 17, 21, 11, 21, 21, -1, 0, 27, 27, 0, 0, 27, 0, 27, 5, 5, 5, 5, 5, 5, -1, 5, 5, -1, 4, 24, 0, 18, 18, 1, -1, -1, -1, 4, -1, -1, -1, 1, 1, 7, -1, 1, -1, -1, 1, 24, 10, 0, 5, 19, 15, 6, 6, -1, 0, -1, -1, -1, -1, 16, -1, 3, 6, 29, -1, -1, 6, 29, 1, 0, 18, 6, 23, 6, 16, 31, 31, -1, 14, 4, 2, -1, 30, 32, 11, 11, -1, 32, 11, -1, 4, -1, -1, -1, -1, 11, 16, 1, 23, -1, 1, 7, 4, -1, 1, -1, 7, -1, -1, 7, 22, 7, 4, 21, 21, -1, 21, 11, 21, 14, 30, 3, 12, -1, 5, -1, 13, -1, -1, 23, 5, 5, 5, 5, 3, 5, 8, 5, 5, 15, -1, 5, 5, -1, -1, 15, 19, 30, -1, 30, 30, 2, 13, 30, -1, -1, 3, 12, 3, -1, 0, 9, 3, -1, 21, 14, 9, 18, 18, 21, 0, 0, -1, 3, 21, 0, -1, -1, 21, 12, -1, 21, 5, 18, 0, 0, 2, 1, -1, 18, 12, 0, 6, -1, 1, 6, 6, 7, -1, 2, 11, 11, 12, 25, 1, 11, 6, -1, -1, 1, 12, 12, 2, 1, 1, -1, 12, 1, 12, -1, 1, 14, 2, -1, 22, -1, 4, 8, 22, 17, 2, -1, 8, 1, -1, 13, 0, 0, 0, 8, 7, 8, 7, 27, 2, 3, -1, 2, 18, 0, -1, 17, 3, 29, 29, 0, 17, 11, 11, 17, 17, -1, 0, -1, 4, 9, 17, 3, 3, 9, 2, 3, 0, 7, 15, 9, -1, -1, 0, 13, 3, 13, 0, -1, 3, -1, -1, 0, 3, -1, 20, 3, 4, 4, 4, 2, 1, 2, 2, 21, 6, 4, 2, 2, 1, 13, 6, 8, 21, 0, 1, 6, 2, 15, 18, 13, 1, 6, 18, 1, 0, 12, 12, -1, 15, 1, 14, 14, 1, 31, 0, 0, 1, 25, 1, 1, 0, -1, 12, -1, -1, 1, 1, 8, 2, 4, 0, -1, 2, 2, 1, -1, -1, -1, 14, 8, 20, 1, 3, -1, 14, 3, 14, -1, 25, 19, 17, 0, 0, 3, 26, 17, 24, -1, 3, 3, 3, 9, 2, 0, 3, 4, 0, 32, 24, 4, 0, -1, 8, 8, 4, 4, 4, 10, 19, 4, 0, 3, 2, 6, -1, 2, 2, 9, 6, -1, 2, 26, 26, 2, 1, 24, -1, -1, -1, -1, 4, 0, 6, 17, 32, -1, 4, 12, -1, 0, 0, -1, -1, 32, 4, 30, -1, 12, 10, 3, 3, -1, 15, 12, 1, 2, 9, 0, 15, 21, 0, 25, 9, 18, 1, 8, 0, -1, -1, 1, 1, -1, 0, 8, 18, -1, 26, 21, 14, -1, -1, 16, 1, 14, 3, 3, 3, 14, 1, 24, 2, -1, 2, -1, 32, 3, 0, 14, 3, -1, 3, 2, 3, 3, 3, 0, 3, 14, 18, 9, 1, 0, 2, 2, 3, 10, 5, 17, 17, 0, 0, 2, 2, 3, 3, 21, 8, 26, 21, 21, 0, -1, -1]\n",
            "-------RUN23-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': \"CountVectorizer(ngram_range=(1, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b891dd10>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[18, 0, 28, 28, -1, 0, -1, 5, 5, 5, -1, 23, 16, 23, 5, -1, 1, 18, -1, 0, -1, 16, 5, 10, 10, 22, -1, 0, 0, 0, 0, 0, 28, 25, 0, -1, 4, -1, -1, 0, 10, -1, 30, 10, 30, -1, 30, 30, 10, 0, 30, 10, 5, 5, -1, 12, 12, 0, 5, 5, 1, 0, 5, 5, -1, 5, 4, 1, -1, 4, 25, 16, 1, 1, 1, 4, 0, -1, 0, 0, 12, 12, -1, 12, 12, 22, 12, 1, 0, 1, 1, 5, 12, -1, -1, 5, 5, 1, -1, -1, 23, 15, 0, 15, 0, 25, -1, -1, -1, -1, -1, 16, -1, -1, 0, -1, -1, 22, 0, -1, -1, -1, 10, 15, 30, 8, 16, 0, 10, 16, 0, 19, 0, 16, 16, 18, 10, 0, 16, -1, 5, 5, 24, 31, 19, 5, 18, 0, 0, -1, 5, 5, 10, 5, -1, 5, 1, -1, -1, 2, 0, -1, 25, 2, -1, -1, -1, 15, -1, 0, 5, -1, 0, -1, 19, 5, 2, -1, 19, -1, 0, 0, -1, -1, 16, 5, -1, 0, 16, 12, -1, 19, 12, 0, 31, -1, 13, 13, 12, 0, 12, 0, 12, 12, -1, 12, 7, 12, 1, 23, 1, 32, -1, 12, 12, -1, -1, 1, 0, -1, 13, 1, 1, 1, -1, 13, -1, -1, -1, 31, 31, 31, 31, 13, 13, 31, 22, 31, 3, 13, 13, 4, 4, -1, 13, -1, 0, 15, 29, 0, -1, 13, 23, -1, -1, 6, 6, 6, 1, 1, -1, 15, -1, -1, 0, -1, 0, -1, -1, 4, 4, 4, 4, 1, 4, 0, 4, 4, 22, -1, 1, -1, 4, 6, 6, -1, 13, 2, -1, 2, 23, 0, 23, 23, -1, 13, 5, -1, 13, 4, 4, 4, 16, 4, 4, 4, 1, 4, 27, 4, 10, 4, 30, 0, -1, -1, -1, -1, 15, -1, 5, 31, 0, -1, -1, 18, -1, 18, 0, 18, 2, 0, 26, 18, 20, 24, 5, 19, 1, -1, 19, -1, 22, 22, -1, -1, 19, 19, 3, 19, -1, -1, 0, 0, 5, 26, 19, 26, 26, -1, 15, 16, 0, 16, 26, 4, 16, 1, 22, -1, 26, 11, 16, 19, 5, 16, 3, 5, 0, 3, 8, 8, 0, 5, 5, 5, 26, 5, 26, 22, 32, -1, -1, 8, 8, 8, 8, 8, 8, 8, -1, 8, 13, 0, 13, -1, 8, -1, 31, 31, 22, -1, -1, 15, 31, 22, 31, -1, 20, -1, -1, -1, -1, 5, 20, 20, 0, 20, 1, 1, 1, -1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 20, 18, 4, -1, 4, 1, 5, 25, 4, 4, 4, 20, 0, 4, 5, 1, 4, 29, 4, -1, 0, 13, 4, 4, 4, 4, 1, 1, 6, 0, 1, 1, 1, 4, 6, -1, 1, 1, 2, 1, -1, -1, -1, 2, 6, -1, 6, -1, 0, 0, 22, 2, -1, 1, 0, 9, 18, 9, 9, 18, 1, -1, 13, 13, 16, 23, 3, -1, 18, 9, 9, 9, 9, 9, 15, 26, 9, 4, 9, 33, 26, -1, 14, 33, 9, -1, 0, 9, -1, 33, -1, 1, 5, -1, 33, 6, -1, 6, 6, 1, 1, 4, 1, 22, 15, 6, 6, 0, 6, 6, 11, 11, 11, 11, 3, 6, 6, 0, 23, 20, 3, 3, 3, 3, 3, 3, -1, 11, 33, -1, 14, 14, 14, 33, 26, 1, 11, 11, 10, -1, 11, 33, 11, -1, 25, 4, 3, 3, 22, 26, -1, 5, 3, 10, 3, 3, 22, 3, 3, 15, 3, 3, 0, 8, 3, 22, 22, 25, -1, 7, 7, 7, 7, 7, 7, 11, 7, 7, 1, 1, -1, 0, -1, -1, 18, 15, 7, 7, -1, 7, 7, 7, 1, 7, 7, 1, 1, 1, 1, 1, 1, 9, 1, 1, 1, -1, 4, 1, 1, 9, 8, 0, 9, 31, 0, 9, 18, 9, 18, 9, 18, 9, -1, 9, 10, 5, 10, 0, -1, -1, 9, 9, 9, 0, 9, 18, 9, 14, 26, 9, 18, 15, 6, 6, 6, 6, 5, 6, 6, 0, -1, 6, 2, 16, 30, 6, 19, 28, 1, 16, -1, 11, 11, 15, 11, 0, -1, 0, 11, -1, 28, 3, -1, -1, -1, 11, 0, -1, -1, 3, 3, -1, 0, -1, 3, 3, 0, 3, 3, -1, 0, 3, -1, 14, 3, 23, 14, 23, 3, 8, 19, 3, 14, 5, -1, 14, 14, 14, 0, -1, 10, 0, 31, 2, 2, 2, 25, -1, 23, 1, -1, 14, 0, 8, 23, 8, 1, -1, 1, -1, -1, -1, 2, 1, -1, 5, 4, 4, 7, 7, 1, 7, 7, 1, 7, 7, 7, 7, 1, 1, 2, 2, 2, 2, 2, -1, -1, 2, 7, 2, 2, 2, 2, -1, 2, 2, 7, 15, -1, 1, 2, -1, 15, 2, 2, 2, 1, -1, 18, 9, 9, 18, -1, 9, 9, 9, -1, 27, 27, 27, 27, 27, 27, 15, 27, -1, 27, -1, 7, 4, 10, 0, 30, 8, 10, -1, 8, 17, -1, 17, 17, 17, 15, 15, -1, 8, 0, 8, 8, 8, 8, 0, 10, 28, 0, 10, 17, 25, 25, 25, 0, -1, 0, 28, 28, 1, 0, 28, 28, 0, 28, 19, 28, -1, 19, 0, -1, -1, 19, 0, 19, 12, 19, 9, 32, 0, -1, -1, -1, 10, -1, 2, 0, 12, 23, 30, -1, 23, 23, 23, 23, -1, 8, 23, -1, 30, 4, 0, 0, -1, 5, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, -1, 2, -1, 2, 2, 2, -1, 2, -1, 2, 9, -1, 10, 15, 33, 26, -1, 20, 12, 0, -1, 13, -1, 32, 33, 19, 25, 27, 11, 27, 0, 27, 11, 10, 0, -1, 29, 29, -1, 29, 10, 11, -1, 30, 32, -1, -1, -1, 25, -1, 25, 32, 0, 32, 10, -1, 32, -1, -1, 26, -1, 4, 10, 29, 8, 5, 3, 21, 21, 21, -1, 0, 5, 5, 16, 5, 8, 0, 5, 17, 24, -1, 0, 24, 8, 17, 17, 8, 0, 9, 17, 17, 17, 0, 17, 2, 17, 21, 10, 21, 21, -1, 0, 28, 28, 0, -1, 28, 0, 28, 24, 24, 18, 18, 24, 24, -1, 24, 24, -1, 4, 22, 0, 20, 20, 1, -1, -1, -1, 4, -1, 25, -1, 1, 1, 5, 5, 1, -1, -1, 1, 22, 9, -1, 18, 16, 13, 6, 6, 5, 0, 5, -1, -1, -1, 15, -1, 3, 6, 30, -1, -1, 6, 30, 1, 0, 20, 6, 26, 6, 15, 33, 33, -1, 14, 4, 2, -1, 29, 32, 10, 10, -1, 32, 10, -1, 4, -1, -1, -1, 29, 10, -1, 1, 26, -1, 1, 5, 4, -1, 1, -1, 5, -1, -1, 5, 19, 5, 4, 21, 21, -1, 21, 10, 21, 14, 29, 3, 11, -1, 24, -1, 12, -1, 5, 26, 24, 24, 24, 24, 3, 24, 7, 24, 24, 13, -1, 24, 24, 5, 5, 13, 16, 29, 29, 29, 29, 2, 12, 29, -1, -1, 3, 11, 3, -1, 0, 8, 3, -1, 21, 14, 8, 20, 20, 21, 0, 0, -1, 3, 21, 0, 5, -1, 21, 11, -1, 21, 18, 20, -1, 0, 2, 1, -1, 20, 11, 0, 6, -1, 1, 6, 6, 5, -1, 2, 10, 10, 11, 25, 1, 10, 6, 15, -1, 1, 11, 11, 2, 1, 1, -1, 11, 1, 11, -1, 1, 14, 2, -1, 19, -1, 4, 7, 19, 17, 2, -1, 7, 1, -1, 12, 0, 0, 0, 7, 5, 7, 5, 28, 2, 3, -1, 2, 20, 0, -1, 17, 3, 30, 30, 0, 17, 10, 10, 17, 17, -1, 0, -1, 4, 8, 17, 3, 3, 8, 2, 3, -1, 5, 13, 8, -1, -1, 0, 12, 3, 12, 0, -1, 3, -1, -1, 0, 3, -1, 23, 3, 4, 4, 4, 2, 1, 2, 2, 21, 6, 4, 2, 2, 1, 12, 6, 7, 21, 0, 1, 6, 2, 13, 20, 12, 1, 6, 20, 1, 0, 11, 11, -1, 13, 1, 14, 14, -1, 33, -1, 0, 1, 25, 1, 1, 0, -1, 11, 1, -1, 1, 1, 7, 2, 4, 0, -1, 2, 2, 1, 17, -1, -1, 14, 7, 23, 1, 3, -1, 14, 3, 14, -1, 25, 16, 17, 0, 0, 3, 27, 17, 22, -1, 3, 3, 3, 8, 2, 0, 3, 4, 0, 32, 22, 4, 0, -1, 7, 7, 4, 4, 4, 9, 16, 4, 0, 3, 2, 6, -1, 2, 2, 8, 6, -1, 2, 27, 27, 2, 1, -1, -1, -1, -1, -1, 4, 0, 6, 17, 32, -1, 4, 11, 25, 0, 0, -1, -1, 32, 4, 29, 29, 11, 9, 3, 3, -1, 13, 11, 1, 2, 8, 0, 13, 21, 0, -1, 8, 20, 1, 7, 0, -1, -1, 1, -1, -1, 0, 7, 20, -1, 27, 21, 14, -1, -1, 15, 1, 14, 3, 3, 3, 14, 1, 22, 2, -1, 2, -1, 32, 3, 0, 14, 3, -1, 3, 2, 3, 3, 3, 0, 3, 14, 20, 8, 1, 0, 2, 2, 3, 9, -1, 17, 17, -1, 0, 2, 2, 3, 3, 21, 7, 27, 21, 21, 0, -1, -1]\n",
            "-------RUN24-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': \"CountVectorizer(ngram_range=(2, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df4a9008410>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[6, 0, 26, 26, -1, 0, -1, 5, 5, 5, -1, 25, 19, -1, 5, -1, 1, 6, -1, 0, -1, 19, 5, 11, 11, 21, -1, 0, 0, 0, 0, 0, 26, 13, 0, -1, 4, -1, -1, -1, 11, -1, 29, 11, 29, -1, 29, 29, 11, 0, 29, 11, 5, -1, 5, 14, 14, 0, 5, 5, 1, 0, -1, 5, -1, 5, 4, 4, -1, 4, 13, 19, 1, 1, 1, 4, 0, -1, 0, 0, 14, 14, -1, 14, 14, 21, 14, 1, 0, 1, 1, 5, 14, -1, 5, 5, 5, 1, -1, -1, 25, 16, 0, 16, 0, 13, -1, -1, -1, -1, -1, 19, -1, 6, 0, -1, -1, 21, 0, -1, 29, -1, 11, 16, 29, 30, 19, 0, 11, 19, 0, 15, 0, 19, 19, 6, 11, 0, 19, -1, 5, 5, 6, 30, 15, 5, 6, 0, 0, -1, 5, 5, 11, 5, -1, 5, 1, -1, -1, 2, 0, -1, 13, 2, -1, 6, 6, 16, -1, 0, 5, -1, 0, -1, 15, 5, 2, 13, 15, -1, 0, 0, 15, -1, 19, 5, -1, 0, 19, 24, 24, 15, 14, 0, 30, 14, 18, 18, 14, 0, 14, 0, 14, 14, -1, 14, 8, 14, 1, 25, 1, -1, -1, 14, 14, -1, -1, 1, 0, -1, 18, 1, 1, 1, -1, 18, -1, -1, -1, 30, 30, 30, 30, 18, 18, 30, 21, 30, 3, 18, -1, 4, 4, -1, 18, -1, 0, 16, 28, 0, -1, 18, 25, 6, -1, 7, 7, 7, 1, 1, -1, 16, 28, -1, 0, -1, 0, -1, -1, 4, 4, 4, 4, 1, 4, 0, 4, 4, 21, -1, 1, -1, 4, 7, -1, -1, 18, 2, -1, 2, 25, 0, 25, 25, -1, 18, 5, 0, 18, 4, 4, 4, 19, 4, 4, 4, 1, 4, 27, 4, 11, 4, 29, 0, 13, -1, -1, -1, 16, 6, 5, 7, 0, -1, -1, 6, -1, 6, 0, 6, 2, -1, 24, 6, 22, 6, -1, 15, 1, -1, 15, -1, 21, 21, -1, -1, 15, 15, 3, 15, -1, 24, 0, 0, 5, 24, 15, 24, -1, 6, 16, 19, 0, 19, 24, 4, 19, 1, 21, -1, 24, 12, 19, 15, 5, 19, 3, 5, 0, 3, 10, 10, 0, 5, 5, 5, 24, 5, 24, 21, 31, -1, 24, 10, 10, 10, 10, 10, 10, 10, -1, 10, 18, 0, 18, -1, 10, 1, 30, 30, 21, 24, -1, 16, 30, 21, 30, -1, 22, -1, -1, -1, -1, 5, 22, 22, 0, 22, 1, 1, 1, 0, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 22, 6, 4, 13, 4, 1, 5, 13, 4, 4, 4, 22, 0, 4, 5, 1, 4, 28, 4, -1, 0, 18, 4, 4, 4, 4, 1, 1, 7, 0, 1, 1, 1, 4, 7, -1, 1, 1, 2, 1, -1, -1, -1, 2, 7, 0, 7, -1, 0, 0, 21, 2, -1, 1, 0, 9, 6, 9, 9, 6, 1, 13, 18, 18, 19, -1, 3, 6, 6, 9, 9, 9, 9, 9, 16, 24, 9, 4, 9, 32, 24, -1, 17, 32, 9, 6, 0, 9, -1, 32, -1, 1, -1, 6, 32, 7, 21, 7, 7, 1, 1, 4, 1, 21, 16, 7, 7, 0, 7, 7, 12, 12, 12, 12, 3, 7, 7, 0, 25, 22, 3, 3, 3, 3, 3, 3, 2, 12, 32, -1, 17, 17, 17, 32, 24, 1, 12, 12, 11, -1, 12, 32, 12, -1, 13, 4, 3, 3, 21, -1, -1, 5, 3, 11, 3, 3, 21, 3, 3, 16, 3, 3, 0, 10, 3, 21, 21, 13, -1, 8, 8, 8, 8, 8, 8, 12, 8, 8, 1, 1, -1, 0, -1, 15, 6, 16, 8, 8, -1, 8, 8, 8, 1, 8, 8, 1, 1, 1, 1, 1, 1, 9, 1, 1, 1, -1, 4, 1, 1, 9, 10, 0, 9, 30, 0, 9, 6, 9, 6, 9, 6, 9, 13, 9, 11, 5, 11, 0, -1, 13, 9, 9, 9, 0, 9, 6, 9, 17, 24, 9, 6, 16, 7, 7, 7, 7, 5, 7, 7, 0, 1, 7, 2, 19, -1, -1, 15, 26, 1, 19, -1, 12, 12, 16, 12, 0, -1, -1, 12, -1, 26, 3, 0, 15, -1, 12, 0, 2, -1, 3, 3, -1, 0, -1, 3, 3, 0, 3, 3, 0, 0, 3, -1, 17, 3, 25, 17, 25, 3, 10, 15, 3, 17, 5, 1, 17, 17, 17, 0, -1, 11, 0, 30, 2, 2, 2, 13, -1, 25, 1, -1, 17, 0, 10, 25, 10, 1, 1, 1, 0, -1, -1, 2, 1, -1, 5, 4, 4, 8, 8, 1, 8, 8, 1, 8, 8, 8, 8, 1, -1, 2, 2, 2, 2, 2, -1, -1, 2, 8, 2, 2, 2, 2, -1, 2, 2, 8, 16, 13, 1, 2, 2, 16, 2, 2, 2, 1, -1, 6, 9, 9, 6, 9, 9, 9, 9, -1, 27, 27, 27, 27, 27, 27, 16, 27, -1, 27, -1, 8, 4, 11, 0, 29, 10, 11, -1, 10, 20, -1, 20, 20, 20, 16, 16, -1, 10, 0, 10, 10, 10, 10, 0, 11, 26, 0, 11, 20, 13, 13, 13, 0, 22, 0, 26, 26, 28, 0, 26, 26, 0, 26, 15, 26, -1, 15, 0, -1, 26, 15, 0, 15, 14, 15, 9, 21, 0, -1, 1, -1, 11, -1, 2, 0, 14, 25, 29, -1, 25, 25, 25, 25, 1, 10, 25, -1, 29, -1, 0, 0, -1, 5, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 0, 2, 2, 2, 2, 2, -1, 2, 2, 2, 9, -1, 11, 16, 32, 24, -1, 22, 14, 0, -1, 18, 13, 31, 32, 15, 13, 27, 12, 27, 0, 27, 12, 11, 0, 0, 28, 28, -1, 28, 11, 12, -1, 29, 31, -1, 31, 0, 13, 0, 13, 31, 0, 31, 11, -1, 31, -1, 31, 24, -1, 4, 11, 28, 10, 5, 3, 23, 23, 23, -1, 0, 5, -1, 19, 5, 10, 0, -1, 20, 6, 6, 0, 6, 10, 20, 20, 10, 0, 9, 20, 20, 20, 0, 20, 2, 20, 23, 11, 23, 23, 0, 0, 26, 26, 0, 0, 26, 0, 26, 6, 6, 6, 6, 6, 6, -1, 6, 6, -1, 4, 21, 0, 22, 22, 1, 0, 13, -1, 4, -1, 13, -1, 1, 1, 5, -1, 1, -1, -1, 1, 21, 9, 0, 6, 19, 18, 7, 7, 5, 0, -1, -1, -1, -1, 16, -1, 3, 7, 29, -1, 5, 7, 29, 1, 0, 22, 7, 24, 7, 16, 32, 32, -1, 17, 4, 2, -1, 28, 31, 11, 11, -1, 31, 11, 21, 4, -1, -1, -1, 28, 11, 16, 1, 24, -1, 1, 5, 4, -1, 1, -1, 5, 15, -1, 5, 15, 5, 4, 23, 23, -1, 23, 11, 23, 17, 28, 3, 12, 13, 6, 6, 14, -1, 5, 24, 6, 6, 6, 6, 3, 6, 8, 6, 6, 18, -1, 6, 6, -1, -1, 18, 19, 28, -1, 28, 28, 2, 14, 28, -1, -1, 3, 12, 3, -1, 0, 10, 3, -1, 23, 17, 10, 22, 22, 23, 0, 0, -1, 3, 23, 0, 5, -1, 23, 12, -1, 23, 6, 22, 0, 0, 2, 1, -1, 22, 12, 0, 7, -1, 1, 7, 7, 5, -1, 2, 11, 11, 12, 13, 1, 11, 7, 16, -1, 1, 12, 12, 2, 1, 1, -1, 12, 1, 12, -1, 1, 17, 2, -1, 15, -1, 4, 8, 15, 20, 2, -1, 8, 1, -1, 14, 0, 0, 0, 8, 5, 8, 5, 26, 2, 3, -1, 2, 22, 0, 20, 20, 3, 29, 29, 0, 20, 11, 11, 20, 20, -1, 0, -1, 4, 10, 20, 3, 3, 10, 2, 3, 0, 5, 18, 10, -1, -1, 0, 14, 3, 14, 0, -1, 3, -1, -1, 0, 3, -1, 25, 3, 4, 4, 4, 2, 1, 2, 2, 23, 7, 4, 2, 2, 1, 14, 7, 8, 23, 0, 1, 7, 2, 18, 22, 14, 1, 7, 22, 1, 0, 12, 12, -1, 18, 1, 17, 17, 1, 32, 0, 0, 1, 13, 1, 1, 0, -1, 12, -1, -1, 1, 1, 8, 2, 4, 0, 13, 2, 2, 1, -1, 15, -1, 17, 8, 25, 1, 3, -1, 17, 3, 17, -1, 13, 19, 20, 0, 0, 3, 27, 20, 21, -1, 3, 3, 3, 10, 2, 0, 3, 4, 0, 31, 21, 4, -1, -1, 8, 8, 4, 4, 4, 9, 19, 4, 0, 3, 2, 7, -1, 2, 2, 10, 7, -1, 2, 27, 27, 2, 1, -1, -1, -1, -1, -1, 4, 0, 7, 20, 31, -1, 4, 12, 13, 0, 0, -1, -1, 31, 4, 28, -1, 12, 9, 3, 3, -1, 18, 12, 1, 2, 10, 0, 18, 23, 0, -1, 10, 22, 1, 8, 0, -1, -1, 1, 1, -1, 0, 8, 22, 1, 27, 23, 17, -1, -1, 16, 1, 17, 3, 3, 3, 17, 1, 21, 2, -1, 2, 2, 31, 3, 0, 17, 3, -1, 3, 2, 3, 3, 3, 0, 3, 17, 22, 10, 1, 0, 2, 2, 3, 9, 6, 20, 20, -1, 0, 2, 2, 3, 3, 23, 8, 27, 23, 23, 0, -1, -1]\n",
            "-------RUN25-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': \"CountVectorizer(stop_words='english')\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[6, 0, 28, 28, -1, 0, 7, 5, 5, 5, -1, 22, 18, 22, 5, -1, 1, 6, -1, 0, -1, 18, 5, 11, 11, 24, -1, 0, 0, 0, 0, -1, 28, 26, 0, 5, -1, -1, -1, 0, 11, -1, 25, 11, 25, -1, 25, 25, 11, 0, 25, 11, 5, 5, -1, 15, 15, 0, 5, -1, 4, -1, 5, 5, -1, 5, 4, 4, 18, 4, 26, 18, 1, 1, 1, 4, 30, -1, 0, -1, 15, -1, -1, 15, 15, 24, 15, 1, 0, 1, 1, 5, 15, -1, -1, 5, 5, 1, -1, -1, 22, 14, 0, 14, 30, 26, -1, -1, -1, -1, -1, 18, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, 11, 14, 25, -1, 18, 0, 11, 18, 0, 20, -1, 18, 18, 6, 11, 0, 18, -1, 5, 5, 6, 29, 20, 5, 6, 0, 0, -1, 5, 5, 11, 5, -1, 5, 1, -1, -1, 2, 0, -1, 26, 2, -1, -1, 6, 14, -1, 0, 5, -1, -1, -1, 20, 5, 2, -1, 20, -1, -1, 0, -1, -1, 18, 5, -1, 0, 18, 12, 12, 20, 15, 0, 29, 15, 17, 17, 15, -1, 15, 0, 15, 15, -1, 15, 8, 15, 1, -1, 1, -1, -1, 15, 15, -1, -1, 1, 0, -1, 17, 1, 1, 1, 5, 17, -1, -1, -1, 29, 29, 29, 29, 17, 17, 29, 24, 29, 3, 17, -1, 4, 4, -1, 17, -1, 0, 14, 31, 0, 14, 17, 22, 6, -1, 7, 7, 7, 1, 1, -1, 14, 31, -1, 0, -1, 0, -1, -1, 4, 4, 4, 4, 1, 4, 30, 4, 4, -1, -1, 1, -1, 4, 7, 7, -1, 17, 2, 22, 2, 22, -1, 22, 22, -1, 17, 5, 30, 17, 4, 4, 4, 18, 4, 4, 4, 4, 4, 27, -1, 11, 4, 25, 0, -1, -1, -1, -1, 14, 6, 5, 7, 30, -1, -1, 6, -1, 6, -1, 6, 2, -1, 12, 6, 21, 6, 5, 20, 1, -1, 20, -1, 24, -1, -1, -1, 20, 20, 3, 20, -1, 12, 0, 0, 5, 12, 20, 12, 12, 6, 14, 18, 0, 18, 12, 4, 18, -1, 24, -1, 12, 13, 18, 20, 5, 18, 3, 5, 0, 3, 9, 9, -1, 5, 5, 5, 12, 5, 12, 24, 32, -1, 12, 9, 9, 9, 9, 9, 9, 9, -1, 9, 17, 0, 17, -1, 9, -1, 29, 29, 24, 12, -1, 14, 29, 24, 29, -1, 21, -1, -1, -1, -1, 5, 21, 21, 0, 21, 1, 1, 1, -1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 21, 6, 4, -1, 4, 1, 5, 26, 4, 4, 4, 21, 0, 4, 5, -1, 4, 31, 4, -1, 0, 17, 4, 4, 4, 4, 1, 1, 7, 0, 1, 1, 1, 4, 7, -1, 1, 1, 2, 1, -1, -1, 7, 2, 7, 30, 7, -1, 0, -1, 24, 2, -1, -1, -1, 10, 6, 10, 10, 6, 1, -1, 17, 17, 18, -1, 3, 6, 6, 10, 10, 10, 10, 10, 14, 12, 10, 4, 10, 12, 12, -1, 19, 12, 10, 6, 0, 10, -1, 12, -1, 1, 5, -1, 12, 7, 24, 7, 7, 1, 1, 4, 1, 24, 14, 7, 7, 0, 7, 7, 13, 13, 13, 13, 3, 7, 7, 0, 22, 21, 3, 3, 3, 3, 3, 3, 2, 13, 12, 12, 19, 19, 19, 12, 12, 1, 13, 13, 11, -1, 13, 12, 13, 12, 26, -1, 3, 3, 24, 12, -1, 5, 3, 11, 3, 3, 24, 3, 3, 14, 3, 3, 0, 9, 3, 24, 24, 26, -1, 8, 8, 8, 8, 8, 8, 13, 8, 8, 1, 1, -1, 0, -1, 20, 6, 14, 8, 8, -1, 8, 8, 8, 1, 8, 8, 1, 1, 1, 1, -1, 1, 10, 1, 1, 4, -1, 4, 1, -1, 10, 9, 0, 10, 29, 0, 10, 6, 10, 6, 10, 6, 10, -1, 10, 11, 5, 11, 0, -1, -1, 10, 10, 10, 0, 10, 6, 10, 19, 12, 10, 6, 14, 7, 7, 7, 7, 5, 7, 7, -1, 4, 7, 2, 18, -1, 7, 20, 28, 1, 18, -1, 13, 13, 14, 13, 0, -1, -1, 13, -1, 28, 3, -1, -1, -1, 13, 0, -1, -1, 3, 3, -1, 30, -1, 3, 3, 0, 3, 3, 30, -1, 3, -1, 19, 3, 22, 19, 22, 3, 9, 20, 3, 19, -1, -1, 19, 19, 19, 0, -1, 11, 0, 29, 2, 2, 2, 26, -1, 22, 1, -1, 19, 0, 9, 22, 9, 1, -1, 1, 30, -1, -1, 2, -1, -1, 5, 4, 4, 8, 8, 1, 8, 8, 1, 8, 8, 8, 8, 1, 1, 2, 2, 2, 2, 2, -1, -1, 2, 8, 2, 2, 2, 2, 14, 2, 2, 8, 14, -1, 1, 2, 2, 14, 2, 2, 2, 1, -1, 6, 10, 10, 6, 10, 10, 10, 10, 25, 27, 27, 27, 27, 27, 27, 14, 27, -1, 27, -1, -1, 4, 11, 0, 25, 9, 11, 25, 9, 16, -1, 16, 16, 16, 14, 14, -1, 9, 30, 9, 9, 9, 9, 0, 11, 28, 0, 11, 16, 26, 26, 26, 0, -1, 0, 28, 28, -1, 0, 28, 28, 0, 28, 20, 28, -1, 20, 0, -1, -1, 20, 0, 20, 15, 20, -1, 24, 0, -1, -1, -1, 11, -1, 2, 0, 15, 22, 25, -1, 22, 22, 22, 22, 1, 9, 22, -1, 25, -1, 0, -1, -1, 5, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, -1, 2, 2, 2, 2, 2, -1, 2, 2, 2, 10, -1, 11, 14, 12, 12, -1, 21, 15, 0, 25, 17, -1, 32, 12, -1, 26, 27, 13, 27, 0, 27, 13, 11, 0, -1, 31, 31, -1, -1, 11, 13, -1, 25, 32, -1, -1, -1, 26, -1, 26, 32, 0, 32, 11, -1, 32, -1, -1, 12, -1, 4, 11, 31, 9, 5, 3, 23, 23, 23, -1, 0, 5, 5, 18, 5, 9, -1, 5, 16, 6, 6, 30, 6, 9, 16, 16, 9, 0, 10, 16, 16, 16, 0, 16, 2, 16, 23, 11, 23, 23, -1, 0, 28, 28, 0, -1, 28, 0, 28, 6, 6, 6, 6, 6, 6, -1, 6, 6, -1, 4, 24, 0, 21, 21, 1, -1, -1, -1, 4, -1, 26, -1, 1, 1, 5, -1, 4, -1, -1, 1, 24, 10, -1, 6, 18, 17, 7, 7, 5, 0, 5, -1, -1, -1, 14, 14, 3, 7, 25, -1, -1, 7, 25, 1, 0, 21, 7, 12, 7, 14, 12, 12, -1, 19, 4, 2, -1, 31, 32, 11, 11, -1, 32, 11, -1, 4, -1, -1, -1, -1, 11, 14, 1, 12, -1, 1, 5, 4, -1, 1, -1, 5, 20, -1, 5, 20, 5, 4, 23, 2, -1, 23, 11, 23, 19, 31, 3, 13, -1, 6, 6, 15, 8, -1, 12, 6, 6, 6, 6, 3, 6, 8, 6, 6, 17, -1, 6, 6, 5, -1, 17, 18, 31, -1, 31, 31, 2, 15, 31, 22, -1, 3, 13, 3, -1, 0, 9, 3, -1, 23, 19, 9, 21, 21, 23, 0, -1, 14, 3, 23, 0, 5, -1, 23, 13, 14, 23, 6, 21, -1, 0, 2, 1, -1, 21, 13, 0, 7, -1, 1, 7, 7, 5, -1, 2, 11, 11, 13, 26, 1, 11, 7, 14, -1, 1, 13, 13, 2, 1, 1, -1, 13, 1, 13, -1, 1, 19, 2, -1, -1, -1, 4, 8, 20, 16, 2, -1, 8, 1, -1, 15, 0, 0, 0, 8, 5, 8, 5, 28, 2, 3, -1, 2, 21, 0, 16, 16, 3, 25, 25, 0, 16, 11, 11, 16, 16, -1, 0, -1, 4, 9, 16, 3, 3, 9, 2, 3, -1, 5, 17, 9, -1, -1, 0, 15, 3, 15, 0, -1, 3, -1, -1, 0, 3, -1, 22, 3, 4, 4, 4, 2, 4, 2, 2, 23, 7, 4, 2, 2, 1, 15, 7, 8, 23, 0, 1, 7, 2, 17, 21, 15, 1, 7, 21, 1, 0, 13, 13, -1, 17, 1, 19, 19, 4, 12, -1, 30, 1, 26, 1, 1, 0, 16, 13, -1, -1, 1, 1, 8, 2, 4, 0, -1, 2, 2, 1, 16, -1, -1, 19, 8, 22, 1, 3, -1, 19, 3, 19, -1, 26, 18, 16, 0, 0, 3, 27, 16, 24, -1, 3, 3, 3, 9, 2, 30, 3, 4, 0, 32, -1, 4, 0, -1, 8, 8, 4, 4, 4, 10, 18, 4, 0, 3, 2, 7, -1, 2, 2, 9, 7, -1, 2, 27, 27, 2, 1, -1, -1, -1, -1, -1, 4, 0, 7, 16, 32, -1, 4, 13, 20, 0, 0, -1, -1, 32, 4, 31, -1, 13, 10, 3, 3, -1, 17, 13, 1, 2, 9, 0, 17, 23, 0, -1, 9, 21, 1, 8, 0, -1, -1, 1, 1, -1, 0, 8, 21, -1, 27, 23, 19, -1, -1, 14, 4, 19, 3, 3, 3, 19, 1, 24, 2, -1, 2, 2, 32, 3, 0, 19, 3, -1, 3, 2, 3, 3, 3, 0, 3, 19, 21, 9, 1, 0, 2, 2, 3, 10, 6, 16, 16, -1, 0, 2, 2, 3, 3, 23, 8, 27, 23, 23, -1, -1, -1]\n",
            "-------RUN26-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': 'CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7df3b00f5c90>)', 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[21, 1, 28, 28, -1, 1, 6, 5, 5, 5, -1, 20, 19, 20, 5, -1, 0, 21, -1, 1, -1, 19, 5, 10, 10, 25, -1, 1, 1, 1, 1, -1, 28, 18, 1, -1, -1, -1, -1, 1, 10, -1, 26, 10, 26, 26, 26, 26, 10, 1, 26, 10, 5, -1, -1, 12, 12, 1, 5, -1, 0, -1, -1, 5, -1, 5, 4, 0, 19, 4, 18, 19, 0, 0, 0, 4, -1, -1, 1, -1, 12, 12, 30, 12, 12, 25, 12, 0, 1, 0, 0, 5, 12, -1, -1, 5, 5, 0, -1, 18, 20, 17, 1, 17, 34, 18, -1, -1, -1, -1, -1, 19, -1, -1, 1, -1, -1, -1, 1, -1, 26, -1, 10, 17, 26, 29, 19, 1, 10, 19, 1, 14, 1, 19, 19, 21, 10, 1, 19, -1, 5, 5, 24, 29, 14, 5, 21, 1, 1, -1, 5, 5, 10, 5, -1, 5, 0, -1, -1, 2, 1, -1, 18, 2, -1, -1, 21, 17, -1, 1, 5, -1, -1, -1, 14, 5, 2, -1, 14, 4, 1, 1, 14, -1, 19, 5, 0, 1, 19, -1, -1, 14, 12, 1, 29, 12, 15, 15, 12, 1, 12, 1, 12, 12, -1, 12, 8, 12, 0, 20, 0, 30, -1, 12, 12, -1, -1, 0, 1, -1, 15, 0, 0, 0, -1, 15, -1, -1, -1, 29, 29, 29, 29, 15, 15, 29, 25, 29, 3, 15, -1, 4, 4, -1, 15, -1, -1, 17, 31, 1, 17, 15, 20, -1, -1, 6, 6, 6, 0, 0, -1, 17, -1, -1, 1, -1, 1, -1, -1, 4, 4, 4, 4, 0, 4, 34, 4, 4, 25, -1, 0, -1, 4, 6, 6, -1, 15, 2, -1, 2, 20, 1, 20, 20, -1, 15, 5, 34, 15, 4, 4, 4, 19, 4, 0, 4, 0, 4, 27, 4, 10, 4, 26, -1, 18, -1, -1, -1, 17, -1, 5, 29, 34, -1, -1, 21, -1, 21, 1, 21, 2, -1, 32, 21, 22, 24, 5, 14, 0, -1, 14, -1, 25, 25, -1, -1, 14, 14, 3, 14, -1, 33, 1, 1, 5, 32, 14, 32, 32, 21, 17, 19, 1, -1, 32, 4, 19, 0, 25, 19, 32, -1, 19, 14, 5, 19, 3, 5, 1, 3, 9, 9, -1, 5, 5, 5, -1, 5, -1, 25, 30, -1, -1, 9, 9, 9, 9, 9, 9, 9, -1, 9, 15, 1, 15, -1, 9, 0, 29, 29, 25, -1, -1, 17, 29, 25, 29, -1, 22, -1, -1, -1, -1, -1, 22, 22, 1, 22, 0, 0, 0, -1, 0, 4, 0, 0, 0, -1, 0, 0, 0, 0, 0, 22, 21, 4, 18, 4, 0, 5, 18, 4, 4, 4, 22, 1, 4, 5, 0, 4, 31, 4, -1, 1, 15, 4, 4, 4, 4, 0, 0, 6, 1, 0, 0, 0, 4, 6, -1, 0, 0, 2, 0, -1, -1, -1, 2, 6, 34, 6, 20, -1, 1, 25, 2, 20, 0, 1, 7, -1, 7, 7, 21, 0, 18, 15, 15, 19, -1, -1, -1, 21, 7, 7, 7, 7, 7, 17, 32, 7, 4, 7, 33, 32, -1, 16, 33, 7, -1, 1, 7, -1, 33, -1, 0, -1, -1, 33, 6, -1, 6, 6, 0, 0, 4, 0, -1, 17, 6, 6, 1, 6, 6, 11, 11, 11, 11, 3, 6, 6, 1, 20, 22, 3, 3, 3, 3, 3, 3, -1, 11, 33, -1, 16, 16, 16, 33, 32, 0, 11, 11, 10, -1, 11, 33, 11, -1, 18, 4, 3, 3, 25, -1, -1, 5, 3, 10, 3, 3, 25, 3, 3, 17, 3, 3, 1, 9, 3, 25, 25, 18, -1, 8, 8, 8, 8, 8, 8, 11, 8, 8, 0, 0, -1, 1, -1, 14, 21, 17, 8, 8, -1, 8, 8, 8, 0, 8, 8, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, -1, 4, 0, -1, 7, 9, 1, 7, 29, -1, 7, 21, 7, -1, 7, 21, 7, 18, 7, 10, 5, 10, 1, -1, -1, 7, 7, 7, 1, 7, 21, 7, 16, 32, 7, 21, 17, 6, 6, 6, 6, -1, 6, 6, 34, 0, 6, 2, 19, 26, 6, 14, 28, 0, 19, -1, 11, 11, 17, 11, 1, -1, 1, -1, -1, 28, 3, -1, -1, -1, 11, 1, -1, -1, 3, 3, -1, 34, -1, 3, 3, 1, 3, 3, 34, 1, 3, -1, 16, 3, 20, 16, 20, 3, 9, 14, 3, 16, 5, 0, 16, 16, 16, 1, -1, 10, 1, 29, 2, 2, 2, 18, -1, 20, 0, -1, 16, 1, 9, 20, 9, 0, 0, 0, 34, -1, -1, 2, -1, -1, 5, 4, 4, 8, 8, 0, 8, 8, 0, 8, 8, 8, 8, 0, -1, 2, 2, 2, 2, 2, -1, -1, 2, 8, 2, 2, 2, 2, -1, 2, 2, 8, 17, 18, 0, 2, -1, 17, 2, 2, 2, 0, -1, 21, 7, 7, 21, 7, 7, 7, 7, 26, 27, 27, 27, 27, 27, 27, 17, 27, -1, 27, -1, -1, 4, 10, 1, 26, 9, 10, -1, 9, 13, -1, 13, 13, 13, 17, 17, -1, 9, 34, 9, 9, 9, 9, 1, 10, 28, 1, 10, 13, 18, 18, 18, 1, -1, 1, 28, 28, -1, 1, 28, 28, 1, 28, 14, 28, -1, 14, 1, -1, 20, 14, 1, 14, 12, 14, 7, 25, 1, -1, 0, 0, 10, -1, 2, 1, 12, 20, 26, -1, 20, 20, 20, 20, 0, 9, 20, -1, 26, -1, 1, 1, -1, 5, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, -1, 2, -1, 2, 2, 2, -1, 2, 2, 2, 7, 22, 10, 17, 33, 32, -1, 22, 12, 1, -1, 15, -1, 30, 33, 14, 18, 27, 11, 27, 1, 27, 11, 10, 1, -1, 31, 31, -1, 31, 10, 11, -1, 26, 30, -1, -1, -1, 18, -1, 18, 30, 1, 30, 10, -1, 30, -1, 30, -1, -1, 4, 10, 31, 9, 5, 3, 23, 23, 23, -1, 1, 5, -1, 19, 5, 9, 1, -1, 13, 24, -1, 34, 24, 9, 13, 13, 9, 1, -1, 13, 13, 13, 1, 13, 2, 13, 23, 10, 23, 23, -1, 1, 28, 28, 1, -1, 28, 1, 28, 24, 24, 21, 21, 24, 24, -1, 24, 24, -1, 4, 25, 1, 22, 22, 0, -1, -1, -1, 4, -1, 18, -1, 0, 0, 5, 5, 0, -1, 5, 0, 25, 7, -1, 21, 19, 15, 6, 6, 5, -1, -1, -1, -1, -1, 17, -1, 3, 6, 26, -1, -1, 6, 26, 0, 1, 22, 6, 32, 6, 17, 33, 33, -1, 16, 4, 2, -1, 31, 30, 10, 10, -1, 30, 10, -1, 4, -1, -1, -1, -1, 10, -1, 0, -1, -1, 0, -1, 4, -1, 0, -1, 5, 14, 20, 5, 14, 5, 4, 23, 23, 31, 23, 10, 23, 16, 31, 3, 11, 18, 24, -1, 12, -1, 5, 32, 24, 24, 24, 24, 3, 24, 8, 24, 24, 15, -1, 24, 24, -1, -1, 15, 19, 31, -1, 31, 31, 2, 12, 31, 7, -1, 3, 11, 3, -1, 1, 9, 3, 0, 23, 16, 9, 22, 22, 23, 1, 1, -1, 3, 23, 1, 5, -1, 23, 11, -1, 23, 21, 22, -1, 1, 2, 0, -1, 22, 11, 1, 6, -1, -1, 6, 6, 5, -1, 2, 10, 10, 11, 18, 0, 10, 6, -1, -1, 0, 11, 11, 2, 0, 0, -1, 11, 0, 11, -1, 0, 16, 2, -1, 14, -1, 4, 8, 14, 13, 2, -1, 8, -1, -1, 12, 1, 1, 1, 8, 5, 8, 5, 28, 2, 3, -1, 2, 22, 1, 13, 13, 3, 26, 26, -1, 13, 10, 10, 13, 13, -1, 1, -1, 4, 9, 13, 3, 3, 9, 2, 3, 17, 5, 15, 9, -1, -1, 1, 12, 3, 12, -1, -1, 3, -1, -1, 1, 3, -1, 20, 3, 4, 4, 4, 2, 0, 2, 2, 23, 6, 4, 2, 2, 0, 12, 6, 8, 23, 1, 0, 6, 2, 15, 22, 12, 0, 6, 22, 0, 1, 11, 11, -1, 15, 0, 16, 16, 0, 33, 1, 34, 0, 18, 0, 0, 1, 13, 11, -1, -1, 0, 0, 8, 2, 4, 1, -1, 2, 2, 0, 13, -1, -1, 16, 8, 20, 0, 3, -1, 16, 3, 16, -1, 18, 19, 13, 1, 1, 3, 27, 13, 25, -1, 3, 3, 3, 9, 2, 34, 3, 4, 1, 30, 25, 4, -1, -1, 8, 8, 4, 4, 4, 7, 19, 4, 1, 3, 2, 6, -1, 2, 2, 9, 6, -1, 2, 27, 27, 2, 0, -1, -1, -1, -1, -1, 4, 1, 6, 13, 30, -1, 4, 11, 18, 1, 1, 14, -1, 30, 4, 31, -1, 11, 7, 3, 3, -1, 15, 11, 0, -1, 9, 1, 15, 23, 1, -1, 9, 22, 0, 8, 1, -1, -1, 0, -1, -1, -1, 8, 22, 0, 27, 23, 16, 5, -1, 17, 0, 16, 3, 3, 3, 16, 0, 25, 2, -1, 2, -1, 30, 3, 1, 16, 3, -1, 3, 2, 3, 3, 3, 1, 3, 16, 22, 9, 0, 1, 2, 2, 3, 7, 24, 13, 13, -1, 1, 2, 2, 3, -1, 23, 8, 27, 23, 23, -1, -1, -1]\n",
            "-------RUN27-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': \"CountVectorizer(stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b02d2c50>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[5, 0, 28, 28, -1, 0, -1, 6, 6, 6, -1, 21, 20, 21, 6, -1, 1, 5, -1, 0, -1, 20, 6, 12, 12, 23, -1, 0, 0, 0, 0, 0, 28, 4, 0, -1, -1, -1, -1, -1, 12, 0, 27, 12, 27, -1, 27, 27, 12, 0, 27, 12, 6, 6, -1, 16, 16, 0, 6, -1, 1, -1, -1, 6, -1, 6, -1, -1, -1, -1, 4, 20, 1, 1, 1, -1, 0, -1, 0, -1, 16, 16, -1, 16, 16, 23, 16, 1, 0, 1, 1, 6, 16, -1, -1, 6, 6, 1, -1, -1, 21, 14, 0, 14, 32, 4, -1, 4, -1, -1, -1, 20, -1, 5, 0, -1, -1, 23, 0, -1, -1, -1, 12, 14, 27, 10, 20, 0, 12, 20, 0, 4, 0, 20, 20, 5, 12, 0, 20, -1, 6, 6, 5, 30, 4, 6, 5, 0, 0, -1, 6, 6, 12, 6, -1, 6, 1, -1, -1, 2, -1, -1, 4, 2, -1, 5, 5, 14, -1, 0, 6, -1, 0, -1, 4, 6, 2, 4, 4, -1, -1, 0, 4, -1, 20, -1, -1, 0, 20, 11, 11, 4, 16, 0, 30, -1, 18, 18, 16, -1, 16, 0, 16, 16, 4, 16, 8, 16, 1, 21, 1, -1, 4, 16, 16, -1, -1, 1, 0, -1, 18, 1, 1, 1, -1, 18, -1, -1, -1, 30, 30, 30, 30, 18, 18, 30, 23, 30, 3, 18, -1, 15, 15, -1, 18, -1, 0, 14, 29, 0, -1, 18, 21, 5, -1, 7, 7, 7, 1, 1, -1, 14, -1, -1, 0, -1, 0, -1, -1, 15, 15, 15, -1, 1, 15, 32, 15, -1, 23, -1, 1, -1, -1, 7, -1, -1, 18, 2, -1, 2, 21, 0, 21, 21, -1, 18, 6, -1, 18, 25, -1, -1, 20, 25, -1, 25, 1, -1, 26, -1, 12, 25, 27, 0, 4, -1, -1, -1, 14, 5, 6, 7, 32, -1, -1, 5, -1, 5, 0, 5, 2, -1, 11, 5, 22, 5, 6, 4, 1, -1, 4, -1, 23, 23, -1, -1, 4, 4, 3, 4, -1, 11, 0, 0, 6, 11, 4, 11, 11, 5, 14, 20, 0, -1, 11, -1, 20, 1, 23, -1, 11, -1, 20, 4, 6, 20, 3, 6, 0, 3, 10, 10, -1, 6, 6, -1, 11, 6, 11, 23, 31, -1, 11, 10, 10, 10, 10, 10, 10, 10, -1, 10, 18, 0, 18, 0, 10, -1, 30, 30, 23, 11, -1, 14, 30, 23, 30, 26, 22, -1, -1, -1, -1, 6, 22, 22, 0, 22, 1, 1, 1, -1, 1, 25, 1, 1, 1, 1, 1, 1, 1, 1, 1, 22, 5, 15, 4, 15, 1, 6, 4, 15, 15, 15, 22, 0, 15, -1, 1, 15, -1, 15, -1, 0, 18, 15, 15, -1, 15, 1, 1, 7, 0, 1, 1, 1, 15, 7, -1, 1, 1, 2, 1, -1, -1, -1, 2, 7, 32, 7, -1, 0, 0, 23, 2, -1, 1, 0, 9, 5, 9, 9, 5, 1, 4, 18, 18, 20, 21, -1, 5, 5, 9, 9, 9, 9, 9, 14, 11, 9, 25, 9, 11, 11, -1, 17, 11, 9, 5, 0, 9, -1, 11, -1, 1, -1, 5, 11, 7, 23, 7, 7, 1, -1, 25, 1, -1, 14, 7, 7, 0, 7, 7, 13, 13, 13, 13, 3, 7, 7, 0, 21, 22, 3, 3, 3, 3, 3, 3, 2, 13, 11, 11, 17, 17, 17, 11, 11, -1, 13, 13, 12, -1, 13, 11, 13, 11, 4, -1, 3, 3, 23, 11, -1, 6, 3, 12, 3, 3, 23, 3, 3, 14, 3, 3, 0, 10, 3, 23, 23, 4, -1, 8, 8, 8, 8, -1, 8, 13, 8, 8, 1, 1, -1, 0, -1, 4, 5, 14, 8, 8, -1, 8, 8, 8, 1, 8, 8, 1, 1, 1, 1, -1, 1, 9, 1, 1, 1, -1, 15, 1, 1, 9, 10, 0, 9, 30, 0, 9, 5, 9, 5, 9, 5, 9, 4, 9, 12, 6, 12, 0, -1, 4, 9, 9, 9, 0, 9, 5, 9, 17, 11, 9, 5, 14, 7, 7, 7, 7, 6, 7, 7, 0, 1, 7, 2, 20, 27, -1, 4, 28, 1, 20, -1, 13, 13, 14, 13, 0, -1, -1, 13, -1, 28, 3, 0, -1, -1, 13, 0, 2, -1, 3, 3, -1, 32, -1, 3, 3, 0, 3, 3, 32, -1, 3, -1, 17, 3, 21, 17, 21, 3, 10, 4, 3, 17, -1, 1, 17, 17, 17, 0, -1, 12, 0, 30, 2, 2, 2, 4, -1, 21, 1, 2, 17, 0, 10, 21, 10, 1, 1, 1, 32, -1, 2, 2, 1, -1, 6, 15, -1, 8, 8, 1, 8, 8, 1, 8, 8, 8, 8, 1, 1, 2, 2, 2, 2, 2, -1, -1, 2, 8, 2, 2, 2, 2, 14, 2, 2, 8, 14, 4, 1, 2, 2, 14, 2, 2, 2, 1, -1, 5, 9, 9, 5, 9, 9, 9, 9, 27, 26, 26, 26, 26, 26, 26, 14, 26, -1, 26, -1, -1, 15, 12, 0, 27, 10, 12, -1, 10, 19, -1, 19, 19, 19, 14, 14, -1, 10, 32, 10, 10, 10, 10, 0, 12, 28, 0, 12, 19, 4, 4, 4, 0, -1, 0, 28, 28, -1, 0, 28, 28, 0, 28, 4, 28, -1, 4, 0, -1, 21, 4, 0, 4, 16, 4, 9, -1, 0, -1, 1, -1, 12, -1, 2, 0, 16, 21, 27, -1, 21, 21, 21, 21, 1, 10, 21, -1, 27, -1, 0, -1, -1, 6, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, -1, 2, 2, 2, 2, 2, -1, 2, 2, 2, 9, -1, 12, 14, 11, 11, 4, 22, 16, 0, 27, 18, 4, 31, 11, 4, 4, 26, 13, 26, 0, 26, 13, 12, 0, -1, 29, 29, -1, 29, 12, 13, -1, 27, 31, -1, -1, 0, 4, -1, 4, 31, 0, 31, 12, 4, 31, -1, 31, 11, -1, -1, 12, 29, 10, 6, 3, 24, 24, 24, -1, 0, 6, -1, 20, 6, 10, 0, -1, 19, 5, 5, 32, 5, 10, 19, 19, 10, 0, 9, 19, 19, 19, 0, 19, 2, 19, 24, 12, 24, 24, 0, 0, 28, 28, 0, 0, 28, 0, 28, 5, 5, 5, 5, 5, 5, -1, 5, 5, -1, 25, 23, 0, 22, 22, 1, 0, 4, -1, 15, -1, 4, -1, 1, 1, 6, -1, 1, -1, -1, 1, 23, 9, -1, 5, 20, 18, 7, 7, 6, 0, 6, -1, 5, 14, 14, -1, 3, 7, 27, -1, -1, 7, 27, 1, 0, 22, 7, 11, 7, 14, 11, 11, -1, 17, 25, 2, -1, 29, 31, 12, 12, -1, 31, 12, -1, 15, -1, -1, -1, 29, 12, 14, 1, 11, -1, 1, -1, -1, -1, 1, -1, 6, 4, -1, 6, 4, 6, 25, 24, 24, 29, 24, 12, 24, 17, 29, 3, 13, 4, 5, 5, 16, -1, -1, 11, 5, 5, 5, 5, 3, 5, 8, 5, 5, 18, -1, 5, 5, -1, -1, 18, 20, 29, 29, 29, 29, 2, 16, 29, 21, 2, 3, 13, 3, -1, 0, 10, 3, -1, 24, 17, 10, 22, 22, 24, 0, 0, 14, 3, 24, 0, 6, -1, 24, 13, 14, 24, 5, 22, 0, 0, 2, 1, -1, 22, 13, 0, 7, -1, 1, 7, 7, 6, -1, 2, 12, 12, 13, 4, 1, 12, 7, 14, -1, 1, 13, 13, 2, 1, 1, -1, 13, 1, 13, -1, 1, 17, 2, -1, 4, -1, 15, 8, 4, 19, 2, -1, 8, 1, -1, 16, 0, 0, 0, 8, 6, 8, 6, 28, 2, 3, -1, 2, 22, 0, -1, 19, 3, 27, 27, 0, 19, 12, 12, 19, 19, -1, 0, -1, 25, 10, 19, 3, 3, 10, 2, 3, 14, 6, 18, 10, -1, -1, 0, 16, 3, 16, 0, -1, 3, -1, -1, 0, 3, 8, 21, 3, 25, 25, 25, 2, 1, 2, 2, 24, 7, 25, 2, 2, 1, 16, 7, 8, 24, 0, 1, 7, 2, 18, 22, 16, 1, 7, 22, 1, 0, 13, 13, -1, 18, 1, 17, 17, 1, 11, 0, 32, 1, 4, 1, 1, 0, -1, 13, -1, -1, 1, 1, 8, 2, -1, -1, 4, 2, 2, 1, -1, -1, -1, 17, 8, 21, 1, 3, -1, 17, 3, 17, -1, 4, 20, 19, 0, 0, 3, 26, 19, 23, -1, 3, 3, 3, 10, 2, -1, 3, 25, 0, 31, 23, 25, -1, -1, 8, 8, 25, 15, 25, 9, 20, -1, 0, 3, 2, 7, -1, 2, 2, 10, 7, -1, 2, 26, 26, 2, 1, 4, -1, -1, -1, -1, 15, 0, 7, 19, 31, -1, 25, 13, 4, 0, 0, 4, 4, 31, 25, 29, -1, 13, 9, 3, 3, -1, 18, 13, 1, 2, 10, 0, 18, 24, 0, 4, 10, 22, 1, 8, 0, -1, -1, 1, 1, -1, -1, 8, 22, 1, 26, 24, 17, -1, -1, 14, 1, 17, 3, 3, 3, 17, 1, 23, 2, -1, 2, 2, 31, 3, 0, 17, 3, -1, 3, 2, 3, 3, 3, 0, 3, 17, 22, 10, 1, 0, 2, 2, 3, 9, 5, 19, 19, -1, 0, 2, 2, 3, 3, 24, 8, 26, 24, 24, -1, -1, 4]\n",
            "-------RUN28-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': \"CountVectorizer(ngram_range=(1, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b891dd10>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[28, 0, 27, 27, -1, 0, 5, 6, 6, 6, -1, 22, 16, 22, 6, -1, 1, 28, -1, 0, -1, 16, 6, 10, 10, 19, -1, 0, 0, 0, 0, 0, 27, 25, 0, -1, -1, -1, -1, 0, 10, -1, 29, 10, 29, -1, 29, 29, 10, 0, 29, 10, 6, -1, -1, 13, 13, 0, 6, -1, 1, -1, -1, 6, -1, 6, 4, -1, -1, 4, 25, 16, 1, 1, 1, 4, 0, -1, 0, -1, 13, 13, -1, 13, 13, 19, 13, 1, 0, 1, 1, 6, 13, -1, -1, 6, 6, 1, -1, -1, 22, 14, 0, 14, 34, 25, -1, -1, -1, -1, -1, 16, -1, -1, 0, -1, -1, -1, 0, -1, 5, -1, 10, 14, 29, 30, 16, 0, 10, 16, 0, 18, 0, 16, 16, 28, 10, 0, 16, -1, 6, 6, 24, 30, 18, 6, 28, 0, 0, -1, -1, 6, 10, 6, -1, 6, 1, -1, -1, 2, 0, -1, 25, 2, -1, -1, 24, 14, -1, 0, 6, -1, 0, -1, 18, 6, 2, -1, 18, 4, 0, 0, -1, -1, 16, -1, 1, 0, 16, 23, 23, 18, 13, 0, 30, -1, 12, 12, 13, 0, 13, 0, 13, 13, -1, 13, 7, 13, 1, 22, 1, 19, -1, 13, 13, -1, -1, 1, 0, -1, 12, 1, 1, 1, 6, 12, -1, -1, -1, 30, 30, 30, 30, 12, 12, 30, 19, 30, 3, 12, -1, 4, 4, -1, 12, -1, 0, 14, 31, 0, -1, 12, 22, -1, -1, 5, 5, 5, 1, 1, -1, 14, 12, -1, 0, -1, 0, -1, -1, 4, 4, 4, 4, 1, 4, 34, 4, 4, 19, -1, 1, -1, 4, 5, -1, -1, 12, 2, -1, 2, 22, -1, 22, 22, -1, 12, 6, -1, 12, 4, 4, 4, 16, 4, 4, 4, -1, 4, 26, 4, 10, 4, 29, 0, -1, -1, -1, -1, 14, -1, -1, 5, 34, -1, -1, 28, -1, 28, 0, 28, 2, 14, 23, 28, 20, 24, -1, 18, 1, -1, 18, -1, 19, 19, -1, -1, 18, 18, 3, 18, -1, -1, 0, 0, 6, 23, 18, 23, 32, -1, 14, 16, 0, 16, 23, 4, 16, 1, 19, -1, 23, 11, 16, 18, 6, 16, 3, 6, 0, 3, 9, 9, -1, -1, 6, -1, 23, 6, 23, 19, 33, -1, 23, 9, 9, 9, 9, 9, 9, 9, -1, 9, 12, 0, 12, 0, 9, -1, 30, 30, 19, 23, -1, 14, 30, 19, 30, -1, 20, -1, -1, -1, -1, 6, 20, 20, 0, 20, 1, 1, 1, -1, 1, 4, 1, 1, 1, -1, 1, 1, 1, 1, 1, 20, 28, 4, -1, 4, 1, 6, 25, 4, 4, 4, 20, 0, 4, -1, 1, 4, -1, 4, -1, 0, 12, 4, 4, 4, 4, 1, 1, 5, 0, 1, 1, 1, 4, 5, -1, 1, 1, 2, 1, -1, -1, -1, 2, 5, 34, 5, -1, 0, -1, 19, 2, -1, -1, 0, 8, -1, 8, 8, 28, 1, -1, 12, 12, 16, 22, -1, -1, 28, 8, 8, 8, 8, 8, 14, 23, 8, 4, 8, 32, 23, -1, 17, 32, 8, -1, 0, 8, -1, 32, 12, 1, -1, -1, 32, 5, 19, 5, 5, 1, -1, 4, 1, -1, 14, 5, 5, 0, 5, 5, 11, 11, 11, 11, 3, 5, 5, 0, 22, 20, 3, 3, 3, 3, 3, 3, 2, 11, 32, -1, 17, 17, 17, 32, 23, 1, 11, 11, 10, -1, 11, 32, 11, 23, 25, 4, 3, 3, 19, -1, -1, 6, 3, 10, 3, 3, 19, 3, 3, 14, 3, 3, 0, 9, 3, 19, 19, 25, -1, 7, 7, 7, 7, 7, 7, 11, 7, 7, 1, 1, -1, 0, -1, 18, 28, 14, 7, 7, -1, 7, 7, 7, 1, 7, 7, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, -1, 4, 1, -1, 8, 9, 0, 8, 30, 0, 8, 28, 8, 28, 8, -1, 8, -1, 8, 10, 6, 10, 0, -1, -1, 8, 8, 8, 0, 8, -1, 8, 17, 23, 8, 28, 14, 5, 5, 5, 5, -1, 5, 5, 0, 1, 5, 2, 16, -1, -1, 18, 27, 1, 16, -1, 11, 11, 14, 11, 0, -1, -1, 11, -1, 27, 3, -1, -1, -1, 11, 0, 2, 0, 3, 3, -1, 0, -1, 3, 3, 0, 3, 3, 34, -1, 3, -1, 17, 3, 22, 17, 22, 3, 9, 18, 3, 17, -1, 1, 17, 17, 17, 0, -1, 10, 0, 30, 2, 2, 2, 25, -1, 22, 1, 2, 17, 0, 9, 22, 9, 1, 1, 1, 34, -1, 2, 2, -1, -1, -1, 4, 4, 7, 7, 1, 7, 7, 1, 7, 7, 7, 7, 1, -1, 2, 2, 2, 2, 2, -1, -1, 2, 7, 2, 2, 2, 2, -1, 2, 2, 7, 14, -1, 1, 2, 2, 14, 2, 2, 2, 1, -1, -1, 8, 8, 28, 8, 8, 8, 8, 29, 26, 26, 26, 26, 26, 26, 14, 26, -1, 26, -1, 7, 4, 10, 0, 29, 9, 10, -1, 9, 15, -1, 15, 15, 15, 14, 14, -1, 9, 34, 9, 9, 9, 9, 0, 10, 27, 0, 10, 15, 25, 25, 25, 0, -1, 0, 27, 27, -1, 0, 27, 27, 0, 27, 18, 27, -1, 18, 0, -1, -1, 18, 0, 18, 13, 18, 8, 19, 0, -1, -1, -1, 10, -1, 2, 0, 13, 22, 29, -1, 22, 22, 22, 22, 1, 9, 22, 0, 29, -1, 0, 0, 34, 6, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, -1, 2, 2, 2, 2, 2, -1, 2, 2, 2, 8, -1, 10, 14, 32, 23, -1, 20, 13, 0, -1, 12, -1, 33, 32, 18, 25, 26, 11, 26, 0, 26, 11, 10, 0, -1, 31, 31, -1, 31, 10, 11, -1, 29, 33, -1, -1, 0, 25, -1, 25, 33, 0, 33, 10, -1, 33, -1, -1, 23, -1, 4, 10, 31, 9, 6, 3, 21, 21, 21, -1, 0, 6, -1, 16, 6, 9, 0, -1, 15, 24, -1, 34, 24, 9, 15, 15, 9, 0, 8, 15, 15, 15, 0, 15, 2, 15, 21, 10, 21, 21, 0, 0, 27, 27, 0, 0, 27, 0, 27, -1, 24, -1, -1, 24, 24, -1, 24, 24, -1, 4, 19, 0, 20, 20, 1, 0, -1, -1, 4, -1, 25, 16, 1, 1, 6, -1, -1, -1, -1, 1, 19, 8, -1, 28, 16, 12, 5, 5, -1, 0, -1, -1, -1, -1, 14, -1, 3, 5, 29, -1, -1, 5, 29, 1, 0, 20, 5, 23, 5, 14, 32, 32, -1, 17, 4, 2, 0, -1, 33, 10, 10, -1, 33, 10, -1, 4, -1, -1, -1, 31, 10, 14, 1, 23, -1, 1, -1, 4, -1, 1, -1, 6, 18, -1, -1, 18, 6, 4, 21, 21, -1, 21, 10, 21, 17, 31, 3, 11, -1, 24, -1, 13, 7, -1, 23, 24, 24, 24, 24, 3, 24, 7, 24, 24, 12, -1, 24, 24, -1, -1, 12, 16, 31, 31, 31, 31, 2, 13, 31, -1, 2, 3, 11, 3, -1, 0, 9, 3, 1, 21, 17, 9, 20, 20, 21, 0, -1, -1, 3, 21, 0, -1, -1, 21, 11, -1, 21, -1, 20, -1, 0, 2, 1, -1, 20, -1, 0, 5, -1, -1, 5, 5, 6, -1, 2, 10, 10, 11, 25, 1, 10, 5, 14, -1, 1, 11, 11, 2, 1, 1, -1, 11, 1, 11, -1, 1, 17, 2, -1, 18, -1, 4, 7, 18, 15, 2, -1, 7, -1, -1, 13, 0, 0, 0, 7, 6, 7, 6, 27, 2, 3, -1, 2, 20, 0, 15, 15, 3, 29, 29, 0, 15, 10, 10, 15, 15, -1, 0, -1, 4, 9, 15, 3, 3, 9, 2, 3, -1, 6, 12, 9, -1, -1, 0, 13, 3, 13, 0, -1, 3, -1, -1, 0, 3, -1, 22, 3, 4, 4, 4, 2, 1, 2, 2, 21, 5, 4, 2, 2, 1, 13, 5, -1, 21, 0, 1, 5, 2, 12, 20, 13, 1, 5, 20, 1, 0, 11, 11, -1, 12, 1, 17, 17, 1, 32, 0, 34, 1, 25, 1, 1, 0, 15, 11, -1, -1, 1, 1, 7, 2, 4, 0, -1, 2, 2, 1, 15, -1, -1, 17, 7, 22, 1, 3, -1, 17, 3, 17, -1, 25, 16, 15, 0, 0, 3, 26, 15, 19, -1, 3, 3, 3, 9, 2, 34, 3, 4, 0, 33, 19, 4, 0, -1, 7, 7, 4, 4, 4, 8, 16, 4, 0, 3, 2, 5, -1, 2, 2, 9, 5, -1, 2, 26, 26, 2, 1, 19, -1, -1, -1, -1, 4, 0, 5, 15, 33, -1, 4, 11, 18, 0, 0, -1, -1, 33, 4, 31, -1, 11, 8, 3, 3, -1, 12, 11, 1, 2, 9, 0, 12, 21, 0, -1, 9, 20, 1, 7, 0, -1, -1, 1, -1, -1, -1, 7, 20, -1, 26, 21, 17, -1, -1, 14, 1, 17, 3, 3, 3, 17, 1, 19, 2, -1, 2, 2, 33, 3, 0, 17, 3, -1, 3, 2, 3, 3, 3, 0, 3, 17, 20, 9, 1, 0, 2, 2, 3, 8, 24, 15, 15, -1, 0, 2, 2, 3, 3, 21, 7, 26, 21, 21, 0, -1, -1]\n",
            "-------RUN29-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': \"CountVectorizer(ngram_range=(2, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df4a9008410>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[18, 0, 27, 27, -1, 0, 5, 4, 4, 4, -1, 26, 20, 26, 4, -1, 1, 18, -1, 0, -1, 20, 4, 9, 9, 19, -1, 0, 0, 0, 0, 0, 27, 28, 0, 4, -1, -1, -1, 0, 9, 0, 32, 9, 32, -1, 32, 32, 9, 0, 32, 9, 4, -1, 4, 14, 14, 0, 4, -1, 1, 0, -1, 4, 0, 4, -1, -1, -1, -1, 28, -1, 1, 1, 1, -1, 0, -1, 0, -1, 14, 14, -1, 14, 14, 19, 14, 1, 0, 1, 1, 4, 14, -1, -1, 4, 4, 1, -1, -1, 26, 11, 0, 11, 0, 28, -1, -1, -1, -1, -1, 20, -1, -1, 0, -1, -1, 19, 0, -1, 5, -1, 9, 11, 32, 31, 20, 0, 9, 20, 0, 12, 0, 20, 20, 18, 9, 0, 20, -1, 4, 4, 25, 31, 12, 4, 18, 0, 0, -1, 4, 4, 9, 4, -1, -1, 1, -1, -1, 2, 0, -1, 28, 2, -1, -1, -1, 11, -1, 0, 4, -1, 0, -1, 12, 4, 2, -1, 12, -1, 0, 0, 12, -1, 20, -1, -1, 0, 20, 21, 21, 12, 14, 0, 31, -1, 13, 13, 14, 0, 14, 0, 14, 14, -1, 14, 6, 14, 1, -1, 1, 19, -1, 14, 14, -1, -1, 1, 0, -1, 13, 1, 1, 1, -1, 13, -1, -1, -1, 31, 31, 31, 31, 13, 13, 31, 19, 31, 3, 13, 13, 16, 16, -1, 13, -1, 0, 11, 29, 0, 11, 13, 26, -1, -1, 5, 5, 5, 1, 1, -1, 11, 29, -1, 0, 0, 0, -1, -1, 16, 16, 16, -1, 1, 16, 0, 16, 16, 19, -1, 1, -1, -1, 5, -1, -1, 13, 2, -1, 2, 26, 0, 26, 26, -1, 13, 4, 0, 13, 24, -1, -1, 20, 24, -1, 24, 1, 24, 30, -1, 9, 24, 32, 0, -1, -1, -1, 0, 11, -1, 4, 31, 0, -1, -1, 18, -1, 18, 0, 18, 2, -1, 21, 18, 22, 25, 4, 12, 1, -1, 12, -1, 19, 19, -1, -1, 12, 12, 3, 12, -1, 33, 0, 0, 4, 21, 12, 21, 33, 18, 11, 20, 0, -1, 21, -1, 20, 1, 19, 20, 21, 10, 20, 12, 4, 20, 3, 4, 0, 3, 8, 8, 0, 4, 4, -1, 21, 4, 21, 19, 34, -1, 21, 8, 8, 8, 8, 8, 8, 8, -1, 8, 13, 0, 13, 0, 8, 1, 31, 31, 19, 21, -1, 11, 31, 19, 31, -1, 22, -1, -1, -1, -1, -1, 22, 22, 0, 22, 1, 1, 1, -1, 1, 24, 1, 1, 1, -1, 1, 1, 1, 1, 1, 22, 18, 16, -1, 16, 1, 4, 28, 16, 16, 16, 22, 0, 16, -1, -1, 16, 29, 16, -1, 0, 13, -1, 24, 16, 16, 1, 1, 5, 0, 1, 1, 1, 16, 5, -1, 1, 1, 2, 1, -1, -1, -1, 2, 5, 0, 5, -1, 0, 0, 19, 2, -1, 1, 0, 7, 18, 7, 7, 18, 1, -1, 13, 13, 20, -1, 3, -1, 18, 7, 7, 7, 7, 7, 11, 21, 7, 24, 7, 33, 21, -1, 15, 33, 7, 18, 0, 7, -1, 33, -1, 1, -1, -1, 33, 5, 19, 5, 5, 1, -1, 24, 1, -1, 11, 5, 5, 0, 5, 5, 10, 10, 10, 10, 3, 5, 5, 0, 26, 22, 3, 3, 3, 3, 3, 3, -1, 10, 33, 21, 15, 15, 15, 33, 21, 1, 10, 10, 9, -1, 10, 33, 10, 21, 28, 24, 3, 3, 19, 21, -1, 4, 3, 9, 3, 3, 19, 3, 3, 11, 3, 3, 0, 8, 3, 19, 19, 28, -1, 6, 6, 6, 6, 6, 6, 10, 6, 6, 1, 1, -1, 0, -1, 12, 18, 11, 6, 6, -1, 6, 6, 6, 1, 6, 6, 1, 1, 1, 1, 1, 1, 7, 1, 1, 1, -1, 16, 1, -1, 7, 8, 0, 7, 31, 0, 7, 18, 7, 18, 7, 18, 7, -1, 7, 9, 4, 9, 0, -1, -1, 7, 7, 7, 0, 7, 18, 7, 15, 21, 7, 18, 11, 5, 5, 5, 5, 4, 5, 5, 0, 1, 5, 2, 20, -1, -1, 12, 27, 1, 20, -1, 10, 10, 11, 10, 0, -1, -1, 10, -1, 27, 3, 0, -1, -1, 10, 0, 2, -1, 3, 3, -1, 0, -1, 3, 3, 0, 3, 3, 0, -1, 3, -1, 15, 3, 26, 15, 26, 3, 8, 12, 3, 15, 4, -1, 15, 15, 15, 0, -1, 9, 0, 31, 2, 2, 2, 28, -1, 26, 1, -1, 15, 0, 8, 26, 8, 1, -1, 1, 0, -1, -1, 2, -1, -1, 4, 16, -1, 6, 6, 1, 6, 6, 1, 6, 6, 6, 6, 1, -1, 2, 2, 2, 2, 2, -1, -1, 2, 6, 2, 2, 2, 2, 11, 2, 2, 6, 11, -1, 1, 2, 2, 11, 2, 2, 2, 1, 22, 18, 7, 7, 18, 7, 7, 7, 7, -1, 30, 30, 30, 30, 30, 30, 11, 30, -1, 30, -1, 6, 16, 9, 0, 32, 8, 9, -1, 8, 17, -1, 17, 17, 17, 11, 11, -1, 8, 0, 8, 8, 8, 8, 0, 9, 27, 0, 9, 17, 28, 28, 28, 0, -1, 0, 27, 27, 1, 0, 27, 27, 0, 27, 12, 27, -1, 12, -1, -1, 27, 12, 0, 12, 14, 12, 7, 19, 0, -1, 1, -1, 9, -1, 2, 0, 14, 26, 32, -1, 26, 26, 26, 26, -1, 8, 26, 0, 32, -1, 0, 0, 0, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, -1, 2, 2, 2, 2, 2, -1, 2, 2, 2, 7, -1, 9, 11, 33, 21, -1, 22, 14, 0, 32, 13, -1, 34, 33, 12, 28, 30, 10, 30, 0, 30, 10, 9, 0, -1, 29, 29, -1, 29, 9, 10, -1, 32, 34, -1, 34, 0, 28, 0, 28, 34, 0, 34, 9, -1, 34, -1, 34, 21, -1, -1, 9, 29, 8, 4, 3, 23, 23, 23, 0, 0, 4, -1, 20, 4, 8, 0, -1, 17, 25, -1, 0, 25, 8, 17, 17, 8, 0, 7, 17, 17, 17, 0, 17, 2, 17, 23, 9, 23, 23, 0, 0, 27, 27, 0, 0, 27, 0, 27, 25, 25, 18, -1, 25, 25, -1, 25, 25, -1, 24, 19, 0, 22, 22, 1, 0, -1, -1, 16, -1, 28, -1, 1, 1, 4, 4, 1, -1, 4, 1, 19, 7, 0, 18, 20, 13, 5, 5, 4, 0, -1, 0, -1, 11, 11, 11, 3, 5, 32, -1, -1, 5, 32, 1, 0, 22, 5, 21, 5, 11, 33, 33, -1, 15, -1, 2, 0, 29, 34, 9, 9, -1, 34, 9, -1, 16, -1, -1, -1, 29, 9, -1, 1, 21, -1, 1, 4, -1, -1, 1, -1, 4, 12, -1, -1, 12, 4, 24, 23, 23, 1, 23, 9, 23, 15, 29, 3, 10, -1, 25, -1, 14, -1, 4, 21, 25, 25, 25, 25, 3, 25, 6, 25, 25, 13, -1, 25, 25, -1, -1, 13, 20, 29, 29, 29, 29, 2, 14, 29, -1, -1, 3, 10, 3, -1, 0, 8, 3, -1, 23, 15, 8, 22, 22, 23, 0, 0, 11, 3, 23, 0, 4, -1, 23, 10, 11, 23, 18, 22, 0, 0, 2, 1, -1, 22, 10, 0, 5, -1, -1, 5, 5, 4, -1, 2, 9, 9, 10, 28, 1, 9, 5, 11, -1, 1, 10, 10, 2, 1, 1, -1, 10, 1, 10, -1, 1, 15, 2, -1, 12, -1, 16, 6, 12, 17, 23, -1, 6, -1, -1, 14, 0, 0, 0, 6, 4, 6, 4, 27, -1, 3, -1, 2, 22, 0, -1, 17, 3, 32, 32, 0, 17, 9, 9, 17, 17, 12, 0, -1, 24, 8, 17, 3, 3, 8, 2, 3, -1, 4, 13, 8, -1, -1, 0, 14, 3, 14, 0, -1, 3, -1, -1, 0, 3, -1, 26, 3, 24, 24, 24, 2, 1, 2, 2, 23, 5, 24, 2, 2, 1, 14, 5, 6, 23, 0, 1, 5, 2, 13, 22, 14, 1, 5, 22, 1, 0, 10, 10, -1, 13, 1, 15, 15, -1, 33, -1, 0, 1, 28, 1, 1, 0, -1, 10, -1, -1, 1, 1, 6, 2, -1, 0, -1, 2, 2, 1, 17, -1, -1, 15, 6, 26, 1, 3, -1, 15, 3, 15, -1, 28, 20, 17, 0, 0, 3, 30, 17, 19, -1, 3, 3, 3, 8, 2, 0, 3, -1, 0, 34, 19, 24, 0, -1, 6, 6, 24, -1, 24, 7, 20, -1, 0, 3, 2, 5, 0, 2, 2, 8, 5, -1, 2, 30, 30, 2, 1, -1, -1, -1, -1, -1, 16, 0, 5, 17, 34, -1, 24, 10, -1, 0, 0, -1, -1, 34, 24, 29, 29, 10, 7, 3, 3, -1, 13, 10, 1, -1, 8, 0, 13, 23, 0, 12, 8, 22, 1, 6, 0, -1, -1, 1, -1, -1, 0, 6, 22, 1, 30, 23, 15, -1, -1, 11, 1, 15, 3, 3, 3, 15, 1, 19, 2, -1, 2, -1, 34, 3, 0, 15, 3, 27, 3, 2, 3, 3, 3, 0, 3, 15, 22, 8, 1, 0, 2, 2, 3, 7, 25, 17, 17, -1, 0, 2, 2, 3, 3, 23, 6, 30, 23, 23, 0, -1, -1]\n",
            "-------RUN30-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': \"CountVectorizer(stop_words='english')\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[37, -1, 23, 23, -1, 39, -1, 10, 10, 10, -1, 25, 14, -1, 10, -1, 47, 37, -1, -1, -1, 14, 10, 8, 8, 17, 42, 43, 19, 50, 38, -1, 23, 24, -1, -1, 1, -1, -1, -1, 8, -1, 31, 8, 31, -1, 31, 31, 8, -1, 31, 8, 10, -1, -1, 12, 12, 27, -1, 58, 46, -1, 58, 10, -1, 10, 1, -1, 14, 1, 24, 14, 3, 3, -1, 1, -1, -1, -1, -1, 12, 12, -1, 12, 12, 17, 12, -1, 19, 3, 3, 10, 12, 48, -1, 10, 10, 3, -1, -1, 25, 30, 39, 41, 33, 24, -1, -1, -1, -1, -1, 14, 42, 57, -1, -1, -1, -1, 38, -1, 4, -1, 8, 30, 31, 29, 14, 19, 8, 14, 19, 11, 50, 14, 14, 37, 8, 38, 14, -1, 10, -1, 22, 29, 11, 10, 37, -1, 27, -1, 10, 10, 8, 10, -1, -1, -1, -1, -1, -1, -1, -1, 24, -1, -1, 57, 22, 30, -1, 39, 10, -1, -1, -1, 11, 10, -1, 53, 11, -1, -1, 38, 11, -1, 14, -1, -1, 19, 14, 20, -1, 11, 12, 19, 29, -1, 13, 13, 12, -1, 12, -1, 12, 12, -1, 12, 5, 12, 3, 52, 32, 17, -1, 12, 12, -1, -1, 3, 19, -1, 13, 51, 32, 32, -1, 13, 42, 42, -1, 29, 29, 29, 29, 13, 13, 29, 17, 29, 0, 13, 13, 1, 1, -1, 13, -1, -1, 30, 28, 43, -1, 13, 25, 57, 54, 4, 4, 4, 32, 32, -1, 30, -1, -1, -1, -1, -1, 42, 48, 1, 1, 1, 1, -1, 1, 33, 1, 1, 17, 42, 3, 54, 1, 4, -1, -1, 13, 35, -1, 35, 25, -1, 25, 25, -1, 13, 10, 33, 13, 1, 1, 1, 14, 1, -1, 1, -1, 1, 26, 1, 8, 1, 31, -1, 53, -1, 54, -1, 30, -1, -1, 29, 33, -1, -1, 37, 45, 37, -1, 37, -1, -1, 20, 37, 18, 22, -1, 11, -1, -1, 11, -1, 17, 17, -1, -1, 11, 11, 0, 11, 55, 36, -1, 27, 49, 20, 11, 20, 20, 37, 30, 14, 39, 14, 20, 1, 14, -1, 17, -1, 20, 9, 14, 11, 49, 14, 0, 49, -1, 0, 6, 6, -1, 49, 49, 49, 20, -1, 20, 17, 34, -1, 20, 6, 6, 6, 6, 6, 6, 6, 48, 6, 13, 27, 13, 43, 6, -1, 29, 29, 17, 20, -1, 41, 29, 17, 29, -1, 18, -1, -1, -1, 48, -1, 18, 18, 38, 18, 32, 3, 32, -1, 3, 1, 3, 3, 3, 56, 3, -1, 3, 47, -1, 18, -1, 1, -1, 1, 32, 10, 24, 1, 1, 1, 18, 27, 1, -1, -1, 1, -1, 1, -1, 27, 13, 1, 1, 1, 1, -1, 3, 4, 19, 3, 47, 56, 1, 4, -1, 3, 47, 35, 3, -1, -1, -1, 35, 4, 33, 4, -1, -1, -1, 17, 35, -1, 46, -1, 7, -1, 7, 7, 40, 3, 53, 13, 13, 14, 52, 0, -1, 40, 7, 7, 7, 7, 7, 30, 20, 7, 1, 7, 36, 20, 52, 15, 36, 7, 40, 39, 7, 52, -1, -1, 3, -1, 57, 36, 4, -1, 4, 4, -1, -1, 1, 3, -1, 30, 4, 4, 19, 4, 4, 9, 9, 9, 9, 0, 4, 4, 19, 25, 18, 0, 0, 0, 0, 0, 0, -1, 9, 36, -1, 15, 15, 15, 36, 20, -1, 9, 9, 8, 48, 9, 36, 9, -1, 24, 1, 0, 0, 17, 20, -1, 10, 0, 8, 0, 0, 17, 0, 0, 30, 0, 0, -1, 6, 0, 17, 17, 24, 48, 5, 5, 5, 5, 5, 5, 9, 5, 5, 3, 3, -1, 27, -1, 11, 40, 41, 5, 5, -1, 5, 5, 5, 3, 5, 5, -1, 3, 3, 3, -1, -1, 7, 3, 3, -1, -1, 1, -1, -1, 7, 6, -1, 7, 29, -1, 7, 40, 7, 40, 7, 40, 7, 53, 7, 8, 10, 8, 19, -1, -1, 7, 7, 7, -1, 7, -1, 7, 15, 20, 7, 37, 30, 4, 4, 4, 4, -1, 4, 4, -1, 46, 4, -1, 14, -1, -1, 11, 23, 3, 14, -1, 9, 9, 41, 9, -1, 52, -1, -1, -1, 23, 0, -1, 11, -1, 9, -1, 44, -1, 0, 0, -1, 33, -1, 0, 0, -1, 0, 0, 33, -1, 0, 55, 15, 0, 25, 15, 25, 0, 6, 11, 0, 15, 10, -1, 15, 15, 15, -1, -1, 8, 38, 29, 2, 2, 2, 24, 33, 25, 32, -1, 15, 39, 6, 25, 6, 32, -1, 32, 33, -1, -1, -1, 56, -1, -1, 1, 1, 5, 5, 51, 5, 5, 3, 5, 5, 5, 5, -1, 56, 2, 2, 2, 2, 2, 55, -1, 2, 5, 2, 2, 2, 2, 45, 2, 2, 5, 30, 53, -1, 2, 44, 41, 2, 2, 2, 32, 18, 40, 7, 7, 40, 7, 7, 7, 7, -1, 26, 26, 26, 26, 26, 26, 30, 26, -1, 26, -1, 5, 1, 8, 19, 31, 6, 8, -1, 6, 16, -1, 16, 16, 16, 30, 30, -1, 6, 33, 6, 6, 6, 6, 38, 8, 23, -1, 8, 16, 24, 24, 24, -1, -1, -1, 23, 23, -1, -1, 23, 23, 19, 23, 11, 23, 55, 11, -1, 11, 23, 11, 27, 11, 12, 11, 52, 17, -1, -1, 46, -1, 8, -1, 2, 19, 12, 25, 31, -1, 25, 25, 25, -1, -1, 6, 25, -1, 31, -1, 38, -1, -1, 10, -1, 2, 2, 2, 2, 2, 2, 2, -1, 35, 2, 0, 2, -1, 2, 44, 2, 35, 2, -1, 44, 44, 2, 7, -1, 8, 30, 36, 20, -1, 18, 12, 19, -1, 13, -1, 34, 36, 11, 24, 26, 9, 26, 27, 26, 9, 8, -1, 33, 28, 28, 28, 28, 8, 9, 55, 31, 34, -1, -1, 43, 24, 43, 24, 34, 27, 34, 8, -1, 34, -1, 34, 20, -1, 1, 8, 28, 6, 10, 0, 21, 21, 21, -1, 27, 10, -1, 14, 10, 6, 50, -1, 16, 22, -1, 33, 22, 6, 16, 16, 6, -1, -1, 16, 16, 16, 39, 16, 2, 16, 21, 8, 21, 21, 43, 39, 23, 23, -1, -1, 23, -1, 23, -1, 22, -1, -1, 22, 22, -1, 22, 22, -1, 1, 17, 50, 18, 18, 3, 43, -1, -1, 1, -1, 24, -1, 51, -1, 10, -1, 46, -1, -1, 3, 17, 7, -1, 37, 14, 13, 4, 4, -1, 27, -1, -1, 57, 45, 41, 45, 0, 4, 31, -1, -1, 4, 31, 3, -1, 18, 4, 20, 4, 41, 36, 36, -1, 15, 1, -1, 43, 28, 34, 8, 8, -1, 34, 8, 17, 1, 54, -1, 54, 28, 8, 54, 51, 20, 0, 3, -1, 1, 54, 3, -1, 10, 11, -1, 58, 11, 10, 1, 21, -1, -1, 21, 8, 21, 15, 28, 0, 9, 53, 22, 57, 12, 12, -1, 20, 22, 22, 22, 22, 0, 22, 5, 22, 22, 13, -1, 22, 22, 58, 58, 13, 14, 28, 28, 28, 28, 35, 12, 28, 52, -1, 0, 9, 0, -1, 27, 6, 0, -1, 21, 15, 6, 18, 18, 21, 50, -1, 45, 0, 21, 27, -1, -1, 21, 9, 45, 21, 37, 18, -1, -1, 2, -1, -1, 18, 9, -1, 4, -1, 56, 4, 4, 10, -1, 2, 8, 8, 9, 24, -1, 8, 4, 41, -1, -1, 9, 9, 2, 3, 47, -1, 9, 32, 9, -1, 3, 15, -1, 0, 11, 42, 1, 5, 11, 16, 35, -1, 5, 56, -1, 12, 19, -1, -1, 5, -1, 5, 49, 23, 35, 0, 45, 35, 18, 27, 16, 16, 0, 31, 31, -1, 16, 8, 8, 16, 16, -1, 19, -1, 1, 6, 16, 0, 0, 6, 2, 0, -1, 10, 13, 6, -1, 0, 27, 12, 0, 12, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, 1, 1, 1, 2, 46, 2, 2, 21, 4, 1, 2, 2, -1, 12, 4, 5, 21, 19, 47, 4, 35, 13, 18, 12, 3, 4, 18, -1, 19, 9, 9, 55, 13, 3, 15, 15, 46, 36, -1, 33, 51, 24, 51, 3, 38, -1, 9, -1, -1, 32, 3, 5, 2, 1, 50, 53, 2, -1, 3, 16, 11, -1, 15, 5, 25, 51, 0, -1, 15, 0, 15, -1, 24, 14, 16, -1, 50, 0, 26, 16, 17, -1, 0, 0, 0, 6, 44, 33, 0, 1, 19, 34, 17, 1, -1, -1, 5, 5, 1, 1, 1, 7, 14, 1, 38, 0, 2, 4, -1, 2, 2, 6, 4, -1, 2, 26, 26, 2, 3, -1, 42, 48, -1, 42, 1, 19, 4, 16, 34, -1, 1, 9, -1, 39, -1, 11, -1, 34, 1, 28, 28, 9, 7, 0, 0, -1, 13, 9, -1, 44, 6, 27, 13, 21, -1, -1, 6, 18, 3, 5, -1, -1, -1, 47, -1, -1, -1, 5, 18, -1, 26, 21, 15, -1, 45, 41, 46, 15, 0, 0, 0, 15, -1, 17, 2, -1, 2, 44, 34, 0, 39, 15, 0, -1, 0, 35, 0, 0, 0, -1, 0, 15, 18, 6, 32, -1, 2, 2, 0, 7, -1, 16, 16, -1, 19, 2, 2, 0, 0, 21, 5, 26, 21, 21, -1, -1, -1]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------RUN31-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': 'CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7df3b00f5c90>)', 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[14, 13, 30, 30, -1, -1, 4, 22, 22, 22, -1, 23, 17, -1, 22, -1, 27, 14, -1, -1, -1, 17, -1, 8, 8, 25, 40, -1, 18, 36, 13, 36, 30, 28, 36, -1, 1, -1, -1, -1, 8, -1, 24, 8, 24, 24, 24, 24, 8, -1, 24, 8, -1, -1, -1, 10, 10, 13, 44, 55, 27, -1, 55, 22, -1, 22, 1, -1, -1, 1, 28, 17, 3, 3, -1, 1, -1, -1, 36, -1, 10, 10, 33, 10, 10, 25, 10, -1, 18, -1, 3, 22, 10, 51, -1, 22, 22, 3, -1, -1, 23, 37, 42, 45, 35, 28, -1, -1, -1, -1, -1, 17, 40, 54, -1, -1, -1, -1, 13, -1, 24, -1, 8, 37, 24, 32, 17, 18, 8, 17, 18, 16, 36, 17, 17, 14, 8, 53, 17, 40, -1, -1, 26, 32, 16, 44, 14, -1, 13, -1, 44, 44, 8, 22, -1, -1, -1, -1, -1, 41, -1, -1, 28, -1, -1, 54, 14, 37, -1, 42, -1, -1, -1, -1, 16, 44, -1, 39, 16, -1, -1, 53, -1, -1, 17, -1, -1, 18, 17, 21, -1, 16, 10, 18, 32, -1, 12, 12, 10, -1, 10, 13, 10, 10, 39, 10, 5, 10, 3, 52, 29, 33, 39, 10, 10, -1, -1, 3, 18, -1, 12, -1, 29, 29, -1, 12, 40, 40, 6, 32, 32, 32, 32, 12, 12, 32, 25, 32, 0, 12, -1, 1, 1, -1, 12, -1, 42, 37, 34, 50, -1, 12, 23, 54, 49, 4, 4, 4, 29, 29, -1, 37, -1, 35, 13, -1, -1, 40, 51, 1, 1, 1, 1, -1, 1, 35, 1, 1, 25, 40, 3, 49, 1, 4, 4, -1, 12, 41, -1, 41, 23, 36, 23, 23, -1, 12, 22, 35, 12, 1, 1, 1, 17, 1, -1, 1, 27, 1, 31, 1, 8, 1, 24, -1, 39, -1, 49, -1, 37, -1, -1, 32, 35, 17, -1, 14, 46, 14, 36, 14, -1, 49, 21, 14, 19, 26, -1, 16, -1, 48, 16, -1, 25, -1, 43, 39, 16, 16, 0, 16, 48, 38, -1, 13, 43, 21, 16, 21, -1, 14, 37, 17, 42, -1, 21, 1, 17, -1, 25, -1, 21, 9, 17, 16, 43, 17, 0, 43, 18, 0, 6, 6, -1, 43, 43, 43, 21, 43, 21, 25, 33, -1, 21, 6, 6, 6, 6, 6, 6, 6, 51, 6, 12, 13, 12, 50, 6, -1, 32, 32, 25, 21, -1, 45, 32, 25, 32, -1, 19, -1, -1, -1, 51, -1, 19, 19, -1, 19, 29, 3, 29, -1, 3, 1, 3, 3, 3, -1, 3, 3, 3, 27, 3, 19, 14, 1, 39, 1, 29, 22, 28, 1, 1, 1, 19, 13, 1, -1, -1, 1, 34, 1, -1, 13, 12, 1, 1, 1, 1, -1, 3, 4, 18, 3, 27, -1, 1, 4, -1, 3, 27, 41, 3, 40, -1, 4, 41, 4, 35, 4, -1, -1, -1, 25, 41, -1, 27, 36, 7, 14, 7, 7, 14, -1, 39, 12, 12, 17, 52, 0, 14, 14, 7, 7, 7, 7, 7, -1, 21, 7, 1, 7, 38, 21, 52, 11, 38, 7, -1, 42, 7, 52, 38, -1, 3, -1, 54, 38, 4, -1, 4, 4, -1, -1, 1, 3, -1, 37, 4, 4, 18, 4, 4, 9, 9, 9, 9, 0, 4, 4, 18, 23, 19, 0, 0, 0, 0, 0, 0, -1, 9, 38, -1, 11, 11, 11, 38, 21, -1, 9, 9, 8, 51, 9, 38, 9, 21, 28, 1, 0, 0, 25, 21, 40, 22, 0, 8, 0, 0, 25, 0, 0, 37, 0, 0, -1, 6, 0, 25, 25, 28, 51, 5, 5, 5, 5, 5, 5, 9, 5, 5, 3, 3, -1, 13, -1, 16, 14, 45, 5, 5, -1, 5, 5, 5, 3, 5, 5, -1, 3, 3, 3, -1, 3, 7, 3, -1, 27, -1, 1, -1, -1, 7, 6, 36, 7, 32, -1, 7, 14, 7, 14, 7, 14, 7, 39, 7, 8, 22, 8, 18, -1, -1, 7, 7, 7, -1, 7, 14, 7, 11, 21, 7, 14, 37, 4, 4, 4, 4, -1, 4, 4, -1, 27, 4, -1, 17, -1, 4, 16, 30, 3, 17, -1, 9, 9, 45, 9, -1, 52, -1, 9, -1, 30, 0, -1, -1, -1, 9, 53, 47, -1, 0, 0, -1, 35, -1, 0, 0, -1, 0, 0, 35, -1, 0, 48, 11, 0, 23, 11, 23, 0, 6, 16, 0, 11, 44, -1, 11, 11, 11, -1, -1, 8, 13, 32, 2, 2, -1, 28, -1, 23, 29, -1, 11, 42, 6, 23, 6, 29, -1, 29, 35, -1, -1, -1, -1, -1, -1, 1, 1, 5, 5, 3, 5, 5, 3, -1, 5, 5, 5, -1, -1, 2, 2, 2, 2, 2, 48, -1, 2, 5, -1, 2, 2, 2, 46, 2, 2, 5, -1, 39, -1, 2, 47, 45, 2, -1, 2, 29, -1, -1, 7, 7, 14, 7, 7, 7, 7, 24, 31, 31, 31, 31, 31, 31, 37, 31, -1, 31, -1, 5, 1, 8, 18, 24, 6, 8, -1, 6, 15, -1, 15, 15, 15, 37, 37, -1, 6, 35, 6, 6, 6, 6, 53, 8, 30, -1, 8, 15, 28, 28, 28, -1, -1, -1, 30, 30, -1, -1, 30, 30, 18, 30, 16, 30, 48, 16, -1, -1, -1, 16, 13, 16, 10, 16, -1, 33, 13, -1, 27, -1, 8, -1, 2, 18, 10, 23, 24, -1, 23, 23, 23, 23, -1, 6, 23, -1, 24, -1, 53, -1, 35, 22, -1, 2, 2, 2, 2, 2, 2, 2, -1, 2, 2, 0, 2, -1, 2, 47, 2, 41, 2, -1, 47, 47, 2, 7, -1, 8, 37, 38, 21, -1, 19, 10, 18, -1, 12, -1, 33, 38, -1, 28, 31, 9, 31, 13, 31, 9, 8, 13, -1, 34, 34, -1, 34, 8, 9, 48, 24, 33, -1, -1, 50, 28, 50, 28, 33, 13, 33, 8, 39, 33, -1, 33, 21, -1, 1, 8, 34, 6, 22, 0, 20, 20, 20, -1, 13, 22, -1, 17, 22, 6, 36, -1, 15, 26, -1, 35, 26, 6, 15, 15, 6, -1, 7, 15, 15, 15, 42, 15, -1, 15, 20, 8, 20, 20, 50, 42, 30, 30, -1, -1, 30, -1, 30, -1, 26, 14, 14, 26, 26, -1, 26, 26, -1, 1, 25, 36, 19, 19, 3, 50, -1, -1, 1, -1, -1, -1, 3, -1, 44, 56, 27, -1, 56, 3, 25, 7, -1, 14, 17, 12, 4, 4, 56, -1, -1, -1, 54, 46, 45, 46, 0, 4, 24, -1, -1, 4, 24, 3, 18, 19, 4, 21, 4, 45, 38, 38, -1, 11, 1, -1, 50, 34, 33, 8, 8, -1, 33, 8, -1, 1, 49, -1, -1, 34, 8, 49, -1, 21, -1, 3, 43, 1, 49, 3, -1, 22, 16, -1, 55, 16, 22, 1, 20, -1, -1, 20, 8, 20, 11, 34, 0, 9, 39, 26, 54, 10, 10, -1, 21, 26, 26, 26, 26, 0, 26, 5, 26, 26, 12, -1, -1, 26, 55, 55, 12, 17, 34, 34, 34, 34, -1, 10, 34, 52, -1, 0, 9, 0, -1, 13, 6, 0, -1, 20, 11, 6, 19, 19, 20, 36, -1, 46, 0, 20, 13, -1, -1, 20, 9, 46, 20, 14, 19, -1, -1, 2, -1, -1, 19, 9, -1, 4, -1, -1, 4, 4, 22, -1, 2, 8, 8, 9, 28, 29, 8, 4, 45, -1, 27, 9, 9, 2, 3, 27, -1, 9, 29, 9, -1, 3, 11, -1, 0, 16, 40, 1, 5, 16, 15, 41, -1, 5, -1, 24, 10, 18, -1, -1, 5, 44, 5, 43, 30, 41, 0, 46, 41, 19, 13, -1, 15, 0, 24, 24, -1, 15, 8, 8, 15, 15, -1, 18, -1, 1, 6, 15, 0, 0, 6, 2, 0, -1, 44, 12, 6, -1, 0, 13, 10, 0, 10, -1, -1, 0, -1, -1, -1, 0, 5, 23, 0, 1, 1, 1, 2, -1, 2, 2, 20, 4, 1, 2, 2, -1, 10, 4, 5, 20, 18, 27, 4, 41, 12, 19, 10, 3, 4, 19, -1, 18, 9, 9, 48, 12, 3, 11, 11, 27, 38, -1, 35, -1, 28, 29, 3, 13, -1, 9, -1, -1, 29, 3, 5, 2, 1, -1, 39, 2, 2, -1, 15, -1, -1, 11, 5, 23, -1, 0, -1, 11, 0, 11, -1, 28, 17, 15, -1, 36, 0, 31, 15, 25, -1, 0, 0, 0, 6, 47, 35, 0, 1, 18, 33, 25, 1, -1, -1, 5, 5, 1, 1, 1, 7, 17, 1, 53, 0, 2, 4, -1, 2, 2, 6, 4, -1, 2, 31, 31, 2, 3, -1, 40, 51, -1, 40, 1, 18, 4, 15, 33, -1, 1, 9, -1, 42, -1, 16, -1, 33, 1, 34, 34, 9, 7, 0, 0, 56, 12, 9, -1, 47, 6, 13, 12, 20, -1, 28, 6, 19, -1, 5, -1, -1, -1, 27, -1, -1, -1, 5, 19, 27, 31, 20, 11, 56, 46, 45, 27, 11, 0, 0, 0, 11, -1, 25, 2, -1, 2, 47, 33, 0, 42, 11, 0, -1, 0, -1, 0, 0, 0, -1, 0, 11, 19, 6, 29, 13, 2, 2, 0, 7, -1, 15, 15, -1, 18, 2, 2, 0, 0, 20, 5, 31, 20, 20, 36, -1, -1]\n",
            "-------RUN32-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': \"CountVectorizer(stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b02d2c50>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[39, 18, 30, 30, -1, 44, 3, 22, 22, 22, -1, 26, 15, -1, 22, -1, 45, 39, -1, -1, -1, 15, -1, 7, 7, 16, 41, -1, 17, 51, 18, -1, 30, 27, 51, -1, -1, -1, -1, -1, 7, -1, 29, 7, 29, -1, 29, 29, 7, -1, 29, 7, -1, -1, -1, 12, 12, 18, 35, 35, 47, -1, 35, 22, -1, 22, 9, 47, 15, -1, 27, 15, 2, 2, -1, -1, -1, -1, 51, -1, 12, 12, 33, 12, 12, 16, 12, 2, 17, 2, 2, 22, 12, 53, -1, 22, 22, 2, -1, -1, 26, 34, 44, 46, 36, 27, -1, -1, -1, -1, -1, 15, 41, -1, -1, 58, -1, 16, -1, -1, 29, 58, 7, 34, 29, 32, 15, 17, 7, 15, 18, 13, 51, 15, 15, 39, 7, 59, 15, -1, 22, -1, 19, 32, 13, 35, 39, -1, 18, -1, 35, 35, 7, 22, -1, -1, 28, -1, -1, 37, -1, -1, 27, -1, -1, -1, 19, 34, -1, -1, 22, -1, 51, -1, 13, 35, 50, 48, 13, -1, -1, 59, -1, -1, 15, -1, -1, 17, 15, -1, -1, 13, 12, 17, 32, 4, 11, 11, 12, -1, 12, -1, 12, 12, -1, 12, 4, 12, 2, 56, 28, 33, -1, 12, 12, -1, -1, 2, 17, -1, 11, -1, 28, 28, -1, 11, 41, 41, -1, 32, 32, 32, 32, 11, 11, 32, 16, 32, 0, 11, -1, 9, 9, -1, 11, -1, -1, 34, 25, -1, -1, 11, 26, -1, 52, 3, 3, 3, 28, 28, -1, 34, 25, -1, 18, -1, -1, 41, 53, 9, 9, 9, -1, 28, 9, 36, 9, -1, 16, 41, 2, 52, -1, 3, 3, -1, 11, 37, -1, 37, 26, -1, 26, 26, 58, 11, 22, 36, 11, 23, -1, -1, 15, 23, -1, 23, 47, -1, 31, -1, 7, 23, 29, -1, 48, -1, 52, -1, 34, -1, 43, 32, 36, -1, -1, 39, 49, 39, -1, 39, 37, 52, 24, 39, 20, 19, -1, 13, -1, -1, 13, -1, 16, 16, -1, -1, 13, 13, 0, 13, 57, 38, 54, 18, 42, 24, 13, 24, 24, -1, 34, 15, -1, 15, 24, -1, 15, 45, 16, -1, 24, -1, 15, 13, 42, 15, 0, 42, 17, 0, 5, 5, -1, 42, 42, 42, 24, 42, 24, 16, 33, -1, 24, 5, 5, 5, 5, 5, 5, 5, 53, 5, 11, -1, 11, 55, 5, -1, 32, 32, 16, 24, -1, 46, 32, 16, 32, -1, 20, -1, -1, -1, 53, -1, 20, 20, -1, 20, 28, 2, 28, -1, 2, 23, 2, 2, 2, -1, 2, 2, 2, 45, 2, 20, 40, 9, -1, 9, 28, 22, 27, 9, 9, 9, 20, 18, 9, -1, -1, 9, 25, 9, -1, 18, 11, 9, 9, -1, 9, -1, 2, 3, 17, 2, 45, 2, 9, 3, -1, 2, 45, 37, 2, -1, -1, 3, 37, 3, 36, 3, -1, -1, -1, 16, 37, -1, 47, -1, 6, 40, 6, 6, 40, 2, 48, 11, 11, 15, 56, 0, -1, 40, 6, 6, 6, 6, 6, -1, 24, 6, 23, 6, 38, 24, 56, 14, 38, 6, -1, 44, 6, 56, 38, 11, 2, -1, -1, 38, 3, 16, 3, 3, -1, -1, 23, 2, 16, 34, 3, 3, 17, 3, 3, 8, 8, 8, 8, 0, 3, 3, 17, 26, 20, 0, 0, 0, 0, 0, 0, -1, 8, 38, -1, 14, 14, 14, 38, 24, -1, 8, 8, 7, 53, 8, 38, 8, -1, 27, 23, 0, 0, 16, 24, 58, 42, 0, 7, 0, 0, 16, 0, 0, 34, 0, 0, -1, 5, 0, 16, 16, 27, 53, 4, 4, 4, 4, -1, 4, 8, 4, 4, 2, 2, -1, 18, -1, 13, 40, 46, 4, 4, -1, 4, 4, 4, 2, 4, 4, 2, 2, 2, 2, -1, -1, 6, 2, 2, 45, -1, 9, -1, -1, 6, 5, 54, 6, 32, -1, 6, 40, 6, 40, 6, 40, 6, 48, 6, 7, 22, 7, 17, -1, -1, 6, 6, 6, 54, 6, 40, 6, 14, 24, 6, 39, 34, 3, 3, 3, 3, -1, 3, 3, -1, 47, 3, -1, 15, -1, 3, 13, 30, 2, 15, -1, 8, 8, 46, 8, -1, 56, -1, 8, -1, 30, 0, -1, 13, 41, 8, 59, 50, -1, 0, 0, -1, 36, -1, 0, 0, -1, 0, 0, 36, -1, 0, 57, 14, 0, 26, 14, 26, 0, 5, 13, 0, 14, 35, -1, 14, 14, 14, 17, -1, 7, 18, 32, 1, 1, 1, 27, -1, -1, 28, -1, 14, 44, 5, 26, 5, 28, -1, 28, 36, -1, -1, -1, -1, -1, -1, 9, -1, 4, 4, 2, 4, 4, 2, -1, 4, 4, 4, -1, -1, 1, 1, 1, 1, 1, 57, -1, 1, 4, 1, 1, 1, 1, 49, 1, 1, 4, -1, 48, -1, 1, 50, 46, 1, 1, 1, 28, -1, -1, 6, 6, 40, 6, 6, 6, 6, 29, 31, 31, 31, 31, 31, 31, 34, 31, 58, 31, -1, -1, 9, 7, 17, 29, 5, 7, -1, 5, 10, -1, 10, 10, 10, 34, 34, -1, 5, 36, 5, 5, 5, 5, -1, 7, 30, -1, 7, 10, 27, 27, 27, -1, -1, -1, 30, 30, -1, -1, 30, 30, 17, 30, 13, 30, 57, 13, -1, -1, -1, 13, 18, 13, 12, 13, -1, 33, 18, -1, 47, -1, 7, -1, 1, 17, 12, 26, 29, -1, 26, 26, 26, 26, -1, 5, 26, -1, 29, -1, 59, -1, 36, 22, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 0, 1, -1, 1, -1, 1, 37, 1, -1, 50, 50, 1, 6, -1, 7, 34, 38, 24, -1, 20, 12, 17, -1, 11, -1, 33, 38, 13, 27, 31, 8, 31, 18, 31, 8, 7, 18, -1, 25, 25, 25, 25, 7, 8, 57, 29, 33, -1, -1, 55, 27, 55, 27, 33, 18, 33, 7, 48, 33, -1, -1, 24, -1, 9, 7, 25, 5, 22, 0, 21, 21, 21, -1, 18, 22, 36, 15, 22, 5, -1, -1, 10, 19, -1, 36, 19, 5, 10, 10, 5, -1, 6, 10, 10, 10, 44, 10, 1, 10, 21, 7, 21, 21, 55, 44, 30, 30, -1, -1, 30, 54, 30, 19, 19, 19, 39, 19, 19, -1, 19, 19, -1, 23, 16, 51, 20, 20, 2, 55, -1, -1, 9, -1, 27, -1, 2, -1, 35, 43, 47, -1, 43, 2, 16, 6, -1, 39, 15, 11, 3, 3, 43, 18, 43, -1, -1, 49, 46, 49, 0, 3, 29, -1, -1, 3, 29, 2, 17, 20, 3, 24, 3, 46, 38, 38, -1, 14, 23, 1, 55, 25, 33, 7, 7, -1, 33, 7, 16, 9, 52, -1, -1, 25, 7, 52, 2, 24, -1, 2, -1, -1, 52, 2, -1, 22, 13, -1, 35, 13, 22, 23, 21, 21, -1, 21, 7, 21, 14, 25, 0, 8, 48, 19, -1, 12, 4, -1, 24, 19, 19, 19, 19, 0, 19, 4, 19, 19, 11, -1, 19, 19, 35, 35, 11, 15, 25, 25, 25, 25, -1, 12, 25, 56, -1, 0, 8, 0, -1, 18, 5, 0, -1, 21, 14, 5, 20, 20, 21, -1, -1, 49, 0, 21, 18, 43, -1, 21, 8, 49, 21, 39, 20, -1, -1, 1, -1, -1, 20, 8, -1, 3, -1, -1, 3, 3, 22, -1, 1, 7, 7, 8, 27, 28, 7, 3, 46, -1, -1, 8, 8, 1, 2, 45, -1, 8, 28, 8, -1, 2, 14, -1, -1, -1, 41, 9, 4, 13, 10, 37, -1, 4, -1, -1, 12, 17, -1, 54, 4, 35, 4, 42, 30, 37, 0, 49, 37, 20, 18, 10, 10, 0, 29, 29, -1, 10, 7, 7, 10, 10, -1, 17, -1, 23, 5, 10, 0, 0, 5, 1, 0, -1, 35, 11, 5, -1, -1, 18, 12, 0, 12, -1, -1, 0, -1, -1, -1, 0, 4, 26, 0, 23, 23, 23, 1, 47, 1, 1, 21, 3, 23, 1, 1, -1, 12, 3, 4, 21, 17, 45, 3, 37, 11, 20, 12, 2, 3, 20, -1, 17, 8, 8, 57, 11, 2, 14, 14, -1, 38, -1, 36, 2, 27, 2, 2, -1, 10, 8, -1, -1, 28, 2, 4, 1, 9, -1, 48, 1, 1, 2, 10, 13, -1, 14, 4, 26, -1, 0, -1, 14, 0, 14, -1, 27, 15, 10, 54, 51, 0, 31, 10, 16, -1, 0, 0, 0, 5, 50, 36, 0, -1, 17, 33, 16, 23, -1, -1, 4, 4, 23, 9, 23, 6, 15, -1, 59, 0, 1, 3, -1, 1, 1, 5, 3, -1, 1, 31, 31, 1, 2, 16, 41, 53, -1, 41, 9, 17, 3, 10, 33, -1, 23, 8, -1, 44, 44, 13, -1, 33, 23, 25, 25, 8, 6, 0, 0, 43, 11, 8, -1, 50, 5, -1, 11, 21, -1, -1, 5, 20, 2, 4, 54, -1, -1, 45, -1, -1, -1, 4, 20, -1, 31, 21, 14, 43, 49, 46, 47, 14, 0, 0, 0, 14, -1, 16, 1, -1, 1, 50, 33, 0, 44, 14, 0, -1, 0, 37, 0, 0, 0, -1, 0, 14, 20, 5, 28, 18, 1, 1, 0, 6, -1, 10, 10, -1, 17, 1, 1, 0, 0, 21, 4, 31, 21, 21, 51, 43, -1]\n",
            "Skipping coherence calculation due to error: unable to interpret topic as either a list of tokens or a list of ids\n",
            "-------RUN33-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': \"CountVectorizer(ngram_range=(1, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b891dd10>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[3, 24, 28, 28, -1, -1, -1, 15, 15, 15, -1, 20, 12, -1, 15, -1, 50, 3, -1, -1, -1, 12, -1, 9, 9, 25, 45, -1, 18, -1, -1, -1, 28, 19, -1, -1, 1, -1, -1, -1, 9, -1, 30, 9, 30, -1, 30, 30, 9, 35, 30, 9, -1, -1, -1, 14, 14, 24, 52, 52, 40, -1, 52, 15, -1, 15, 1, 40, 12, 1, 19, 12, 4, 4, -1, 1, -1, -1, -1, -1, 14, 14, -1, 14, 14, 25, 14, -1, 18, 4, 4, 15, 14, 48, -1, 15, 15, 4, -1, -1, 20, 32, 47, 41, 26, 19, -1, 10, -1, 10, -1, 12, -1, 3, -1, -1, -1, -1, 35, -1, 30, -1, 9, 32, 30, 31, 12, 18, 9, 12, 18, 10, -1, 12, 12, 3, 9, 35, 12, -1, 15, -1, 3, 31, 10, -1, 3, -1, 24, -1, 15, 15, 9, 15, 26, -1, 23, -1, -1, 33, -1, -1, 19, -1, 10, 3, 3, 32, -1, 47, 15, -1, -1, -1, 10, -1, -1, 38, 10, 12, -1, 35, 10, -1, 12, -1, -1, 18, 12, -1, -1, 10, 14, 18, 31, 6, 17, 17, 14, -1, 14, -1, 14, 14, 38, 14, 6, 14, 4, 46, 23, 34, -1, 14, 14, -1, -1, 4, 18, -1, 17, -1, 23, 23, -1, 17, 45, 45, 7, 31, 31, 31, 31, 17, 17, 31, 25, 31, 0, 17, -1, 1, 1, -1, 17, 26, -1, 32, 27, -1, -1, 17, 20, 3, 53, 5, 5, 5, 23, 23, -1, 32, -1, 26, -1, -1, 55, 45, 48, 1, 1, 1, 1, 23, 1, 26, 1, 1, 25, 45, 4, 53, 1, 5, 5, -1, 17, 33, -1, 33, 20, -1, 20, 20, -1, 17, 15, 26, 17, 1, 1, 1, 12, 1, -1, 1, 40, 1, 29, -1, 9, 1, 30, -1, 38, -1, 53, -1, 32, 3, 37, 31, 26, 12, -1, 3, 43, 3, -1, 3, 33, -1, 44, 3, 21, 3, -1, 10, -1, -1, 10, -1, 25, 25, -1, -1, 10, 10, 0, 10, 54, -1, -1, 24, 42, 44, 10, 44, -1, 3, 32, 12, 47, 12, 39, 1, 12, -1, 25, -1, 39, 11, 12, 10, 42, 12, 0, 42, 18, 0, 7, 7, -1, 42, 42, 42, 39, 42, 39, 25, 34, -1, 39, 7, 7, 7, 7, 7, 7, 7, 48, 7, 17, 24, 17, 51, 7, -1, 31, 31, 25, 39, -1, 41, 31, 25, 31, -1, 21, -1, -1, -1, 48, -1, 21, 21, -1, 21, 23, 4, 23, 55, 4, 1, 4, 4, 4, -1, 4, 4, 4, 50, -1, 21, 3, 1, 38, 1, 23, 15, 19, 1, 1, 1, 21, 24, 1, -1, -1, 1, -1, 1, -1, 24, 17, 1, 1, 1, 1, -1, 4, 5, 18, 4, 50, -1, 1, 5, -1, 4, 50, 33, 4, -1, -1, -1, 33, 5, 26, 5, 20, -1, -1, 25, 33, 20, -1, -1, 8, 3, 8, 8, 3, -1, 38, 17, 17, 12, 46, -1, 3, 3, 8, 8, 8, 8, 8, -1, -1, 8, 1, 8, 36, 44, 46, 13, 36, 8, 3, 47, 8, 46, 36, -1, 4, -1, 3, 36, 5, -1, 5, 5, -1, -1, 1, 4, -1, 32, 5, 5, 18, 5, 5, 11, 11, 11, 11, 0, 5, 5, 18, 20, 21, 0, 0, 0, 0, 0, 0, -1, 11, 36, -1, 13, 13, 13, 36, 44, -1, 11, 11, 9, 48, 11, 36, 11, -1, 19, 1, 0, 0, 25, 39, -1, -1, 0, 9, 0, 0, 25, 0, 0, 32, 0, 0, -1, 7, 0, 25, 25, 19, 48, 6, 6, 6, 6, -1, 6, 11, 6, 6, 4, 4, 20, 24, -1, 10, 3, 41, 6, 6, -1, 6, 6, 6, 4, 6, 6, -1, 4, 4, 4, -1, 4, 8, 4, 4, -1, -1, 1, -1, -1, 8, 7, -1, 8, 31, -1, 8, 3, 8, 3, 8, 3, 8, 38, 8, 9, 15, 9, -1, -1, 19, 8, 8, 8, -1, 8, 3, 8, 13, 44, 8, 3, 32, 5, 5, 5, 5, -1, 5, 5, -1, 40, 5, -1, 12, -1, 5, 10, 28, 4, 12, -1, 11, 11, 41, 11, -1, 46, -1, 11, -1, 28, 0, -1, 10, -1, 11, 35, 49, -1, 0, 0, -1, 26, -1, 0, 0, 24, 0, 0, 26, -1, 0, 13, 13, 0, 20, 13, 20, 0, 7, 10, 0, 13, -1, -1, 13, 13, 13, 18, -1, 9, 35, 31, 2, 2, 2, 19, 26, 20, 23, -1, 13, 35, 7, 20, 7, 23, -1, 23, 26, -1, -1, -1, -1, -1, 37, 1, 1, 6, 6, 23, 6, 6, 4, 6, 6, 6, 6, -1, -1, 2, 2, 2, 2, 2, 54, -1, 2, 6, 2, 2, 2, 2, 43, 2, 2, 6, 32, 38, -1, 2, 49, 41, 2, 2, 2, 23, -1, 3, 8, 8, 3, 8, 8, 8, 8, -1, 29, 29, 29, 29, 29, 29, 32, 29, -1, 29, -1, 6, 1, 9, 18, 30, 7, 9, -1, 7, 16, -1, 16, 16, 16, 32, 32, 37, 7, 26, 7, 7, 7, 7, 35, 9, 28, -1, 9, 16, 19, 19, 19, -1, -1, -1, 28, 28, -1, -1, 28, 28, 18, 28, 10, 28, 54, 10, 18, 10, 20, 10, 24, 10, 14, 10, 46, 25, 35, -1, 40, -1, 9, -1, 2, 18, 14, 20, 30, -1, 20, 20, 20, 20, -1, 7, 20, -1, 30, -1, 35, -1, 26, 15, -1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, -1, 2, -1, 2, 33, 2, -1, 49, 49, 2, 8, -1, 9, 32, 36, 44, -1, 21, 14, 18, -1, 17, 19, 34, 36, 10, 19, 29, 11, 29, 24, 29, 11, 9, 24, -1, 27, 27, 27, 27, 9, 11, 54, 30, 34, -1, -1, 51, 19, 51, 19, 34, 24, 34, 9, 38, 34, -1, 34, 39, 55, 1, 9, 27, 7, 15, 0, 22, 22, 22, -1, 24, 15, 26, 12, 15, 7, -1, -1, 16, 3, 3, 26, 3, 7, 16, 16, 7, -1, 8, 16, 16, 16, 47, 16, 2, 16, 22, 9, 22, 22, 51, -1, 28, 28, -1, -1, 28, -1, 28, 3, 3, 3, 3, 3, 3, -1, 3, 3, -1, 1, 25, -1, 21, 21, 4, 51, 19, -1, 1, -1, 19, -1, -1, 23, -1, 37, 40, -1, 37, 4, 25, 8, -1, 3, 12, 17, 5, 5, 37, -1, 37, -1, 3, 43, 41, 43, 0, 5, 30, -1, -1, 5, 30, 4, 18, 21, 5, 44, 5, 41, 36, 36, -1, 13, 1, 2, 51, 27, 34, 9, 9, -1, 34, 9, -1, 1, 53, -1, -1, 27, 9, 53, 4, 39, -1, 4, 42, 1, 53, 4, -1, 15, 10, 20, 52, 10, 15, 1, 22, 22, -1, 22, 9, 22, 13, 27, 0, 11, 38, 3, 3, 14, -1, -1, -1, 3, 3, 3, 3, 0, 3, 6, 3, 3, 17, -1, 3, 3, 52, 52, 17, 12, 27, 27, 27, 27, 2, 14, 27, 46, -1, 0, 11, 0, -1, 24, 7, 0, -1, 22, 13, 7, 21, 21, 22, -1, -1, 43, 0, 22, 24, 37, -1, 22, 11, 43, 22, 3, 21, -1, -1, 2, -1, -1, 21, 11, 24, 5, -1, -1, 5, 5, 15, -1, 2, 9, 9, 11, 19, 23, 9, 5, 41, -1, -1, 11, 11, 2, 4, 50, -1, 11, 23, 11, -1, 4, 13, 33, -1, 10, 45, 1, 6, 10, 16, 33, -1, 6, -1, -1, 14, 18, -1, -1, 6, 15, 6, 42, 28, 33, 0, 43, 33, 21, 24, 16, 16, 0, 30, 30, -1, 16, 9, 9, 16, 16, 10, 18, -1, 1, 7, 16, 0, 0, 7, 2, 0, -1, 15, 17, 7, -1, -1, 24, 14, 0, 14, -1, -1, 0, -1, -1, -1, 0, 6, 20, 0, 1, 1, 1, 2, 40, 2, 2, 22, 5, 1, 2, 2, -1, 14, 5, 6, 22, 18, 50, 5, 33, 17, 21, 14, 4, 5, 21, -1, 18, 11, 11, 54, 17, 4, 13, 13, 40, 36, 55, 26, -1, 19, 23, 4, 35, 16, 11, -1, -1, 23, 4, 6, 2, 1, -1, 19, 2, 2, 4, -1, 10, 27, 13, 6, 20, -1, 0, -1, 13, 0, 13, -1, 19, 12, 16, -1, -1, 0, 29, 16, 25, -1, 0, 0, 0, 7, 49, 26, 0, 1, 18, 34, 25, 1, -1, -1, 6, 6, 1, 1, 1, 8, 12, 1, 35, 0, 2, 5, -1, 2, 2, 7, 5, -1, 2, 29, 29, 2, 4, 19, 45, 48, -1, 45, 1, 18, 5, 16, -1, -1, 1, 11, 19, 47, -1, 10, -1, 34, 1, 27, 27, 11, 8, 0, 0, 37, 17, 11, -1, 49, 7, 24, 17, 22, -1, 10, 7, 21, 4, 6, -1, -1, -1, 50, -1, -1, -1, 6, 21, -1, 29, 22, 13, 37, 43, 41, 40, 13, 0, 0, 0, 13, -1, 25, 2, -1, 2, 49, 34, 0, 47, 13, 0, -1, 0, 33, 0, 0, 0, -1, 0, 13, 21, 7, 23, 24, 2, 2, 0, 8, 3, 16, 16, 55, 18, 2, 2, 0, 0, 22, 6, 29, 22, 22, -1, -1, -1]\n",
            "Skipping coherence calculation due to error: unable to interpret topic as either a list of tokens or a list of ids\n",
            "-------RUN34-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': \"CountVectorizer(ngram_range=(2, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df4a9008410>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[39, -1, 32, 32, -1, -1, 4, 22, 22, 22, -1, 27, 10, -1, 22, -1, 18, 39, -1, -1, -1, 10, 16, 8, 8, 19, 50, -1, 17, 35, 23, 35, 32, 24, 35, -1, -1, -1, -1, -1, 8, -1, 28, 8, 28, -1, 28, 28, 8, -1, 28, 8, 16, -1, -1, 13, 13, 23, 16, 16, 18, 54, 16, 22, -1, 22, 1, 18, 10, 1, 24, 10, 3, 3, 18, 1, 37, -1, 35, -1, 13, 13, 36, 13, 13, 19, 13, 3, 17, 3, 3, 22, 13, 53, -1, 22, 22, 3, -1, -1, 27, 33, 41, 46, 26, 24, -1, -1, -1, 15, -1, 10, -1, -1, 54, 57, 57, -1, 37, -1, 4, 57, 8, 33, 28, 30, 10, 17, 8, 10, -1, 15, 35, 10, 10, 39, 8, 37, 10, -1, 22, -1, -1, 30, 15, 16, 39, -1, 23, -1, 16, 16, 8, 22, 26, -1, -1, -1, -1, -1, 41, -1, 24, -1, -1, -1, 25, 33, -1, 41, -1, -1, 35, -1, 15, 16, -1, 44, 15, -1, 54, 37, -1, -1, 10, -1, -1, 17, 10, -1, -1, 15, 13, 17, 30, -1, 14, 14, 13, 54, 13, 54, 13, 13, -1, 13, 5, 13, 3, 51, 31, 36, 44, 13, 13, -1, -1, 3, 17, -1, 14, -1, 31, 31, -1, 14, 50, 50, 30, 30, 30, 30, 30, 14, 14, 30, 19, 30, 0, 14, 14, 1, 1, -1, 14, 26, -1, 33, 34, 52, -1, 14, 27, -1, 48, 4, 4, 4, 31, 31, -1, 33, -1, 26, 23, -1, -1, 50, 53, 1, 1, 1, 1, -1, 1, 26, 1, 1, 19, 50, 3, 48, 1, 4, 4, -1, 14, 40, -1, 40, 27, 35, 27, 27, 57, 14, 22, 26, 14, 1, 1, 1, 10, 1, -1, 1, 18, 1, 29, 1, 8, 1, 28, -1, 44, -1, 48, -1, 33, -1, 16, 30, 26, 10, -1, 39, 49, 39, 35, 39, -1, 48, 42, 39, 21, 25, -1, 15, -1, -1, 15, -1, 19, 19, -1, -1, 15, 15, 0, 15, -1, 38, 23, 23, 47, 42, 15, 42, 42, -1, 33, 10, 41, 10, 45, 1, 10, 18, 19, 10, 45, 9, 10, 15, 47, 10, 0, 47, 17, 0, 6, 6, -1, 47, 47, 47, 45, -1, 45, 19, 36, -1, 45, 6, 6, 6, 6, 6, 6, 6, 53, 6, 14, 23, 14, 52, 6, -1, 30, 30, 19, 45, -1, 46, 30, 19, 30, -1, 21, -1, -1, -1, 53, -1, 21, 21, 37, 21, 31, 3, 31, -1, 3, 1, 3, 3, 3, 55, 3, 3, 3, 18, -1, 21, 43, 1, 44, 1, 31, 22, 24, 1, 1, 1, 21, 23, 1, -1, -1, 1, -1, 1, -1, 23, 14, 1, 1, 1, 1, -1, 3, 4, 17, 3, 18, -1, 1, 4, -1, 3, 18, 40, 3, -1, -1, 4, 40, 4, 26, 4, -1, -1, -1, 19, 40, -1, 18, -1, 7, 43, 7, 7, 43, 3, 44, 14, 14, 10, 51, 0, -1, 43, 7, 7, 7, 7, 7, 33, -1, 7, 1, 7, 38, 42, 51, 11, 38, 7, -1, 41, 7, 51, 38, -1, 3, -1, -1, -1, 4, 19, 4, 4, -1, 18, 1, 3, -1, 33, 4, 4, 17, 4, 4, 9, 9, 9, 9, 0, 4, 4, 17, 27, 21, 0, 0, 0, 0, 0, 0, -1, 9, 38, -1, 11, 11, 11, 38, 42, -1, 9, 9, 8, 53, 9, 38, 9, 45, 24, 1, 0, 0, 19, -1, 57, 22, 0, 8, 0, 0, 19, 0, 0, 33, 0, 0, -1, 6, 0, 19, 19, 24, 53, 5, 5, 5, 5, 5, 5, 9, 5, 5, 3, 3, -1, 23, -1, 15, 43, 46, 5, 5, -1, 5, 5, 5, 3, 5, 5, -1, 3, 3, 3, -1, -1, 7, 3, 3, 18, -1, 1, -1, 55, 7, 6, 35, 7, 30, -1, 7, 43, 7, 43, 7, 43, 7, 44, 7, 8, 22, 8, 17, -1, 24, 7, 7, 7, -1, 7, 43, 7, 11, 42, 7, 39, 33, 4, 4, 4, 4, -1, 4, 4, -1, 18, 4, -1, 10, -1, 4, 15, 32, 3, 10, -1, 9, 9, 46, 9, -1, 51, -1, 9, -1, 32, 0, -1, -1, -1, 9, 37, -1, -1, 0, 0, -1, 26, -1, 0, 0, -1, 0, 0, 26, -1, 0, 11, 11, 0, 27, 11, 27, 0, 6, 15, 0, 11, 16, -1, 11, 11, 11, -1, -1, 8, 37, 30, 2, 2, 2, 24, 26, 51, 31, -1, 11, 41, 6, 27, 6, 31, -1, 31, 26, -1, -1, -1, 55, -1, 16, 1, 1, 5, 5, -1, 5, 5, 3, 5, 5, 5, 5, -1, 55, 2, 2, 2, 2, 2, -1, -1, 2, 5, 2, 2, 2, 2, 49, 2, 2, 5, 33, 44, -1, 2, 56, 46, 2, 2, 2, 31, -1, -1, 7, 7, 43, 7, 7, 7, 7, 28, 29, 29, 29, 29, 29, 29, 33, 29, -1, 29, 57, 5, 1, 8, 17, 28, 6, 8, -1, 6, 12, -1, 12, 12, 12, 33, 33, -1, 6, 26, 6, 6, 6, 6, 37, 8, 32, -1, 8, 12, 24, 24, 24, -1, -1, -1, 32, 32, -1, 54, 32, 32, 17, 32, 15, 32, -1, 15, -1, -1, -1, 15, 23, 15, 13, 15, 7, 19, 37, -1, 18, -1, 8, -1, 2, 17, 13, 27, 28, -1, 27, 27, 27, 27, 18, 6, 27, -1, 28, -1, 37, -1, 26, 22, -1, 2, 2, 2, 2, 2, 2, 2, -1, 2, 2, 0, 2, -1, 2, -1, 2, 40, 2, -1, 56, 56, 2, 7, -1, 8, 33, 38, 42, -1, 21, 13, 17, 28, 14, 24, 36, 38, -1, 24, 29, 9, 29, 23, 29, 9, 8, 23, 26, 34, 34, -1, 34, 8, 9, -1, 28, 36, -1, -1, 52, 24, 52, 24, 36, 23, 36, 8, 44, 36, -1, -1, 45, -1, 1, 8, 34, 6, 22, 0, 20, 20, 20, -1, 23, 22, -1, 10, 22, 6, 35, -1, 12, 25, -1, 26, 25, 6, 12, 12, 6, -1, -1, 12, 12, 12, 41, 12, -1, 12, 20, 8, 20, 20, 52, 41, 32, 32, -1, -1, 32, -1, 32, 25, 25, -1, -1, 25, 25, -1, 25, 25, -1, 1, 19, 35, 21, 21, 3, 52, -1, -1, 1, -1, 24, -1, 3, -1, 16, 16, 18, -1, 16, 3, 19, 7, -1, 39, 10, 14, 4, 4, 16, -1, 16, -1, -1, 49, 46, 49, 0, 4, 28, -1, -1, 4, 28, 3, 17, 21, 4, 42, 4, 46, 38, 38, -1, 11, 1, 2, 52, 34, 36, 8, 8, -1, 36, 8, 19, 1, 48, -1, 48, 34, 8, 48, 3, 45, -1, 3, 47, 1, 48, 3, -1, 22, 15, -1, 16, 15, 22, 1, 20, 20, -1, 20, 8, 20, 11, 34, 0, 9, 44, 25, -1, 13, -1, -1, 42, 25, 25, 25, 25, 0, 25, 5, 25, 25, 14, -1, 25, 25, 16, 16, 14, 10, 34, 34, 34, 34, 2, 13, 34, 51, -1, 0, 9, 0, -1, 23, 6, 0, -1, 20, 11, 6, 21, 21, 20, 35, -1, 49, 0, 20, 23, -1, -1, 20, 9, 49, 20, 39, 21, -1, -1, 2, -1, -1, 21, 9, 54, 4, -1, -1, 4, 4, 22, -1, 2, 8, 8, 9, 24, -1, 8, 4, 46, -1, 18, 9, 9, 2, 3, 18, -1, 9, 31, 9, -1, 3, 11, -1, 0, -1, 50, 1, 5, 15, 12, 40, -1, 5, 55, -1, 13, 17, -1, -1, 5, 16, 5, 47, 32, 40, 0, 49, 40, 21, 23, 12, 12, 0, 28, 28, -1, 12, 8, 8, 12, 12, 15, 17, -1, 1, 6, 12, 0, 0, 6, 2, 0, -1, 16, 14, 6, -1, -1, 23, 13, 0, 13, -1, -1, 0, -1, -1, -1, 0, 5, 27, 0, 1, 1, 1, 2, 18, 2, 2, 20, 4, 1, 2, 2, 31, 13, 4, 5, 20, 17, -1, 4, 40, 14, 21, 13, 3, 4, 21, -1, 17, 9, 9, -1, 14, 3, 11, 11, 18, 38, -1, 26, 3, 24, 31, 3, 37, 12, 9, 55, -1, 31, 3, 5, 2, 1, -1, 44, 2, -1, 3, 12, -1, -1, 11, 5, 27, -1, 0, -1, 11, 0, 11, -1, 24, 10, 12, 23, 35, 0, 29, 12, 19, -1, 0, 0, 0, 6, 56, 26, 0, 1, 17, 36, 19, 1, -1, -1, 5, 5, 1, 1, 1, 7, 10, 1, 37, 0, 2, 4, -1, 2, 2, 6, 4, -1, 2, 29, 29, 2, 3, -1, 50, 53, -1, 50, 1, 17, 4, 12, 36, -1, 1, 9, 24, 41, -1, 15, -1, 36, 1, 34, 34, 9, 7, 0, 0, 16, 14, 9, -1, 56, 6, -1, 14, 20, 37, -1, 6, 21, 3, 5, 23, -1, -1, 18, -1, -1, -1, 5, 21, 18, 29, 20, 11, 16, 49, 46, 18, 11, 0, 0, 0, 11, -1, 19, 2, -1, 2, 56, 36, 0, 41, 11, 0, -1, 0, -1, 0, 0, 0, -1, 0, 11, 21, 6, 31, 23, 2, 2, 0, 7, -1, 12, 12, -1, 17, 2, 2, 0, 0, 20, 5, 29, 20, 20, 35, -1, -1]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------RUN35-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': \"CountVectorizer(stop_words='english')\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[11, 0, 10, 10, 5, 0, 5, 1, 1, 1, 1, 10, 5, 10, 1, 0, 15, 11, 1, 0, 1, 5, 1, 16, 16, 8, 3, 0, 0, 0, 0, 0, 10, 8, 0, 1, 7, 6, 0, 0, 16, 0, 5, 16, 5, 5, 5, 5, 16, 0, 5, 16, 1, 1, 1, 3, 3, 0, 1, 1, 15, 0, 1, 1, 0, 1, 7, 7, 7, 7, 8, 7, 6, 6, 15, 7, 2, 2, 0, 0, 3, 3, 8, 3, 3, 8, 3, 6, 0, 6, 6, 1, 3, 7, 1, 1, 1, 6, 1, 8, 10, 2, 2, 2, 0, 8, 0, 1, 1, 1, 7, 5, 5, 11, 0, 8, 8, 8, 2, 1, 5, 8, 16, 2, 5, 12, 5, 0, 16, 5, 0, 1, 0, 5, 5, 11, 16, 2, 5, 5, 1, 1, 11, 12, 1, 1, 11, 2, 0, 2, 1, 1, 16, 1, 2, 1, 6, 13, 2, 4, 2, 4, 8, 4, 1, 11, 11, 2, 8, 2, 1, 1, 0, 2, 1, 1, 4, 1, 1, 7, 0, 2, 1, 2, 5, 1, 15, 0, 5, 3, 3, 1, 3, 0, 12, 3, 13, 13, 3, 0, 3, 0, 3, 3, 8, 3, 3, 3, 6, 10, 6, 8, 1, 3, 3, 8, 8, 6, 0, 6, 13, 6, 6, 6, 1, 13, 3, 3, 12, 9, 12, 12, 12, 13, 13, 12, 8, 12, 9, 13, 13, 7, 7, 2, 13, 0, 2, 2, 13, 0, 2, 13, 10, 11, 2, 5, 5, 5, 6, 6, 5, 2, 13, 0, 0, 0, 1, 3, 7, 7, 7, 7, 7, 6, 7, 0, 7, 7, 8, 3, 6, 2, 7, 5, 5, 0, 13, 4, 10, 4, 10, 0, 10, 10, 8, 13, 1, 0, 13, 7, 7, 7, 7, 7, 7, 7, 15, 7, 2, 7, 16, 7, 5, 0, 1, 5, 2, 2, 2, 11, 1, 12, 0, 7, 2, 11, 2, 11, 0, 11, 4, 2, 3, 11, 19, 11, 1, 1, 6, 2, 1, 5, 8, 8, 1, 1, 1, 1, 9, 1, 2, 3, 0, 0, 1, 3, 1, 3, 3, 11, 2, 5, 2, 5, 3, 7, 5, 15, 8, 7, 3, 17, 5, 1, 1, 5, 9, 1, 0, 9, 12, 12, 0, 1, 1, 1, 3, 1, 3, 8, 8, 1, 3, 12, 12, 12, 12, 12, 12, 12, 7, 12, 13, 0, 13, 0, 12, 15, 12, 12, 8, 3, 0, 2, 12, 8, 12, 2, 19, 2, 2, 6, 7, 1, 19, 19, 2, 19, 6, 6, 6, 1, 6, 7, 6, 6, 6, 6, 6, 6, 6, 15, 6, 19, 11, 7, 8, 7, 6, 1, 8, 7, 7, 7, 19, 0, 7, 1, 15, 7, 13, 7, 2, 0, 13, 7, 7, 7, 7, 6, 6, 5, 0, 6, 15, 6, 7, 5, 15, 6, 15, 4, 6, 3, 10, 5, 4, 5, 0, 5, 10, 2, 0, 8, 4, 10, 15, 0, 14, 11, 14, 14, 11, 6, 1, 13, 13, 5, 10, 9, 11, 11, 14, 14, 14, 14, 14, 2, 3, 14, 7, 14, 3, 3, 14, 18, 3, 14, 11, 2, 14, 14, 3, 13, 6, 1, 11, 3, 5, 8, 5, 5, 15, 15, 7, 6, 8, 2, 5, 5, 0, 5, 5, 17, 17, 17, 17, 9, 5, 5, 0, 10, 19, 9, 9, 9, 9, 9, 9, 11, 17, 3, 3, 18, 18, 18, 3, 3, 6, 17, 17, 16, 7, 17, 3, 17, 3, 8, 7, 9, 9, 8, 3, 8, 1, 9, 16, 9, 9, 8, 9, 9, 2, 9, 9, 0, 12, 9, 8, 8, 8, 7, 3, 3, 3, 3, 3, 3, 17, 3, 3, 6, 6, 10, 0, 10, 1, 11, 2, 3, 3, 3, 3, 3, 3, 6, 3, 3, 6, 6, 6, 6, 15, 6, 14, 6, 6, 15, 1, 7, 15, 6, 14, 12, 0, 14, 12, 2, 14, 11, 14, 11, 14, 11, 14, 1, 14, 16, 1, 16, 0, 1, 8, 14, 14, 14, 0, 14, 11, 14, 18, 3, 14, 11, 2, 5, 5, 5, 5, 1, 5, 5, 2, 15, 5, 4, 5, 5, 5, 1, 10, 6, 5, 5, 17, 17, 2, 17, 0, 14, 0, 2, 1, 10, 9, 2, 0, 5, 17, 2, 4, 0, 9, 9, 7, 0, 7, 9, 9, 0, 9, 9, 0, 0, 9, 18, 18, 9, 10, 18, 10, 9, 12, 1, 9, 18, 1, 15, 18, 18, 18, 0, 1, 16, 0, 12, 4, 4, 4, 8, 0, 10, 6, 11, 18, 2, 12, 10, 12, 6, 15, 6, 0, 1, 11, 4, 6, 15, 1, 7, 7, 3, 3, 6, 3, 3, 6, 3, 3, 3, 3, 6, 6, 4, 4, 4, 4, 4, 2, 2, 4, 3, 4, 4, 4, 4, 2, 4, 4, 3, 2, 1, 6, 4, 11, 2, 4, 4, 4, 6, 2, 11, 14, 14, 11, 14, 14, 14, 14, 5, 2, 2, 2, 2, 2, 2, 2, 2, 8, 2, 8, 3, 7, 16, 0, 5, 12, 16, 5, 12, 10, 18, 10, 10, 10, 2, 2, 1, 12, 0, 12, 12, 12, 12, 2, 16, 10, 0, 16, 10, 8, 8, 8, 0, 2, 0, 10, 10, 15, 0, 10, 10, 0, 10, 1, 10, 2, 1, 0, 1, 10, 1, 0, 1, 3, 1, 14, 8, 0, 0, 7, 7, 16, 7, 4, 0, 3, 10, 5, 6, 10, 10, 10, 10, 15, 12, 10, 0, 5, 7, 2, 0, 0, 1, 15, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 9, 4, 0, 4, 11, 4, 4, 4, 1, 4, 11, 4, 14, 2, 16, 2, 3, 3, 8, 19, 3, 0, 5, 13, 8, 8, 3, 1, 8, 2, 17, 2, 0, 2, 17, 16, 0, 0, 13, 13, 2, 13, 16, 17, 18, 5, 8, 15, 8, 0, 8, 0, 8, 8, 0, 8, 16, 1, 8, 8, 8, 3, 1, 7, 16, 13, 12, 1, 9, 4, 4, 4, 0, 0, 1, 1, 7, 1, 12, 0, 1, 10, 11, 11, 0, 11, 12, 10, 10, 12, 0, 14, 10, 10, 10, 2, 10, 4, 10, 4, 16, 4, 4, 0, 2, 10, 10, 2, 2, 10, 0, 10, 11, 11, 11, 11, 11, 11, 0, 11, 11, 0, 7, 8, 0, 19, 19, 6, 0, 8, 1, 7, 7, 8, 5, 6, 6, 1, 1, 15, 1, 1, 6, 8, 14, 0, 11, 5, 13, 5, 5, 1, 0, 1, 0, 11, 2, 2, 2, 9, 5, 5, 3, 1, 5, 5, 6, 0, 19, 5, 3, 5, 2, 3, 3, 1, 18, 7, 4, 0, 13, 8, 16, 16, 5, 8, 16, 8, 7, 2, 15, 2, 13, 16, 2, 6, 3, 18, 6, 1, 7, 2, 6, 6, 1, 1, 10, 1, 1, 1, 7, 4, 4, 15, 4, 16, 4, 18, 13, 9, 17, 1, 11, 11, 3, 3, 1, 3, 11, 11, 11, 11, 9, 11, 3, 11, 11, 13, 3, 11, 11, 1, 1, 13, 5, 13, 13, 13, 13, 4, 3, 13, 14, 11, 9, 17, 9, 1, 0, 12, 9, 15, 4, 18, 12, 19, 19, 4, 0, 0, 2, 9, 4, 0, 1, 7, 4, 17, 2, 4, 11, 19, 2, 0, 4, 6, 3, 19, 17, 0, 5, 6, 6, 5, 5, 1, 5, 4, 16, 16, 17, 8, 6, 16, 5, 2, 1, 15, 17, 17, 4, 6, 15, 0, 17, 6, 17, 0, 6, 18, 4, 9, 1, 3, 7, 3, 1, 10, 4, 1, 3, 6, 5, 3, 0, 2, 0, 3, 1, 3, 1, 10, 4, 9, 2, 4, 19, 0, 10, 10, 9, 5, 5, 0, 10, 16, 16, 10, 10, 0, 0, 1, 7, 12, 10, 9, 9, 12, 4, 9, 2, 1, 13, 12, 0, 4, 0, 3, 9, 3, 0, 15, 9, 5, 2, 2, 9, 3, 10, 9, 7, 7, 7, 4, 7, 4, 4, 4, 5, 7, 4, 4, 15, 3, 5, 3, 4, 0, 15, 5, 4, 13, 19, 3, 6, 5, 19, 15, 0, 17, 17, 18, 13, 6, 18, 18, 7, 3, 1, 0, 6, 8, 6, 6, 0, 10, 17, 6, 2, 6, 6, 3, 4, 7, 0, 8, 4, 4, 6, 10, 0, 2, 18, 3, 10, 6, 9, 5, 18, 9, 18, 0, 8, 7, 10, 0, 0, 9, 2, 10, 8, 1, 9, 9, 9, 12, 4, 0, 9, 7, 0, 8, 8, 7, 0, 0, 3, 3, 7, 7, 7, 14, 5, 7, 2, 9, 4, 5, 0, 4, 4, 12, 5, 0, 4, 2, 2, 4, 6, 8, 3, 7, 7, 3, 7, 0, 5, 10, 8, 13, 7, 17, 8, 2, 2, 1, 1, 8, 7, 13, 13, 17, 14, 9, 9, 1, 13, 17, 6, 4, 12, 0, 13, 4, 2, 8, 12, 19, 6, 3, 0, 15, 1, 15, 6, 2, 0, 3, 19, 15, 2, 4, 18, 1, 2, 2, 15, 18, 9, 9, 9, 18, 6, 8, 4, 1, 4, 4, 8, 9, 2, 18, 9, 2, 9, 4, 9, 9, 9, 0, 9, 18, 19, 12, 6, 0, 4, 4, 9, 14, 11, 10, 10, 1, 0, 4, 4, 9, 9, 4, 3, 2, 4, 4, 0, 1, 1]\n",
            "-------RUN36-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': 'CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7df3b00f5c90>)', 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[10, 0, 6, 6, 8, 0, 8, 5, 5, 5, 0, 6, 2, 6, 5, 0, 1, 10, 5, 0, 5, 2, 5, 15, 15, 9, 3, 0, 0, 0, 0, 0, 6, 13, 0, 5, 2, 1, 0, 0, 15, 0, 8, 15, 8, 8, 8, 8, 15, 0, 8, 15, 5, 5, 5, 3, 3, 0, 5, 5, 2, 5, 5, 5, 0, 5, 2, 2, 2, 2, 9, 2, 1, 1, 1, 2, 0, 12, 0, 0, 3, 3, 9, 3, 3, 9, 3, 1, 0, 1, 1, 5, 3, 2, 5, 5, 5, 1, 13, 9, 6, 12, 0, 12, 0, 9, 13, 13, 13, 13, 2, 8, 8, 10, 0, 9, 9, 9, 0, 5, 8, 9, 15, 12, 8, 11, 2, 0, 15, 2, 0, 13, 0, 2, 2, 10, 15, 0, 2, 8, 5, 5, 10, 11, 13, 5, 10, 12, 0, 12, 5, 5, 15, 5, 12, 5, 1, 10, 12, 4, 0, 4, 9, 4, 13, 10, 10, 12, 9, 0, 5, 5, 0, 19, 13, 5, 10, 9, 13, 2, 5, 0, 13, 19, 2, 5, 2, 0, 2, 3, 3, 13, 3, 0, 7, 3, 14, 14, 3, 0, 3, 0, 3, 3, 9, 3, 3, 3, 1, 6, 1, 9, 9, 3, 3, 9, 9, 1, 0, 1, 14, 1, 1, 1, 5, 14, 3, 3, 11, 7, 7, 7, 7, 14, 14, 7, 9, 7, 7, 14, 5, 2, 2, 12, 14, 0, 0, 12, 14, 0, 12, 14, 6, 10, 12, 8, 8, 8, 1, 1, 8, 12, 14, 12, 0, 0, 0, 3, 2, 2, 2, 2, 2, 1, 2, 0, 2, 2, 9, 3, 1, 12, 2, 8, 8, 0, 14, 4, 6, 4, 6, 0, 6, 6, 9, 14, 5, 0, 14, 2, 2, 2, 2, 2, 2, 2, 2, 2, 19, 2, 15, 2, 8, 0, 9, 8, 12, 12, 12, 10, 5, 8, 0, 2, 12, 10, 12, 10, 0, 10, 4, 12, 3, 10, 18, 10, 5, 13, 1, 10, 13, 8, 9, 9, 5, 5, 13, 13, 7, 13, 10, 3, 0, 0, 5, 3, 13, 3, 3, 10, 12, 2, 0, 8, 3, 2, 2, 1, 9, 2, 3, 16, 8, 13, 5, 2, 7, 5, 0, 7, 11, 11, 0, 5, 5, 5, 3, 5, 3, 9, 9, 5, 3, 11, 11, 11, 11, 11, 11, 11, 2, 11, 14, 0, 14, 0, 11, 2, 7, 7, 9, 3, 0, 12, 7, 9, 7, 19, 18, 12, 12, 1, 2, 5, 18, 18, 12, 18, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 18, 10, 2, 9, 2, 1, 5, 9, 2, 2, 2, 18, 0, 2, 5, 1, 2, 14, 2, 12, 0, 14, 2, 2, 2, 2, 1, 1, 8, 0, 1, 1, 1, 2, 8, 2, 1, 1, 4, 1, 3, 6, 8, 4, 8, 0, 8, 6, 12, 0, 9, 4, 6, 2, 0, 6, 10, 6, 6, 10, 1, 9, 14, 14, 2, 6, 7, 10, 10, 6, 6, 6, 6, 6, 12, 3, 6, 2, 6, 3, 3, 6, 17, 3, 6, 10, 0, 6, 6, 3, 14, 1, 5, 10, 3, 8, 9, 8, 8, 1, 1, 2, 1, 9, 12, 8, 8, 0, 8, 8, 16, 16, 16, 16, 7, 8, 8, 0, 6, 18, 7, 7, 7, 7, 7, 7, 10, 16, 3, 3, 17, 17, 17, 3, 3, 1, 16, 16, 15, 2, 16, 3, 16, 3, 9, 2, 7, 7, 9, 3, 9, 5, 7, 15, 7, 7, 9, 7, 7, 12, 7, 7, 0, 11, 7, 9, 9, 13, 2, 3, 3, 3, 3, 3, 3, 16, 3, 3, 1, 1, 6, 0, 6, 13, 10, 12, 3, 3, 3, 3, 3, 3, 1, 3, 3, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 13, 2, 1, 1, 6, 11, 0, 6, 11, 0, 6, 10, 6, 10, 6, 10, 6, 9, 6, 15, 5, 15, 0, 5, 9, 6, 6, 6, 0, 6, 10, 6, 17, 3, 6, 10, 12, 8, 8, 8, 8, 5, 8, 8, 0, 2, 8, 4, 2, 8, 8, 13, 6, 1, 2, 8, 16, 16, 12, 16, 0, 6, 5, 16, 13, 6, 7, 0, 13, 8, 16, 0, 10, 0, 7, 7, 2, 0, 2, 7, 7, 0, 7, 7, 0, 0, 7, 10, 17, 7, 6, 17, 6, 7, 11, 13, 7, 17, 5, 1, 17, 17, 17, 0, 5, 15, 0, 7, 4, 4, 4, 13, 0, 6, 1, 10, 17, 0, 11, 6, 11, 1, 1, 1, 0, 5, 10, 4, 1, 1, 5, 2, 2, 3, 3, 1, 3, 3, 1, 3, 3, 3, 3, 1, 1, 4, 4, 4, 4, 4, 10, 19, 4, 3, 4, 4, 4, 4, 12, 4, 4, 3, 12, 9, 1, 4, 10, 12, 4, 4, 4, 1, 18, 10, 6, 6, 10, 6, 6, 6, 6, 8, 19, 19, 19, 19, 19, 19, 12, 19, 9, 19, 9, 3, 2, 15, 0, 8, 11, 15, 8, 11, 11, 11, 11, 11, 11, 12, 12, 5, 11, 0, 11, 11, 11, 11, 0, 15, 6, 0, 15, 11, 9, 9, 13, 0, 12, 0, 6, 6, 1, 0, 6, 6, 0, 6, 13, 6, 10, 13, 0, 13, 6, 13, 0, 13, 3, 13, 6, 9, 0, 0, 2, 2, 15, 2, 4, 0, 3, 6, 8, 1, 6, 6, 6, 6, 1, 11, 6, 0, 8, 2, 0, 0, 0, 5, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 7, 4, 12, 4, 10, 4, 4, 4, 5, 10, 10, 4, 6, 18, 15, 12, 3, 3, 9, 18, 3, 0, 8, 14, 9, 9, 3, 13, 9, 19, 16, 19, 0, 19, 16, 15, 0, 12, 14, 14, 12, 14, 15, 16, 10, 8, 9, 1, 9, 0, 9, 0, 9, 9, 0, 9, 15, 9, 9, 9, 9, 3, 5, 2, 15, 14, 11, 5, 7, 4, 4, 4, 0, 0, 5, 5, 2, 5, 11, 0, 5, 11, 10, 10, 0, 10, 11, 11, 11, 11, 0, 6, 11, 11, 11, 0, 11, 4, 11, 4, 15, 4, 4, 0, 0, 6, 6, 0, 0, 6, 0, 6, 10, 10, 10, 10, 10, 10, 0, 10, 10, 12, 2, 9, 0, 18, 18, 1, 0, 9, 5, 2, 2, 13, 8, 1, 1, 5, 5, 2, 5, 5, 1, 9, 6, 0, 10, 2, 14, 8, 8, 5, 0, 5, 0, 10, 12, 12, 12, 7, 8, 8, 3, 5, 8, 8, 1, 0, 18, 8, 3, 8, 12, 3, 3, 5, 17, 2, 4, 0, 14, 9, 15, 15, 8, 9, 15, 9, 2, 12, 14, 12, 14, 15, 12, 1, 3, 19, 1, 5, 2, 12, 1, 1, 5, 13, 6, 5, 13, 5, 2, 4, 4, 14, 4, 15, 4, 17, 14, 7, 16, 9, 10, 10, 3, 3, 5, 3, 10, 10, 10, 10, 7, 10, 3, 10, 10, 14, 3, 10, 10, 5, 5, 14, 2, 14, 14, 14, 14, 4, 3, 14, 6, 4, 7, 16, 7, 13, 0, 11, 7, 1, 4, 17, 11, 18, 18, 4, 0, 0, 12, 7, 4, 0, 5, 2, 4, 16, 12, 4, 10, 18, 12, 0, 4, 1, 3, 18, 16, 0, 8, 1, 1, 8, 8, 5, 8, 4, 15, 15, 16, 9, 1, 15, 8, 12, 13, 1, 16, 16, 4, 1, 1, 0, 16, 1, 16, 0, 1, 17, 4, 7, 13, 3, 2, 3, 13, 11, 4, 5, 3, 1, 8, 3, 0, 0, 0, 3, 5, 3, 5, 6, 4, 7, 12, 4, 18, 0, 11, 11, 7, 8, 8, 0, 11, 15, 15, 11, 11, 0, 0, 13, 2, 11, 11, 7, 7, 11, 4, 7, 12, 5, 14, 11, 0, 4, 0, 3, 7, 3, 0, 1, 7, 8, 19, 0, 7, 3, 6, 7, 2, 2, 2, 4, 2, 4, 4, 4, 8, 2, 4, 4, 1, 3, 8, 3, 4, 0, 1, 8, 4, 14, 18, 3, 1, 8, 18, 1, 0, 16, 16, 10, 14, 1, 17, 17, 2, 3, 5, 0, 1, 9, 1, 1, 0, 11, 16, 1, 12, 1, 1, 3, 4, 2, 0, 9, 4, 4, 1, 11, 13, 12, 17, 3, 6, 1, 7, 8, 17, 7, 17, 0, 13, 2, 11, 0, 0, 7, 19, 11, 9, 5, 7, 7, 7, 11, 10, 0, 7, 2, 0, 9, 9, 2, 0, 13, 3, 3, 2, 2, 2, 6, 2, 2, 0, 7, 4, 8, 12, 4, 4, 11, 8, 0, 4, 19, 19, 4, 1, 9, 3, 2, 2, 3, 2, 0, 8, 11, 9, 10, 2, 16, 13, 0, 0, 13, 13, 9, 2, 14, 14, 16, 6, 7, 7, 5, 14, 16, 1, 10, 11, 0, 14, 4, 0, 13, 11, 18, 1, 3, 0, 1, 13, 1, 1, 12, 0, 3, 18, 1, 19, 4, 17, 5, 12, 12, 2, 17, 7, 7, 7, 17, 1, 9, 4, 5, 4, 10, 9, 7, 0, 17, 7, 0, 7, 4, 7, 7, 7, 0, 7, 17, 18, 11, 1, 0, 4, 4, 7, 6, 10, 11, 11, 13, 0, 4, 4, 7, 7, 4, 3, 19, 4, 4, 0, 5, 13]\n",
            "-------RUN37-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': \"CountVectorizer(stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b02d2c50>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[4, 0, 5, 5, 12, 0, 12, 6, 6, 6, 0, 5, 2, 5, 6, 0, 1, 4, 0, 0, 6, 2, 6, 15, 15, 9, 3, 0, 0, 0, 0, 0, 5, 13, 0, 6, 2, 1, 0, 0, 15, 0, 12, 15, 12, 9, 12, 12, 15, 0, 12, 15, 6, 6, 6, 3, 3, 0, 6, 6, 2, 0, 6, 6, 0, 6, 2, 2, 2, 2, 13, 2, 1, 1, 1, 2, 0, 10, 0, 0, 3, 3, 9, 3, 3, 9, 3, 1, 0, 1, 1, 6, 3, 2, 6, 6, 6, 1, 0, 9, 5, 10, 0, 10, 0, 13, 0, 13, 13, 13, 2, 2, 12, 4, 0, 9, 9, 9, 0, 6, 12, 9, 15, 10, 12, 11, 2, 0, 15, 2, 0, 13, 0, 2, 2, 4, 15, 0, 2, 9, 6, 6, 4, 11, 13, 6, 4, 10, 0, 10, 6, 6, 15, 6, 10, 6, 1, 4, 10, 7, 10, 7, 13, 7, 13, 4, 4, 10, 9, 0, 6, 6, 0, 10, 13, 6, 7, 9, 13, 2, 0, 0, 13, 10, 2, 6, 1, 0, 2, 3, 3, 13, 3, 0, 11, 3, 14, 14, 3, 0, 3, 0, 3, 3, 9, 3, 3, 3, 1, 5, 1, 9, 13, 3, 3, 9, 9, 1, 0, 1, 14, 1, 1, 1, 6, 14, 3, 3, 11, 8, 11, 11, 11, 14, 14, 11, 9, 8, 8, 14, 14, 2, 2, 10, 14, 0, 0, 10, 14, 0, 10, 14, 5, 4, 10, 12, 12, 12, 1, 1, 9, 10, 14, 0, 0, 0, 0, 3, 2, 2, 2, 2, 2, 1, 2, 0, 2, 2, 9, 3, 1, 10, 2, 12, 12, 0, 14, 7, 5, 7, 5, 0, 5, 5, 9, 14, 6, 0, 14, 2, 2, 2, 2, 2, 2, 2, 2, 2, 19, 2, 15, 2, 12, 0, 9, 9, 10, 10, 10, 4, 6, 11, 0, 2, 10, 4, 10, 4, 0, 4, 7, 10, 3, 4, 17, 4, 6, 13, 1, 4, 13, 9, 9, 9, 6, 6, 13, 13, 8, 13, 10, 3, 0, 0, 6, 3, 13, 3, 3, 4, 10, 2, 0, 12, 3, 2, 2, 1, 9, 2, 3, 16, 2, 13, 6, 2, 8, 6, 0, 8, 11, 11, 0, 6, 6, 6, 3, 6, 3, 9, 9, 6, 3, 11, 11, 11, 11, 11, 11, 11, 2, 11, 14, 0, 14, 0, 11, 2, 8, 8, 9, 3, 0, 10, 11, 9, 8, 19, 17, 10, 10, 1, 2, 6, 17, 17, 10, 17, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 17, 4, 2, 9, 2, 1, 6, 13, 2, 2, 2, 17, 0, 2, 6, 1, 2, 14, 2, 10, 0, 14, 2, 2, 2, 2, 1, 1, 12, 0, 1, 1, 1, 2, 12, 2, 1, 1, 7, 1, 3, 5, 12, 7, 12, 0, 12, 5, 10, 0, 9, 7, 5, 2, 0, 5, 4, 5, 5, 4, 1, 9, 14, 14, 2, 5, 8, 4, 4, 5, 5, 5, 5, 5, 10, 3, 5, 2, 5, 3, 3, 5, 4, 3, 5, 4, 0, 5, 5, 3, 14, 1, 6, 4, 3, 12, 9, 12, 12, 1, 1, 2, 1, 9, 10, 12, 12, 0, 12, 12, 16, 16, 16, 16, 8, 12, 12, 0, 5, 17, 8, 8, 8, 8, 8, 8, 4, 16, 3, 3, 4, 4, 4, 3, 3, 1, 16, 16, 15, 2, 16, 3, 16, 3, 13, 2, 8, 8, 9, 3, 9, 6, 8, 15, 8, 8, 9, 8, 8, 10, 8, 8, 0, 11, 8, 9, 9, 13, 2, 3, 3, 3, 3, 3, 3, 16, 3, 3, 1, 1, 5, 0, 5, 13, 4, 10, 3, 3, 3, 3, 3, 3, 1, 3, 3, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 13, 2, 1, 1, 5, 11, 0, 5, 11, 0, 5, 4, 5, 4, 5, 4, 5, 9, 5, 15, 6, 15, 0, 6, 9, 5, 5, 5, 0, 5, 4, 5, 4, 3, 5, 4, 10, 12, 12, 12, 12, 6, 12, 12, 0, 2, 12, 7, 2, 12, 12, 13, 5, 1, 2, 9, 16, 16, 10, 16, 0, 5, 6, 16, 13, 5, 8, 0, 13, 12, 16, 10, 4, 0, 8, 8, 2, 0, 2, 8, 8, 0, 8, 8, 0, 0, 8, 4, 4, 8, 5, 4, 5, 8, 11, 13, 8, 4, 6, 1, 4, 4, 4, 0, 13, 15, 0, 8, 7, 7, 7, 13, 0, 5, 1, 4, 4, 0, 11, 5, 11, 1, 1, 1, 0, 6, 4, 7, 1, 1, 6, 2, 2, 3, 3, 1, 3, 3, 1, 3, 3, 3, 3, 1, 1, 7, 7, 7, 7, 7, 4, 10, 7, 3, 7, 7, 7, 7, 10, 7, 7, 3, 10, 9, 1, 7, 4, 10, 7, 7, 7, 1, 10, 4, 5, 5, 4, 5, 5, 5, 5, 12, 19, 19, 19, 19, 19, 19, 10, 19, 9, 19, 9, 3, 2, 15, 0, 12, 11, 15, 12, 11, 11, 4, 11, 11, 11, 10, 10, 6, 11, 0, 11, 11, 11, 11, 0, 15, 5, 0, 15, 11, 13, 13, 13, 0, 10, 0, 5, 5, 1, 0, 5, 5, 0, 5, 13, 5, 4, 13, 0, 13, 5, 13, 0, 13, 3, 13, 5, 9, 0, 0, 2, 2, 15, 2, 7, 0, 3, 5, 12, 1, 5, 5, 5, 5, 1, 11, 5, 0, 12, 2, 0, 0, 0, 6, 1, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 7, 0, 7, 4, 7, 7, 7, 6, 4, 4, 7, 5, 10, 15, 10, 3, 3, 9, 17, 3, 0, 12, 14, 9, 9, 3, 13, 13, 19, 16, 19, 0, 19, 16, 15, 0, 0, 14, 14, 10, 14, 15, 16, 4, 12, 9, 1, 9, 0, 13, 0, 13, 9, 0, 9, 15, 13, 9, 9, 9, 3, 0, 2, 15, 14, 11, 6, 8, 18, 18, 18, 0, 0, 6, 6, 2, 6, 11, 0, 6, 11, 4, 4, 0, 4, 11, 11, 11, 11, 0, 5, 11, 11, 11, 0, 11, 7, 11, 18, 15, 18, 18, 0, 0, 5, 5, 10, 10, 5, 0, 5, 4, 4, 4, 4, 4, 4, 0, 4, 4, 0, 2, 9, 0, 17, 17, 1, 0, 9, 6, 2, 2, 13, 9, 1, 1, 6, 6, 2, 6, 6, 1, 9, 5, 0, 4, 2, 14, 12, 12, 6, 0, 6, 0, 4, 10, 10, 10, 8, 12, 12, 3, 6, 12, 12, 1, 0, 17, 12, 3, 12, 10, 3, 3, 0, 4, 2, 7, 0, 14, 9, 15, 15, 9, 9, 15, 9, 2, 10, 14, 10, 14, 15, 10, 1, 3, 19, 1, 6, 2, 10, 1, 1, 6, 13, 5, 6, 13, 6, 2, 18, 18, 14, 18, 15, 18, 4, 14, 8, 16, 9, 4, 4, 3, 3, 6, 3, 4, 4, 4, 4, 8, 4, 3, 4, 4, 14, 3, 4, 4, 6, 6, 14, 2, 14, 14, 14, 14, 7, 3, 14, 5, 4, 8, 16, 8, 13, 0, 11, 8, 1, 18, 4, 11, 17, 17, 18, 0, 0, 10, 8, 18, 0, 6, 2, 18, 16, 10, 18, 4, 17, 10, 0, 7, 1, 3, 17, 16, 0, 12, 1, 1, 12, 12, 6, 12, 7, 15, 15, 16, 9, 1, 15, 12, 10, 13, 1, 16, 16, 7, 1, 1, 0, 16, 1, 16, 0, 1, 4, 7, 8, 13, 3, 2, 3, 13, 11, 7, 6, 3, 1, 9, 3, 0, 10, 0, 3, 6, 3, 6, 5, 7, 8, 10, 7, 17, 0, 11, 11, 8, 12, 12, 0, 11, 15, 15, 11, 11, 13, 0, 13, 2, 11, 11, 8, 8, 11, 7, 8, 10, 6, 14, 11, 0, 7, 0, 3, 8, 3, 0, 1, 8, 9, 19, 0, 8, 3, 5, 8, 2, 2, 2, 7, 2, 7, 7, 18, 12, 2, 7, 7, 1, 3, 12, 3, 18, 0, 1, 12, 7, 14, 17, 3, 1, 12, 17, 1, 0, 16, 16, 4, 14, 1, 4, 4, 2, 3, 0, 0, 1, 13, 1, 1, 0, 11, 16, 1, 10, 1, 1, 3, 7, 2, 0, 9, 7, 7, 1, 11, 13, 10, 4, 3, 5, 1, 8, 9, 4, 8, 4, 0, 13, 2, 11, 0, 0, 8, 19, 11, 9, 6, 8, 8, 8, 11, 4, 0, 8, 2, 0, 9, 9, 2, 0, 13, 3, 3, 2, 2, 2, 5, 2, 2, 10, 8, 7, 12, 0, 7, 7, 11, 12, 0, 7, 19, 19, 7, 1, 9, 3, 2, 2, 3, 2, 0, 12, 11, 9, 10, 2, 16, 13, 0, 10, 13, 13, 9, 2, 14, 14, 16, 5, 8, 8, 6, 14, 16, 1, 4, 11, 0, 14, 18, 10, 13, 11, 17, 1, 3, 0, 2, 13, 1, 1, 10, 0, 3, 17, 1, 19, 18, 4, 6, 10, 10, 2, 4, 8, 8, 8, 4, 1, 9, 7, 6, 7, 4, 9, 8, 0, 4, 8, 5, 8, 7, 8, 8, 8, 0, 8, 4, 17, 11, 1, 0, 7, 7, 8, 5, 4, 11, 11, 13, 0, 7, 7, 8, 8, 18, 3, 19, 18, 18, 0, 6, 13]\n",
            "-------RUN38-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': \"CountVectorizer(ngram_range=(1, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b891dd10>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[11, 0, 10, 10, 13, 0, 7, 8, 8, 8, 0, 10, 13, 10, 8, 0, 2, 11, 0, 0, 8, 13, 8, 16, 16, 7, 1, 0, 0, 0, 0, 0, 10, 4, 0, 8, 6, 13, 0, 0, 16, 0, 7, 16, 13, 7, 7, 7, 16, 0, 7, 16, 8, 8, 8, 1, 1, 0, 8, 8, 6, 0, 8, 8, 0, 8, 6, 6, 13, 6, 4, 13, 2, 2, 2, 6, 3, 3, 0, 0, 1, 1, 4, 1, 1, 7, 1, 2, 0, 2, 2, 8, 1, 13, 8, 8, 8, 2, 4, 4, 10, 3, 3, 3, 3, 4, 0, 4, 4, 4, 6, 13, 7, 11, 0, 1, 1, 7, 3, 0, 7, 4, 16, 3, 7, 12, 13, 0, 16, 13, 0, 4, 0, 13, 13, 11, 16, 3, 13, 7, 8, 8, 11, 12, 4, 8, 11, 3, 0, 3, 8, 8, 16, 8, 3, 8, 2, 11, 3, 5, 3, 5, 4, 5, 0, 11, 11, 3, 4, 3, 8, 8, 0, 18, 4, 8, 5, 4, 4, 6, 0, 3, 4, 18, 13, 8, 6, 0, 13, 1, 1, 4, 1, 0, 9, 1, 14, 14, 1, 0, 1, 0, 1, 1, 4, 1, 1, 1, 2, 15, 2, 4, 4, 1, 1, 4, 4, 2, 0, 2, 14, 2, 2, 2, 8, 14, 1, 1, 12, 9, 9, 9, 9, 14, 14, 9, 7, 9, 9, 14, 14, 6, 6, 3, 14, 3, 3, 3, 14, 0, 3, 14, 10, 11, 3, 7, 7, 7, 2, 2, 7, 3, 14, 3, 0, 0, 0, 1, 13, 6, 6, 6, 6, 2, 6, 0, 6, 6, 4, 1, 2, 3, 6, 7, 7, 0, 14, 5, 10, 5, 10, 0, 10, 10, 1, 14, 8, 0, 14, 6, 6, 6, 13, 6, 6, 6, 6, 6, 18, 6, 16, 6, 7, 0, 4, 13, 3, 3, 3, 11, 8, 7, 0, 6, 3, 11, 3, 11, 0, 11, 5, 3, 1, 11, 19, 11, 8, 4, 2, 11, 4, 7, 7, 7, 8, 8, 4, 4, 9, 4, 3, 1, 0, 0, 8, 1, 4, 1, 1, 11, 3, 13, 3, 13, 1, 6, 13, 14, 7, 6, 1, 17, 13, 4, 8, 13, 9, 8, 0, 9, 12, 12, 0, 8, 8, 8, 1, 8, 1, 7, 4, 8, 1, 12, 12, 12, 12, 12, 12, 12, 13, 12, 14, 0, 14, 0, 12, 6, 9, 9, 7, 1, 0, 3, 9, 7, 9, 18, 19, 3, 3, 13, 13, 8, 19, 19, 3, 19, 2, 2, 2, 0, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 19, 11, 6, 4, 6, 2, 8, 4, 6, 6, 6, 19, 0, 6, 8, 2, 6, 14, 6, 3, 0, 14, 6, 6, 6, 6, 2, 2, 7, 0, 2, 2, 2, 6, 7, 6, 2, 2, 5, 2, 1, 10, 7, 5, 7, 0, 7, 10, 3, 0, 7, 5, 10, 6, 0, 15, 11, 15, 15, 11, 2, 4, 14, 14, 13, 15, 9, 11, 11, 15, 15, 15, 15, 15, 3, 1, 15, 6, 15, 1, 1, 15, 10, 1, 15, 11, 3, 15, 15, 1, 14, 2, 8, 11, 1, 7, 7, 7, 7, 2, 2, 6, 2, 7, 3, 7, 7, 0, 7, 7, 17, 17, 17, 17, 9, 7, 7, 0, 10, 19, 9, 9, 9, 9, 9, 9, 11, 17, 1, 1, 10, 10, 10, 1, 1, 2, 17, 17, 16, 13, 17, 1, 17, 1, 4, 6, 9, 9, 7, 1, 4, 8, 9, 16, 9, 9, 7, 9, 9, 3, 9, 9, 0, 12, 9, 7, 7, 4, 13, 1, 1, 1, 1, 1, 1, 17, 1, 1, 2, 2, 10, 0, 10, 4, 11, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 15, 2, 2, 6, 4, 6, 2, 2, 15, 12, 0, 15, 12, 3, 15, 11, 15, 11, 15, 11, 15, 4, 15, 16, 8, 16, 0, 8, 4, 15, 15, 15, 0, 15, 11, 15, 10, 1, 15, 11, 3, 7, 7, 7, 7, 8, 7, 7, 3, 6, 7, 5, 13, 7, 7, 4, 10, 2, 13, 8, 17, 17, 3, 17, 0, 15, 0, 17, 4, 10, 9, 3, 4, 7, 17, 3, 5, 0, 9, 9, 6, 3, 6, 9, 9, 0, 9, 9, 3, 0, 9, 11, 10, 9, 10, 10, 10, 9, 12, 4, 9, 10, 8, 2, 10, 10, 10, 0, 8, 16, 0, 9, 5, 5, 5, 4, 3, 10, 2, 11, 10, 3, 12, 10, 12, 2, 2, 2, 0, 8, 11, 5, 2, 13, 8, 6, 6, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 5, 5, 5, 5, 5, 11, 18, 5, 1, 5, 5, 5, 5, 3, 5, 5, 1, 3, 4, 2, 5, 11, 3, 5, 5, 5, 2, 3, 11, 15, 15, 11, 15, 15, 15, 15, 13, 18, 18, 18, 18, 18, 18, 3, 18, 4, 18, 4, 1, 6, 16, 0, 7, 12, 16, 13, 12, 12, 10, 12, 12, 12, 3, 3, 8, 12, 3, 12, 12, 12, 12, 3, 16, 10, 0, 16, 12, 4, 4, 4, 0, 3, 0, 10, 10, 2, 0, 10, 10, 0, 10, 4, 10, 11, 4, 0, 4, 10, 4, 0, 4, 1, 4, 15, 4, 3, 0, 6, 6, 16, 6, 5, 0, 1, 10, 7, 2, 10, 10, 10, 10, 2, 12, 10, 0, 7, 6, 3, 0, 0, 8, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 9, 5, 3, 5, 11, 5, 5, 5, 8, 5, 11, 5, 15, 3, 16, 3, 1, 1, 4, 19, 1, 0, 13, 14, 4, 4, 1, 4, 4, 18, 17, 18, 0, 18, 17, 16, 0, 3, 14, 14, 3, 14, 16, 17, 10, 7, 4, 13, 4, 0, 4, 0, 4, 4, 0, 4, 16, 4, 4, 4, 4, 1, 0, 6, 16, 14, 12, 8, 9, 5, 5, 5, 0, 0, 8, 8, 13, 8, 12, 0, 8, 12, 11, 11, 3, 11, 12, 12, 12, 12, 0, 15, 12, 12, 12, 3, 12, 5, 12, 5, 16, 5, 5, 0, 3, 10, 10, 3, 3, 10, 0, 10, 11, 11, 11, 11, 11, 11, 3, 11, 11, 3, 6, 7, 0, 19, 19, 2, 0, 4, 8, 6, 6, 4, 13, 2, 2, 8, 8, 6, 8, 8, 2, 7, 15, 0, 11, 13, 14, 7, 7, 8, 0, 8, 0, 11, 3, 3, 3, 9, 7, 7, 1, 8, 7, 7, 2, 0, 19, 7, 1, 7, 3, 1, 1, 4, 10, 6, 5, 0, 14, 4, 16, 16, 13, 4, 16, 4, 6, 3, 6, 3, 14, 16, 3, 2, 1, 18, 2, 8, 6, 3, 2, 2, 8, 4, 10, 8, 4, 8, 6, 5, 5, 14, 5, 16, 5, 10, 14, 9, 17, 4, 11, 11, 1, 1, 8, 1, 11, 11, 11, 11, 9, 11, 1, 11, 11, 14, 1, 11, 11, 8, 8, 14, 13, 14, 14, 14, 14, 5, 1, 14, 15, 11, 9, 17, 9, 4, 0, 12, 9, 2, 5, 10, 12, 19, 19, 5, 0, 0, 3, 9, 5, 0, 8, 6, 5, 17, 3, 5, 11, 19, 3, 0, 5, 2, 1, 19, 17, 0, 7, 2, 2, 7, 7, 8, 7, 5, 16, 16, 17, 4, 2, 16, 7, 3, 4, 2, 17, 17, 5, 2, 2, 0, 17, 2, 17, 0, 2, 10, 5, 9, 4, 1, 6, 1, 4, 12, 5, 8, 1, 2, 7, 1, 0, 3, 0, 1, 8, 1, 8, 10, 5, 9, 3, 5, 19, 0, 10, 12, 9, 7, 7, 0, 12, 16, 16, 12, 12, 0, 0, 4, 6, 12, 12, 9, 9, 12, 5, 9, 3, 8, 14, 12, 0, 5, 0, 1, 9, 1, 0, 2, 9, 13, 18, 0, 9, 1, 10, 9, 6, 6, 6, 5, 6, 5, 5, 5, 7, 6, 5, 5, 2, 1, 7, 1, 5, 0, 2, 7, 5, 14, 19, 1, 2, 7, 19, 2, 0, 17, 17, 11, 14, 2, 10, 10, 6, 1, 0, 3, 2, 4, 2, 2, 0, 10, 17, 2, 3, 2, 2, 1, 5, 6, 0, 4, 5, 5, 2, 10, 4, 3, 10, 1, 10, 2, 9, 13, 10, 9, 10, 0, 4, 13, 12, 0, 0, 9, 18, 12, 7, 8, 9, 9, 9, 12, 11, 3, 9, 6, 0, 4, 7, 6, 0, 0, 1, 1, 6, 6, 6, 15, 13, 6, 3, 9, 5, 7, 3, 5, 5, 12, 7, 0, 5, 18, 18, 5, 2, 4, 1, 13, 6, 1, 6, 0, 7, 12, 4, 11, 6, 17, 4, 3, 3, 0, 4, 4, 6, 14, 14, 17, 15, 9, 9, 8, 14, 17, 2, 11, 12, 0, 14, 5, 3, 4, 12, 19, 2, 1, 0, 13, 0, 2, 2, 3, 0, 1, 19, 6, 18, 5, 10, 8, 3, 3, 6, 10, 9, 9, 9, 10, 2, 4, 5, 8, 5, 11, 4, 9, 3, 10, 9, 10, 9, 5, 9, 9, 9, 0, 9, 10, 19, 12, 2, 0, 5, 5, 9, 15, 11, 12, 12, 4, 0, 5, 5, 9, 9, 5, 1, 18, 5, 5, 0, 8, 4]\n",
            "-------RUN39-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': \"CountVectorizer(ngram_range=(2, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df4a9008410>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[10, 0, 3, 3, 16, 0, 6, 7, 7, 7, 0, 3, 16, 3, 7, 0, 1, 10, 0, 0, 7, 16, 7, 15, 15, 6, 9, 0, 0, 0, 0, 0, 3, 5, 0, 7, 8, 16, 0, 0, 15, 0, 6, 15, 6, 6, 6, 6, 15, 0, 6, 15, 7, 7, 7, 9, 9, 0, 7, 7, 8, 0, 7, 7, 0, 7, 8, 8, 16, 8, 5, 16, 1, 1, 1, 8, 0, 17, 0, 0, 9, 9, 5, 9, 9, 6, 9, 1, 0, 1, 1, 7, 9, 12, 7, 7, 7, 1, 0, 5, 3, 11, 0, 11, 0, 5, 0, 5, 5, 5, 8, 16, 6, 10, 0, 6, 6, 6, 0, 7, 6, 6, 15, 11, 6, 2, 16, 0, 15, 16, 0, 5, 0, 16, 16, 10, 15, 0, 16, 6, 7, 7, 10, 2, 5, 7, 10, 0, 0, 11, 7, 7, 15, 7, 11, 7, 1, 10, 11, 4, 0, 4, 5, 4, 0, 10, 10, 11, 5, 0, 7, 7, 0, 17, 5, 7, 4, 5, 5, 8, 0, 0, 5, 17, 16, 7, 1, 0, 16, 9, 9, 5, 9, 0, 2, 9, 13, 13, 9, 0, 9, 0, 9, 9, 5, 9, 12, 9, 1, 3, 1, 5, 5, 9, 9, 5, 5, 1, 0, 1, 13, 1, 1, 1, 7, 13, 9, 9, 2, 2, 2, 2, 2, 13, 13, 2, 6, 2, 2, 13, 13, 8, 8, 11, 13, 0, 0, 11, 13, 0, 11, 13, 3, 10, 11, 6, 6, 6, 1, 1, 6, 11, 13, 0, 0, 0, 0, 9, 12, 8, 8, 8, 8, 1, 8, 0, 8, 8, 5, 9, 1, 11, 8, 6, 6, 0, 13, 4, 3, 4, 3, 0, 3, 3, 6, 13, 7, 0, 13, 8, 8, 8, 16, 8, 8, 8, 1, 8, 17, 8, 15, 8, 6, 0, 5, 6, 11, 11, 11, 10, 7, 2, 0, 16, 11, 10, 11, 10, 0, 10, 4, 11, 9, 10, 19, 10, 7, 5, 1, 10, 5, 6, 6, 6, 7, 7, 5, 5, 2, 5, 11, 9, 0, 0, 7, 9, 5, 9, 9, 10, 11, 16, 0, 16, 9, 8, 16, 1, 6, 8, 9, 14, 16, 5, 7, 16, 2, 7, 0, 2, 2, 2, 0, 7, 7, 7, 9, 7, 9, 6, 5, 7, 9, 2, 2, 2, 2, 2, 2, 2, 12, 2, 13, 0, 13, 0, 2, 8, 2, 2, 6, 9, 0, 11, 2, 6, 2, 17, 19, 11, 17, 1, 12, 7, 19, 19, 0, 19, 1, 1, 1, 0, 1, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 19, 10, 8, 5, 8, 1, 7, 5, 8, 8, 8, 19, 0, 8, 7, 1, 8, 13, 8, 11, 0, 13, 8, 8, 8, 8, 1, 1, 6, 0, 1, 1, 1, 8, 6, 8, 1, 1, 4, 1, 9, 3, 6, 4, 6, 0, 6, 3, 0, 0, 6, 4, 3, 8, 0, 3, 10, 3, 3, 10, 1, 5, 13, 13, 16, 3, 2, 10, 10, 3, 3, 3, 3, 3, 11, 9, 3, 8, 3, 9, 9, 3, 18, 9, 3, 10, 0, 3, 3, 9, 13, 1, 7, 10, 9, 6, 6, 6, 6, 1, 1, 8, 1, 6, 11, 6, 6, 0, 6, 6, 14, 14, 14, 14, 14, 6, 6, 0, 3, 19, 2, 2, 2, 2, 2, 14, 10, 14, 9, 9, 18, 18, 18, 9, 9, 1, 14, 14, 15, 12, 14, 9, 14, 9, 5, 8, 2, 2, 6, 9, 6, 7, 2, 15, 2, 2, 6, 2, 2, 11, 2, 2, 0, 2, 2, 6, 6, 5, 12, 12, 12, 12, 12, 12, 12, 14, 12, 12, 1, 1, 3, 0, 17, 5, 10, 11, 12, 12, 12, 12, 12, 12, 1, 12, 12, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 5, 8, 1, 1, 3, 2, 0, 3, 2, 0, 3, 10, 3, 10, 3, 10, 3, 5, 3, 15, 7, 15, 0, 7, 5, 3, 3, 3, 0, 3, 10, 3, 18, 9, 3, 10, 11, 6, 6, 6, 6, 7, 6, 6, 0, 8, 6, 4, 16, 6, 6, 5, 3, 1, 16, 6, 14, 14, 11, 14, 0, 3, 0, 14, 5, 3, 14, 0, 5, 6, 14, 0, 4, 0, 2, 2, 8, 0, 8, 2, 2, 0, 2, 2, 0, 0, 2, 18, 18, 2, 3, 18, 3, 2, 2, 5, 2, 18, 7, 1, 18, 18, 18, 0, 7, 15, 0, 2, 4, 4, 4, 5, 0, 3, 1, 10, 18, 0, 2, 3, 2, 1, 1, 1, 0, 7, 10, 4, 1, 1, 7, 8, 8, 12, 12, 1, 12, 12, 1, 12, 12, 12, 12, 1, 1, 4, 4, 4, 4, 4, 17, 17, 4, 12, 4, 4, 4, 4, 11, 4, 4, 12, 11, 5, 1, 4, 4, 11, 4, 4, 4, 1, 19, 10, 3, 3, 10, 3, 3, 3, 3, 6, 17, 17, 17, 17, 17, 17, 11, 17, 5, 17, 5, 12, 8, 15, 0, 6, 2, 15, 16, 2, 3, 17, 3, 3, 3, 11, 11, 7, 2, 0, 2, 2, 2, 2, 0, 15, 3, 0, 15, 3, 5, 5, 5, 0, 19, 0, 3, 3, 1, 0, 3, 3, 0, 3, 5, 3, 10, 5, 0, 5, 3, 5, 0, 5, 9, 5, 3, 5, 0, 0, 8, 8, 15, 8, 4, 0, 9, 3, 6, 1, 3, 3, 3, 3, 1, 2, 3, 0, 6, 8, 0, 0, 0, 7, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 0, 4, 10, 4, 4, 4, 7, 4, 4, 4, 3, 19, 15, 11, 9, 9, 5, 19, 9, 0, 16, 13, 5, 5, 9, 5, 5, 17, 14, 17, 0, 17, 14, 15, 0, 0, 13, 11, 11, 11, 15, 14, 17, 6, 5, 1, 5, 0, 5, 0, 5, 5, 0, 5, 15, 5, 5, 5, 5, 9, 0, 8, 15, 13, 2, 7, 2, 4, 4, 4, 0, 0, 7, 7, 16, 7, 2, 0, 7, 3, 10, 10, 0, 10, 2, 3, 3, 2, 0, 3, 3, 3, 3, 0, 3, 4, 3, 4, 15, 4, 4, 0, 0, 3, 3, 0, 0, 3, 0, 3, 10, 10, 10, 10, 10, 10, 0, 10, 10, 0, 8, 6, 0, 19, 19, 1, 0, 5, 7, 8, 8, 5, 16, 1, 1, 7, 7, 8, 7, 7, 1, 6, 3, 0, 10, 16, 13, 6, 6, 7, 0, 7, 0, 10, 11, 11, 11, 2, 6, 6, 9, 7, 6, 6, 1, 0, 19, 6, 9, 6, 11, 9, 9, 5, 18, 8, 4, 0, 13, 5, 15, 15, 6, 5, 15, 5, 8, 11, 13, 11, 11, 15, 11, 1, 9, 17, 1, 7, 8, 11, 1, 1, 7, 5, 3, 7, 5, 7, 8, 4, 4, 1, 4, 15, 4, 18, 13, 2, 14, 5, 10, 10, 9, 12, 7, 9, 10, 10, 10, 10, 14, 10, 12, 10, 10, 13, 12, 10, 10, 7, 7, 13, 16, 13, 11, 13, 13, 4, 9, 13, 3, 10, 2, 14, 2, 5, 0, 2, 2, 1, 4, 18, 2, 19, 19, 4, 0, 0, 11, 2, 4, 0, 7, 8, 4, 14, 11, 4, 10, 19, 0, 0, 4, 1, 9, 19, 14, 0, 6, 1, 1, 6, 6, 7, 6, 4, 15, 15, 14, 5, 1, 15, 6, 11, 5, 1, 14, 14, 4, 1, 1, 0, 14, 1, 14, 0, 1, 18, 4, 17, 5, 9, 8, 12, 5, 3, 4, 7, 12, 1, 6, 9, 0, 0, 0, 12, 7, 12, 7, 3, 4, 2, 11, 4, 19, 0, 3, 3, 2, 6, 6, 0, 3, 15, 15, 3, 3, 5, 0, 5, 8, 2, 3, 2, 2, 2, 4, 2, 11, 7, 13, 2, 0, 4, 0, 9, 2, 9, 0, 1, 2, 6, 17, 0, 2, 12, 3, 14, 8, 8, 8, 4, 8, 4, 4, 4, 6, 8, 4, 4, 1, 9, 6, 12, 4, 0, 1, 6, 4, 13, 19, 9, 1, 6, 19, 1, 0, 14, 14, 17, 13, 1, 18, 18, 8, 9, 7, 0, 1, 5, 1, 1, 0, 3, 14, 1, 11, 1, 1, 12, 4, 8, 0, 5, 4, 4, 1, 3, 5, 11, 18, 12, 3, 1, 2, 6, 18, 2, 18, 0, 5, 16, 3, 0, 0, 2, 17, 3, 6, 7, 2, 2, 2, 2, 10, 0, 2, 8, 0, 5, 5, 8, 0, 0, 12, 12, 8, 8, 8, 3, 16, 8, 0, 2, 4, 6, 0, 4, 4, 2, 6, 0, 4, 17, 17, 4, 1, 5, 9, 12, 8, 9, 8, 0, 6, 3, 5, 10, 8, 14, 5, 0, 0, 5, 5, 5, 8, 13, 11, 14, 3, 2, 2, 7, 13, 14, 1, 4, 2, 0, 13, 4, 0, 5, 2, 19, 1, 12, 0, 16, 5, 1, 1, 11, 0, 12, 19, 1, 17, 4, 18, 7, 11, 11, 8, 18, 2, 2, 2, 18, 1, 5, 4, 7, 4, 4, 5, 2, 0, 18, 2, 0, 14, 4, 2, 2, 2, 0, 2, 18, 19, 2, 1, 0, 4, 4, 2, 3, 10, 3, 3, 5, 0, 4, 4, 2, 2, 4, 12, 17, 4, 4, 0, 11, 5]\n",
            "-------RUN40-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': \"CountVectorizer(stop_words='english')\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 1, 0, 0, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 1, 1, -1, 0, 0, 0, 1, 0, 1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 1, 1, 0, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "-------RUN41-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': 'CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7df3b00f5c90>)', 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 1, 0, 0, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 1, 1, -1, 0, 0, 0, 1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 1, 1, 0, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, -1, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "-------RUN42-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': \"CountVectorizer(stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b02d2c50>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 1, 0, 0, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 1, 1, -1, 0, 0, 0, 1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 1, 1, 0, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "-------RUN43-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': \"CountVectorizer(ngram_range=(1, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b891dd10>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 1, 0, 0, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 1, 1, -1, 0, 0, 0, 1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 1, 1, 0, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "-------RUN44-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': \"CountVectorizer(ngram_range=(2, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df4a9008410>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 1, 0, 0, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 1, 1, -1, 0, 0, 0, 1, 0, 1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 1, 1, 0, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "-------RUN45-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': \"CountVectorizer(stop_words='english')\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, 1, -1, 0, -1, 0, -1, -1, 0, 1, -1, 0, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 1, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 1, 1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, -1, -1, 1, -1, -1, -1, -1, 0, -1, 2, -1, -1, 2, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 1, 2, -1, -1, 0, 0, 0, -1, 0, 2, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, 2, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, 1, -1, 2, 0, -1, -1, 0, -1, 0, 2, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, 1, -1, -1, -1, -1, 1, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, 0, 3, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 2, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, 3, -1, 0, -1, 1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, 3, -1, -1, 3, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, 3, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, 3, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 3, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 1, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, 2, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 1, 0, -1, -1, -1, -1, 0, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 1, -1, 0, -1, -1, -1, -1, -1, 0, 1, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 2, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 1, -1, -1, -1, -1, 3, -1, -1, -1, -1, 3, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, 1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, 2, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 3, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 3, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0]\n",
            "-------RUN46-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': 'CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7df3b00f5c90>)', 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[-1, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, 1, -1, 0, -1, 0, -1, -1, 0, 1, -1, 0, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 1, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 1, 1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, -1, -1, 1, -1, -1, -1, -1, 0, -1, 2, -1, -1, 2, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 1, 2, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, 2, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, 1, -1, 2, 0, -1, -1, 0, -1, 0, 2, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, 1, -1, -1, -1, -1, 1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 2, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 3, -1, -1, -1, 0, 0, 0, -1, -1, -1, 3, -1, 0, -1, 1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, 3, -1, -1, 3, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, 3, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, 3, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 3, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 1, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, 2, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 1, 0, -1, -1, -1, -1, 0, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 1, -1, 0, -1, -1, -1, -1, 0, 0, 1, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 2, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 1, -1, -1, -1, -1, 3, -1, -1, -1, -1, 3, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, 1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, 2, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 3, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 3, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0]\n",
            "-------RUN47-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': \"CountVectorizer(stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b02d2c50>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, 1, -1, 0, -1, 0, -1, -1, 0, 1, -1, 0, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 1, 1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, -1, -1, 1, -1, -1, -1, -1, 0, -1, 2, -1, -1, 2, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, 4, -1, 4, -1, 4, -1, -1, -1, -1, 0, -1, 0, -1, 1, 2, -1, -1, 0, 0, 0, -1, 0, 2, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, 2, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, 1, -1, 2, 0, -1, -1, 0, -1, 0, 2, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, 1, -1, -1, -1, -1, 1, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 2, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, 3, -1, 0, -1, 1, -1, 0, 0, -1, -1, -1, 1, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, 3, -1, -1, 3, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, 3, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 4, -1, -1, 0, 0, -1, 4, -1, -1, -1, -1, 4, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, 4, -1, -1, -1, 3, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 3, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, 1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 1, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, 2, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 1, 0, -1, -1, -1, -1, 0, -1, 4, 2, -1, -1, -1, 4, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 1, -1, 0, -1, -1, -1, -1, 0, 0, 1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 2, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 1, -1, -1, -1, -1, 3, -1, -1, -1, -1, 3, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, 1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, 2, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 3, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 3, 4, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0]\n",
            "-------RUN48-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': \"CountVectorizer(ngram_range=(1, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b891dd10>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[-1, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, 1, -1, 0, -1, 0, -1, -1, 0, 1, -1, 0, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 1, 1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, -1, -1, 1, -1, -1, -1, -1, 0, -1, 2, -1, -1, 2, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 1, 2, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, 2, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, 1, -1, 2, 0, -1, -1, 0, -1, 0, 2, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, 1, -1, -1, -1, -1, 1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 2, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, 3, -1, 0, -1, 1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, 3, -1, -1, 3, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, 3, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, 3, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 3, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, 1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 1, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, 2, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 1, 0, -1, -1, -1, -1, 0, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 1, -1, 0, -1, -1, -1, -1, -1, 0, 1, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 2, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 3, -1, -1, -1, -1, 3, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, 1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, 2, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 3, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 3, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0]\n",
            "-------RUN49-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': \"CountVectorizer(ngram_range=(2, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df4a9008410>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[-1, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, 1, -1, 0, -1, 0, -1, -1, 0, 1, -1, 0, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 1, 1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, -1, -1, 1, -1, -1, -1, -1, 0, -1, 2, -1, -1, 2, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 1, 2, -1, -1, 0, 0, -1, -1, 0, 2, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, 2, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, 1, -1, 2, 0, -1, -1, 0, -1, 0, 2, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, 1, -1, -1, -1, -1, 1, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 2, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, 3, -1, 0, -1, 1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, 3, -1, -1, 3, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, 3, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, 3, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 3, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, 1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 1, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, 2, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 1, 0, -1, -1, -1, -1, 0, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 1, -1, 0, -1, -1, -1, -1, -1, 0, 1, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 3, -1, -1, -1, -1, 3, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, 1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, 2, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 3, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 3, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0]\n",
            "-------RUN50-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': \"CountVectorizer(stop_words='english')\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 1, 0, 1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 1, 1, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, 2, -1, 0, -1, 2, -1, 0, -1, 0, 2, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, -1, -1, 0, 2, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, -1, 1, -1, -1, 0, -1, 0, 1, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, 2, -1, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, 0, -1, 1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 2, -1, -1, 0, 0, 0]\n",
            "-------RUN51-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': 'CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7df3b00f5c90>)', 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 1, 0, 1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 1, 1, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, 2, -1, 0, -1, 2, -1, 0, -1, 0, 2, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 2, -1, -1, 0, 2, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 2, 0, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, -1, 1, -1, -1, 0, -1, 0, 1, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, 2, -1, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 2, -1, -1, 0, 0, 0]\n",
            "-------RUN52-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': \"CountVectorizer(stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b02d2c50>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 1, 0, 1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 1, 1, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, 2, -1, 0, -1, 2, -1, 0, -1, 0, 2, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 2, -1, -1, 0, 2, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 2, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, -1, 1, -1, -1, 0, -1, 0, 1, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, 2, -1, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, 0, -1, 1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 2, -1, -1, 0, 0, 0]\n",
            "-------RUN53-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': \"CountVectorizer(ngram_range=(1, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b891dd10>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 1, 0, 1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 1, 1, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, 2, -1, 0, -1, 2, -1, 0, -1, 0, 2, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 2, -1, -1, 0, 2, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, -1, 1, -1, -1, 0, -1, 0, 1, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, 2, -1, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, 0, -1, 1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 2, -1, -1, 0, 0, 0]\n",
            "-------RUN54-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': \"CountVectorizer(ngram_range=(2, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df4a9008410>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 1, 0, 1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 1, 1, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, 2, -1, 0, -1, 2, -1, 0, -1, 0, 2, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 2, -1, -1, 0, 2, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, -1, 1, -1, -1, 0, -1, 0, 1, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, 2, -1, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, 0, -1, 1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 2, -1, -1, 0, 0, 0]\n",
            "-------RUN55-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': \"CountVectorizer(stop_words='english')\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[6, 8, 6, 8, 3, 0, 14, 9, 4, 1, 15, 6, 9, 7, 1, 4, 5, 16, 0, 0, 10, 9, 4, 2, 2, 1, 3, 0, 4, 0, 0, 0, 6, 0, 0, 4, 16, 16, 14, 4, 2, 4, 9, 2, 3, 1, 11, 3, 2, 6, 3, 2, 1, 9, 4, 3, 3, 0, 0, 0, 5, 0, 0, 1, 4, 0, 7, 3, 5, 5, 1, 5, 5, 17, 10, 5, 0, 2, 0, 14, 3, 4, 1, 3, 3, 11, 3, 17, 4, 9, 17, 1, 1, 8, 1, 4, 1, 17, 10, 4, 4, 2, 0, 2, 0, 1, 4, 1, 2, 4, 10, 3, 11, 16, 4, 11, 11, 1, 2, 0, 1, 1, 2, 6, 11, 18, 7, 4, 0, 11, 4, 4, 0, 3, 5, 10, 2, 2, 3, 1, 1, 1, 1, 14, 4, 4, 14, 0, 6, 15, 9, 9, 0, 1, 2, 0, 5, 7, 10, 10, 2, 13, 1, 16, 4, 16, 16, 0, 1, 2, 1, 9, 0, 14, 4, 0, 6, 4, 11, 7, 0, 0, 1, 14, 9, 7, 10, 4, 3, 9, 1, 1, 3, 4, 14, 3, 7, 7, 3, 4, 3, 4, 3, 9, 1, 3, 1, 3, 17, 14, 5, 1, 1, 1, 9, 7, 7, 17, 4, 1, 7, 7, 5, 5, 1, 19, 3, 11, 14, 12, 12, 14, 12, 7, 10, 14, 1, 14, 15, 7, 7, 5, 16, 0, 15, 9, 6, 0, 7, 0, 0, 7, 6, 16, 2, 11, 11, 11, 5, 5, 9, 0, 15, 0, 0, 0, 1, 3, 0, 16, 5, 5, 7, 17, 5, 2, 5, 5, 1, 11, 5, 0, 5, 11, 1, 4, 1, 0, 6, 16, 8, 4, 8, 8, 1, 7, 1, 2, 5, 5, 16, 10, 9, 17, 5, 5, 5, 16, 2, 7, 2, 5, 3, 4, 1, 16, 2, 2, 0, 14, 7, 11, 0, 7, 14, 10, 10, 16, 0, 16, 10, 2, 9, 16, 2, 4, 1, 1, 5, 6, 4, 0, 1, 11, 7, 1, 1, 6, 15, 1, 6, 9, 6, 4, 3, 9, 4, 9, 9, 14, 2, 3, 0, 3, 9, 5, 0, 10, 11, 5, 9, 13, 3, 4, 7, 11, 12, 11, 14, 14, 18, 18, 6, 7, 3, 3, 9, 0, 9, 1, 7, 11, 9, 18, 18, 18, 18, 18, 18, 12, 13, 18, 7, 0, 7, 4, 18, 5, 12, 12, 9, 3, 4, 10, 14, 1, 15, 2, 0, 2, 2, 10, 16, 9, 2, 10, 2, 10, 7, 17, 5, 1, 5, 5, 17, 17, 17, 3, 17, 3, 17, 5, 10, 2, 6, 5, 1, 5, 7, 1, 1, 5, 7, 5, 0, 0, 5, 0, 7, 5, 13, 5, 2, 6, 7, 5, 5, 5, 16, 17, 17, 11, 4, 17, 5, 3, 5, 11, 9, 17, 5, 6, 17, 3, 14, 14, 15, 11, 2, 11, 14, 2, 0, 11, 15, 14, 7, 0, 8, 8, 8, 8, 6, 7, 1, 7, 7, 14, 8, 15, 8, 8, 8, 16, 8, 8, 8, 10, 9, 8, 5, 16, 2, 9, 8, 14, 9, 8, 6, 0, 8, 8, 9, 15, 2, 3, 16, 9, 9, 1, 11, 11, 10, 10, 5, 17, 9, 0, 11, 18, 6, 11, 11, 19, 19, 19, 19, 15, 1, 11, 4, 8, 9, 12, 12, 14, 12, 19, 12, 6, 19, 0, 9, 6, 15, 6, 9, 3, 10, 19, 19, 2, 8, 19, 9, 19, 3, 1, 5, 12, 12, 1, 9, 4, 3, 12, 2, 12, 12, 1, 14, 12, 9, 12, 12, 1, 18, 12, 1, 11, 1, 10, 7, 3, 9, 3, 8, 3, 9, 3, 3, 17, 17, 14, 0, 0, 4, 8, 0, 1, 3, 9, 3, 3, 3, 5, 3, 3, 10, 17, 17, 17, 7, 3, 10, 17, 10, 5, 0, 5, 5, 11, 5, 8, 4, 8, 14, 0, 10, 6, 8, 6, 8, 6, 8, 3, 8, 2, 3, 2, 0, 9, 1, 8, 8, 8, 6, 8, 6, 8, 18, 2, 8, 16, 0, 3, 11, 11, 9, 9, 11, 11, 2, 5, 11, 6, 3, 3, 11, 4, 8, 17, 11, 1, 19, 19, 10, 10, 6, 8, 4, 14, 4, 8, 15, 6, 4, 14, 19, 2, 0, 0, 12, 12, 10, 0, 16, 12, 15, 0, 12, 12, 2, 0, 14, 6, 14, 12, 6, 6, 8, 12, 18, 1, 12, 7, 0, 10, 15, 6, 15, 14, 1, 2, 2, 15, 15, 13, 15, 1, 2, 8, 5, 16, 0, 6, 18, 8, 18, 5, 17, 7, 0, 0, 0, 6, 3, 10, 9, 5, 7, 1, 3, 5, 3, 3, 17, 9, 3, 3, 3, 5, 3, 13, 13, 13, 13, 13, 6, 15, 13, 3, 13, 13, 15, 13, 10, 13, 13, 3, 15, 1, 10, 13, 14, 10, 13, 15, 15, 5, 6, 8, 8, 8, 6, 2, 8, 8, 8, 11, 2, 12, 2, 2, 2, 2, 0, 2, 1, 2, 3, 3, 5, 2, 11, 11, 18, 2, 3, 18, 18, 14, 18, 8, 14, 0, 6, 0, 18, 2, 18, 18, 18, 18, 2, 2, 8, 4, 2, 8, 1, 4, 4, 0, 0, 0, 16, 8, 10, 4, 8, 8, 4, 8, 1, 2, 6, 1, 4, 6, 2, 4, 6, 4, 3, 1, 8, 1, 0, 14, 9, 10, 2, 16, 13, 1, 3, 2, 11, 3, 16, 8, 8, 8, 5, 12, 16, 6, 3, 10, 2, 18, 9, 1, 2, 13, 12, 13, 12, 15, 13, 13, 13, 13, 13, 14, 13, 0, 15, 14, 15, 13, 13, 10, 6, 4, 15, 8, 6, 10, 4, 9, 9, 1, 6, 1, 4, 7, 7, 1, 1, 0, 4, 1, 2, 19, 2, 6, 2, 19, 2, 6, 0, 7, 3, 0, 4, 2, 19, 0, 11, 3, 4, 9, 4, 4, 4, 1, 1, 0, 9, 2, 1, 3, 1, 1, 3, 0, 7, 2, 5, 18, 1, 12, 13, 15, 7, 14, 0, 1, 9, 7, 9, 18, 4, 0, 6, 1, 2, 0, 4, 18, 8, 6, 18, 4, 8, 18, 8, 18, 6, 8, 15, 6, 13, 2, 16, 14, 6, 6, 8, 8, 2, 6, 8, 6, 8, 2, 6, 6, 16, 4, 14, 2, 14, 4, 2, 16, 1, 0, 10, 2, 17, 4, 9, 0, 5, 8, 4, 1, 5, 5, 1, 4, 10, 7, 0, 17, 1, 8, 0, 5, 10, 7, 11, 11, 0, 4, 0, 14, 16, 6, 16, 10, 14, 11, 9, 3, 11, 11, 9, 10, 1, 10, 11, 9, 11, 10, 2, 2, 0, 8, 5, 13, 4, 7, 1, 2, 2, 4, 1, 2, 1, 7, 2, 7, 0, 10, 0, 10, 5, 9, 14, 5, 4, 7, 2, 17, 7, 1, 6, 14, 0, 4, 3, 5, 13, 13, 10, 16, 0, 15, 6, 7, 12, 7, 1, 4, 16, 3, 3, 11, 3, 1, 4, 4, 4, 15, 1, 7, 4, 14, 7, 7, 6, 4, 2, 0, 7, 9, 7, 1, 7, 7, 7, 3, 7, 8, 16, 12, 7, 12, 4, 4, 18, 12, 10, 15, 13, 18, 9, 10, 13, 0, 4, 10, 13, 13, 6, 7, 16, 15, 7, 10, 15, 7, 2, 2, 4, 13, 3, 3, 10, 13, 4, 11, 3, 3, 11, 9, 0, 11, 13, 2, 2, 19, 1, 17, 2, 11, 10, 4, 5, 19, 19, 12, 17, 5, 9, 19, 5, 19, 0, 17, 6, 16, 14, 4, 3, 5, 3, 1, 6, 6, 0, 7, 3, 3, 3, 4, 6, 4, 3, 0, 11, 3, 14, 15, 12, 10, 15, 7, 0, 6, 18, 12, 11, 11, 6, 8, 2, 2, 6, 6, 0, 4, 1, 5, 8, 18, 12, 12, 12, 13, 12, 0, 1, 7, 18, 10, 7, 0, 3, 12, 3, 8, 2, 15, 1, 15, 6, 12, 9, 6, 19, 16, 5, 10, 13, 5, 13, 13, 15, 11, 10, 15, 13, 10, 3, 11, 3, 14, 4, 10, 11, 13, 7, 10, 3, 17, 11, 10, 5, 4, 19, 19, 6, 7, 17, 6, 7, 2, 2, 1, 0, 5, 1, 5, 17, 2, 15, 19, 3, 2, 5, 5, 3, 15, 5, 0, 1, 13, 15, 17, 6, 4, 10, 14, 3, 8, 10, 12, 9, 6, 12, 13, 6, 0, 9, 14, 6, 4, 12, 12, 8, 11, 15, 12, 12, 12, 18, 16, 2, 2, 5, 0, 4, 1, 16, 14, 6, 9, 3, 5, 5, 5, 8, 3, 7, 2, 14, 13, 11, 6, 13, 19, 18, 11, 0, 13, 2, 2, 13, 17, 1, 3, 10, 16, 3, 5, 4, 11, 14, 7, 7, 5, 19, 1, 2, 0, 0, 1, 9, 5, 10, 10, 19, 16, 12, 12, 7, 7, 19, 17, 13, 18, 4, 7, 15, 2, 0, 18, 2, 17, 3, 0, 3, 11, 5, 3, 6, 4, 3, 2, 5, 12, 15, 6, 12, 16, 10, 10, 13, 12, 12, 12, 6, 5, 1, 13, 9, 13, 6, 1, 12, 2, 4, 12, 6, 19, 16, 12, 12, 19, 4, 12, 0, 2, 12, 5, 6, 13, 13, 12, 8, 10, 6, 8, 1, 4, 13, 13, 14, 14, 13, 3, 2, 15, 13, 6, 15, 1]\n",
            "-------RUN56-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': 'CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7df3b00f5c90>)', 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[5, 10, 5, 10, 2, 0, 9, 7, 1, 3, 11, 5, 9, 6, 3, 6, 4, 5, 9, 0, 6, 6, 1, 7, 7, 3, 2, 0, 1, 9, 0, 1, 18, 9, 9, 9, 11, 5, 1, 1, 7, 1, 2, 7, 2, 3, 2, 14, 7, 0, 2, 7, 9, 9, 1, 2, 2, 1, 9, 9, 4, 9, 9, 3, 1, 9, 15, 2, 4, 4, 3, 15, 4, 17, 13, 4, 0, 16, 1, 0, 2, 1, 3, 2, 2, 14, 2, 17, 1, 2, 17, 3, 3, 10, 3, 1, 3, 17, 6, 1, 1, 7, 0, 0, 0, 1, 1, 3, 16, 1, 13, 2, 14, 5, 1, 14, 3, 3, 7, 9, 3, 3, 7, 5, 14, 18, 15, 1, 7, 6, 1, 1, 1, 2, 4, 13, 0, 0, 2, 3, 3, 3, 3, 19, 1, 9, 5, 0, 1, 11, 9, 9, 9, 3, 16, 7, 4, 6, 9, 11, 0, 5, 3, 13, 1, 5, 5, 0, 3, 0, 3, 9, 1, 5, 6, 9, 11, 1, 18, 4, 1, 7, 3, 16, 9, 6, 6, 1, 2, 2, 3, 3, 2, 1, 8, 2, 15, 15, 6, 1, 2, 6, 3, 2, 3, 2, 3, 2, 17, 18, 4, 3, 3, 6, 2, 6, 6, 17, 1, 3, 11, 6, 4, 4, 3, 15, 2, 14, 1, 8, 8, 19, 8, 15, 13, 19, 9, 8, 11, 15, 9, 4, 11, 0, 11, 16, 0, 0, 6, 0, 0, 15, 5, 5, 7, 14, 14, 14, 6, 4, 9, 1, 11, 0, 1, 1, 3, 6, 0, 10, 4, 4, 15, 17, 4, 7, 4, 4, 3, 6, 13, 0, 4, 14, 3, 6, 9, 0, 0, 11, 10, 1, 10, 10, 3, 6, 3, 7, 15, 4, 4, 13, 2, 17, 4, 4, 4, 4, 16, 4, 7, 4, 2, 18, 3, 5, 7, 0, 7, 0, 15, 18, 0, 15, 0, 11, 13, 5, 0, 5, 11, 7, 2, 5, 16, 5, 3, 3, 4, 5, 1, 9, 3, 14, 6, 3, 3, 5, 8, 3, 1, 7, 1, 1, 6, 2, 1, 2, 2, 5, 7, 2, 0, 2, 2, 15, 9, 13, 14, 4, 13, 12, 2, 3, 6, 8, 8, 6, 0, 8, 19, 19, 0, 6, 2, 6, 2, 9, 2, 3, 6, 14, 2, 19, 19, 19, 19, 19, 19, 19, 11, 18, 15, 0, 15, 1, 19, 4, 8, 8, 2, 2, 1, 13, 8, 3, 12, 0, 0, 0, 0, 6, 6, 9, 16, 9, 16, 13, 6, 17, 4, 9, 4, 4, 17, 17, 17, 2, 17, 2, 17, 4, 13, 7, 5, 4, 3, 4, 6, 3, 3, 4, 4, 4, 0, 1, 4, 9, 15, 4, 12, 4, 16, 1, 15, 4, 4, 4, 5, 17, 17, 14, 1, 17, 4, 2, 4, 14, 13, 17, 15, 0, 17, 2, 16, 6, 11, 14, 13, 14, 16, 16, 9, 14, 11, 16, 6, 9, 10, 10, 10, 13, 5, 6, 3, 15, 11, 16, 10, 11, 18, 5, 10, 5, 10, 10, 10, 0, 2, 10, 4, 10, 7, 2, 10, 5, 7, 10, 0, 0, 10, 10, 7, 11, 13, 6, 5, 7, 2, 3, 14, 14, 13, 13, 4, 17, 9, 0, 14, 19, 1, 14, 14, 8, 15, 15, 15, 12, 3, 14, 1, 10, 9, 8, 8, 19, 8, 8, 8, 5, 15, 7, 7, 5, 11, 5, 7, 2, 13, 15, 15, 7, 10, 15, 7, 15, 2, 9, 4, 8, 8, 3, 15, 1, 2, 8, 7, 8, 8, 3, 12, 8, 9, 8, 12, 3, 19, 8, 3, 14, 3, 13, 15, 2, 2, 6, 10, 2, 9, 2, 2, 17, 17, 16, 9, 0, 1, 10, 0, 3, 2, 2, 2, 6, 2, 4, 2, 2, 13, 17, 17, 17, 4, 2, 13, 17, 13, 4, 9, 4, 4, 14, 13, 18, 1, 10, 19, 0, 13, 5, 10, 5, 10, 5, 10, 3, 10, 7, 2, 7, 1, 9, 3, 10, 10, 10, 1, 10, 0, 10, 18, 13, 10, 11, 0, 2, 14, 14, 2, 9, 14, 6, 16, 4, 14, 5, 2, 2, 14, 1, 18, 17, 3, 3, 15, 15, 13, 15, 0, 10, 1, 0, 1, 10, 11, 5, 5, 2, 15, 7, 0, 1, 8, 8, 13, 0, 10, 19, 16, 1, 8, 8, 7, 0, 12, 5, 5, 8, 5, 5, 10, 8, 18, 3, 8, 11, 9, 13, 11, 5, 5, 16, 3, 7, 0, 15, 12, 12, 12, 3, 7, 10, 4, 5, 0, 0, 18, 10, 19, 4, 17, 6, 0, 9, 9, 5, 6, 6, 9, 4, 15, 6, 6, 4, 14, 2, 17, 9, 2, 2, 6, 4, 3, 12, 12, 12, 12, 12, 0, 11, 12, 2, 12, 12, 12, 12, 13, 12, 12, 2, 0, 3, 6, 12, 5, 0, 12, 11, 12, 4, 0, 10, 10, 10, 5, 13, 10, 10, 10, 14, 16, 16, 16, 16, 16, 16, 1, 16, 3, 16, 2, 6, 4, 7, 6, 14, 19, 7, 2, 19, 18, 0, 18, 18, 18, 1, 5, 11, 19, 7, 19, 18, 19, 19, 7, 7, 10, 1, 7, 18, 3, 1, 1, 0, 0, 0, 10, 10, 13, 1, 10, 10, 1, 10, 3, 0, 5, 3, 1, 1, 0, 1, 1, 1, 2, 3, 10, 3, 0, 0, 2, 13, 13, 10, 12, 6, 2, 16, 14, 6, 10, 10, 10, 18, 4, 19, 10, 1, 2, 13, 7, 18, 16, 3, 7, 12, 12, 12, 8, 12, 12, 12, 15, 12, 12, 19, 12, 0, 12, 5, 12, 11, 12, 11, 5, 5, 12, 10, 0, 13, 1, 7, 7, 9, 0, 3, 1, 6, 11, 3, 3, 0, 6, 3, 16, 8, 16, 0, 16, 15, 7, 1, 0, 4, 6, 9, 1, 7, 8, 0, 14, 6, 6, 9, 1, 3, 1, 3, 3, 0, 2, 7, 3, 6, 3, 9, 2, 1, 15, 7, 4, 19, 3, 8, 12, 11, 6, 0, 0, 3, 9, 4, 2, 19, 1, 9, 18, 3, 0, 1, 5, 19, 18, 5, 18, 1, 10, 18, 18, 18, 0, 18, 11, 5, 11, 7, 5, 5, 0, 0, 10, 10, 16, 0, 10, 1, 18, 16, 5, 5, 5, 1, 13, 16, 5, 1, 0, 4, 3, 1, 13, 0, 17, 1, 9, 9, 4, 13, 1, 3, 4, 4, 3, 9, 13, 6, 0, 17, 3, 10, 0, 4, 6, 9, 14, 14, 9, 6, 11, 19, 11, 0, 5, 13, 16, 14, 2, 2, 14, 14, 2, 13, 3, 11, 14, 2, 14, 13, 7, 7, 11, 18, 6, 11, 1, 6, 3, 7, 7, 1, 3, 7, 3, 15, 0, 15, 0, 13, 7, 13, 4, 15, 0, 13, 6, 15, 7, 17, 15, 3, 0, 18, 1, 1, 2, 4, 12, 12, 11, 5, 9, 11, 5, 15, 8, 15, 3, 6, 5, 2, 2, 14, 2, 3, 6, 1, 1, 12, 3, 6, 6, 0, 15, 6, 5, 6, 7, 0, 11, 2, 15, 3, 6, 11, 6, 2, 15, 10, 4, 8, 6, 8, 1, 1, 19, 8, 13, 11, 11, 19, 7, 11, 5, 1, 6, 0, 12, 12, 1, 6, 4, 11, 15, 0, 9, 11, 7, 16, 1, 12, 17, 2, 13, 12, 1, 14, 14, 2, 14, 2, 9, 6, 11, 7, 7, 15, 3, 17, 7, 14, 13, 1, 4, 8, 15, 8, 17, 17, 2, 15, 4, 8, 0, 17, 5, 5, 16, 1, 2, 4, 2, 3, 5, 5, 0, 6, 2, 2, 3, 1, 11, 1, 2, 9, 14, 2, 16, 11, 8, 13, 11, 6, 1, 0, 19, 8, 2, 14, 5, 18, 7, 7, 5, 5, 0, 1, 3, 4, 18, 18, 8, 8, 19, 12, 8, 0, 3, 15, 18, 6, 11, 1, 2, 8, 2, 18, 7, 12, 3, 16, 0, 8, 2, 0, 8, 4, 4, 13, 12, 4, 12, 12, 11, 14, 13, 12, 12, 6, 2, 14, 2, 5, 3, 13, 14, 11, 11, 11, 2, 17, 14, 11, 17, 1, 15, 15, 1, 4, 17, 5, 11, 13, 7, 9, 0, 4, 3, 4, 17, 0, 5, 8, 6, 7, 6, 4, 6, 12, 4, 9, 3, 12, 0, 17, 5, 1, 13, 5, 2, 18, 13, 8, 7, 5, 8, 11, 0, 1, 15, 18, 1, 1, 8, 16, 18, 14, 9, 8, 8, 8, 19, 11, 16, 16, 4, 0, 6, 3, 6, 5, 5, 2, 14, 4, 4, 4, 13, 2, 6, 7, 18, 12, 14, 11, 12, 15, 19, 14, 9, 12, 16, 16, 12, 17, 3, 2, 13, 5, 2, 4, 1, 14, 18, 15, 6, 4, 8, 3, 0, 0, 1, 3, 9, 4, 4, 0, 8, 4, 19, 8, 6, 15, 8, 17, 12, 19, 1, 15, 11, 0, 9, 19, 13, 17, 2, 0, 2, 14, 4, 2, 0, 1, 14, 0, 4, 16, 11, 0, 8, 5, 13, 4, 5, 19, 8, 8, 5, 17, 3, 12, 9, 12, 5, 3, 8, 0, 1, 8, 0, 8, 11, 8, 8, 8, 1, 8, 0, 16, 19, 4, 0, 12, 12, 8, 10, 13, 5, 10, 3, 1, 12, 12, 16, 18, 11, 2, 16, 11, 11, 5, 11, 3]\n",
            "-------RUN57-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': \"CountVectorizer(stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b02d2c50>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[9, 6, 9, 6, 8, 2, 17, 12, 4, 3, 1, 11, 8, 11, 3, 4, 10, 9, 7, 2, 8, 8, 4, 12, 12, 7, 15, 2, 4, 1, 1, 2, 9, 2, 7, 7, 11, 11, 17, 4, 12, 4, 8, 12, 5, 3, 8, 5, 12, 9, 8, 12, 7, 12, 4, 5, 5, 2, 7, 7, 0, 7, 7, 3, 4, 2, 0, 5, 0, 0, 7, 8, 10, 10, 1, 0, 1, 18, 2, 9, 5, 4, 7, 5, 5, 3, 5, 10, 4, 8, 10, 3, 7, 6, 7, 4, 3, 10, 0, 4, 4, 1, 1, 1, 1, 4, 2, 7, 18, 4, 6, 5, 15, 11, 4, 3, 3, 3, 12, 3, 3, 7, 12, 9, 15, 16, 8, 4, 12, 3, 4, 4, 2, 8, 0, 11, 1, 1, 8, 3, 3, 7, 3, 17, 4, 7, 9, 2, 9, 17, 7, 8, 2, 3, 18, 12, 0, 0, 7, 11, 1, 17, 3, 11, 4, 11, 11, 2, 7, 1, 3, 7, 2, 17, 4, 7, 11, 4, 3, 0, 2, 12, 3, 18, 8, 7, 6, 4, 8, 8, 7, 3, 5, 4, 17, 5, 0, 19, 5, 4, 5, 4, 5, 7, 7, 5, 3, 5, 10, 6, 0, 7, 3, 3, 8, 0, 7, 10, 4, 3, 0, 0, 0, 0, 7, 19, 8, 15, 9, 14, 14, 17, 14, 0, 0, 17, 7, 17, 17, 0, 7, 0, 11, 2, 1, 18, 1, 1, 0, 2, 1, 0, 9, 11, 1, 15, 15, 15, 0, 0, 8, 2, 11, 1, 2, 2, 4, 3, 1, 6, 10, 0, 0, 10, 0, 12, 0, 0, 3, 3, 10, 1, 10, 15, 3, 4, 7, 1, 1, 11, 6, 4, 6, 6, 3, 7, 7, 12, 0, 10, 11, 0, 8, 10, 0, 0, 0, 11, 18, 0, 12, 0, 15, 4, 3, 11, 12, 1, 12, 9, 0, 16, 1, 0, 17, 11, 11, 11, 1, 11, 11, 12, 8, 11, 18, 4, 7, 3, 0, 9, 4, 12, 3, 3, 0, 3, 3, 9, 17, 3, 2, 12, 2, 2, 5, 8, 4, 8, 8, 9, 12, 8, 1, 8, 8, 8, 7, 0, 15, 0, 8, 17, 5, 4, 0, 15, 14, 3, 9, 17, 16, 16, 2, 0, 5, 5, 8, 7, 8, 3, 7, 15, 8, 16, 16, 16, 16, 16, 16, 14, 11, 16, 19, 2, 0, 2, 16, 0, 14, 14, 8, 5, 2, 1, 17, 3, 17, 1, 1, 1, 1, 8, 11, 8, 18, 1, 18, 1, 0, 10, 0, 7, 10, 0, 10, 5, 10, 5, 10, 5, 10, 0, 11, 12, 9, 0, 7, 0, 0, 3, 7, 10, 0, 0, 2, 2, 0, 7, 0, 0, 13, 0, 18, 2, 0, 0, 0, 0, 9, 10, 10, 15, 2, 10, 0, 5, 0, 15, 8, 10, 0, 1, 10, 5, 18, 6, 11, 15, 18, 15, 18, 18, 7, 15, 1, 18, 0, 7, 6, 6, 6, 6, 9, 8, 3, 7, 0, 18, 6, 17, 16, 9, 6, 11, 6, 6, 6, 2, 8, 6, 10, 6, 12, 8, 6, 17, 12, 6, 11, 1, 6, 6, 12, 1, 12, 3, 11, 12, 8, 7, 15, 15, 11, 0, 10, 10, 8, 1, 15, 16, 2, 15, 15, 19, 19, 19, 19, 17, 3, 15, 4, 6, 8, 14, 14, 17, 14, 14, 18, 9, 19, 12, 8, 9, 1, 11, 12, 5, 0, 19, 19, 12, 6, 19, 12, 19, 5, 7, 0, 14, 14, 3, 8, 4, 5, 14, 12, 14, 14, 3, 17, 14, 7, 14, 17, 3, 16, 14, 3, 3, 7, 6, 8, 5, 8, 5, 6, 5, 8, 5, 8, 10, 10, 9, 2, 2, 2, 6, 1, 3, 5, 8, 3, 5, 5, 10, 5, 5, 0, 10, 10, 10, 0, 5, 11, 10, 10, 10, 12, 10, 0, 15, 6, 16, 2, 6, 17, 1, 18, 9, 6, 9, 6, 9, 6, 3, 6, 12, 5, 12, 2, 7, 3, 6, 6, 6, 2, 6, 9, 6, 16, 18, 6, 11, 1, 5, 15, 15, 8, 1, 15, 3, 18, 8, 15, 9, 8, 15, 15, 4, 6, 10, 3, 3, 19, 19, 1, 1, 9, 6, 4, 1, 4, 6, 17, 9, 4, 8, 19, 12, 2, 2, 14, 14, 6, 1, 6, 17, 18, 2, 14, 14, 12, 2, 17, 9, 17, 14, 9, 9, 6, 14, 16, 7, 14, 11, 7, 0, 13, 9, 11, 18, 7, 12, 1, 17, 13, 13, 13, 3, 12, 6, 0, 11, 1, 1, 16, 6, 16, 10, 10, 0, 9, 7, 1, 9, 5, 4, 7, 0, 0, 3, 5, 10, 5, 5, 10, 8, 5, 5, 5, 10, 5, 13, 13, 13, 13, 13, 2, 1, 13, 5, 13, 13, 17, 13, 0, 13, 13, 5, 1, 3, 10, 13, 9, 1, 13, 11, 13, 10, 2, 6, 6, 6, 9, 6, 6, 6, 6, 15, 18, 18, 18, 18, 18, 18, 2, 18, 7, 18, 7, 8, 0, 12, 3, 15, 16, 12, 8, 16, 16, 9, 16, 16, 9, 2, 9, 2, 16, 12, 16, 16, 16, 16, 12, 1, 6, 4, 12, 16, 3, 4, 7, 2, 2, 2, 11, 6, 0, 4, 6, 6, 4, 6, 4, 9, 9, 3, 4, 2, 9, 4, 2, 2, 5, 3, 6, 3, 1, 9, 8, 6, 12, 11, 13, 3, 5, 18, 8, 5, 11, 6, 6, 6, 0, 14, 6, 2, 8, 1, 1, 16, 18, 3, 12, 13, 14, 13, 14, 17, 13, 13, 19, 13, 13, 17, 13, 1, 13, 17, 17, 13, 13, 1, 9, 4, 13, 6, 2, 1, 4, 12, 8, 7, 1, 3, 4, 8, 11, 3, 3, 12, 4, 7, 18, 19, 18, 2, 18, 19, 12, 2, 1, 0, 6, 1, 4, 12, 19, 2, 15, 5, 4, 7, 2, 4, 4, 3, 3, 2, 7, 12, 3, 7, 3, 7, 5, 2, 0, 12, 0, 16, 7, 14, 13, 13, 11, 9, 2, 3, 8, 0, 7, 16, 2, 7, 9, 3, 1, 2, 4, 16, 16, 9, 16, 2, 6, 16, 16, 16, 1, 16, 13, 9, 13, 12, 11, 17, 2, 1, 6, 6, 18, 9, 6, 9, 9, 1, 9, 9, 11, 2, 17, 18, 17, 4, 1, 11, 3, 2, 6, 1, 10, 2, 7, 2, 0, 6, 4, 3, 10, 0, 7, 7, 11, 0, 1, 10, 7, 6, 2, 0, 8, 7, 15, 15, 7, 11, 2, 17, 11, 2, 11, 0, 18, 15, 8, 5, 3, 15, 8, 10, 7, 11, 15, 8, 15, 1, 12, 12, 2, 9, 0, 11, 2, 0, 7, 12, 12, 4, 7, 12, 3, 0, 1, 0, 1, 1, 12, 1, 10, 8, 17, 10, 4, 19, 12, 10, 0, 3, 2, 18, 2, 4, 3, 10, 13, 13, 0, 11, 1, 17, 11, 0, 14, 0, 3, 4, 11, 5, 5, 15, 5, 3, 4, 4, 4, 17, 3, 5, 4, 9, 0, 0, 9, 4, 12, 2, 0, 8, 0, 7, 0, 11, 0, 5, 0, 6, 11, 14, 17, 14, 4, 4, 16, 14, 8, 13, 11, 16, 12, 1, 11, 2, 4, 2, 17, 13, 2, 7, 11, 1, 0, 2, 1, 0, 12, 18, 2, 13, 5, 8, 1, 13, 4, 15, 5, 5, 15, 8, 1, 3, 13, 12, 12, 19, 7, 10, 12, 15, 1, 4, 0, 19, 19, 14, 10, 10, 8, 19, 0, 19, 2, 10, 11, 11, 18, 4, 5, 0, 5, 3, 9, 11, 1, 0, 5, 5, 5, 4, 1, 2, 5, 12, 5, 5, 18, 1, 14, 0, 13, 8, 2, 9, 16, 14, 15, 15, 9, 9, 12, 12, 9, 9, 2, 4, 7, 10, 16, 16, 14, 14, 14, 13, 14, 1, 7, 0, 16, 6, 11, 2, 5, 14, 5, 4, 12, 17, 3, 1, 9, 14, 8, 2, 19, 11, 0, 6, 13, 10, 13, 13, 1, 15, 12, 13, 13, 4, 5, 15, 8, 9, 4, 1, 15, 13, 0, 1, 5, 10, 15, 11, 10, 4, 19, 19, 2, 0, 10, 11, 0, 18, 12, 7, 2, 10, 3, 10, 10, 1, 17, 19, 5, 1, 0, 10, 5, 13, 0, 1, 7, 13, 1, 10, 9, 4, 8, 17, 5, 9, 6, 14, 12, 9, 14, 13, 9, 2, 8, 16, 2, 4, 14, 18, 6, 15, 1, 14, 14, 14, 16, 11, 1, 18, 0, 1, 4, 3, 11, 9, 9, 8, 5, 10, 0, 10, 6, 5, 0, 1, 17, 13, 15, 1, 13, 19, 16, 15, 2, 13, 18, 18, 13, 10, 3, 8, 0, 9, 5, 0, 4, 15, 16, 0, 0, 10, 19, 3, 1, 1, 2, 4, 7, 10, 0, 1, 19, 11, 14, 14, 0, 19, 19, 10, 13, 16, 4, 0, 17, 1, 2, 16, 18, 10, 5, 2, 5, 15, 0, 5, 9, 4, 5, 12, 10, 18, 17, 9, 17, 11, 1, 0, 11, 14, 14, 14, 11, 10, 7, 13, 7, 13, 9, 3, 14, 1, 4, 14, 9, 19, 11, 14, 14, 14, 2, 14, 1, 18, 14, 0, 2, 13, 13, 14, 6, 8, 9, 6, 3, 4, 13, 13, 17, 17, 13, 5, 18, 1, 13, 2, 13, 3]\n",
            "-------RUN58-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': \"CountVectorizer(ngram_range=(1, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b891dd10>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[6, 11, 6, 11, 8, 0, 12, 12, 1, 1, 12, 13, 8, 9, 1, 14, 7, 13, 12, 0, 14, 8, 1, 3, 3, 1, 4, 2, 0, 12, 2, 0, 6, 12, 12, 1, 13, 13, 6, 1, 3, 0, 8, 3, 4, 14, 8, 4, 3, 6, 8, 3, 12, 12, 14, 4, 4, 0, 12, 12, 7, 12, 12, 1, 0, 12, 9, 4, 15, 15, 1, 8, 7, 18, 7, 15, 2, 3, 0, 6, 4, 14, 1, 4, 4, 14, 4, 18, 0, 7, 18, 4, 4, 11, 1, 14, 1, 18, 14, 14, 14, 3, 2, 2, 2, 1, 0, 1, 3, 0, 7, 4, 16, 13, 0, 16, 16, 1, 3, 12, 1, 1, 3, 6, 8, 17, 8, 0, 3, 14, 0, 0, 0, 4, 15, 13, 2, 2, 8, 1, 1, 1, 1, 17, 14, 12, 6, 2, 0, 9, 8, 8, 12, 1, 2, 12, 9, 9, 12, 2, 2, 10, 1, 7, 0, 13, 13, 0, 1, 2, 1, 8, 0, 6, 14, 12, 2, 1, 14, 9, 12, 12, 1, 3, 8, 9, 14, 1, 8, 8, 1, 14, 4, 1, 5, 4, 9, 19, 4, 0, 4, 14, 4, 4, 1, 4, 4, 4, 18, 6, 7, 1, 1, 14, 4, 9, 9, 18, 0, 4, 9, 9, 9, 15, 1, 19, 8, 16, 6, 5, 5, 17, 5, 9, 7, 5, 12, 5, 5, 9, 9, 15, 13, 2, 12, 8, 2, 2, 9, 0, 2, 9, 6, 13, 3, 16, 16, 16, 9, 15, 8, 0, 10, 2, 0, 0, 1, 14, 2, 11, 15, 15, 9, 18, 15, 3, 15, 15, 1, 14, 7, 2, 15, 16, 1, 14, 12, 2, 2, 13, 11, 1, 11, 11, 1, 9, 1, 3, 7, 7, 13, 7, 8, 7, 15, 7, 15, 15, 6, 15, 3, 7, 8, 1, 1, 13, 3, 2, 2, 6, 9, 17, 2, 9, 6, 13, 7, 13, 2, 13, 7, 3, 8, 13, 3, 14, 1, 1, 15, 13, 1, 12, 1, 16, 9, 14, 1, 6, 5, 1, 0, 3, 0, 0, 4, 8, 0, 8, 8, 6, 3, 8, 2, 8, 8, 8, 12, 7, 16, 7, 8, 5, 4, 1, 14, 5, 5, 14, 6, 5, 17, 17, 6, 14, 4, 14, 8, 12, 8, 1, 9, 16, 8, 17, 17, 17, 17, 17, 17, 5, 10, 17, 9, 0, 9, 0, 17, 15, 5, 5, 8, 4, 0, 2, 5, 14, 5, 2, 2, 2, 2, 14, 13, 12, 3, 12, 3, 2, 9, 18, 7, 12, 7, 15, 18, 18, 18, 4, 18, 4, 18, 15, 7, 3, 6, 15, 1, 15, 9, 1, 1, 15, 15, 15, 2, 0, 15, 12, 9, 15, 10, 15, 3, 0, 9, 15, 15, 15, 13, 18, 18, 16, 0, 18, 7, 4, 15, 16, 7, 18, 7, 2, 18, 4, 6, 14, 13, 16, 3, 16, 17, 3, 12, 16, 2, 6, 9, 12, 11, 11, 11, 11, 13, 8, 1, 9, 9, 8, 11, 2, 11, 6, 11, 13, 11, 11, 11, 2, 8, 11, 7, 11, 3, 8, 11, 6, 3, 11, 2, 2, 11, 6, 8, 2, 3, 14, 13, 3, 8, 1, 16, 16, 7, 7, 7, 18, 8, 2, 16, 17, 0, 16, 16, 19, 19, 19, 19, 5, 1, 16, 0, 11, 8, 5, 5, 5, 5, 19, 5, 6, 19, 12, 8, 13, 2, 13, 3, 4, 7, 19, 19, 3, 11, 19, 3, 19, 4, 12, 15, 5, 5, 1, 8, 0, 4, 5, 3, 5, 5, 1, 5, 5, 12, 5, 5, 14, 17, 5, 1, 14, 1, 14, 9, 4, 8, 4, 11, 4, 8, 4, 8, 18, 18, 6, 12, 2, 0, 11, 2, 4, 4, 8, 14, 4, 4, 7, 4, 4, 7, 18, 18, 18, 7, 4, 13, 18, 7, 7, 12, 15, 15, 16, 7, 17, 0, 11, 5, 2, 3, 6, 11, 0, 11, 6, 11, 4, 11, 3, 14, 3, 0, 12, 1, 11, 11, 11, 0, 11, 2, 11, 17, 3, 11, 13, 2, 4, 16, 16, 8, 12, 16, 14, 6, 8, 16, 13, 8, 16, 16, 0, 11, 18, 14, 1, 19, 19, 2, 7, 2, 11, 0, 6, 0, 11, 6, 13, 0, 8, 19, 3, 0, 0, 5, 5, 7, 2, 11, 5, 5, 0, 5, 5, 3, 0, 5, 0, 6, 5, 6, 13, 11, 5, 17, 1, 5, 9, 12, 7, 10, 13, 13, 6, 1, 3, 2, 5, 10, 10, 10, 1, 3, 11, 7, 13, 2, 2, 17, 11, 17, 7, 7, 14, 6, 12, 12, 6, 15, 14, 12, 15, 9, 14, 4, 15, 4, 4, 18, 8, 4, 4, 4, 15, 4, 10, 10, 10, 10, 10, 0, 2, 10, 4, 10, 10, 5, 10, 7, 10, 10, 4, 2, 1, 14, 10, 6, 2, 10, 13, 10, 7, 2, 11, 11, 11, 6, 11, 11, 11, 11, 16, 3, 5, 3, 3, 3, 3, 12, 3, 1, 3, 4, 8, 15, 3, 14, 16, 17, 3, 8, 17, 17, 6, 17, 17, 6, 0, 6, 0, 17, 3, 17, 17, 17, 17, 3, 3, 11, 0, 3, 17, 1, 1, 1, 0, 2, 12, 13, 11, 7, 0, 11, 11, 0, 11, 1, 2, 0, 1, 0, 0, 6, 0, 0, 0, 4, 1, 11, 1, 2, 6, 8, 7, 3, 13, 10, 14, 4, 3, 8, 15, 13, 11, 11, 11, 7, 5, 11, 0, 8, 7, 3, 17, 3, 1, 3, 10, 5, 10, 5, 10, 10, 10, 10, 10, 10, 5, 10, 2, 10, 6, 5, 10, 10, 2, 6, 0, 10, 11, 0, 2, 0, 3, 8, 12, 2, 4, 0, 8, 9, 1, 1, 2, 14, 1, 3, 19, 3, 0, 3, 19, 3, 0, 2, 7, 14, 2, 0, 3, 19, 12, 16, 4, 14, 12, 0, 1, 0, 1, 1, 2, 8, 3, 1, 4, 1, 1, 4, 0, 9, 3, 7, 17, 1, 5, 10, 10, 13, 6, 0, 1, 8, 9, 8, 17, 0, 12, 6, 14, 2, 12, 13, 17, 6, 6, 17, 0, 11, 17, 17, 17, 2, 17, 10, 6, 10, 3, 13, 6, 2, 2, 11, 11, 3, 6, 11, 0, 6, 2, 6, 6, 13, 0, 6, 3, 6, 0, 2, 15, 1, 0, 7, 3, 18, 0, 12, 12, 15, 11, 1, 1, 15, 7, 1, 12, 7, 9, 2, 18, 1, 11, 2, 15, 7, 9, 16, 16, 12, 14, 0, 17, 13, 0, 13, 7, 6, 16, 8, 4, 16, 16, 8, 7, 1, 13, 16, 8, 16, 2, 3, 3, 0, 6, 15, 13, 0, 9, 1, 3, 3, 14, 1, 3, 1, 9, 2, 9, 2, 7, 12, 2, 15, 8, 6, 7, 14, 15, 3, 18, 9, 1, 0, 6, 12, 1, 4, 15, 10, 10, 7, 13, 12, 6, 13, 9, 5, 9, 1, 14, 13, 4, 4, 16, 4, 14, 14, 14, 0, 5, 1, 9, 14, 6, 9, 9, 13, 14, 3, 0, 9, 8, 9, 1, 9, 9, 9, 4, 9, 11, 13, 5, 14, 5, 0, 0, 17, 5, 7, 10, 13, 17, 12, 2, 13, 0, 14, 2, 10, 10, 0, 9, 13, 2, 9, 2, 12, 9, 3, 3, 0, 10, 18, 8, 2, 10, 0, 16, 4, 4, 16, 8, 12, 14, 10, 3, 3, 19, 1, 18, 3, 16, 2, 1, 7, 19, 19, 5, 18, 7, 8, 19, 15, 19, 0, 18, 13, 13, 5, 0, 4, 15, 4, 1, 6, 13, 2, 9, 4, 4, 4, 0, 2, 0, 4, 12, 16, 4, 6, 2, 5, 7, 10, 9, 0, 6, 17, 5, 16, 16, 6, 6, 3, 3, 6, 6, 0, 0, 1, 7, 17, 17, 5, 5, 5, 10, 5, 2, 1, 9, 17, 14, 9, 0, 4, 5, 4, 14, 3, 5, 1, 2, 6, 5, 8, 2, 19, 15, 15, 7, 10, 7, 10, 10, 2, 16, 7, 10, 10, 14, 4, 16, 8, 6, 14, 7, 16, 10, 9, 2, 4, 18, 16, 9, 7, 0, 19, 19, 0, 9, 18, 13, 9, 3, 3, 12, 12, 15, 1, 7, 18, 2, 6, 19, 14, 3, 15, 15, 4, 10, 15, 12, 1, 10, 2, 18, 0, 0, 12, 6, 4, 6, 7, 5, 3, 13, 5, 10, 6, 0, 8, 17, 0, 0, 5, 5, 11, 16, 12, 5, 5, 5, 17, 13, 3, 3, 15, 2, 1, 1, 13, 6, 6, 8, 16, 15, 15, 15, 11, 4, 9, 3, 6, 10, 16, 2, 10, 19, 17, 16, 12, 10, 3, 3, 10, 18, 1, 4, 7, 13, 4, 15, 0, 16, 17, 9, 9, 15, 19, 1, 2, 2, 0, 1, 1, 7, 7, 2, 19, 13, 5, 5, 9, 9, 19, 18, 10, 17, 0, 9, 10, 2, 12, 17, 3, 18, 4, 2, 4, 16, 7, 4, 2, 0, 4, 3, 7, 5, 12, 2, 5, 13, 12, 7, 13, 5, 5, 5, 13, 18, 4, 10, 12, 10, 0, 1, 5, 2, 0, 5, 6, 19, 13, 5, 5, 19, 0, 5, 2, 3, 5, 15, 0, 10, 10, 5, 11, 7, 6, 11, 1, 1, 10, 10, 5, 6, 10, 4, 3, 2, 10, 0, 10, 1]\n",
            "-------RUN59-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': \"CountVectorizer(ngram_range=(2, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df4a9008410>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[7, 9, 7, 9, 3, 2, 12, 8, 0, 0, 12, 14, 13, 5, 0, 5, 13, 14, 12, 1, 5, 13, 0, 8, 8, 0, 3, 2, 1, 12, 2, 1, 7, 12, 12, 0, 14, 14, 7, 0, 8, 1, 13, 8, 3, 0, 13, 3, 8, 7, 13, 8, 12, 12, 1, 3, 3, 1, 12, 12, 11, 12, 12, 0, 1, 12, 6, 3, 6, 6, 0, 13, 6, 19, 11, 6, 2, 17, 1, 7, 3, 1, 0, 3, 3, 15, 3, 19, 1, 13, 19, 0, 3, 9, 0, 1, 0, 19, 5, 1, 1, 8, 2, 2, 2, 0, 1, 0, 17, 1, 11, 3, 15, 14, 1, 15, 15, 0, 8, 12, 0, 0, 8, 7, 15, 18, 13, 1, 8, 5, 1, 1, 1, 3, 6, 11, 2, 2, 3, 0, 0, 0, 0, 18, 1, 12, 7, 2, 1, 16, 13, 13, 12, 0, 17, 12, 6, 11, 12, 11, 2, 10, 0, 11, 1, 14, 14, 2, 0, 2, 0, 13, 1, 7, 5, 12, 2, 0, 15, 16, 12, 8, 0, 17, 13, 5, 5, 1, 13, 13, 0, 0, 3, 1, 4, 3, 16, 16, 5, 1, 3, 5, 3, 13, 0, 3, 0, 3, 19, 17, 11, 0, 0, 5, 3, 5, 5, 19, 1, 0, 16, 5, 6, 6, 0, 16, 3, 15, 7, 4, 4, 18, 4, 16, 11, 4, 12, 4, 17, 16, 12, 6, 14, 2, 12, 13, 2, 2, 5, 2, 2, 16, 7, 14, 8, 15, 15, 15, 5, 6, 13, 1, 10, 2, 1, 12, 0, 5, 2, 9, 6, 6, 11, 19, 6, 8, 6, 6, 0, 5, 11, 2, 6, 15, 0, 5, 12, 2, 2, 14, 9, 1, 9, 9, 0, 5, 0, 8, 11, 6, 14, 11, 13, 19, 6, 6, 6, 6, 17, 6, 8, 6, 3, 15, 0, 14, 8, 2, 8, 7, 16, 18, 2, 16, 7, 14, 11, 14, 2, 14, 11, 8, 13, 14, 17, 5, 0, 0, 6, 14, 1, 12, 0, 15, 5, 0, 0, 7, 4, 0, 1, 8, 1, 1, 5, 13, 1, 13, 13, 7, 8, 3, 2, 5, 13, 13, 12, 11, 15, 6, 13, 4, 3, 0, 5, 4, 4, 5, 7, 4, 18, 18, 7, 5, 3, 5, 13, 12, 13, 0, 5, 15, 13, 18, 18, 18, 18, 18, 18, 17, 16, 18, 16, 1, 16, 1, 18, 6, 4, 4, 13, 3, 1, 2, 4, 0, 10, 2, 2, 2, 2, 5, 14, 12, 17, 12, 17, 11, 5, 19, 6, 12, 6, 6, 19, 19, 19, 3, 19, 3, 19, 6, 11, 8, 7, 6, 0, 6, 5, 0, 0, 6, 6, 6, 2, 1, 6, 12, 16, 6, 10, 6, 17, 1, 16, 6, 6, 6, 14, 19, 19, 15, 1, 19, 11, 3, 6, 15, 13, 19, 13, 2, 19, 3, 17, 5, 14, 15, 17, 15, 17, 8, 12, 15, 2, 17, 5, 12, 9, 9, 9, 9, 14, 5, 0, 13, 16, 17, 9, 7, 9, 7, 9, 14, 9, 9, 9, 2, 13, 9, 6, 9, 8, 13, 9, 7, 8, 9, 2, 2, 9, 9, 13, 11, 8, 5, 14, 8, 13, 0, 15, 15, 11, 11, 6, 19, 13, 2, 15, 18, 7, 15, 15, 4, 16, 16, 16, 4, 0, 15, 1, 9, 13, 4, 4, 4, 4, 4, 4, 7, 16, 8, 13, 14, 2, 14, 8, 3, 11, 16, 16, 8, 9, 16, 8, 16, 3, 12, 6, 4, 4, 0, 13, 1, 3, 4, 8, 4, 4, 0, 4, 4, 12, 4, 4, 0, 18, 4, 0, 15, 0, 11, 16, 3, 13, 5, 9, 3, 13, 3, 5, 19, 19, 7, 12, 2, 1, 9, 2, 0, 3, 13, 3, 5, 3, 11, 3, 3, 11, 19, 19, 19, 11, 3, 11, 19, 11, 6, 12, 6, 6, 15, 11, 18, 1, 9, 4, 2, 17, 7, 9, 7, 9, 7, 9, 3, 9, 8, 3, 8, 1, 12, 0, 9, 9, 9, 1, 9, 2, 9, 18, 17, 9, 14, 2, 3, 15, 15, 13, 12, 15, 5, 17, 13, 15, 7, 3, 3, 15, 1, 9, 19, 15, 0, 16, 16, 2, 11, 2, 9, 1, 7, 1, 9, 7, 7, 1, 13, 16, 8, 1, 1, 4, 4, 11, 2, 9, 4, 17, 1, 4, 4, 8, 12, 4, 7, 7, 4, 7, 7, 9, 4, 18, 0, 4, 14, 12, 11, 10, 14, 14, 17, 0, 8, 2, 16, 10, 10, 10, 0, 8, 9, 6, 14, 2, 2, 18, 9, 18, 6, 19, 5, 7, 12, 12, 7, 5, 5, 13, 6, 16, 5, 5, 6, 3, 3, 19, 13, 3, 3, 5, 6, 3, 10, 10, 10, 10, 10, 7, 2, 10, 3, 10, 10, 4, 10, 11, 10, 10, 3, 2, 0, 5, 10, 7, 2, 10, 14, 10, 6, 2, 9, 9, 9, 7, 9, 9, 9, 9, 15, 17, 17, 17, 17, 17, 17, 12, 8, 0, 17, 3, 5, 6, 8, 5, 15, 18, 8, 3, 18, 18, 7, 18, 18, 7, 1, 7, 12, 18, 8, 18, 18, 18, 18, 8, 8, 9, 1, 8, 18, 0, 0, 0, 2, 2, 12, 14, 9, 11, 1, 9, 9, 1, 9, 0, 2, 1, 0, 1, 1, 2, 1, 1, 1, 3, 0, 9, 0, 2, 7, 13, 11, 8, 14, 10, 5, 3, 17, 13, 5, 14, 9, 9, 9, 11, 4, 9, 1, 3, 11, 8, 18, 17, 0, 8, 10, 4, 10, 4, 10, 10, 10, 16, 10, 10, 4, 10, 2, 10, 7, 10, 10, 10, 2, 7, 1, 10, 9, 1, 2, 1, 8, 13, 0, 2, 0, 1, 5, 16, 0, 0, 8, 5, 0, 17, 4, 17, 1, 17, 16, 8, 1, 2, 11, 5, 12, 1, 8, 4, 12, 15, 5, 5, 12, 1, 0, 1, 0, 0, 2, 13, 8, 0, 5, 0, 0, 3, 1, 16, 8, 11, 18, 0, 4, 10, 10, 14, 7, 1, 0, 13, 6, 13, 18, 1, 12, 7, 5, 17, 12, 14, 18, 18, 7, 18, 1, 9, 18, 18, 18, 2, 18, 10, 7, 10, 8, 14, 7, 2, 2, 9, 9, 17, 7, 9, 7, 9, 17, 7, 7, 14, 1, 17, 17, 7, 1, 2, 6, 0, 1, 11, 8, 19, 1, 12, 12, 6, 17, 0, 0, 6, 11, 0, 12, 11, 5, 2, 19, 0, 9, 2, 6, 11, 12, 15, 15, 12, 5, 1, 18, 14, 2, 14, 11, 17, 15, 13, 3, 15, 15, 13, 11, 0, 14, 15, 13, 15, 11, 8, 8, 12, 7, 5, 14, 1, 5, 0, 8, 8, 1, 0, 8, 0, 16, 2, 16, 2, 11, 8, 11, 6, 13, 7, 11, 5, 16, 8, 19, 11, 0, 1, 17, 12, 1, 3, 6, 10, 10, 11, 14, 12, 7, 14, 16, 4, 16, 0, 5, 14, 3, 3, 15, 3, 0, 5, 1, 1, 4, 0, 5, 5, 7, 16, 5, 14, 5, 8, 12, 11, 13, 16, 0, 11, 14, 5, 3, 11, 9, 14, 4, 5, 4, 1, 1, 18, 4, 11, 10, 14, 18, 13, 11, 14, 1, 5, 2, 10, 10, 1, 5, 14, 2, 16, 2, 12, 11, 8, 17, 1, 10, 3, 13, 11, 10, 1, 15, 3, 3, 15, 13, 12, 5, 10, 8, 8, 16, 0, 19, 8, 15, 11, 1, 6, 4, 16, 4, 19, 19, 13, 16, 6, 4, 2, 19, 14, 14, 17, 1, 3, 6, 3, 0, 7, 14, 2, 5, 3, 3, 3, 1, 2, 1, 3, 12, 15, 3, 17, 2, 4, 11, 10, 5, 1, 7, 18, 4, 3, 15, 7, 7, 8, 8, 7, 7, 2, 1, 0, 6, 18, 18, 4, 4, 4, 10, 4, 2, 0, 16, 18, 5, 16, 1, 3, 4, 3, 1, 8, 4, 0, 17, 7, 4, 13, 2, 4, 6, 6, 11, 10, 6, 10, 10, 2, 15, 11, 10, 10, 5, 3, 15, 3, 7, 0, 11, 15, 10, 16, 2, 3, 19, 15, 11, 19, 1, 16, 16, 7, 11, 19, 14, 16, 8, 8, 12, 12, 6, 0, 6, 19, 2, 7, 4, 5, 8, 5, 6, 5, 10, 6, 12, 0, 10, 2, 19, 1, 1, 11, 7, 3, 7, 11, 4, 8, 14, 4, 10, 7, 1, 13, 17, 1, 1, 4, 17, 9, 15, 12, 4, 4, 4, 18, 14, 8, 17, 6, 2, 5, 0, 14, 7, 7, 13, 3, 6, 6, 6, 9, 3, 5, 8, 7, 10, 15, 2, 10, 16, 18, 15, 12, 10, 17, 17, 10, 19, 0, 3, 11, 14, 3, 6, 1, 15, 18, 16, 5, 6, 16, 0, 2, 2, 1, 0, 13, 6, 11, 2, 4, 14, 4, 4, 5, 16, 16, 19, 10, 18, 1, 16, 10, 2, 12, 18, 17, 19, 3, 2, 3, 15, 6, 3, 2, 1, 3, 8, 6, 17, 12, 2, 4, 14, 11, 11, 14, 4, 4, 4, 14, 6, 0, 10, 12, 10, 7, 0, 4, 2, 1, 4, 7, 4, 14, 4, 4, 4, 1, 4, 2, 17, 4, 6, 2, 10, 10, 4, 9, 11, 7, 9, 0, 1, 10, 10, 17, 7, 10, 3, 17, 17, 10, 7, 10, 0]\n",
            "-------RUN60-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': \"CountVectorizer(stop_words='english')\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[-1, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 1, -1, -1, 0, -1, -1, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, 1, 0, 0, 0, -1, -1, -1, -1, -1, 1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, 1, -1, 1, -1, 1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 1, -1, 0, -1, 0, -1, -1, 0, -1, 1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, 1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, -1, -1, 0, 0, 1, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, -1, 1, -1, 1, 0, -1, -1, 1, -1, -1, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, 1, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, 1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, 1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, 1, 0, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, 1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, 1, -1, -1, 0, 0, 0, 0, 1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1]\n",
            "-------RUN61-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': 'CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7df3b00f5c90>)', 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[-1, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 1, -1, -1, 0, -1, -1, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, 1, 0, 0, 0, -1, -1, -1, -1, -1, 1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, 1, -1, 1, -1, 1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 1, -1, 0, -1, 0, -1, -1, 0, -1, 1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, 1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, -1, -1, 0, 0, 1, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, -1, 1, -1, 1, 0, -1, 0, 1, -1, -1, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, 1, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, 1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, 1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, 1, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, 1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, 1, -1, -1, 0, 0, 0, 0, 1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1]\n",
            "-------RUN62-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': \"CountVectorizer(stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b02d2c50>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[-1, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 1, -1, -1, 0, -1, -1, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, 1, 0, 0, 0, -1, -1, -1, -1, -1, 1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, 1, 0, 1, -1, 1, -1, 1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 1, -1, 0, -1, 0, -1, -1, 0, -1, 1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, 1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, -1, -1, 0, 0, 1, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, -1, 1, -1, 1, 0, -1, 0, 1, -1, -1, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, 1, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, 1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, 1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, 1, 0, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, 1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, 1, -1, -1, 0, 0, 0, 0, 1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1]\n",
            "-------RUN63-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': \"CountVectorizer(ngram_range=(1, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b891dd10>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[-1, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 1, -1, -1, 0, -1, -1, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, 1, 0, 0, 0, -1, -1, -1, -1, -1, 1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, 1, 0, 1, -1, 1, -1, 1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 1, -1, 0, -1, 0, -1, -1, 0, -1, 1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, 1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, -1, -1, 0, 0, 1, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, -1, 1, -1, 1, 0, -1, 0, 1, -1, -1, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, 1, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, 1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, 1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, 1, 0, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, 1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, 1, -1, -1, 0, 0, 0, 0, 1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1]\n",
            "-------RUN64-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': \"CountVectorizer(ngram_range=(2, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df4a9008410>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[-1, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 1, -1, -1, 0, -1, -1, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, 1, 0, 0, 0, -1, -1, -1, -1, -1, 1, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, 1, 0, 1, -1, 1, -1, 1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 1, -1, 0, -1, 0, -1, -1, 0, -1, 1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, 1, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, -1, -1, 0, 0, 1, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, -1, 1, -1, 1, 0, -1, -1, 1, -1, -1, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, 1, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, 1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, 1, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, 1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, 1, -1, -1, 0, 0, 0, 0, 1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1]\n",
            "-------RUN65-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': \"CountVectorizer(stop_words='english')\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[-1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 1, 1, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 1, 0, -1, -1, -1, -1, -1, -1, 1, 0, -1, 1, -1, -1, -1, 4, 4, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 5, 0, -1, -1, 3, -1, -1, -1, -1, 0, -1, 4, -1, -1, 4, -1, -1, 4, -1, 0, -1, 3, -1, -1, -1, -1, -1, 0, 3, -1, -1, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 1, 0, -1, -1, -1, 0, 1, -1, 0, 0, -1, -1, -1, -1, 0, 0, 4, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 4, -1, -1, 4, -1, -1, -1, -1, 4, -1, -1, -1, -1, -1, -1, -1, 3, -1, -1, 0, -1, -1, -1, -1, -1, 3, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 5, -1, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 5, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, 5, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 0, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 2, -1, -1, -1, -1, 6, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 6, 6, -1, 6, 6, 6, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 3, -1, 3, -1, 3, -1, 3, -1, -1, -1, -1, -1, -1, -1, -1, 5, -1, -1, 0, 5, -1, -1, -1, 0, 5, -1, -1, 5, -1, -1, 0, 0, -1, 5, 5, -1, -1, -1, 3, -1, 0, -1, -1, -1, -1, -1, -1, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 5, 3, -1, 0, -1, -1, 0, -1, -1, -1, 7, -1, -1, -1, -1, -1, 0, -1, -1, -1, 2, -1, -1, -1, -1, -1, 7, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 1, -1, 7, -1, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, 0, -1, 2, -1, -1, -1, 0, -1, 2, -1, -1, 0, -1, -1, -1, -1, 4, -1, -1, -1, -1, -1, 3, -1, 0, -1, 0, 0, 0, -1, -1, 4, -1, -1, -1, -1, -1, 4, 4, -1, -1, -1, 3, -1, -1, -1, 3, -1, -1, -1, 5, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, 1, -1, 1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 3, -1, -1, 7, 7, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 2, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 0, -1, -1, 2, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, 0, 0, -1, 4, -1, -1, -1, -1, -1, 2, 2, 4, -1, 0, -1, -1, 0, -1, 2, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, 6, -1, -1, -1, -1, 6, -1, 1, -1, 0, 1, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 7, 1, 0, 0, -1, -1, 0, 0, 1, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, 1, -1, -1, -1, -1, 4, -1, -1, 1, -1, 6, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 6, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, 2, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 3, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, 3, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, 0, -1, 0, 1, 1, -1, 0, 1, 0, -1, 0, -1, 0, -1, 1, -1, -1, -1, 0, -1, -1, -1, -1, 3, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, 6, 2, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 2, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, -1, 0, -1, 1, -1, -1, -1, -1, -1, -1, -1, 3, -1, -1, 7, -1, -1, 0, 3, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, 0, -1, -1, -1, -1, -1, 0, -1, 1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 2, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, 4, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 5, -1, 0, -1, -1, 3, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 2, -1, 0, -1, -1, -1, 2, 2, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 7, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, 2, -1, -1, 7, -1, -1, -1, -1, -1, -1, 0, 0, 6, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
            "-------RUN66-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': 'CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7df3b00f5c90>)', 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[-1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 1, 1, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 1, 0, -1, -1, -1, -1, -1, -1, 1, 0, -1, 1, -1, -1, -1, 4, 4, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 7, 0, -1, -1, 3, -1, -1, -1, -1, 0, -1, 4, -1, -1, 4, -1, -1, 4, -1, 0, -1, 3, -1, -1, -1, -1, -1, 0, 3, -1, -1, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 1, 0, -1, -1, -1, 0, 1, -1, 0, 0, 0, -1, -1, -1, 0, 0, 4, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 4, -1, -1, 4, -1, -1, -1, -1, 4, -1, -1, -1, -1, -1, -1, -1, 3, -1, -1, 0, -1, -1, -1, -1, -1, 3, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 7, -1, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 7, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 0, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 7, -1, -1, -1, 0, -1, -1, 2, -1, -1, -1, -1, 5, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 5, 5, -1, 5, 5, 5, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 2, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 3, -1, 3, -1, 3, -1, 3, -1, -1, -1, -1, -1, -1, -1, -1, 7, -1, -1, 0, 7, -1, -1, -1, 0, 7, -1, -1, 7, -1, -1, 0, 0, -1, 7, -1, -1, -1, -1, 3, -1, 0, -1, -1, -1, -1, -1, -1, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 3, -1, 0, -1, -1, 0, -1, -1, 6, 6, -1, -1, -1, -1, -1, 0, -1, -1, -1, 2, -1, -1, -1, -1, -1, 6, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 1, -1, 6, -1, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, 0, -1, 2, -1, -1, -1, 0, -1, 2, -1, -1, 0, -1, -1, -1, -1, 4, -1, -1, -1, -1, -1, 3, -1, 0, -1, 0, 0, 0, -1, -1, 4, -1, -1, -1, -1, -1, -1, 4, -1, -1, -1, 3, -1, -1, -1, 3, -1, -1, -1, 7, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, 1, -1, 1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 3, -1, -1, 6, 6, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 2, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 0, -1, -1, 2, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, 0, 0, -1, 4, -1, -1, -1, -1, -1, 2, 2, 4, 0, 0, -1, -1, 0, -1, 2, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, -1, -1, 5, -1, 0, -1, 0, 0, 0, -1, 5, -1, -1, -1, -1, 5, -1, -1, -1, 0, 1, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 6, 1, 0, 0, -1, -1, 0, 0, 1, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, 1, -1, -1, -1, -1, 4, -1, -1, 1, -1, 5, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 5, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, 2, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 3, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, 3, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, 0, -1, 0, 1, 1, -1, 0, 1, 0, -1, 0, -1, 0, -1, 1, -1, -1, -1, 0, -1, -1, -1, -1, 3, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, 5, 2, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 2, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, -1, 0, -1, 1, -1, -1, -1, -1, -1, -1, -1, 3, -1, -1, 6, -1, -1, 0, 3, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, 0, -1, -1, -1, -1, -1, 0, -1, 1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 2, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, 4, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 7, -1, 0, -1, -1, 3, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 2, -1, -1, -1, -1, -1, 2, 2, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 6, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, 2, -1, -1, 6, -1, -1, -1, -1, -1, -1, 0, 0, 5, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
            "-------RUN67-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': \"CountVectorizer(stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b02d2c50>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 1, -1, -1, 0, -1, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, -1, -1, 1, 0, 0, 0, -1, 0, -1, -1, -1, 1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 1, -1, 0, -1, 0, -1, -1, 0, -1, 1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, 1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, 1, 0, -1, 0, 1, -1, 0, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, 1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, 1, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, 1, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, 1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 1, -1, -1, 0, 0, 0, 0, 1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1]\n",
            "-------RUN68-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': \"CountVectorizer(ngram_range=(1, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b891dd10>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 1, -1, -1, 0, 0, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, -1, -1, 1, 0, 0, 0, -1, 0, -1, -1, -1, 1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 1, -1, 1, -1, 1, -1, 1, -1, 1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 1, -1, 0, -1, 0, -1, -1, 0, -1, 1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, 1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, 1, 0, -1, 0, 1, -1, 0, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, 1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, 1, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, 1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 1, -1, -1, 0, 0, 0, 0, 1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0]\n",
            "-------RUN69-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': \"CountVectorizer(ngram_range=(2, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df4a9008410>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 1, -1, -1, 0, -1, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, -1, -1, 1, 0, 0, 0, -1, 0, -1, -1, -1, 1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 1, -1, 1, -1, 1, -1, 1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 1, -1, 0, -1, 0, -1, -1, 0, -1, 1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, 1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, 1, 0, -1, 0, 1, -1, 0, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, 1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, 0, 1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, 1, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, 1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 1, -1, -1, 0, 0, 0, 0, 1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1]\n",
            "-------RUN70-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': \"CountVectorizer(stop_words='english')\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[-1, -1, -1, -1, -1, 0, -1, -1, -1, 15, -1, -1, -1, -1, 15, -1, -1, -1, -1, 0, -1, 9, -1, 2, 2, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 2, 0, -1, -1, -1, -1, -1, 10, 2, -1, -1, 2, -1, -1, -1, 3, 3, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 14, 4, 0, 14, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 3, -1, -1, 3, -1, 0, -1, 1, -1, -1, -1, -1, -1, 15, 1, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 2, 0, -1, -1, -1, 0, 2, -1, 0, -1, 0, -1, -1, -1, 0, 0, 3, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 15, 0, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, -1, 0, 15, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 10, -1, -1, 3, 0, -1, 3, -1, -1, -1, -1, 3, -1, 3, -1, 0, -1, -1, -1, 1, -1, 14, 0, -1, -1, -1, -1, -1, 1, 0, -1, -1, -1, -1, 9, 0, -1, 10, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 4, -1, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, 12, -1, -1, 9, 9, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, 4, 4, -1, -1, 4, -1, 4, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 11, -1, 11, -1, -1, -1, 0, -1, -1, -1, 4, 4, 9, -1, -1, -1, 14, -1, -1, -1, 2, -1, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 10, -1, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 5, -1, -1, -1, 6, 6, 0, -1, -1, -1, 10, -1, -1, 0, -1, -1, -1, 6, 6, 6, 6, 6, 6, -1, -1, -1, -1, 0, -1, 0, 6, 1, -1, 7, -1, 10, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 9, -1, 9, 0, 1, -1, 1, -1, 1, -1, 1, -1, -1, 1, -1, -1, -1, 4, -1, 4, 9, -1, -1, 4, -1, -1, -1, 0, 4, -1, -1, 4, -1, -1, 0, 0, -1, 4, -1, -1, -1, -1, 1, -1, 0, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, 12, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 11, 17, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 17, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 18, -1, -1, -1, -1, -1, 14, 4, 1, -1, 0, 12, -1, 0, 10, -1, 7, 7, -1, -1, -1, -1, -1, 0, -1, -1, 5, 5, -1, -1, -1, -1, -1, 7, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 2, -1, 7, 18, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 5, -1, 0, -1, 5, -1, -1, -1, 15, 6, 5, -1, -1, 0, -1, -1, -1, -1, -1, -1, 3, -1, 3, -1, 1, -1, -1, -1, 0, 0, -1, -1, -1, 3, -1, 3, -1, -1, -1, -1, 3, -1, 1, -1, 1, -1, -1, -1, 1, -1, -1, -1, 4, -1, -1, -1, 11, 0, -1, -1, 0, -1, 0, 17, 0, -1, 0, -1, 0, -1, 2, -1, 2, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 17, -1, 0, -1, 12, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 10, 0, -1, 1, -1, -1, 7, 7, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 5, -1, -1, 0, -1, 0, 0, 5, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 2, 0, -1, 8, 8, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 4, -1, -1, -1, 1, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 8, -1, 19, 0, 0, -1, 3, -1, -1, -1, -1, -1, 8, 8, -1, 0, 0, -1, -1, 0, -1, 8, -1, -1, -1, 0, 11, -1, -1, 0, -1, -1, -1, 17, -1, 16, 16, 0, -1, 16, 16, 0, -1, -1, 16, -1, -1, 4, 2, -1, -1, -1, 2, -1, -1, 6, -1, -1, -1, -1, 0, 0, -1, 6, -1, -1, -1, -1, 6, -1, -1, -1, 0, 2, 11, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 3, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 8, 8, -1, -1, -1, -1, 8, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, 3, 0, -1, -1, 0, 0, -1, -1, -1, 16, -1, 16, 0, -1, 7, 2, -1, 0, -1, -1, 0, 0, 2, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, 2, -1, -1, 15, -1, 10, -1, -1, 2, -1, 6, -1, 5, -1, 19, -1, -1, 0, -1, -1, 9, -1, -1, -1, -1, -1, 13, -1, -1, -1, 6, -1, 0, -1, 0, -1, -1, -1, -1, 0, 11, -1, 0, 8, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 13, -1, -1, -1, 0, -1, 0, 1, 0, -1, 0, 4, -1, 0, -1, 1, -1, -1, -1, -1, -1, 0, 1, -1, -1, 0, -1, 9, -1, 12, 12, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 12, -1, 18, 18, -1, -1, -1, -1, 0, -1, 0, 2, 2, -1, 0, 2, 0, -1, 0, -1, 0, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 5, -1, 0, 13, -1, -1, -1, -1, -1, 13, 13, 13, -1, -1, -1, -1, 13, -1, -1, -1, -1, 13, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 11, -1, -1, -1, -1, 0, -1, 6, 5, -1, -1, -1, 6, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, 2, -1, 0, -1, 2, -1, -1, -1, -1, 7, -1, -1, 1, -1, -1, 7, 9, 7, 0, 1, -1, -1, -1, 0, 10, -1, 3, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, 5, -1, 19, -1, 0, -1, -1, -1, 10, -1, -1, -1, 2, -1, -1, -1, -1, 0, -1, 4, -1, -1, -1, -1, 5, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, 3, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 4, -1, 4, -1, -1, -1, 8, -1, -1, -1, -1, -1, -1, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 12, -1, -1, 0, 7, 7, -1, -1, 1, -1, -1, -1, 18, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, 9, 1, -1, -1, 4, -1, 0, -1, -1, 1, -1, -1, 14, 0, 3, 0, -1, -1, -1, -1, 5, -1, 0, 0, -1, -1, 0, 0, 5, -1, 11, -1, -1, -1, 5, 5, 6, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, 3, 4, -1, -1, -1, -1, -1, -1, -1, 8, -1, 0, -1, -1, 6, -1, 0, -1, -1, -1, 8, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 4, 7, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, 5, -1, -1, 7, -1, -1, -1, -1, -1, -1, 0, 0, 6, 0, -1, -1, 0, -1, -1, 14, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 5, 5, 19, -1, -1, -1, -1, 8, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 5, -1, -1, -1, -1, -1, -1, 8, -1, 17, -1, -1, -1, 0, 0, -1, -1, -1, -1, 19, 3, -1, -1, -1, 0, -1, -1]\n",
            "-------RUN71-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': 'CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7df3b00f5c90>)', 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[-1, -1, -1, -1, -1, 0, -1, -1, -1, 18, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 9, -1, 3, 3, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 3, 0, -1, -1, -1, -1, -1, 10, 3, -1, -1, 3, -1, -1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 16, 6, 0, 16, -1, 2, -1, -1, -1, -1, 0, -1, -1, -1, -1, 1, -1, 10, 1, -1, 0, -1, 2, -1, -1, -1, -1, -1, 18, 2, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 3, 0, -1, -1, -1, 0, 3, -1, 0, -1, 0, -1, -1, -1, 0, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 18, 0, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, -1, 0, 18, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 10, -1, -1, 1, 0, -1, 1, -1, -1, -1, -1, 1, -1, 1, -1, 0, -1, -1, -1, 2, -1, 16, 0, -1, -1, -1, -1, -1, 2, 0, -1, -1, -1, -1, 9, 0, -1, 10, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 6, -1, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, 12, -1, -1, 9, 9, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 6, 6, -1, 2, 6, -1, 6, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 11, -1, 11, -1, -1, -1, 0, -1, -1, -1, 15, 15, 9, -1, -1, -1, 16, -1, -1, -1, 3, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 10, -1, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 4, -1, -1, -1, 5, 5, 0, -1, -1, -1, 10, -1, -1, 0, -1, -1, -1, 5, 5, 5, 5, 5, 5, -1, -1, -1, -1, 0, -1, 0, 5, -1, -1, 7, -1, 10, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 9, -1, 9, 0, 2, -1, 2, -1, 2, -1, 2, -1, -1, 2, -1, -1, -1, 6, -1, 6, 9, -1, -1, 6, -1, -1, -1, 0, 6, -1, -1, 6, -1, -1, 0, 0, -1, 6, -1, -1, -1, -1, 2, -1, 0, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, 12, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 11, 17, 17, 0, -1, -1, -1, -1, -1, -1, -1, 11, -1, 17, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 19, -1, -1, -1, -1, -1, 16, -1, 2, -1, 0, 12, -1, 0, 10, -1, 7, 7, -1, -1, -1, -1, -1, 0, -1, -1, 4, 4, -1, -1, -1, -1, -1, 7, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 3, -1, 7, 19, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 4, -1, 0, -1, 4, -1, -1, -1, 18, 5, 4, -1, -1, 0, -1, -1, -1, -1, 1, -1, 1, -1, 1, -1, 2, -1, -1, -1, 0, 0, 11, -1, -1, 1, -1, 1, 1, -1, -1, -1, 1, -1, 2, -1, 2, -1, -1, -1, 2, -1, -1, -1, 6, -1, -1, -1, 11, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 3, -1, 3, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 17, -1, 0, -1, 12, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 10, 0, -1, 2, -1, -1, 7, 7, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 4, -1, -1, 0, -1, 0, 0, 4, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 3, 0, -1, 8, 8, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 6, -1, -1, -1, 2, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 8, -1, 20, 0, 0, -1, 1, -1, -1, -1, -1, -1, 8, 8, -1, 0, 0, -1, -1, -1, -1, 8, -1, -1, -1, 0, 11, -1, -1, 0, -1, -1, -1, 17, -1, 13, 13, 0, -1, 13, 13, 0, -1, -1, 13, -1, -1, 6, 3, -1, -1, -1, 3, -1, -1, 5, -1, -1, -1, -1, 0, 0, -1, 5, -1, -1, -1, -1, 5, -1, -1, -1, 0, 3, 11, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 8, 8, -1, 4, -1, -1, 8, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, 1, 0, -1, -1, 0, 0, -1, -1, -1, 13, -1, 13, 0, -1, 7, 3, -1, 0, -1, -1, 0, 0, 3, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, 3, -1, -1, 18, -1, 10, -1, -1, 3, -1, 5, -1, 4, -1, 20, -1, -1, 0, -1, -1, 9, -1, -1, -1, -1, -1, 14, -1, -1, -1, 5, -1, 0, -1, 0, -1, 5, -1, -1, 0, 11, -1, 0, 8, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 14, -1, -1, -1, 0, -1, -1, 2, 0, -1, 0, 6, -1, 0, -1, 2, -1, -1, -1, -1, -1, 0, 2, -1, -1, 0, -1, 9, -1, 12, 12, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 12, -1, 19, 19, -1, -1, -1, -1, 0, -1, 0, 3, 3, -1, 0, 3, 0, -1, 0, -1, 0, -1, 3, -1, -1, -1, 0, -1, -1, -1, -1, 2, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 4, -1, 0, 14, -1, -1, -1, -1, 1, 14, 14, 14, -1, -1, -1, -1, 14, -1, -1, -1, -1, 14, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 11, -1, -1, -1, -1, 0, -1, 5, 4, -1, -1, -1, 5, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 13, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 3, 3, -1, 0, -1, 3, -1, -1, -1, -1, 7, -1, -1, 2, -1, -1, 7, 9, 7, 0, 2, -1, -1, -1, 0, 10, -1, 1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, 4, -1, -1, -1, 0, -1, -1, -1, 10, -1, -1, -1, 3, -1, -1, -1, -1, 0, -1, 15, -1, -1, -1, -1, 4, 8, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 15, -1, 15, -1, -1, -1, 8, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 12, -1, -1, 0, 7, 7, -1, -1, 2, -1, -1, -1, 19, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, 9, 2, 1, -1, 6, -1, 0, -1, -1, 2, -1, -1, -1, 0, 1, -1, -1, -1, -1, -1, 4, -1, 0, 0, -1, -1, 0, 0, 4, -1, 11, -1, -1, -1, 4, 4, 5, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, 1, 15, -1, -1, -1, -1, -1, -1, -1, 8, -1, 0, -1, -1, 5, -1, 0, -1, -1, -1, 8, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 15, 7, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, 4, -1, -1, 7, -1, -1, -1, -1, -1, 20, 0, 0, 5, -1, -1, -1, 0, -1, -1, 16, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 4, 20, -1, -1, -1, -1, 8, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 4, -1, -1, -1, -1, -1, -1, -1, -1, 17, -1, -1, -1, 0, 0, -1, -1, -1, -1, 20, 1, -1, -1, -1, -1, -1, -1]\n",
            "-------RUN72-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': \"CountVectorizer(stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b02d2c50>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 1, 0, 0, 0, -1, 0, 1, 1, 1, -1, 0, -1, 0, 1, 0, 0, -1, 0, 1, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1]\n",
            "-------RUN73-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': \"CountVectorizer(ngram_range=(1, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b891dd10>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[-1, -1, -1, -1, -1, 0, -1, -1, -1, 16, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 2, 2, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 2, 0, -1, -1, -1, -1, -1, 10, 2, -1, -1, 2, -1, -1, -1, 4, 4, -1, -1, -1, 3, -1, -1, -1, 0, -1, -1, -1, 15, 3, 0, 15, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 4, -1, -1, 4, -1, 0, -1, 1, -1, -1, -1, -1, -1, 16, 1, -1, -1, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 2, 0, -1, -1, -1, 0, 2, -1, 0, 0, 0, -1, -1, -1, 0, 0, 4, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 16, 0, -1, -1, 1, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, -1, 0, 16, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 10, -1, -1, 4, 0, -1, 4, -1, -1, -1, -1, 4, -1, 4, -1, 0, -1, -1, -1, 1, -1, 15, 0, -1, -1, -1, -1, -1, 1, 0, -1, 14, -1, -1, 12, 0, -1, 10, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 3, -1, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, 20, -1, -1, 12, 12, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 3, 3, -1, 1, 3, -1, 3, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 9, 0, 9, -1, -1, -1, 0, -1, -1, -1, 3, 3, -1, -1, -1, -1, 15, -1, -1, -1, 2, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, 3, -1, -1, -1, -1, 0, -1, -1, 0, 10, -1, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 5, -1, -1, -1, 6, 6, 0, -1, -1, -1, 10, -1, -1, 0, -1, -1, -1, 6, 6, 6, 6, 6, 6, -1, -1, -1, -1, 0, -1, 0, 6, -1, -1, -1, -1, 10, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 12, -1, 12, 0, 1, -1, 1, -1, 1, -1, 1, -1, -1, 1, -1, -1, -1, 3, -1, 3, 12, -1, -1, 3, -1, -1, -1, 0, 3, -1, -1, 3, -1, -1, 0, 0, 14, 3, -1, -1, -1, -1, 1, -1, 0, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 9, 17, -1, 0, -1, -1, -1, -1, -1, -1, -1, 9, -1, 17, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 18, -1, -1, -1, -1, -1, 15, 3, 1, -1, 0, 20, -1, 0, 10, -1, -1, 8, -1, -1, -1, -1, -1, 0, -1, -1, 5, 5, -1, -1, -1, -1, -1, 8, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 2, -1, 8, 18, 8, -1, -1, -1, -1, 5, -1, -1, -1, -1, -1, 2, 5, -1, 0, -1, 5, -1, -1, -1, 16, 6, 5, -1, -1, 0, -1, -1, -1, -1, 4, -1, 4, -1, 4, -1, 1, -1, -1, -1, 0, 0, -1, -1, -1, 4, -1, 4, -1, -1, -1, -1, 4, -1, 1, -1, 1, -1, -1, -1, 1, -1, -1, -1, 3, -1, -1, -1, 9, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 2, -1, 2, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 17, -1, 0, -1, 20, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 10, 0, -1, 1, -1, -1, 8, 8, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 5, -1, -1, 0, -1, 0, -1, 5, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 14, -1, 0, -1, -1, 2, 0, -1, 7, 7, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 3, -1, -1, -1, 1, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 7, -1, 19, 0, 0, -1, 4, -1, -1, -1, -1, -1, 7, 7, -1, 0, 0, -1, -1, -1, -1, 7, -1, -1, -1, 0, 9, -1, -1, 0, -1, -1, -1, 17, -1, 11, 11, 0, -1, 11, 11, 0, -1, -1, 11, -1, -1, 3, 2, -1, -1, -1, 2, -1, -1, 6, -1, 9, 9, -1, 0, 0, -1, 6, -1, -1, -1, -1, 6, -1, -1, -1, 0, 2, 9, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 4, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, 4, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 7, 7, -1, 5, -1, -1, 7, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 4, 0, -1, -1, 0, 0, -1, -1, -1, 11, -1, 11, 0, -1, 8, 2, -1, 0, -1, -1, 0, 0, 2, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, 2, -1, -1, 16, -1, 10, -1, -1, 2, -1, 6, -1, 5, -1, 19, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 13, -1, -1, -1, 6, -1, 0, -1, 0, -1, 9, -1, -1, 0, 9, -1, 0, 7, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 1, 0, -1, 0, 3, -1, 0, -1, 1, -1, -1, -1, -1, -1, 0, 1, -1, -1, 0, -1, -1, -1, 20, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 20, -1, 18, 18, -1, -1, -1, -1, 0, -1, 0, 2, 2, -1, 0, 2, 0, -1, 0, -1, 0, -1, 2, -1, -1, -1, 0, -1, -1, -1, -1, 1, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 5, -1, 0, 13, -1, -1, -1, -1, -1, 13, 13, 13, -1, -1, -1, -1, 13, -1, -1, -1, -1, 13, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 9, -1, -1, -1, -1, 0, -1, 6, 5, -1, -1, -1, 6, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 11, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, 2, -1, 0, -1, 2, -1, -1, -1, -1, 8, -1, -1, 1, -1, -1, 8, 12, 8, 0, 1, -1, -1, -1, 0, 10, -1, 4, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, 5, -1, 19, -1, 0, -1, -1, -1, 10, -1, -1, -1, 2, -1, -1, -1, -1, 0, -1, 3, -1, -1, -1, -1, 5, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, 4, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 3, -1, 3, -1, -1, -1, 7, -1, 0, -1, -1, -1, -1, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 8, 8, -1, -1, 1, -1, 14, -1, 18, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, 12, 1, -1, -1, 3, -1, 0, -1, -1, 1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 5, -1, 0, 0, -1, -1, 0, 0, 5, -1, 9, -1, -1, -1, 5, 5, 6, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, 4, 3, -1, -1, -1, -1, -1, -1, -1, 7, -1, 0, -1, -1, 6, -1, 0, -1, -1, -1, 7, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 3, 8, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, 5, -1, 14, 8, -1, -1, -1, -1, 14, -1, 0, 0, 6, -1, -1, -1, 0, -1, -1, 15, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 5, 5, 19, -1, -1, -1, -1, 7, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 5, -1, -1, -1, -1, -1, -1, -1, -1, 17, -1, -1, -1, 0, 0, -1, -1, -1, -1, 19, 4, -1, -1, -1, 0, -1, -1]\n",
            "-------RUN74-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': \"CountVectorizer(ngram_range=(2, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df4a9008410>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[-1, -1, -1, -1, -1, 0, -1, -1, -1, 14, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 9, -1, 2, 2, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 2, 0, -1, -1, -1, -1, -1, 10, 2, -1, -1, 2, -1, -1, -1, -1, 3, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 13, 6, 0, 13, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 3, -1, 10, 3, -1, 0, -1, 1, -1, -1, -1, -1, -1, 14, 1, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 2, 0, -1, -1, -1, 0, 2, -1, 0, -1, 0, -1, -1, -1, 0, 0, 3, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 14, 0, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, -1, 0, 14, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 3, 0, -1, 3, -1, -1, -1, -1, 3, -1, 3, -1, 0, -1, -1, -1, 1, -1, 13, 0, -1, -1, -1, -1, -1, 1, 0, -1, -1, -1, -1, 9, 0, -1, 10, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 6, -1, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, 11, -1, -1, 9, 9, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 6, 6, -1, 1, 6, -1, 6, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 9, -1, -1, -1, 13, -1, -1, -1, 2, -1, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 10, -1, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 5, -1, -1, -1, 4, 4, 0, -1, -1, -1, 10, -1, -1, 0, -1, -1, -1, 4, 4, 4, 4, 4, 4, -1, -1, -1, -1, 0, -1, 0, 4, -1, -1, -1, -1, 10, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 9, -1, 9, 0, 1, -1, 1, -1, 1, -1, 1, -1, -1, 1, -1, -1, -1, 6, -1, 6, 9, -1, -1, 6, -1, -1, -1, 0, 6, -1, -1, 6, -1, -1, 0, 0, -1, 6, -1, -1, -1, -1, 1, -1, 0, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, 11, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 16, -1, 0, -1, -1, -1, -1, -1, -1, -1, 17, -1, 16, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 18, -1, -1, -1, -1, -1, 13, -1, 1, -1, 0, 11, -1, 0, 10, -1, 8, 8, -1, -1, -1, -1, -1, 0, -1, -1, 5, 5, -1, -1, -1, -1, -1, 8, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 2, -1, 8, 18, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 5, -1, 0, -1, 5, -1, -1, -1, 14, 4, 5, -1, -1, 0, -1, -1, -1, -1, -1, -1, 3, -1, 3, -1, 1, -1, -1, -1, 0, 0, -1, -1, -1, 3, -1, 3, -1, -1, -1, -1, 3, -1, 1, -1, 1, -1, -1, -1, 1, -1, -1, -1, 6, -1, -1, -1, 17, 0, -1, -1, 0, -1, 0, 16, 0, -1, 0, -1, 0, -1, 2, -1, 2, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 16, -1, 0, -1, 11, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 10, 0, -1, 1, -1, -1, 8, 8, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 5, -1, -1, 0, -1, 0, 0, 5, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 2, 0, -1, 7, 7, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 6, -1, -1, -1, 1, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 7, -1, -1, 0, 0, -1, 3, -1, -1, -1, -1, -1, 7, 7, -1, 0, 0, -1, -1, 0, -1, 7, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, 16, -1, 15, 15, 0, -1, 15, 15, 0, -1, -1, 15, -1, -1, 6, 2, -1, -1, -1, 2, -1, -1, 4, -1, -1, -1, -1, 0, 0, -1, 4, -1, -1, -1, -1, 4, -1, -1, -1, 0, 2, 17, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 3, 14, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 7, 7, -1, 7, -1, -1, 7, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, 3, 0, -1, -1, 0, 0, -1, -1, -1, 15, -1, 15, 0, -1, 8, 2, -1, 0, -1, -1, 0, 0, 2, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, 2, -1, -1, 14, -1, 3, -1, -1, 2, -1, 4, -1, 5, -1, -1, -1, -1, 0, -1, -1, 9, -1, -1, -1, -1, -1, 12, -1, -1, -1, 4, -1, 0, -1, 0, -1, -1, -1, -1, 0, 17, -1, 0, 7, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 12, -1, -1, -1, 0, -1, -1, 1, 0, -1, 0, 6, -1, 0, -1, 1, -1, -1, -1, -1, -1, 0, 1, -1, -1, 0, -1, 9, -1, 11, 11, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 11, -1, -1, -1, -1, -1, -1, 11, -1, 18, 18, -1, -1, -1, -1, 0, -1, 0, 2, 2, -1, 0, 2, 0, -1, 0, -1, 0, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 5, -1, 0, 12, -1, -1, -1, -1, -1, 12, 12, 12, -1, -1, -1, -1, 12, -1, -1, -1, -1, 12, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 13, 0, -1, -1, -1, -1, 0, -1, 4, 5, -1, -1, -1, 4, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, 2, -1, 0, -1, 2, -1, -1, -1, -1, 8, -1, -1, 1, -1, -1, 8, 9, 8, 0, 1, -1, -1, -1, 0, 10, -1, 3, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, 5, -1, -1, -1, 0, -1, -1, -1, 10, -1, -1, -1, 2, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 5, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, 3, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 7, -1, -1, -1, -1, -1, -1, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 11, -1, -1, 0, 8, 8, -1, -1, 1, -1, -1, -1, 18, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, 9, 1, -1, -1, 6, -1, 0, -1, -1, 1, -1, -1, -1, 0, 3, 0, -1, -1, -1, -1, 5, -1, 0, 0, -1, -1, 0, 0, 5, -1, 17, -1, -1, -1, 5, 5, 4, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, 3, -1, -1, -1, -1, -1, -1, -1, -1, 7, -1, 0, -1, -1, 4, -1, 0, -1, -1, -1, 7, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 8, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, 5, -1, -1, 8, -1, -1, -1, -1, -1, -1, 0, 0, 4, -1, -1, -1, 0, -1, -1, 13, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 5, -1, -1, -1, -1, -1, 7, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 5, -1, -1, -1, -1, -1, -1, -1, -1, 16, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 3, -1, -1, -1, -1, -1, -1]\n",
            "-------RUN75-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': \"CountVectorizer(stop_words='english')\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[6, 6, 6, 6, 3, 0, 18, 7, 7, 7, 0, 0, 1, 8, 7, 2, 1, 13, 0, 0, 8, 12, 2, 15, 15, 4, 9, 0, 2, 0, 0, 2, 6, 0, 2, 4, 8, 13, 7, 2, 15, 2, 9, 15, 3, 4, 9, 9, 15, 6, 9, 15, 8, 15, 8, 3, 3, 2, 0, 4, 12, 7, 7, 7, 2, 2, 1, 1, 1, 12, 4, 1, 14, 14, 11, 12, 0, 18, 2, 6, 3, 2, 4, 3, 3, 9, 3, 14, 2, 14, 14, 7, 3, 10, 4, 7, 7, 14, 1, 4, 2, 0, 0, 11, 11, 4, 2, 7, 18, 7, 12, 9, 9, 8, 7, 7, 3, 4, 18, 7, 4, 4, 15, 6, 9, 17, 1, 2, 15, 3, 2, 2, 2, 3, 1, 13, 6, 0, 3, 4, 7, 2, 7, 17, 7, 7, 13, 0, 6, 1, 7, 7, 15, 7, 0, 0, 1, 1, 0, 13, 11, 6, 4, 13, 2, 8, 8, 0, 4, 0, 7, 4, 0, 6, 2, 4, 13, 4, 7, 12, 7, 0, 7, 18, 3, 7, 10, 2, 1, 3, 4, 3, 3, 2, 5, 3, 1, 1, 3, 2, 3, 6, 3, 1, 4, 3, 3, 3, 14, 11, 1, 4, 4, 2, 3, 1, 1, 14, 2, 7, 1, 1, 1, 1, 4, 19, 9, 9, 6, 5, 5, 17, 5, 1, 1, 5, 4, 5, 5, 1, 8, 12, 12, 0, 13, 18, 0, 0, 8, 0, 11, 1, 6, 8, 18, 9, 9, 9, 1, 1, 4, 0, 0, 18, 0, 2, 4, 4, 6, 6, 12, 12, 12, 14, 12, 18, 12, 1, 4, 8, 14, 0, 12, 9, 4, 8, 4, 15, 0, 13, 10, 2, 11, 10, 7, 4, 2, 11, 19, 12, 12, 12, 1, 12, 12, 12, 1, 12, 11, 12, 15, 12, 9, 7, 4, 8, 11, 0, 0, 6, 1, 17, 11, 12, 0, 8, 8, 8, 0, 8, 13, 11, 3, 8, 11, 8, 4, 7, 1, 0, 2, 4, 4, 9, 8, 7, 4, 6, 5, 7, 2, 15, 2, 2, 7, 18, 8, 3, 4, 8, 18, 3, 0, 9, 3, 1, 6, 1, 9, 12, 18, 19, 1, 2, 7, 5, 5, 7, 11, 5, 17, 17, 2, 8, 7, 7, 3, 7, 3, 4, 4, 9, 3, 17, 17, 17, 17, 17, 17, 5, 13, 17, 1, 0, 8, 2, 17, 1, 5, 5, 9, 3, 2, 0, 5, 4, 5, 0, 0, 11, 11, 3, 10, 18, 11, 11, 18, 11, 1, 14, 1, 4, 14, 12, 14, 14, 14, 7, 14, 14, 14, 1, 10, 11, 6, 12, 4, 12, 1, 7, 4, 12, 1, 12, 11, 2, 12, 0, 1, 12, 16, 12, 18, 6, 1, 12, 12, 12, 6, 14, 14, 9, 2, 14, 1, 9, 12, 9, 18, 14, 1, 13, 14, 3, 11, 10, 13, 9, 11, 9, 11, 18, 0, 9, 13, 11, 1, 0, 10, 10, 10, 10, 8, 7, 4, 1, 1, 18, 11, 8, 17, 10, 10, 10, 10, 10, 10, 10, 3, 10, 12, 10, 15, 3, 10, 5, 15, 10, 0, 0, 10, 10, 15, 11, 11, 7, 8, 15, 9, 4, 9, 9, 11, 1, 12, 14, 4, 0, 9, 17, 6, 9, 9, 19, 19, 19, 19, 5, 4, 9, 2, 10, 7, 5, 5, 5, 5, 5, 5, 13, 19, 15, 15, 6, 0, 8, 15, 3, 1, 19, 19, 15, 10, 19, 15, 19, 3, 4, 12, 5, 5, 4, 18, 4, 7, 5, 15, 5, 5, 4, 5, 5, 7, 5, 5, 7, 17, 5, 4, 7, 4, 10, 3, 3, 3, 3, 10, 3, 18, 3, 3, 14, 14, 11, 11, 11, 2, 10, 0, 3, 3, 3, 3, 3, 3, 14, 3, 3, 1, 14, 14, 14, 1, 14, 10, 14, 14, 12, 15, 12, 1, 7, 10, 17, 2, 10, 5, 0, 11, 6, 10, 6, 10, 8, 10, 4, 10, 15, 3, 15, 2, 4, 4, 10, 10, 10, 2, 10, 6, 10, 17, 11, 10, 13, 0, 9, 9, 9, 9, 18, 9, 8, 11, 9, 9, 6, 3, 9, 9, 2, 6, 14, 7, 4, 19, 19, 0, 1, 0, 10, 2, 11, 2, 6, 0, 6, 2, 11, 19, 18, 6, 0, 5, 5, 10, 0, 10, 5, 5, 2, 5, 5, 18, 0, 5, 6, 6, 5, 6, 13, 10, 5, 17, 2, 5, 13, 15, 1, 13, 6, 8, 11, 7, 15, 0, 19, 16, 16, 16, 4, 11, 11, 1, 8, 6, 6, 17, 10, 17, 1, 1, 1, 11, 13, 15, 13, 7, 2, 15, 12, 1, 7, 3, 14, 3, 3, 14, 3, 3, 3, 3, 14, 7, 19, 16, 16, 16, 13, 6, 0, 16, 3, 16, 16, 5, 16, 8, 16, 16, 3, 0, 4, 14, 16, 6, 0, 16, 13, 16, 1, 0, 11, 10, 10, 8, 10, 10, 10, 10, 9, 18, 18, 18, 18, 18, 18, 2, 18, 7, 18, 4, 3, 12, 15, 7, 9, 17, 15, 9, 17, 17, 6, 17, 17, 6, 0, 6, 2, 17, 15, 17, 17, 17, 17, 18, 15, 6, 2, 15, 17, 4, 2, 2, 0, 0, 0, 12, 6, 1, 6, 6, 6, 2, 10, 4, 6, 13, 7, 2, 0, 6, 2, 2, 2, 3, 4, 10, 4, 0, 11, 3, 11, 11, 10, 16, 4, 3, 11, 9, 3, 10, 10, 6, 10, 1, 5, 10, 0, 9, 8, 18, 6, 15, 7, 11, 16, 16, 16, 5, 16, 16, 16, 13, 16, 16, 5, 16, 0, 16, 13, 16, 13, 16, 1, 13, 13, 16, 10, 0, 15, 2, 15, 18, 4, 0, 3, 2, 3, 8, 4, 4, 6, 7, 4, 18, 19, 18, 0, 18, 19, 15, 8, 0, 1, 2, 0, 2, 15, 19, 0, 9, 4, 2, 4, 2, 4, 2, 4, 4, 0, 4, 15, 4, 7, 7, 4, 3, 4, 8, 15, 1, 17, 4, 5, 16, 13, 8, 11, 0, 7, 7, 1, 7, 17, 2, 0, 6, 7, 11, 2, 8, 17, 17, 6, 17, 2, 6, 17, 17, 17, 0, 17, 0, 6, 13, 15, 13, 13, 0, 0, 6, 6, 18, 6, 10, 6, 6, 11, 8, 8, 10, 8, 8, 18, 8, 8, 11, 12, 4, 2, 11, 11, 14, 2, 4, 0, 12, 11, 2, 2, 14, 1, 7, 2, 0, 8, 0, 14, 4, 10, 0, 10, 12, 1, 9, 9, 2, 8, 8, 5, 8, 8, 8, 8, 11, 9, 9, 7, 7, 9, 9, 14, 4, 0, 9, 18, 9, 0, 15, 15, 0, 6, 8, 1, 2, 1, 4, 15, 15, 6, 4, 15, 4, 12, 11, 1, 0, 0, 15, 11, 14, 1, 0, 14, 8, 12, 15, 14, 1, 4, 0, 11, 4, 2, 7, 12, 16, 16, 1, 13, 15, 13, 13, 1, 5, 19, 4, 8, 8, 3, 3, 7, 3, 7, 8, 8, 8, 5, 7, 3, 8, 8, 1, 3, 8, 8, 11, 11, 1, 3, 1, 2, 1, 13, 13, 3, 1, 10, 8, 5, 7, 5, 2, 2, 17, 5, 11, 16, 13, 17, 4, 0, 13, 2, 7, 8, 5, 16, 2, 7, 8, 13, 1, 8, 0, 13, 11, 18, 2, 16, 14, 3, 11, 19, 7, 9, 3, 7, 9, 9, 0, 9, 13, 15, 15, 19, 4, 14, 15, 9, 0, 2, 1, 19, 19, 16, 14, 12, 7, 19, 1, 19, 0, 14, 8, 13, 5, 2, 9, 12, 3, 7, 6, 13, 11, 3, 7, 9, 3, 2, 0, 2, 3, 7, 3, 4, 11, 13, 5, 8, 13, 1, 0, 6, 17, 5, 9, 9, 6, 10, 15, 15, 6, 6, 2, 2, 4, 12, 17, 17, 5, 5, 5, 16, 5, 0, 7, 8, 17, 11, 1, 2, 3, 5, 3, 6, 18, 5, 7, 18, 0, 5, 3, 6, 19, 12, 12, 11, 16, 12, 16, 16, 13, 9, 12, 16, 16, 1, 3, 9, 3, 6, 2, 1, 9, 13, 1, 0, 3, 14, 9, 1, 1, 2, 19, 19, 6, 1, 14, 13, 1, 18, 15, 4, 0, 14, 4, 14, 14, 0, 6, 19, 3, 0, 1, 14, 3, 16, 12, 0, 2, 16, 6, 14, 6, 2, 1, 6, 3, 6, 11, 5, 18, 6, 5, 13, 6, 2, 1, 17, 2, 2, 5, 18, 17, 9, 18, 5, 5, 5, 17, 13, 18, 18, 12, 0, 4, 4, 8, 6, 0, 3, 3, 12, 12, 12, 10, 3, 1, 18, 6, 16, 9, 0, 16, 16, 17, 9, 0, 16, 18, 18, 16, 14, 4, 9, 10, 10, 3, 12, 2, 9, 17, 1, 8, 12, 19, 4, 0, 0, 2, 7, 4, 12, 1, 0, 19, 10, 5, 5, 8, 19, 19, 14, 13, 17, 2, 1, 13, 0, 0, 17, 11, 14, 3, 0, 3, 7, 1, 14, 0, 2, 3, 11, 1, 18, 13, 6, 5, 8, 0, 1, 13, 5, 5, 5, 13, 1, 4, 16, 4, 16, 8, 4, 5, 0, 2, 5, 6, 19, 13, 5, 5, 5, 2, 5, 6, 11, 5, 1, 6, 16, 16, 5, 10, 8, 6, 10, 4, 2, 16, 16, 5, 5, 13, 3, 18, 13, 16, 0, 0, 7]\n",
            "-------RUN76-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': 'CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7df3b00f5c90>)', 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[3, 3, 1, 8, 10, 0, 11, 9, 9, 9, 0, 0, 2, 2, 9, 1, 11, 13, 0, 0, 5, 10, 1, 7, 7, 4, 12, 0, 1, 0, 0, 1, 1, 0, 0, 4, 5, 13, 9, 4, 7, 1, 12, 7, 10, 4, 12, 12, 7, 1, 12, 7, 5, 7, 5, 17, 17, 1, 4, 4, 11, 9, 9, 19, 4, 1, 2, 10, 2, 18, 4, 11, 15, 15, 11, 18, 0, 11, 1, 1, 17, 1, 4, 17, 17, 12, 17, 15, 1, 15, 15, 9, 17, 3, 19, 9, 9, 15, 2, 19, 1, 0, 0, 0, 0, 19, 1, 19, 0, 9, 11, 10, 12, 5, 9, 19, 19, 19, 0, 9, 19, 4, 7, 1, 12, 8, 11, 1, 7, 17, 1, 1, 0, 10, 2, 13, 0, 0, 10, 4, 9, 19, 9, 8, 9, 9, 13, 0, 1, 2, 19, 9, 7, 9, 0, 0, 2, 2, 11, 13, 0, 1, 4, 13, 1, 5, 5, 0, 4, 0, 9, 4, 0, 0, 1, 4, 13, 4, 9, 18, 9, 0, 19, 0, 10, 9, 3, 1, 10, 10, 4, 19, 17, 1, 6, 17, 2, 2, 10, 1, 10, 1, 17, 10, 4, 10, 17, 10, 15, 3, 2, 4, 4, 1, 10, 2, 4, 15, 1, 19, 2, 2, 2, 2, 19, 2, 10, 12, 1, 14, 6, 8, 6, 2, 2, 8, 4, 6, 6, 2, 5, 18, 18, 0, 11, 6, 0, 0, 5, 0, 0, 2, 1, 5, 7, 12, 12, 12, 2, 2, 4, 0, 11, 0, 0, 1, 19, 10, 1, 18, 18, 18, 11, 15, 18, 0, 18, 18, 4, 5, 15, 0, 18, 12, 19, 5, 4, 7, 0, 13, 3, 1, 3, 3, 19, 4, 1, 9, 11, 18, 18, 18, 10, 11, 18, 18, 2, 18, 3, 18, 7, 18, 12, 19, 4, 5, 7, 0, 0, 0, 2, 8, 0, 11, 0, 5, 5, 5, 0, 5, 13, 0, 10, 5, 3, 5, 4, 19, 2, 0, 1, 4, 4, 12, 9, 9, 19, 1, 6, 9, 1, 7, 1, 1, 9, 11, 5, 10, 10, 5, 0, 10, 0, 12, 10, 11, 10, 2, 12, 11, 11, 5, 10, 19, 9, 6, 6, 9, 0, 6, 8, 8, 1, 9, 9, 9, 10, 9, 10, 4, 4, 19, 10, 8, 8, 8, 8, 8, 8, 6, 13, 8, 2, 1, 2, 1, 8, 2, 6, 14, 12, 10, 1, 11, 6, 19, 6, 0, 0, 0, 0, 17, 13, 11, 3, 11, 7, 3, 2, 15, 2, 4, 15, 18, 15, 15, 15, 9, 15, 15, 15, 2, 2, 7, 3, 18, 4, 18, 2, 19, 4, 18, 18, 18, 0, 1, 18, 0, 2, 18, 16, 18, 0, 1, 2, 18, 18, 18, 1, 15, 15, 12, 1, 15, 2, 15, 18, 12, 11, 15, 2, 13, 15, 10, 6, 3, 13, 12, 9, 12, 6, 7, 0, 12, 13, 6, 2, 4, 3, 3, 3, 3, 5, 9, 19, 2, 2, 11, 3, 5, 8, 3, 3, 3, 3, 3, 3, 3, 10, 3, 18, 3, 7, 10, 3, 6, 7, 3, 0, 0, 3, 3, 10, 11, 3, 9, 5, 7, 12, 4, 12, 12, 3, 2, 18, 15, 4, 0, 12, 8, 1, 12, 12, 14, 14, 14, 14, 6, 4, 12, 1, 3, 9, 6, 14, 6, 6, 14, 6, 13, 14, 7, 10, 5, 0, 5, 7, 10, 11, 14, 14, 7, 3, 14, 7, 14, 10, 4, 11, 6, 6, 4, 11, 4, 9, 6, 7, 14, 6, 4, 6, 14, 9, 14, 14, 9, 8, 14, 4, 19, 4, 3, 17, 17, 10, 17, 3, 17, 11, 17, 17, 15, 15, 3, 19, 0, 1, 3, 0, 17, 17, 10, 17, 17, 17, 15, 17, 17, 2, 15, 15, 15, 2, 15, 3, 15, 15, 11, 7, 18, 2, 9, 3, 8, 1, 3, 6, 0, 3, 5, 3, 1, 3, 5, 3, 4, 3, 7, 17, 7, 1, 10, 4, 3, 3, 3, 1, 3, 0, 3, 8, 3, 3, 13, 0, 12, 12, 12, 12, 11, 12, 5, 0, 11, 12, 1, 10, 12, 12, 1, 8, 15, 19, 19, 14, 14, 11, 2, 0, 3, 1, 0, 1, 8, 0, 1, 1, 10, 14, 0, 1, 0, 14, 6, 3, 0, 3, 14, 6, 1, 14, 14, 7, 0, 14, 5, 5, 14, 1, 13, 3, 6, 8, 4, 6, 13, 7, 2, 13, 13, 5, 6, 9, 7, 0, 14, 16, 16, 16, 19, 7, 3, 2, 5, 0, 0, 8, 3, 8, 2, 11, 2, 9, 13, 7, 13, 9, 1, 7, 18, 2, 9, 17, 15, 17, 17, 15, 17, 17, 17, 17, 15, 19, 16, 16, 16, 16, 13, 0, 0, 16, 17, 16, 16, 16, 16, 5, 16, 16, 17, 0, 4, 15, 16, 1, 11, 16, 13, 16, 2, 0, 3, 3, 3, 5, 3, 3, 3, 3, 12, 6, 6, 0, 6, 6, 7, 1, 7, 19, 6, 19, 10, 18, 7, 9, 12, 8, 7, 10, 8, 8, 13, 8, 8, 8, 0, 1, 1, 8, 7, 8, 8, 8, 8, 0, 7, 3, 1, 7, 8, 4, 4, 4, 0, 0, 0, 18, 3, 2, 1, 3, 3, 1, 3, 4, 0, 13, 19, 1, 1, 0, 1, 1, 1, 17, 19, 3, 4, 0, 0, 11, 11, 7, 3, 16, 1, 17, 7, 12, 17, 3, 3, 1, 3, 2, 6, 3, 1, 10, 11, 0, 8, 7, 19, 0, 16, 16, 16, 6, 16, 16, 16, 13, 16, 16, 6, 16, 0, 16, 13, 16, 13, 16, 2, 13, 5, 16, 3, 0, 7, 1, 7, 10, 4, 0, 17, 1, 10, 5, 4, 4, 0, 19, 4, 6, 14, 6, 1, 6, 14, 7, 5, 0, 2, 1, 0, 1, 7, 14, 0, 12, 4, 1, 4, 1, 4, 1, 4, 19, 0, 4, 7, 4, 9, 19, 4, 10, 4, 11, 7, 2, 8, 19, 14, 16, 13, 5, 5, 0, 9, 9, 2, 9, 8, 1, 0, 8, 9, 5, 0, 5, 8, 8, 0, 8, 1, 3, 8, 8, 8, 0, 8, 0, 1, 13, 7, 13, 13, 0, 0, 3, 3, 0, 0, 3, 1, 8, 11, 5, 5, 13, 5, 5, 0, 5, 5, 0, 18, 12, 1, 2, 0, 15, 1, 4, 0, 18, 11, 1, 1, 15, 2, 9, 4, 11, 5, 0, 15, 4, 3, 0, 13, 10, 2, 12, 12, 9, 5, 5, 6, 5, 5, 5, 5, 6, 12, 10, 19, 19, 12, 12, 15, 4, 0, 12, 10, 12, 11, 7, 7, 0, 8, 2, 2, 1, 2, 4, 7, 7, 19, 4, 7, 19, 18, 0, 2, 0, 11, 7, 11, 15, 11, 0, 15, 5, 18, 7, 15, 2, 19, 0, 8, 4, 19, 9, 18, 16, 16, 2, 13, 7, 13, 13, 2, 6, 2, 4, 5, 5, 10, 17, 19, 17, 9, 5, 5, 5, 6, 9, 17, 5, 5, 2, 17, 5, 5, 0, 1, 2, 10, 2, 4, 2, 2, 13, 10, 2, 3, 5, 6, 9, 6, 1, 1, 8, 14, 11, 16, 13, 8, 4, 0, 13, 1, 9, 5, 16, 16, 1, 9, 5, 13, 2, 5, 0, 11, 7, 0, 1, 16, 15, 10, 11, 14, 9, 12, 10, 9, 12, 4, 0, 12, 13, 7, 7, 14, 4, 15, 7, 12, 11, 1, 2, 14, 14, 16, 15, 11, 11, 14, 2, 14, 0, 15, 5, 13, 6, 1, 12, 18, 17, 19, 1, 13, 0, 17, 9, 4, 10, 1, 0, 1, 17, 9, 17, 19, 3, 13, 14, 5, 13, 2, 1, 0, 8, 6, 12, 12, 1, 3, 7, 7, 1, 1, 1, 1, 4, 18, 8, 8, 14, 6, 6, 16, 14, 0, 9, 2, 8, 1, 2, 1, 10, 6, 10, 1, 7, 6, 19, 0, 0, 6, 10, 0, 14, 18, 11, 11, 16, 11, 16, 16, 13, 12, 11, 16, 16, 2, 10, 12, 17, 13, 1, 2, 12, 13, 2, 0, 10, 15, 12, 2, 2, 1, 14, 14, 0, 2, 15, 13, 2, 11, 7, 4, 0, 15, 4, 15, 15, 0, 0, 14, 17, 0, 2, 15, 17, 16, 18, 0, 4, 16, 0, 15, 1, 1, 0, 5, 17, 3, 3, 14, 11, 5, 14, 13, 1, 1, 11, 8, 1, 1, 6, 6, 8, 12, 11, 14, 6, 6, 8, 13, 0, 6, 18, 0, 4, 4, 18, 1, 0, 17, 17, 18, 18, 18, 3, 17, 2, 0, 8, 16, 10, 0, 16, 16, 8, 12, 0, 16, 6, 7, 16, 15, 4, 10, 11, 3, 10, 18, 1, 12, 8, 11, 5, 18, 14, 4, 0, 0, 1, 19, 4, 10, 2, 0, 14, 3, 8, 6, 5, 2, 14, 15, 13, 8, 1, 2, 13, 0, 0, 8, 0, 15, 17, 0, 10, 19, 2, 15, 0, 1, 17, 3, 11, 6, 13, 0, 6, 5, 11, 2, 13, 6, 14, 14, 13, 2, 19, 16, 4, 16, 5, 4, 14, 0, 1, 14, 7, 14, 13, 14, 14, 14, 1, 14, 0, 3, 6, 2, 1, 16, 16, 14, 3, 5, 3, 3, 4, 1, 16, 16, 6, 8, 13, 17, 6, 13, 16, 0, 0, 9]\n",
            "-------RUN77-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': \"CountVectorizer(stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b02d2c50>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[6, 6, 6, 6, 11, 1, 0, 5, 5, 5, 0, 0, 2, 2, 5, 1, 11, 4, 0, 0, 2, 11, 1, 14, 14, 3, 9, 0, 1, 0, 0, 1, 6, 0, 1, 3, 4, 4, 5, 1, 14, 1, 9, 14, 15, 3, 9, 9, 14, 6, 9, 14, 5, 14, 5, 15, 15, 1, 0, 3, 11, 5, 5, 5, 1, 1, 2, 11, 2, 17, 3, 11, 13, 13, 0, 17, 0, 12, 1, 6, 15, 1, 3, 15, 15, 9, 15, 18, 1, 13, 13, 5, 15, 10, 3, 5, 5, 13, 2, 3, 1, 0, 0, 0, 0, 5, 1, 5, 12, 5, 11, 15, 9, 4, 5, 5, 18, 3, 12, 5, 3, 3, 14, 6, 9, 16, 11, 1, 14, 15, 1, 1, 1, 11, 2, 4, 6, 0, 15, 3, 5, 1, 5, 16, 5, 5, 4, 0, 6, 2, 5, 5, 14, 5, 0, 0, 2, 2, 0, 8, 0, 6, 3, 4, 1, 4, 4, 0, 3, 0, 5, 3, 1, 6, 1, 3, 4, 3, 5, 17, 5, 0, 18, 12, 11, 5, 10, 1, 15, 11, 3, 18, 15, 1, 7, 18, 2, 2, 15, 1, 15, 6, 15, 15, 3, 15, 18, 15, 13, 12, 2, 3, 3, 15, 15, 2, 2, 13, 1, 5, 2, 2, 2, 2, 5, 19, 9, 9, 6, 7, 7, 16, 7, 2, 2, 7, 3, 12, 7, 2, 4, 17, 17, 0, 0, 12, 0, 0, 4, 0, 0, 2, 6, 4, 0, 9, 9, 9, 2, 2, 3, 1, 0, 0, 1, 1, 3, 11, 6, 6, 17, 17, 11, 13, 17, 12, 17, 17, 3, 5, 13, 0, 17, 9, 9, 6, 3, 14, 0, 8, 10, 1, 10, 10, 18, 3, 1, 12, 19, 17, 17, 17, 11, 11, 17, 17, 2, 17, 6, 4, 14, 17, 9, 5, 3, 4, 0, 0, 0, 6, 2, 16, 0, 11, 0, 4, 4, 4, 0, 4, 4, 0, 11, 4, 12, 4, 3, 3, 2, 0, 1, 3, 3, 9, 5, 5, 3, 6, 7, 5, 1, 14, 1, 1, 5, 11, 4, 11, 11, 4, 0, 11, 0, 9, 11, 11, 6, 2, 9, 11, 11, 19, 15, 1, 5, 7, 7, 5, 0, 7, 16, 16, 1, 5, 5, 5, 11, 18, 11, 3, 3, 9, 11, 16, 16, 16, 16, 16, 16, 7, 4, 16, 2, 1, 2, 1, 16, 2, 7, 19, 9, 11, 1, 0, 7, 5, 7, 0, 0, 0, 0, 15, 4, 11, 12, 0, 12, 0, 2, 13, 2, 3, 13, 17, 13, 13, 13, 18, 13, 13, 13, 2, 2, 12, 6, 17, 3, 17, 2, 3, 3, 17, 17, 17, 0, 1, 17, 0, 2, 17, 8, 17, 12, 6, 2, 17, 17, 17, 6, 13, 13, 9, 1, 13, 2, 13, 17, 9, 11, 13, 2, 4, 13, 18, 12, 18, 8, 9, 12, 9, 12, 12, 0, 9, 8, 12, 4, 3, 10, 10, 10, 10, 4, 5, 3, 2, 2, 12, 10, 4, 16, 4, 10, 4, 10, 10, 10, 10, 11, 10, 17, 10, 14, 11, 10, 7, 14, 10, 0, 0, 10, 10, 11, 0, 12, 5, 4, 14, 9, 3, 9, 9, 10, 2, 17, 13, 3, 0, 9, 16, 6, 9, 9, 19, 19, 19, 19, 7, 3, 9, 1, 10, 5, 7, 7, 7, 7, 7, 7, 8, 19, 14, 14, 4, 0, 4, 14, 18, 2, 19, 19, 14, 10, 19, 14, 19, 18, 3, 11, 7, 7, 3, 11, 3, 18, 7, 14, 7, 7, 3, 7, 7, 5, 7, 7, 5, 16, 7, 3, 5, 3, 10, 15, 15, 11, 18, 10, 18, 12, 18, 15, 13, 13, 6, 5, 0, 1, 10, 0, 18, 18, 11, 15, 15, 18, 13, 18, 18, 2, 13, 13, 13, 2, 13, 10, 13, 13, 11, 14, 17, 2, 18, 10, 16, 1, 10, 7, 0, 12, 4, 10, 1, 10, 4, 10, 3, 10, 14, 18, 14, 1, 11, 3, 10, 10, 10, 1, 10, 4, 10, 16, 12, 10, 4, 0, 9, 9, 9, 9, 0, 9, 4, 12, 11, 9, 6, 15, 9, 9, 1, 6, 13, 18, 3, 19, 19, 0, 2, 0, 10, 1, 0, 1, 6, 0, 6, 1, 6, 19, 0, 6, 1, 7, 7, 10, 0, 10, 7, 12, 1, 7, 7, 12, 0, 7, 4, 6, 7, 6, 4, 10, 7, 16, 1, 7, 4, 14, 2, 4, 4, 4, 12, 5, 14, 0, 19, 8, 8, 8, 3, 14, 10, 2, 4, 6, 6, 16, 10, 16, 2, 11, 2, 5, 14, 14, 8, 5, 1, 14, 17, 2, 18, 15, 13, 18, 18, 13, 15, 15, 18, 15, 13, 5, 19, 8, 8, 8, 8, 6, 0, 8, 15, 7, 8, 7, 8, 5, 8, 8, 15, 0, 3, 13, 8, 6, 0, 8, 8, 8, 2, 0, 6, 10, 10, 4, 10, 10, 10, 10, 9, 12, 12, 0, 12, 12, 12, 1, 12, 3, 12, 3, 11, 17, 14, 5, 9, 16, 14, 11, 16, 16, 6, 16, 16, 6, 1, 6, 1, 16, 14, 16, 16, 16, 16, 0, 14, 6, 1, 14, 16, 3, 1, 3, 1, 0, 0, 17, 6, 2, 6, 6, 6, 1, 10, 3, 6, 4, 5, 1, 1, 6, 1, 1, 1, 15, 3, 10, 3, 0, 0, 11, 11, 12, 10, 8, 3, 15, 12, 9, 18, 10, 10, 6, 10, 2, 7, 10, 1, 9, 4, 0, 6, 12, 5, 0, 8, 8, 8, 7, 8, 8, 8, 8, 8, 8, 7, 8, 0, 8, 4, 8, 8, 8, 2, 6, 4, 8, 10, 0, 14, 1, 14, 11, 3, 0, 15, 1, 11, 4, 3, 3, 6, 5, 3, 12, 19, 12, 1, 12, 19, 14, 1, 0, 2, 1, 0, 1, 14, 19, 0, 9, 3, 1, 3, 1, 3, 1, 3, 3, 0, 3, 14, 3, 5, 5, 3, 11, 6, 4, 14, 2, 16, 3, 7, 8, 8, 4, 4, 1, 5, 5, 2, 5, 16, 1, 0, 6, 5, 12, 1, 4, 16, 16, 6, 16, 1, 6, 16, 16, 16, 0, 16, 0, 6, 8, 14, 4, 8, 0, 0, 6, 6, 12, 6, 10, 1, 6, 0, 4, 4, 4, 1, 4, 0, 4, 5, 0, 17, 9, 1, 2, 0, 13, 1, 3, 1, 17, 12, 1, 15, 13, 2, 5, 1, 0, 2, 0, 13, 3, 10, 0, 4, 11, 2, 9, 9, 5, 4, 4, 7, 4, 4, 4, 4, 12, 9, 9, 18, 18, 9, 9, 13, 3, 0, 9, 11, 9, 0, 14, 14, 0, 6, 2, 2, 1, 2, 3, 14, 14, 6, 3, 14, 3, 17, 0, 2, 0, 0, 14, 0, 13, 11, 0, 13, 5, 17, 14, 13, 2, 3, 1, 6, 3, 1, 5, 17, 8, 8, 2, 8, 14, 8, 4, 2, 7, 2, 3, 5, 4, 15, 15, 5, 15, 5, 5, 5, 5, 7, 5, 15, 5, 4, 2, 18, 4, 5, 0, 1, 2, 11, 2, 1, 2, 2, 4, 15, 2, 10, 4, 7, 5, 7, 1, 1, 16, 7, 2, 8, 4, 16, 3, 0, 4, 1, 5, 4, 7, 8, 1, 5, 4, 8, 2, 4, 0, 4, 12, 12, 1, 8, 13, 11, 0, 19, 5, 9, 18, 5, 9, 9, 0, 9, 8, 14, 14, 19, 3, 13, 14, 9, 0, 1, 2, 19, 19, 7, 13, 11, 11, 19, 2, 19, 1, 13, 4, 4, 12, 1, 9, 17, 18, 18, 6, 8, 0, 15, 5, 3, 15, 1, 0, 1, 15, 5, 18, 3, 12, 8, 7, 4, 8, 2, 1, 6, 16, 7, 9, 9, 6, 10, 14, 14, 6, 6, 1, 1, 3, 17, 16, 16, 19, 7, 7, 8, 7, 0, 5, 4, 16, 6, 2, 1, 15, 7, 15, 6, 12, 7, 18, 0, 0, 7, 11, 6, 19, 17, 11, 11, 8, 11, 8, 8, 8, 9, 11, 8, 8, 2, 15, 9, 18, 6, 1, 2, 9, 8, 2, 0, 15, 13, 9, 2, 2, 1, 19, 19, 6, 2, 13, 4, 2, 12, 14, 3, 0, 13, 3, 13, 13, 0, 6, 19, 18, 0, 2, 13, 15, 8, 17, 0, 3, 8, 0, 13, 6, 1, 2, 6, 15, 6, 10, 7, 12, 6, 7, 4, 6, 1, 11, 16, 1, 1, 7, 12, 6, 9, 0, 7, 7, 7, 16, 4, 0, 12, 17, 0, 3, 3, 17, 6, 0, 15, 18, 17, 17, 17, 10, 15, 2, 12, 6, 8, 9, 0, 8, 8, 16, 9, 0, 8, 12, 12, 8, 13, 3, 9, 10, 10, 18, 17, 1, 9, 16, 2, 2, 17, 19, 3, 0, 0, 1, 5, 3, 11, 2, 0, 19, 10, 7, 7, 5, 19, 19, 13, 8, 16, 1, 2, 4, 0, 1, 16, 12, 13, 18, 0, 11, 18, 2, 13, 0, 1, 18, 12, 11, 12, 8, 6, 7, 4, 0, 2, 4, 7, 7, 7, 4, 2, 3, 8, 3, 8, 4, 3, 7, 0, 1, 7, 6, 19, 4, 7, 7, 7, 1, 7, 6, 12, 7, 2, 6, 8, 8, 7, 10, 4, 6, 10, 3, 1, 8, 8, 7, 7, 8, 18, 12, 8, 8, 1, 0, 5]\n",
            "-------RUN78-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': \"CountVectorizer(ngram_range=(1, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b891dd10>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[6, 6, 6, 6, 2, 0, 17, 4, 4, 4, 3, 0, 10, 10, 4, 1, 8, 13, 0, 0, 10, 7, 1, 14, 14, 5, 5, 0, 0, 0, 0, 0, 6, 0, 1, 1, 11, 13, 4, 1, 14, 1, 5, 14, 2, 1, 5, 5, 14, 6, 5, 14, 4, 14, 11, 2, 2, 6, 1, 1, 7, 4, 4, 4, 1, 1, 8, 7, 8, 7, 1, 7, 18, 18, 3, 7, 0, 3, 0, 6, 2, 6, 1, 2, 2, 5, 2, 18, 1, 8, 18, 4, 2, 12, 5, 4, 4, 18, 10, 1, 0, 3, 0, 3, 3, 1, 0, 4, 3, 4, 7, 5, 5, 11, 4, 4, 2, 1, 3, 4, 1, 1, 14, 6, 5, 16, 17, 1, 14, 2, 1, 0, 0, 2, 8, 13, 6, 3, 2, 1, 4, 1, 4, 16, 4, 4, 13, 0, 6, 10, 4, 4, 14, 4, 0, 0, 8, 10, 10, 13, 3, 0, 1, 13, 0, 11, 11, 0, 1, 3, 4, 1, 0, 6, 0, 1, 13, 1, 4, 7, 4, 3, 4, 3, 2, 4, 6, 1, 8, 2, 1, 1, 2, 1, 17, 2, 10, 10, 2, 1, 2, 6, 2, 2, 1, 2, 2, 2, 18, 17, 8, 1, 1, 1, 2, 10, 1, 18, 1, 4, 10, 8, 8, 8, 4, 10, 5, 5, 6, 9, 17, 17, 17, 10, 8, 9, 1, 17, 17, 10, 10, 7, 7, 0, 10, 3, 0, 0, 10, 0, 17, 10, 11, 11, 3, 5, 5, 5, 8, 8, 5, 0, 10, 3, 0, 0, 1, 1, 6, 6, 7, 7, 7, 8, 7, 3, 7, 7, 1, 4, 8, 0, 7, 5, 5, 11, 5, 14, 0, 13, 12, 1, 12, 12, 4, 1, 1, 3, 19, 7, 7, 7, 7, 7, 7, 7, 8, 7, 6, 7, 14, 7, 5, 4, 1, 11, 3, 3, 0, 11, 10, 16, 0, 7, 0, 13, 10, 11, 0, 11, 13, 3, 2, 11, 3, 11, 1, 1, 8, 0, 1, 3, 1, 5, 4, 4, 1, 0, 9, 4, 0, 14, 0, 0, 4, 3, 11, 2, 2, 11, 3, 2, 0, 5, 2, 7, 6, 8, 5, 7, 3, 19, 8, 1, 4, 17, 9, 4, 17, 9, 16, 16, 0, 4, 4, 4, 2, 2, 5, 5, 1, 5, 2, 16, 16, 16, 16, 16, 16, 17, 7, 16, 10, 0, 10, 0, 16, 8, 9, 9, 5, 2, 0, 3, 17, 4, 9, 3, 0, 3, 3, 2, 13, 3, 3, 3, 3, 3, 8, 18, 8, 1, 8, 7, 18, 18, 18, 4, 18, 18, 18, 8, 8, 3, 6, 7, 1, 7, 8, 1, 1, 7, 7, 7, 11, 0, 7, 3, 10, 7, 15, 7, 3, 6, 10, 7, 7, 7, 6, 18, 18, 5, 0, 18, 8, 18, 7, 5, 3, 18, 8, 13, 18, 2, 17, 12, 13, 5, 4, 5, 17, 3, 0, 5, 13, 17, 10, 1, 12, 6, 12, 12, 11, 4, 1, 10, 10, 17, 12, 11, 16, 11, 12, 12, 12, 12, 12, 12, 2, 12, 7, 12, 14, 2, 12, 17, 14, 12, 11, 0, 12, 12, 14, 17, 3, 4, 11, 14, 5, 5, 5, 5, 12, 8, 7, 18, 5, 0, 5, 16, 0, 5, 5, 19, 19, 19, 19, 9, 1, 5, 1, 12, 4, 9, 9, 9, 9, 9, 17, 13, 19, 14, 14, 11, 3, 11, 14, 2, 10, 19, 19, 14, 6, 19, 14, 19, 2, 1, 7, 9, 9, 1, 3, 1, 4, 9, 14, 9, 9, 1, 17, 9, 4, 9, 9, 4, 16, 9, 1, 4, 1, 12, 2, 2, 2, 2, 12, 2, 17, 2, 2, 18, 18, 6, 4, 0, 0, 12, 0, 2, 2, 2, 2, 2, 2, 8, 2, 2, 8, 18, 18, 18, 8, 18, 12, 18, 8, 7, 14, 7, 8, 4, 12, 16, 0, 12, 17, 0, 3, 11, 12, 11, 12, 11, 12, 5, 12, 14, 2, 14, 0, 2, 1, 12, 12, 12, 0, 12, 11, 12, 16, 3, 12, 13, 0, 5, 5, 5, 5, 3, 5, 11, 0, 5, 5, 6, 2, 5, 5, 1, 6, 18, 1, 2, 19, 19, 3, 10, 0, 12, 4, 17, 1, 6, 0, 11, 11, 6, 19, 3, 0, 0, 9, 9, 12, 0, 12, 9, 17, 0, 9, 9, 3, 0, 9, 11, 11, 9, 6, 13, 12, 9, 16, 1, 9, 13, 14, 8, 10, 11, 11, 17, 4, 14, 0, 17, 15, 15, 15, 1, 3, 12, 8, 11, 6, 6, 16, 6, 16, 8, 8, 8, 11, 13, 14, 13, 4, 0, 14, 7, 10, 4, 2, 18, 2, 2, 18, 2, 2, 2, 2, 8, 4, 19, 15, 15, 15, 13, 0, 0, 15, 2, 15, 15, 9, 15, 10, 15, 15, 2, 3, 1, 8, 15, 11, 3, 15, 13, 15, 8, 0, 6, 12, 12, 11, 12, 12, 12, 12, 5, 3, 17, 3, 3, 3, 3, 0, 3, 4, 3, 4, 2, 7, 14, 4, 5, 16, 14, 5, 16, 16, 6, 16, 16, 6, 0, 6, 0, 16, 14, 16, 16, 16, 16, 3, 14, 6, 0, 14, 16, 1, 1, 1, 0, 0, 0, 7, 6, 10, 6, 6, 6, 1, 12, 1, 6, 13, 4, 1, 0, 6, 0, 0, 0, 2, 1, 12, 1, 0, 0, 2, 6, 3, 12, 15, 1, 2, 6, 5, 2, 12, 6, 6, 12, 8, 9, 12, 0, 5, 17, 3, 6, 14, 4, 3, 15, 15, 15, 9, 15, 15, 15, 13, 15, 15, 9, 15, 0, 15, 10, 15, 13, 15, 10, 13, 11, 15, 12, 0, 14, 1, 14, 2, 1, 0, 2, 1, 2, 10, 1, 1, 6, 4, 1, 3, 19, 3, 0, 3, 19, 14, 11, 0, 8, 8, 0, 1, 14, 19, 0, 5, 5, 1, 1, 0, 1, 0, 1, 1, 0, 10, 14, 1, 4, 4, 1, 2, 6, 17, 14, 8, 16, 1, 9, 15, 13, 10, 11, 0, 4, 4, 8, 4, 16, 1, 3, 6, 4, 17, 0, 11, 16, 6, 6, 16, 0, 6, 16, 16, 16, 0, 16, 0, 6, 13, 14, 13, 13, 0, 0, 6, 6, 3, 0, 12, 0, 6, 3, 11, 11, 13, 11, 17, 3, 11, 4, 3, 7, 5, 0, 8, 3, 18, 0, 1, 0, 7, 17, 1, 1, 8, 8, 4, 1, 3, 10, 0, 18, 1, 12, 0, 13, 7, 10, 5, 5, 10, 11, 10, 17, 11, 11, 11, 11, 17, 5, 5, 2, 4, 5, 5, 18, 1, 0, 5, 2, 5, 3, 14, 14, 0, 6, 8, 10, 0, 8, 1, 14, 14, 6, 1, 14, 1, 7, 3, 10, 0, 0, 14, 3, 8, 10, 0, 8, 10, 7, 14, 18, 8, 1, 0, 6, 1, 1, 4, 7, 15, 15, 10, 13, 10, 13, 13, 8, 9, 10, 1, 4, 11, 2, 2, 4, 2, 4, 4, 4, 11, 9, 4, 2, 4, 11, 10, 2, 11, 4, 3, 0, 10, 17, 10, 1, 8, 10, 13, 2, 10, 12, 13, 9, 4, 9, 1, 11, 16, 9, 8, 15, 13, 16, 3, 0, 13, 0, 4, 11, 9, 15, 0, 4, 11, 13, 10, 11, 3, 10, 3, 3, 0, 15, 18, 7, 3, 19, 4, 5, 2, 4, 5, 5, 0, 5, 13, 14, 14, 19, 1, 8, 14, 5, 3, 1, 8, 19, 19, 9, 18, 7, 4, 19, 8, 19, 0, 18, 11, 13, 17, 1, 5, 7, 2, 1, 6, 13, 3, 2, 4, 5, 2, 1, 0, 0, 2, 4, 2, 4, 17, 13, 9, 11, 13, 10, 0, 0, 16, 9, 5, 5, 6, 11, 14, 14, 6, 6, 0, 1, 1, 7, 16, 16, 19, 9, 9, 15, 9, 0, 4, 10, 16, 6, 0, 0, 2, 9, 2, 6, 3, 9, 2, 3, 0, 9, 2, 0, 19, 7, 7, 7, 15, 7, 15, 15, 13, 5, 7, 15, 15, 8, 2, 5, 2, 6, 1, 8, 5, 13, 10, 3, 2, 18, 5, 8, 8, 1, 19, 19, 0, 10, 18, 13, 10, 3, 14, 1, 0, 8, 1, 8, 8, 3, 6, 19, 2, 3, 8, 18, 2, 15, 7, 3, 1, 15, 11, 18, 6, 11, 0, 11, 2, 6, 12, 9, 3, 11, 9, 13, 0, 0, 7, 16, 0, 1, 9, 17, 6, 5, 3, 9, 9, 9, 16, 13, 3, 17, 7, 0, 1, 1, 7, 6, 0, 2, 2, 7, 7, 7, 12, 2, 10, 3, 6, 15, 5, 0, 15, 15, 16, 5, 0, 15, 3, 17, 15, 18, 1, 5, 12, 12, 2, 7, 1, 5, 16, 10, 10, 7, 19, 1, 3, 0, 0, 1, 4, 7, 10, 0, 19, 12, 9, 9, 4, 10, 19, 18, 13, 16, 1, 10, 13, 3, 0, 16, 3, 18, 2, 0, 2, 2, 8, 18, 0, 0, 2, 3, 8, 17, 13, 6, 9, 11, 3, 10, 11, 9, 9, 9, 13, 8, 4, 15, 1, 15, 11, 1, 9, 3, 1, 9, 6, 19, 13, 9, 9, 9, 0, 9, 6, 3, 17, 8, 6, 15, 15, 9, 12, 10, 6, 12, 1, 1, 15, 15, 17, 9, 13, 2, 3, 13, 15, 0, 17, 4]\n",
            "-------RUN79-------\n",
            "{'Topic extraction': \"SentenceTransformer(\\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\\n  (2): Normalize()\\n)\", 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': \"CountVectorizer(ngram_range=(2, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df4a9008410>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=SentenceTransformer(...), hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[8, 8, 8, 8, 14, 0, 17, 4, 4, 4, 0, 0, 3, 3, 4, 1, 17, 5, 0, 0, 3, 12, 1, 16, 16, 2, 9, 0, 1, 0, 0, 1, 8, 0, 1, 2, 5, 6, 4, 1, 16, 1, 9, 16, 13, 13, 9, 9, 16, 8, 9, 16, 4, 16, 4, 14, 14, 1, 0, 2, 17, 4, 4, 4, 1, 1, 3, 13, 3, 12, 2, 17, 15, 15, 17, 12, 0, 10, 1, 8, 13, 1, 2, 13, 13, 9, 13, 15, 1, 15, 15, 4, 14, 11, 2, 4, 4, 15, 3, 2, 1, 0, 0, 0, 0, 2, 1, 4, 10, 4, 12, 13, 9, 5, 4, 4, 14, 2, 10, 4, 2, 2, 16, 8, 9, 18, 17, 1, 16, 13, 1, 1, 1, 13, 3, 5, 0, 0, 13, 2, 4, 1, 4, 18, 4, 4, 5, 0, 8, 3, 4, 4, 16, 4, 0, 0, 3, 3, 17, 6, 0, 1, 2, 11, 1, 5, 5, 0, 2, 0, 4, 2, 0, 8, 1, 2, 5, 2, 4, 12, 4, 0, 4, 10, 13, 4, 8, 1, 13, 14, 2, 13, 13, 1, 17, 14, 3, 3, 13, 1, 13, 8, 13, 13, 2, 13, 14, 13, 15, 8, 3, 2, 2, 13, 13, 3, 3, 15, 1, 4, 3, 3, 3, 3, 4, 19, 9, 9, 1, 7, 17, 18, 7, 3, 3, 7, 2, 17, 17, 3, 17, 12, 12, 0, 17, 10, 0, 0, 5, 0, 0, 3, 8, 5, 10, 9, 9, 9, 3, 3, 2, 0, 17, 10, 0, 1, 2, 2, 0, 8, 12, 12, 12, 15, 12, 10, 12, 3, 2, 4, 15, 0, 12, 9, 9, 5, 2, 16, 0, 6, 8, 1, 11, 11, 4, 2, 1, 10, 17, 12, 12, 12, 17, 12, 12, 12, 3, 12, 8, 17, 16, 12, 9, 4, 2, 5, 10, 0, 0, 8, 3, 18, 0, 17, 0, 5, 5, 5, 0, 5, 6, 10, 14, 5, 10, 5, 2, 2, 3, 0, 1, 2, 2, 9, 4, 4, 2, 8, 7, 4, 1, 16, 1, 1, 4, 17, 5, 14, 2, 5, 10, 13, 0, 9, 14, 17, 13, 3, 9, 12, 17, 19, 13, 1, 4, 7, 7, 4, 0, 7, 18, 18, 1, 4, 4, 4, 14, 4, 13, 2, 2, 9, 14, 18, 18, 18, 18, 18, 18, 7, 17, 18, 3, 0, 3, 1, 18, 3, 7, 19, 9, 14, 1, 0, 7, 4, 7, 0, 0, 0, 0, 13, 11, 17, 10, 0, 10, 11, 3, 15, 3, 2, 15, 12, 15, 15, 15, 4, 15, 15, 15, 3, 11, 10, 8, 12, 2, 12, 3, 2, 2, 12, 3, 12, 0, 1, 12, 0, 3, 12, 6, 12, 10, 1, 3, 12, 12, 12, 8, 15, 15, 9, 1, 15, 3, 9, 12, 9, 10, 15, 3, 5, 15, 14, 18, 11, 6, 9, 10, 9, 10, 10, 0, 9, 6, 10, 3, 2, 11, 8, 11, 11, 5, 4, 2, 3, 3, 10, 8, 5, 8, 5, 11, 5, 11, 11, 11, 11, 14, 11, 12, 11, 16, 14, 8, 7, 16, 11, 0, 0, 11, 5, 13, 17, 11, 4, 5, 16, 9, 2, 9, 9, 11, 3, 12, 15, 2, 0, 9, 18, 1, 9, 9, 19, 19, 19, 19, 7, 2, 9, 1, 11, 4, 7, 7, 7, 7, 7, 17, 6, 19, 16, 13, 5, 0, 5, 16, 14, 17, 19, 19, 16, 8, 19, 16, 19, 14, 2, 12, 7, 7, 2, 17, 2, 4, 7, 16, 7, 7, 2, 7, 7, 4, 7, 7, 4, 18, 7, 2, 4, 2, 11, 14, 13, 14, 14, 11, 14, 17, 14, 14, 15, 15, 8, 2, 0, 1, 11, 0, 14, 14, 13, 13, 13, 14, 15, 14, 14, 3, 15, 15, 15, 3, 15, 11, 15, 15, 12, 16, 12, 3, 4, 11, 8, 1, 11, 7, 0, 11, 5, 11, 1, 11, 5, 11, 2, 11, 16, 14, 16, 1, 2, 2, 11, 11, 11, 1, 11, 5, 11, 18, 10, 11, 5, 0, 9, 9, 9, 9, 17, 9, 5, 10, 17, 9, 1, 13, 9, 9, 1, 8, 15, 4, 2, 19, 19, 0, 3, 0, 11, 1, 0, 1, 8, 0, 0, 1, 13, 19, 10, 1, 0, 7, 7, 11, 0, 12, 7, 10, 1, 7, 7, 10, 0, 7, 5, 5, 7, 8, 5, 11, 7, 18, 1, 7, 5, 16, 3, 5, 5, 5, 10, 4, 16, 0, 17, 6, 6, 6, 2, 10, 8, 3, 5, 0, 0, 18, 8, 18, 3, 3, 3, 4, 16, 16, 11, 4, 1, 16, 12, 3, 4, 13, 15, 14, 14, 15, 14, 13, 14, 13, 11, 4, 19, 6, 6, 6, 6, 0, 0, 6, 13, 7, 6, 7, 6, 5, 6, 6, 13, 0, 2, 15, 6, 8, 0, 6, 6, 6, 3, 0, 8, 11, 11, 5, 11, 11, 11, 11, 9, 10, 10, 10, 10, 10, 10, 1, 10, 4, 10, 2, 14, 12, 16, 4, 9, 18, 16, 9, 18, 18, 8, 18, 8, 8, 0, 8, 1, 18, 16, 18, 18, 18, 18, 10, 16, 8, 1, 16, 18, 2, 1, 2, 0, 0, 0, 12, 8, 3, 8, 8, 8, 1, 8, 2, 8, 5, 4, 1, 1, 8, 1, 1, 1, 13, 2, 11, 2, 0, 0, 14, 8, 10, 11, 6, 2, 13, 10, 9, 14, 11, 8, 8, 8, 3, 7, 11, 1, 9, 17, 0, 8, 10, 4, 0, 6, 7, 6, 7, 6, 6, 6, 6, 6, 6, 7, 6, 0, 6, 5, 7, 6, 6, 3, 5, 5, 6, 11, 0, 16, 1, 16, 14, 2, 0, 13, 1, 17, 17, 2, 2, 0, 4, 2, 10, 19, 10, 1, 10, 19, 16, 1, 0, 3, 13, 0, 1, 16, 19, 0, 9, 2, 1, 2, 1, 2, 1, 2, 2, 0, 2, 16, 2, 4, 4, 2, 14, 2, 17, 16, 3, 18, 2, 7, 6, 6, 5, 5, 0, 4, 4, 3, 4, 18, 1, 0, 8, 4, 10, 1, 5, 18, 8, 8, 18, 1, 8, 18, 18, 18, 0, 18, 0, 8, 6, 16, 6, 6, 0, 0, 8, 8, 10, 0, 11, 1, 8, 17, 5, 5, 5, 1, 5, 10, 5, 4, 0, 12, 9, 1, 3, 0, 15, 1, 2, 0, 12, 10, 1, 13, 15, 3, 4, 1, 17, 3, 0, 15, 2, 11, 0, 11, 13, 3, 9, 9, 1, 5, 5, 17, 5, 5, 5, 5, 10, 9, 9, 14, 4, 9, 9, 15, 2, 0, 9, 14, 9, 0, 16, 16, 0, 8, 3, 3, 1, 3, 2, 16, 16, 8, 2, 16, 2, 12, 0, 3, 0, 0, 16, 0, 15, 17, 0, 15, 4, 12, 16, 15, 14, 2, 0, 8, 2, 1, 4, 12, 6, 6, 3, 6, 16, 6, 5, 3, 7, 3, 2, 4, 5, 13, 13, 4, 14, 4, 4, 4, 5, 7, 4, 13, 4, 5, 3, 14, 5, 4, 0, 1, 3, 13, 3, 2, 3, 3, 5, 13, 3, 8, 5, 7, 4, 7, 1, 1, 18, 7, 3, 6, 5, 18, 2, 0, 5, 1, 4, 5, 7, 6, 1, 4, 5, 6, 17, 5, 0, 17, 10, 10, 1, 6, 15, 14, 11, 19, 4, 9, 14, 4, 9, 17, 0, 9, 6, 16, 16, 19, 2, 15, 16, 9, 0, 2, 3, 19, 19, 7, 15, 12, 17, 19, 3, 19, 0, 15, 5, 5, 10, 1, 9, 12, 14, 4, 8, 6, 0, 13, 4, 9, 13, 1, 0, 1, 13, 4, 14, 2, 10, 6, 7, 5, 6, 3, 1, 0, 18, 7, 9, 9, 8, 5, 16, 16, 8, 8, 1, 1, 2, 12, 8, 18, 19, 7, 7, 6, 7, 0, 4, 17, 18, 8, 17, 1, 13, 7, 13, 8, 10, 7, 4, 10, 0, 7, 14, 0, 19, 12, 12, 12, 6, 12, 6, 6, 6, 9, 12, 6, 6, 3, 13, 9, 14, 8, 1, 3, 9, 6, 3, 0, 13, 15, 9, 0, 3, 1, 19, 19, 0, 3, 15, 6, 3, 10, 16, 2, 0, 15, 2, 15, 15, 0, 0, 19, 14, 0, 3, 15, 13, 6, 12, 0, 2, 6, 0, 15, 1, 1, 0, 5, 13, 8, 11, 7, 10, 5, 7, 6, 0, 1, 17, 18, 1, 1, 7, 10, 8, 9, 17, 7, 7, 7, 18, 5, 10, 10, 12, 0, 2, 2, 12, 8, 0, 14, 14, 12, 12, 12, 11, 13, 3, 10, 8, 6, 9, 0, 6, 6, 18, 9, 0, 6, 10, 10, 6, 15, 2, 9, 11, 11, 14, 12, 1, 9, 8, 17, 4, 12, 19, 2, 0, 0, 1, 4, 2, 12, 3, 0, 19, 11, 7, 7, 17, 19, 19, 15, 6, 18, 1, 3, 6, 0, 0, 18, 10, 15, 14, 0, 14, 13, 3, 15, 0, 1, 14, 10, 3, 10, 6, 8, 7, 5, 0, 3, 5, 7, 7, 7, 5, 3, 2, 6, 2, 6, 5, 2, 7, 0, 1, 7, 8, 19, 5, 7, 7, 7, 1, 7, 0, 10, 18, 3, 1, 6, 6, 7, 11, 16, 8, 11, 2, 1, 6, 6, 7, 7, 6, 14, 10, 6, 6, 0, 17, 4]\n",
            "-------RUN80-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': \"CountVectorizer(stop_words='english')\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[25, 2, 30, 30, -1, -1, -1, 5, -1, -1, -1, -1, -1, -1, 15, 24, 18, 25, -1, -1, -1, 14, 5, 9, 9, 21, 35, 2, 24, -1, -1, -1, 30, 32, -1, -1, 1, 11, 0, -1, 9, 24, 26, 9, 26, 8, 26, 26, 9, -1, 26, 9, 5, -1, -1, 22, -1, 2, 5, 5, 1, 5, 5, 5, 24, 24, -1, -1, 14, 14, 31, 14, 1, 1, 20, 4, 18, -1, -1, 2, 22, 2, -1, 22, 2, 21, 22, 1, 2, 32, 1, 5, 22, -1, 5, 10, -1, 1, -1, -1, -1, 20, -1, -1, -1, 31, -1, -1, 0, -1, 4, 14, 35, -1, 2, -1, 6, 5, -1, -1, 26, 6, 9, 20, 26, -1, 14, 2, 9, 14, 2, 15, 24, 14, 14, 25, 9, 18, 14, 26, 5, 5, -1, 10, 15, 5, 25, -1, 2, 0, -1, -1, -1, 5, -1, 5, 1, 28, -1, 3, -1, 3, 31, 3, 24, -1, -1, 20, -1, -1, 5, 5, -1, 0, 15, 5, 18, -1, 15, 14, 11, 18, 15, 0, 14, 5, -1, 2, 14, 14, 15, 15, 22, 2, -1, 6, 16, 16, 22, -1, 22, 2, 22, -1, 5, -1, 6, 22, 1, -1, 1, -1, 2, 2, 22, 22, -1, 1, 2, -1, 16, -1, 1, 1, 5, 16, 35, 35, 2, 0, 0, 0, 0, 16, 16, 0, 21, 0, -1, 16, -1, 4, 4, -1, 16, -1, -1, -1, 11, -1, -1, 16, 12, 25, -1, 8, 8, -1, 1, 1, 35, 20, 11, -1, -1, 2, -1, 35, -1, 4, 4, 4, -1, 1, 4, -1, 4, 4, 21, -1, 1, 11, 4, 8, 8, 4, 16, 34, 19, 34, 19, 2, 19, 19, 6, -1, -1, -1, 16, 4, 4, 4, 14, 4, -1, 4, 1, 4, 19, -1, 9, 4, 26, 11, -1, -1, -1, -1, 20, -1, 5, 10, -1, -1, 0, 25, -1, 25, 2, 25, -1, -1, 13, 25, 29, -1, 5, 15, 1, 15, 15, 21, 21, 5, -1, -1, 15, 15, 0, 15, -1, 9, -1, 2, 5, 0, -1, 13, 13, 25, 18, 14, -1, -1, 13, -1, 14, 18, 21, 14, -1, 0, 14, 31, 5, 14, 0, 5, 11, 0, 12, 12, 24, 5, 5, 5, 13, 5, 13, 21, 33, -1, -1, 12, 12, 12, 12, 12, 12, 27, -1, 12, 16, -1, 16, -1, 12, 1, 0, 0, 21, 13, 1, 20, 0, 21, 0, -1, 29, 34, -1, -1, -1, -1, -1, -1, 18, 29, 1, 1, 1, 18, 1, 4, 1, -1, 1, 1, 1, 1, 1, 1, -1, 29, 25, 4, -1, 4, 1, -1, 32, 4, 4, 4, 29, 2, 4, -1, 4, 4, 11, 4, 27, 2, 16, 4, 4, 4, 4, 1, 1, 8, -1, 1, -1, 1, 4, -1, -1, 1, -1, 34, 1, 5, 19, -1, 34, 8, 2, 8, 19, 20, -1, 21, 34, 19, 16, -1, 7, 7, 7, 0, 25, 1, -1, 16, 16, -1, 7, -1, 7, 25, 7, 7, 7, 7, 7, 20, 13, 7, 4, 7, 13, 13, 7, 17, 13, 7, 2, -1, 7, 7, 13, 0, 1, 1, 2, 13, 8, -1, 32, 8, 0, -1, 4, 1, 8, 18, 8, 8, 2, 26, 8, 0, 0, 0, -1, 0, 8, 8, 2, 19, 29, 0, 0, 0, 0, 0, 0, -1, 0, 13, -1, 17, 17, 17, 13, 13, 0, 0, 0, 9, -1, 0, 13, 0, 13, 31, 13, 0, 0, 21, 5, 21, 5, 0, 9, 0, 0, 21, 0, 0, 20, 0, 0, 5, 12, 0, 21, 21, 32, -1, 6, 6, 6, 6, 6, 6, 0, 6, 6, 1, 1, 19, -1, 10, 15, -1, 20, 13, 6, -1, 6, 6, 6, 1, 6, 6, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 2, 4, -1, -1, 7, 7, -1, 7, 0, 2, 7, 2, 7, 24, 7, 25, 7, -1, 7, 9, 1, 9, 2, 5, -1, 7, 7, 7, -1, 7, -1, 7, 0, -1, 7, 25, -1, 8, 8, 8, 8, -1, 8, 8, -1, 1, 8, -1, 14, 8, 8, -1, 30, 1, 14, -1, 0, 0, 20, 18, 2, 7, 15, 0, -1, 30, 0, -1, -1, -1, 0, 18, 9, 2, 0, 0, 19, -1, -1, 0, 0, -1, 0, 0, 9, -1, 0, 17, 17, 0, 2, 17, 19, 0, 10, 31, 0, 17, 5, 1, 17, 17, 17, 2, -1, 9, 18, 0, 3, 3, 3, 31, 9, 18, 1, 0, 10, -1, 10, 19, 12, 1, 1, 0, -1, -1, -1, -1, 1, -1, 5, 4, 4, 6, 6, 1, 6, 6, 1, 6, 6, 6, 6, 1, -1, 3, 3, 3, 3, 3, 2, 0, 3, 6, 3, 3, 3, 3, -1, 3, 3, 6, 20, -1, 1, 3, -1, 20, 3, 3, 3, 1, -1, -1, 7, 7, -1, 7, 7, 7, 7, -1, 27, 27, -1, 27, 27, 27, 2, 27, 21, 27, 21, -1, 4, 9, 2, 26, 12, 9, -1, 12, 10, 10, 10, 10, 10, 2, 10, 5, 12, 9, 12, 12, 12, 12, 18, 9, 30, 2, 9, 10, -1, 31, 32, 2, -1, 2, 30, 30, 20, -1, 30, 30, 2, 30, -1, 30, -1, 15, 15, 15, 19, 15, -1, 15, 22, -1, 7, -1, 18, 15, 4, -1, 1, 19, 3, 2, 22, 19, 26, -1, 19, 19, 19, 12, 1, 0, 19, 2, 26, 4, 18, -1, -1, 5, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, -1, 3, -1, 3, 3, 3, 16, 3, -1, 3, 7, -1, 9, 2, 13, 13, 8, -1, 22, 2, -1, 16, -1, 33, 24, 15, 32, 27, 0, 27, 2, 27, 0, 9, 2, -1, 11, -1, 11, 11, 9, 0, 2, 26, 33, -1, 33, -1, 32, -1, 31, 33, 2, 33, 9, 15, 33, -1, 33, 13, 18, -1, 9, 1, 12, -1, 0, 23, 23, -1, 26, -1, -1, 5, 14, -1, 12, 2, -1, 10, 28, 27, -1, 28, 12, 10, 10, 12, -1, -1, 10, 10, 10, -1, 10, 3, 10, 23, -1, 23, 23, 24, -1, 30, 30, -1, 2, 30, 2, -1, 0, 28, 25, -1, -1, 28, 0, 28, 28, 20, 4, 21, 2, 29, 29, 1, -1, -1, -1, 4, 4, 31, 14, 1, 1, -1, -1, -1, 5, 5, 1, 21, 7, 11, 25, 14, 3, 8, -1, 5, 2, 5, -1, -1, 2, 20, -1, 10, -1, 8, 32, 8, 8, 26, 1, 2, 25, 8, 13, 8, 20, 13, 13, -1, 17, 4, 3, -1, 11, 33, 9, 9, -1, 33, 9, 32, 4, 11, 22, 11, 11, 9, 20, 1, 13, -1, 1, 16, 14, -1, 1, 11, -1, -1, 19, -1, 15, 5, 4, 23, 23, 18, 23, -1, 23, 17, 11, 0, 3, -1, 31, -1, -1, 6, 5, 13, 28, 28, 28, 31, 0, 28, 6, 28, 28, 16, -1, 28, 28, 5, 5, 16, 14, 11, 11, 11, 11, 34, 22, 11, 7, 34, 0, 0, 0, 2, 2, 12, 0, 1, 0, 17, 12, 29, 29, 23, 24, -1, -1, 0, 23, 2, -1, -1, 23, -1, -1, 23, 25, -1, 18, 2, 3, 1, 13, 29, 0, 2, 8, 8, 1, 8, -1, 32, 8, 3, 9, 9, -1, 32, 1, 9, 8, 20, 2, 1, 0, 0, 3, 1, 1, 0, 0, 1, 0, 24, 1, 17, 3, -1, 32, 35, 4, 6, 15, 10, 34, 5, 6, -1, -1, 22, 2, 11, 24, 6, 5, 6, -1, 30, 34, 0, -1, 34, 0, 2, 10, 10, 0, 35, 26, 2, 10, 9, 9, 10, 10, 11, 2, 24, 4, 12, 10, 0, 0, 0, 3, 0, 2, 5, -1, -1, -1, 17, -1, 22, 0, 22, 2, 1, -1, 8, 0, -1, 0, 6, 24, 0, 4, 4, 4, 3, 4, 3, 3, 23, 8, 4, 3, 3, 2, 22, 8, 6, 23, 2, -1, 21, 3, 16, 29, 22, 1, -1, 29, 1, 2, 0, 0, 24, 16, 1, 17, 17, -1, 13, -1, 18, 1, 31, -1, 1, 18, 2, 0, -1, -1, 1, 1, 6, 3, -1, -1, -1, 3, 3, 1, 2, 15, 11, 17, 6, 19, -1, 0, 26, 17, 0, 17, 24, 31, 14, 10, 2, 24, 0, 0, 10, 21, -1, 0, 0, 0, 12, 11, 20, 0, 4, 2, 33, 33, 4, 2, -1, 6, 6, 4, 4, 4, 7, 14, -1, 2, 0, 3, -1, 11, 3, 3, 12, 8, 32, 3, 27, 27, 3, 1, 1, 35, -1, 4, 35, 4, 2, 8, 10, -1, 28, 4, 0, 15, -1, -1, 15, -1, -1, 4, 11, 11, 0, 19, 0, 3, 5, 16, 0, 1, 11, 12, 2, 16, 23, 18, -1, 12, 29, 1, 6, -1, -1, -1, -1, 1, 20, 6, 6, 29, 1, 27, 23, 17, 5, -1, 20, 24, 17, 0, 0, 0, 17, 1, 21, 3, 5, 3, 11, 33, 0, 18, -1, 0, 2, 0, 3, 0, 0, 0, -1, 0, 17, 29, -1, 1, 2, 3, 3, 0, 7, 4, 10, 10, 2, 2, 3, 3, 0, 0, 23, 6, 27, 23, 23, -1, 0, 31]\n",
            "Skipping coherence calculation due to error: unable to interpret topic as either a list of tokens or a list of ids\n",
            "-------RUN81-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': 'CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7df3b00f5c90>)', 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[27, 1, 32, 32, -1, -1, -1, 19, -1, -1, -1, -1, -1, -1, 13, 25, 18, 27, -1, 25, -1, 14, -1, 8, 8, 24, -1, 1, 25, -1, -1, -1, 32, 33, -1, -1, 0, 11, -1, -1, 8, 25, 21, 8, 21, 7, 21, 21, 8, -1, 21, 8, -1, -1, -1, 23, -1, 1, 19, 19, -1, 19, 19, 17, 25, 25, -1, -1, 14, 3, 30, 14, 0, 0, 22, 3, 18, -1, -1, 1, 23, 1, -1, 23, 1, 24, 23, 0, 1, 33, 0, 17, -1, -1, 17, 12, 24, 0, -1, -1, -1, 22, -1, -1, -1, 30, -1, -1, -1, -1, -1, 14, -1, -1, 1, -1, 5, 17, 18, 1, -1, 5, 8, 22, 21, -1, 14, 1, 8, 14, 1, 13, 25, 14, 14, 27, 8, 18, 14, 21, -1, -1, 31, -1, 13, 19, 27, -1, -1, -1, -1, -1, -1, 17, -1, 19, 0, 31, -1, 2, -1, 2, 30, 2, 25, -1, -1, 22, -1, -1, 17, 17, 1, -1, 13, 19, -1, -1, 13, 14, 11, 18, 13, -1, 14, -1, -1, 1, 14, 14, 13, 13, 23, 1, -1, 5, 15, 15, 23, -1, 23, 1, 23, -1, -1, -1, 5, 23, 0, -1, 0, -1, 1, 1, 23, 23, -1, 0, 1, -1, 15, -1, 0, 0, 17, 15, 21, 21, 1, 4, -1, -1, -1, 15, 15, -1, 24, -1, -1, 15, -1, 3, 3, -1, 15, -1, -1, -1, 11, 1, -1, 15, 9, -1, -1, 7, 7, -1, 0, 0, 21, 22, 11, -1, 1, 1, 30, 21, -1, 3, 3, 3, -1, 0, 3, -1, 3, 3, 24, -1, 0, 11, 3, 7, 7, 3, 15, 36, 34, 36, 34, 1, 34, 34, 5, -1, -1, -1, 15, 3, 3, 3, 14, 3, -1, 3, -1, 3, -1, -1, 8, 3, 21, -1, -1, -1, -1, -1, 22, -1, 19, -1, -1, -1, 4, 27, -1, 27, 1, 27, -1, -1, 16, 27, 28, -1, 19, 13, 0, 13, 13, -1, -1, 17, -1, -1, 13, 13, -1, 13, -1, 8, -1, 1, 17, -1, -1, 16, 16, 27, 18, 14, -1, -1, 16, -1, 14, 18, 24, 14, -1, -1, 14, 30, 17, 14, -1, 17, 11, -1, 9, 9, 25, 17, 17, 17, 16, 17, -1, 24, 35, -1, -1, 9, 9, 9, 9, 9, 9, 29, -1, 9, 15, -1, 15, -1, 9, 0, -1, -1, 24, -1, 0, 22, -1, 24, -1, -1, 28, 36, -1, -1, -1, -1, -1, -1, 18, 28, 0, 0, 0, 18, 0, 3, 0, -1, 0, 0, 0, 0, 0, 0, -1, 28, 27, 3, -1, 3, 0, -1, 33, 3, 3, 3, 28, 1, 3, 1, 3, 3, 11, 3, 29, 1, 15, 3, 3, 3, 3, 0, 0, 7, -1, 0, -1, 0, 3, -1, -1, 0, -1, 36, 0, 17, -1, -1, 36, 7, 1, 7, -1, 22, -1, 24, 36, -1, 15, -1, 6, 6, 6, -1, 27, 0, -1, 15, 15, -1, 6, -1, 6, 27, 6, 6, 6, 6, 6, 22, 16, 6, 3, 6, 16, 16, 6, 20, 16, 6, -1, -1, 6, -1, 16, -1, 0, 0, 1, 16, 7, -1, 33, 7, 10, -1, 3, 0, 7, 18, 7, 7, 1, 21, 7, 10, 10, 10, -1, -1, 7, 7, 1, 34, 28, 4, 4, 4, -1, 4, 10, -1, 10, 16, -1, 20, 20, 20, 16, 16, 10, 10, 10, 8, -1, 10, 16, 10, 16, 30, 16, -1, -1, 24, 19, 24, 17, 4, 8, 4, 4, -1, 4, 4, 22, 4, 4, -1, 9, 4, 24, -1, 33, -1, 5, 5, 5, 5, 5, 5, 10, 5, 5, 0, 0, -1, -1, -1, 13, -1, 22, -1, 5, -1, 5, 5, -1, 0, 5, 5, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 1, 3, -1, -1, 6, 6, -1, 6, -1, -1, 6, -1, 6, 25, 6, 27, 6, -1, 6, 8, 0, 8, 1, 17, -1, 6, 6, 6, -1, 6, -1, 6, -1, -1, 6, 27, 1, 7, 7, 7, 7, -1, 7, 7, -1, 0, 7, -1, 14, 7, 7, 1, 32, 0, 14, -1, 10, 10, 22, 18, -1, -1, 13, 10, -1, 32, -1, 1, 1, -1, 10, 18, -1, -1, 4, 4, -1, 1, -1, 4, 4, 1, 4, 4, 8, -1, 4, -1, 20, 4, 1, 20, 34, 4, -1, 30, 4, 20, 17, 0, 20, 20, 20, 1, -1, 8, 18, -1, 2, 2, 2, 30, 8, 18, 0, -1, 12, -1, -1, 34, 9, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, 3, 3, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 0, -1, 2, 2, 2, 2, 2, 1, 4, 2, 5, 2, 2, 2, 2, -1, 2, 2, 5, 22, -1, 0, 2, -1, 22, 2, 2, 2, 0, -1, -1, 6, 6, -1, 6, 6, 6, 6, -1, 29, 29, -1, 29, 29, 29, 1, 29, 24, 29, 24, 5, 3, 8, 1, 21, 9, 8, -1, 9, 12, 12, 12, 12, 12, 1, 12, 19, 9, -1, 9, 9, 9, 9, 18, 8, 32, 1, 8, 12, 30, 30, 33, 1, 1, 1, 32, 32, 22, -1, 32, 32, 1, 32, -1, 32, -1, 13, 13, 13, 34, 13, 1, 13, 23, -1, 6, -1, 18, 13, -1, -1, 0, -1, 2, 1, 23, 34, 21, -1, 34, 34, 34, 9, 0, -1, 34, 1, -1, 3, 18, -1, -1, 17, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, -1, 2, -1, 2, 2, 2, 15, 2, -1, 2, 6, -1, 8, 1, 16, 16, 7, 1, 23, 1, -1, 15, -1, 35, 25, 13, 33, 29, 10, 29, 1, 29, 10, 8, 1, -1, 11, -1, 11, 11, 8, 10, 1, 21, 35, -1, 35, -1, 33, -1, 30, 35, 1, 35, 8, 13, 35, 24, 35, -1, 18, -1, 8, 0, 9, -1, 4, 26, 26, -1, -1, 1, -1, 17, 14, -1, 9, 1, -1, 12, 31, 29, -1, 31, 9, 12, 12, 9, -1, -1, 12, 12, 12, -1, 12, 2, 12, 26, -1, 26, 26, 25, -1, 32, 32, 11, 1, 32, 1, -1, -1, 31, 27, -1, -1, 31, -1, 31, -1, 22, 3, 24, 1, 28, 28, 0, -1, -1, -1, 3, 3, 30, 14, 0, 0, -1, -1, -1, 19, 19, 0, 24, 6, 11, 27, 14, 2, 7, -1, 19, 1, 19, -1, -1, 1, 22, -1, -1, -1, 21, 33, 7, 7, 21, 0, 1, 27, 7, 16, 7, 22, 16, 16, -1, 20, 3, 2, -1, 11, 35, 8, 8, -1, 35, 8, 33, 3, 11, 23, 11, 11, 8, 22, 0, -1, -1, 0, 15, 14, -1, 0, 11, -1, -1, -1, -1, 13, 17, 3, 26, 26, 18, 26, -1, 26, 20, 11, 4, -1, -1, 30, -1, -1, 5, 17, 16, 31, 31, 31, 30, -1, 31, 5, 31, 31, 15, -1, 31, 31, 19, 19, 15, 14, 11, 11, 11, 11, 36, 23, 11, 6, 36, -1, -1, 4, 1, 1, 9, 4, 0, 4, 20, 9, 28, 28, 26, 25, -1, -1, 4, 26, 1, -1, -1, 26, 11, -1, 26, 27, 28, 18, 1, 2, 0, -1, 28, -1, 1, 7, 7, 0, 7, -1, 33, 7, 2, 8, 8, -1, 33, 0, 8, 7, 22, 1, 0, 10, 10, 2, 0, 0, -1, 10, 0, 10, 25, 0, 20, 2, -1, 33, 21, 3, -1, 13, 12, 36, 19, 5, -1, -1, -1, 1, -1, 25, 5, 19, -1, -1, 32, 36, 4, -1, 36, 10, 1, 12, 12, 4, 21, 21, 1, 12, 8, 8, 12, 12, 11, 1, 25, 3, 9, 12, 4, 4, -1, 2, 10, -1, 19, -1, -1, -1, 20, -1, 23, 4, 23, 1, 0, -1, 7, 4, -1, 4, 5, -1, 4, 3, 3, 3, 2, 3, 2, 2, 26, 7, 3, 2, 2, 1, 23, 7, 5, 26, 1, -1, 24, 2, 15, 28, 23, 0, -1, 28, 0, 1, 10, 10, 25, 15, 0, 20, 20, -1, 16, -1, 18, 0, 30, -1, 0, 18, 1, 10, -1, -1, 0, 0, -1, 2, -1, -1, -1, 2, 2, 0, -1, 13, 11, 20, 5, 34, -1, 4, 21, 20, 4, 20, 25, 30, 14, 12, 1, 25, 4, -1, 12, -1, -1, 4, 4, 4, 9, -1, 22, 4, 3, 1, 35, 35, -1, 1, -1, 5, 5, 3, 3, 3, 6, 14, -1, 1, 4, 2, -1, 11, 2, 2, 9, 7, 33, 2, 29, 29, 2, 0, 0, 21, -1, 3, -1, 3, 1, 7, 12, -1, 31, 3, 10, 13, -1, -1, 13, -1, -1, 3, 11, 11, 10, -1, 4, 2, 19, 15, 10, 0, -1, 9, 1, 15, 26, 18, 33, 9, 28, 0, 5, -1, -1, -1, -1, 0, 22, 5, 5, 28, 0, 29, 26, 20, 19, -1, 22, 25, 20, 4, 4, 4, 20, 0, 24, 2, 19, 2, -1, 35, 4, 18, -1, 4, 1, 10, 2, 4, 4, 4, -1, 4, 20, 28, -1, 0, 1, 2, 2, 4, 6, -1, 12, 12, 1, 1, 2, 2, 4, -1, 26, 5, 29, 26, 26, -1, -1, 30]\n",
            "-------RUN82-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': \"CountVectorizer(stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b02d2c50>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[26, 1, 32, 32, -1, 11, -1, 18, -1, -1, -1, -1, -1, -1, 14, 11, 24, 26, 8, 11, -1, 15, -1, 8, 8, 23, 37, 11, 11, -1, -1, -1, 32, 33, -1, -1, 0, 10, -1, -1, 8, 11, 35, 8, 35, 7, 35, 35, 8, -1, 35, 8, -1, -1, -1, 19, -1, 1, 18, 18, 0, 18, 18, 22, 11, 11, -1, -1, 15, 3, 28, 15, 0, 0, 16, 3, 24, -1, -1, 1, 19, 1, -1, 19, 1, 23, 19, 0, 1, 33, 0, 22, 19, -1, 22, -1, 23, 0, -1, -1, -1, 16, -1, 16, -1, 28, -1, -1, -1, -1, 3, 15, 37, 0, 1, -1, 5, 22, -1, 1, -1, 5, 8, 16, 35, -1, 15, 1, 8, 15, 1, 14, 11, 15, 15, 26, 8, 24, 15, 35, -1, -1, 30, -1, 14, 18, 26, -1, 1, -1, -1, -1, -1, 22, -1, 18, 0, 30, -1, 2, -1, 2, 28, 2, 11, -1, -1, 16, -1, -1, 22, 22, -1, -1, 14, 18, -1, -1, 14, 3, 10, 24, 14, -1, 15, -1, -1, 1, 15, 15, 14, 14, 19, 1, -1, 5, 17, 17, 19, -1, 19, 1, 19, -1, -1, -1, 5, 19, 0, -1, 0, 19, 1, 1, 19, 19, 19, 0, 1, -1, 17, -1, 0, 0, 22, 17, 37, 37, 1, 4, -1, -1, -1, 17, 17, -1, 23, -1, -1, 17, -1, 3, 3, -1, 17, -1, -1, -1, 10, 11, -1, 17, -1, 26, -1, 7, 7, -1, 0, 0, 37, 16, 10, -1, -1, 1, 28, 37, -1, 3, 3, 3, -1, 0, 3, -1, 3, 3, 23, -1, 0, 10, 3, 7, 7, 3, 17, 36, 27, 36, 27, 1, 27, 27, 5, -1, -1, -1, 17, 3, 3, 3, 15, 3, -1, 3, 0, 3, 27, -1, 8, 3, 35, 10, -1, -1, -1, -1, -1, -1, 18, -1, -1, -1, 4, 26, 11, 26, 1, 26, -1, -1, 21, 26, 31, -1, 18, 14, 0, 14, 14, -1, -1, 22, -1, -1, 14, 14, -1, 14, -1, 8, 11, 1, 22, -1, -1, 21, 21, 26, 16, 15, 11, -1, 21, -1, 15, 24, 23, 15, -1, -1, 15, 28, -1, 15, -1, 22, 10, -1, 12, 12, 11, 22, 22, 22, 21, 22, -1, 23, 34, -1, -1, 12, 12, 12, 12, 12, 12, 29, -1, 12, 17, -1, 17, -1, 12, 0, -1, -1, 23, -1, 0, 16, -1, 23, -1, -1, 31, 36, -1, -1, -1, 1, -1, 16, 24, 31, 0, 0, 0, 24, 0, 3, 0, -1, 0, 0, 0, 0, 0, 0, -1, 31, 26, 3, -1, 3, 0, -1, 33, 3, 3, 3, 31, 1, 3, 1, 3, 3, 10, 3, 29, 1, 17, 3, 3, 3, 3, 0, 0, 7, -1, 0, -1, 0, 3, -1, -1, 0, -1, 36, 0, 22, -1, -1, 36, 7, 1, 7, -1, 16, -1, 23, 36, -1, 17, -1, 6, 6, 6, -1, 26, 0, -1, 17, 17, -1, 6, -1, 6, 26, 6, 6, 6, 6, 6, -1, 21, 6, 3, 6, 21, 21, 6, 20, 21, 6, -1, 16, 6, 6, 21, -1, 0, 0, 1, 21, 7, -1, 33, 7, 9, -1, 3, 0, 7, -1, 7, 7, 1, 35, 7, 9, 9, 9, -1, -1, 7, 7, 1, 27, 31, 4, 4, 4, -1, 4, 9, -1, 9, 21, 5, 20, 20, 20, 21, 21, 9, 9, 9, 8, -1, 9, 21, 9, 21, 28, 21, -1, -1, 23, 18, 23, 22, 4, 8, 4, 4, -1, 4, 4, 16, 4, 4, -1, 12, 4, 23, 23, 33, -1, 5, 5, 5, 5, 5, 5, 9, 5, 5, 0, 0, -1, -1, -1, 14, 1, 16, -1, 5, -1, 5, 5, 5, 0, 5, 5, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 1, 3, -1, -1, 6, 6, -1, 6, 4, -1, 6, 1, 6, 11, 6, 26, 6, -1, 6, 8, 0, 8, 1, 22, -1, 6, 6, 6, -1, 6, -1, 6, -1, -1, 6, 26, 1, 7, 7, 7, 7, -1, 7, 7, -1, 0, 7, -1, 15, 7, 7, 1, 32, 0, 15, -1, 9, 9, 16, 24, 1, 6, 14, 9, -1, 32, -1, -1, 1, -1, 9, 24, 8, -1, 4, 4, 27, 1, -1, 4, 4, 1, 4, 4, 8, -1, 4, -1, 20, 4, 1, 20, 27, 4, -1, 28, 4, 20, 22, 0, 20, 20, 20, 1, -1, 8, 24, -1, 2, 2, 2, 28, 8, 24, 0, -1, 13, -1, -1, 27, 12, 0, 0, -1, -1, -1, -1, -1, 0, 1, 18, 3, 3, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 0, 10, 2, 2, 2, 2, 2, 1, -1, 2, 5, 2, 2, 2, 2, -1, 2, 2, 5, 16, -1, 0, 2, -1, 16, 2, 2, 2, 0, -1, -1, 6, 6, -1, 6, 6, 6, 6, -1, 29, 29, -1, 29, 29, 29, 1, 29, -1, 29, 23, 5, 3, 8, 1, 37, 12, 8, -1, 12, 13, 13, 13, 13, 13, 1, 13, 18, 12, 8, 12, 12, 12, 12, -1, 8, 32, 11, 8, 13, 28, 28, 33, 1, -1, 1, 32, 32, 16, -1, 32, 32, 1, 32, -1, 32, -1, 14, 14, 14, -1, 14, 1, 14, 19, -1, 6, -1, 24, 14, 3, -1, 0, 27, 2, 1, 19, 27, 35, -1, 27, 27, 27, 13, 0, -1, 27, 1, -1, 3, 24, -1, -1, 22, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, -1, 2, -1, 2, 2, 2, 17, 2, -1, 2, 6, -1, 8, 1, 21, 21, 7, -1, 19, 1, -1, 17, 23, 34, 11, 14, 33, 29, 9, 29, 1, 29, 9, 8, 1, -1, 10, -1, 10, 10, 8, 9, 1, 35, 34, -1, 34, -1, 33, -1, 28, 34, 1, 34, 8, 14, 34, 23, 34, -1, 24, -1, 8, 0, 12, -1, 4, 25, 25, 10, -1, -1, 23, 22, 15, -1, 12, 1, -1, 13, 30, 29, -1, 30, 12, 13, 13, 12, -1, -1, 13, 13, 13, -1, 13, 2, 13, 25, -1, 25, 25, 11, -1, 32, 32, -1, 1, 32, 1, -1, -1, 30, 26, -1, -1, 30, -1, 30, -1, 16, 3, 23, 1, 31, 31, 0, -1, -1, -1, 3, 3, 28, 15, 0, 0, -1, -1, 24, 18, 18, 0, 23, 6, 10, 26, 15, 2, 7, -1, 18, 1, 18, 24, -1, 11, 16, -1, -1, -1, -1, 33, 7, 7, 35, 0, 1, 26, 7, 21, 7, 16, 21, 21, -1, 20, 3, 2, -1, 10, 34, 8, 8, -1, 34, 8, 33, 3, 10, 19, 10, 10, 8, 16, 0, -1, -1, 0, 17, 15, -1, 0, 10, -1, -1, -1, -1, 14, 22, 3, 25, 25, 24, 25, -1, 25, 20, 10, 4, 2, -1, 28, -1, -1, 5, 22, 21, 30, 30, 30, 28, -1, 30, 5, 30, 30, 17, -1, 30, 30, 18, 18, 17, 15, 10, 10, 10, 10, 36, 19, 10, 6, 36, -1, 9, 4, 1, 1, 12, 4, 0, 4, 20, 12, 31, 31, 25, 11, -1, -1, 4, 25, 1, -1, 27, 25, -1, -1, 25, 26, -1, 24, 1, 2, 0, -1, 31, -1, 1, 7, 7, 0, 7, -1, 33, 7, 2, 8, 8, -1, 33, 0, 8, 7, 16, 1, 0, 9, 9, 2, 0, 0, -1, 9, 0, 9, 11, 0, 20, 2, -1, 33, 37, 3, 5, 14, 13, 36, 18, 5, -1, -1, 19, 1, 10, 11, 5, 18, 5, 37, 32, 36, 4, -1, 36, 9, 1, 13, 13, 4, -1, 35, 1, 13, 8, 8, 13, 13, 10, 1, 11, 3, 12, 13, 4, 4, 4, 2, 9, 1, 18, -1, -1, -1, 20, -1, 19, -1, 19, 1, 0, -1, 7, 4, -1, 4, 5, 11, 4, 3, 3, 3, 2, 3, 2, 2, 25, 7, 3, 2, 2, 1, 19, 7, 5, 25, 1, -1, 23, 2, 17, 31, 19, 0, -1, 31, 0, 1, 9, 9, 11, 17, 0, 20, 20, -1, 21, -1, 24, 0, 28, 0, 0, 16, 1, 9, -1, -1, 0, 0, -1, 2, 3, 11, -1, 2, 2, 0, 1, 14, 10, 20, 5, 27, -1, 4, -1, 20, 4, 20, 11, 28, 15, 13, 1, 11, 4, 4, 13, -1, -1, 4, 4, 4, 12, 10, 16, 4, 3, 1, 34, 34, 3, 1, 1, 5, 5, 3, 3, 3, 6, 15, -1, 1, 4, 2, -1, 10, 2, 2, 12, 7, -1, 2, 29, 29, 2, 0, 0, 37, -1, 3, 37, 3, 1, 7, 13, -1, 30, 3, 9, 14, -1, -1, 14, -1, 24, 3, 10, 10, 9, 27, 4, 2, 18, 17, 9, 0, 10, 12, 1, 17, 25, 24, 33, 12, 31, 0, 5, -1, -1, -1, -1, 0, 16, 5, 5, 31, 0, 29, 25, 20, 18, -1, 16, 11, 20, 4, 4, 4, 20, 0, 23, 2, 18, 2, -1, 34, 4, 24, -1, 4, 1, 9, 2, 4, 4, 4, -1, 4, 20, 31, -1, 0, 1, 2, 2, 4, 6, 3, 13, 13, 1, 1, 2, 2, 4, -1, 25, 5, 29, 25, 25, -1, -1, 28]\n",
            "-------RUN83-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': \"CountVectorizer(ngram_range=(1, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b891dd10>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[26, 2, 31, 31, -1, 14, -1, 23, -1, -1, -1, -1, -1, -1, 11, 14, 22, 26, -1, 14, -1, 12, 19, 8, 8, 21, 36, -1, 14, -1, -1, -1, 31, 32, -1, -1, -1, 9, 0, -1, 8, 14, 28, 8, 28, 6, 28, 28, 8, -1, 28, 8, -1, -1, -1, 18, -1, 2, 23, 23, 1, 23, 23, 19, 14, 14, -1, -1, 12, 12, 27, 12, 1, 1, 20, 4, 22, -1, -1, 2, 18, 2, -1, -1, 2, 21, 18, 1, 2, 32, 1, 19, 18, -1, 19, -1, 21, 1, -1, -1, -1, 20, 2, -1, -1, 27, -1, -1, 0, -1, 4, 12, 36, -1, 2, -1, 7, 19, 22, 2, -1, 7, 8, 20, 28, -1, 12, 2, 8, 12, 2, 11, 14, 12, 12, 26, -1, 22, 12, 28, -1, -1, -1, -1, 11, 23, 26, -1, 2, 0, -1, -1, -1, 19, -1, 23, 1, -1, -1, 3, -1, 3, 27, 3, 14, -1, -1, 20, -1, -1, 19, 19, 2, 0, 11, 23, 2, -1, 11, 12, 9, 22, 11, 0, 12, -1, -1, 2, 12, 12, 11, 11, 18, 2, -1, 18, 15, 15, 18, -1, 18, 2, 18, -1, -1, -1, -1, 18, 1, -1, 1, 18, 2, 2, 18, 18, 18, 1, 2, -1, 15, -1, 1, 1, 19, 15, 36, 36, 2, 0, 0, 0, 0, 15, 15, 0, 21, 0, -1, 15, 25, 4, 4, -1, 15, -1, -1, -1, 9, 14, -1, 15, 10, -1, -1, 6, 6, -1, 1, 1, 36, 20, 9, -1, -1, 2, 27, 36, -1, 4, 4, 4, -1, 1, 4, -1, 4, 4, 21, -1, 1, 9, 4, 6, 6, -1, 15, 34, 29, 34, 29, 2, 29, 29, 7, -1, -1, -1, 15, 4, 4, 4, 12, 4, -1, 4, 1, 4, -1, -1, 8, 4, 28, 9, -1, -1, -1, -1, 20, -1, 23, -1, -1, -1, 0, 26, 14, 26, 2, 26, -1, -1, 16, 26, 30, -1, 23, 11, 1, 11, 11, 21, 21, -1, -1, -1, 11, 11, 0, 11, -1, 8, 14, 2, 19, 0, -1, 16, 16, 26, 22, 12, -1, -1, 16, -1, 12, 22, 21, 12, 22, 0, 12, 27, -1, 12, 0, 19, 9, 0, 10, 10, 14, 19, 19, 19, 16, 19, -1, 21, -1, -1, -1, 10, 10, 10, 10, 10, 10, 25, -1, 10, 15, -1, 15, -1, 10, 1, 0, 0, 21, -1, 1, 20, 0, 21, 0, -1, 30, 34, -1, -1, -1, -1, -1, -1, 22, 30, 1, 1, 1, 22, 1, 4, 1, -1, 1, 1, 1, 1, 1, 1, -1, 30, -1, 4, -1, 4, 1, -1, 32, 4, 4, 4, 30, 2, 4, -1, 4, 4, 9, 4, 25, 2, 15, 4, 4, 4, 4, 1, 1, 6, -1, 1, -1, 1, 4, -1, -1, 1, -1, 34, 1, 19, -1, -1, 34, 6, 2, 6, -1, 20, -1, 21, 34, -1, 15, -1, 5, 5, 5, 0, 26, 1, -1, 15, 15, -1, 5, -1, 5, 26, 5, 5, 5, 5, 5, 20, 16, 5, 4, 5, 16, 16, 5, 17, 16, 5, -1, -1, 5, -1, 16, 0, 1, 1, 2, 16, 6, -1, 32, 6, 0, -1, 4, 1, 6, -1, 6, 6, 2, 28, 6, 0, 0, 0, -1, 0, 6, 6, 2, 29, 30, 0, 0, 0, 0, 0, 0, -1, 0, 16, 7, 17, 17, 17, 16, 16, 0, 0, 0, 8, -1, 0, 16, 0, 16, 27, 16, 0, 0, 21, -1, 21, 19, 0, 8, 0, 0, -1, 0, 0, 20, 0, 0, -1, 10, 0, 21, -1, 32, -1, 7, 7, 7, 7, 7, 7, 0, 7, 7, 1, 1, 29, -1, -1, 11, -1, 20, -1, 7, -1, 7, 7, 7, 1, 7, 7, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 2, 4, 9, -1, 5, 5, -1, 5, 0, -1, 5, -1, 5, 14, 5, 26, 5, -1, 5, 8, 1, 8, 2, 19, -1, 5, 5, 5, -1, 5, -1, 5, 0, -1, 5, 26, 2, 6, 6, 6, 6, -1, 6, 6, -1, 1, 6, -1, 12, -1, 6, -1, 31, 1, 12, -1, 0, 0, 20, 22, -1, 5, 11, 0, -1, 31, 0, -1, 2, -1, 0, 22, -1, 2, 0, 0, -1, 2, -1, 0, 0, -1, 0, 0, 8, -1, 0, 34, 17, 0, 2, 17, 29, 0, -1, 27, 0, 17, 19, 1, 17, 17, 17, 2, -1, 8, 22, 0, 3, 3, 3, 27, 8, 22, 1, 0, 13, -1, -1, 29, 10, 1, 1, 0, -1, -1, -1, -1, 1, -1, 19, 4, 4, 7, 7, 1, 7, 7, 1, 7, 7, -1, 7, 1, 9, 3, 3, 3, 3, 3, 2, 0, 3, 7, 3, 3, 3, 3, -1, 3, 3, 7, 20, -1, 1, 3, -1, 20, 3, 3, 3, 1, -1, -1, 5, 5, -1, 5, 5, 5, 5, -1, 25, 25, -1, 25, 25, 25, 2, 25, 21, 25, 21, -1, 4, 8, 2, 28, 10, 8, -1, 10, 13, 13, 13, 13, 13, 2, 13, -1, 10, -1, 10, 10, 10, 10, -1, 8, 31, 2, 8, 13, 27, 27, 32, 2, 2, 2, 31, 31, -1, -1, 31, 31, 2, 31, -1, 31, -1, 11, 11, 11, 29, 11, 2, 11, 18, -1, 5, -1, 22, 11, 4, -1, 1, -1, 3, 2, 18, 29, 28, -1, 29, 29, 29, 10, 1, 0, 29, -1, 28, 4, 22, -1, -1, 19, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, -1, 3, -1, 3, 3, 3, 15, 3, -1, 3, 5, 20, 8, 2, 16, 16, 6, 2, 18, 2, -1, 15, -1, 35, 14, 11, 32, 25, 0, 25, 2, 25, 0, 8, 2, -1, 9, -1, 9, 9, 8, 0, 2, 28, 35, -1, 35, -1, 32, -1, 27, 35, 2, 35, 8, 11, 35, -1, 35, -1, 22, -1, 8, 1, 10, -1, 0, 24, 24, -1, 28, -1, 21, 19, 12, -1, 10, 2, -1, 13, 33, 25, -1, 33, 10, 13, 13, 10, -1, -1, 13, 13, 13, 14, 13, 3, 13, 24, -1, 24, 24, 14, -1, 31, 31, -1, 2, 31, 2, -1, 0, 33, 26, -1, -1, 33, 0, 33, 33, 20, 4, 21, 2, 30, 30, 1, -1, -1, -1, 4, 4, 27, 12, 1, 1, -1, -1, -1, 23, 23, 1, 21, 5, 9, 26, 12, 3, 6, -1, 23, 2, 23, -1, -1, 14, 20, -1, -1, -1, -1, 32, 6, 6, 28, 1, 2, 26, 6, 16, 6, 20, 16, 16, -1, 17, 4, 3, -1, 9, 35, 8, 8, 6, 35, 8, 32, 4, 9, 18, 9, 9, 8, 20, 1, -1, -1, 1, 15, 12, -1, 1, 9, -1, -1, 29, -1, 11, 19, 4, 24, 24, 22, 24, -1, 24, 17, 9, 0, 3, -1, 27, -1, -1, 7, 19, 16, 33, 33, 33, 27, 0, 33, 7, 33, 33, 15, -1, 33, 33, 23, 23, 15, 12, 9, 9, 9, 9, 34, 18, 9, 5, 34, 0, 0, 0, 2, 2, 10, 0, 1, 0, 17, 10, 30, 30, 24, -1, -1, -1, 0, 24, 2, -1, -1, 24, -1, -1, 24, 26, -1, 22, 2, 3, 1, -1, 30, 0, 2, 6, 6, 1, 6, -1, 32, 6, 3, 8, 8, -1, 32, 1, 8, 6, 20, 2, 1, 0, 0, 3, 1, 1, 0, 0, 1, 0, 14, 1, 17, 3, -1, 32, 36, 4, -1, 11, 13, 34, 23, 7, -1, -1, 18, 2, 9, 14, 7, 23, -1, 36, 31, 34, 0, -1, 34, 0, 2, 13, 13, 0, -1, 28, 2, 13, 8, 8, 13, 13, 9, 2, 14, 4, 10, 13, 0, 0, 0, 3, 0, -1, 23, -1, -1, -1, 17, -1, 18, 0, 18, 2, 1, -1, 6, 0, -1, 0, 7, 14, 0, 4, 4, 4, 3, 4, 3, 3, 24, 6, 4, 3, 3, 2, 18, 6, 7, 24, 2, -1, 21, 3, 15, 30, 18, 1, 36, 30, 1, 2, 0, 0, 14, 15, 1, 17, 17, -1, 16, -1, 22, 1, 27, 9, 1, 20, 2, 0, -1, -1, 1, 1, -1, 3, -1, -1, -1, 3, 3, 1, -1, 11, 9, 17, 7, 29, -1, 0, 28, 17, 0, 17, 14, 27, 12, 13, 2, 14, 0, 0, 13, -1, -1, 0, 0, 0, 10, 9, -1, 0, 4, 2, 35, 35, 4, 2, -1, 7, 7, 4, 4, 4, 5, 12, -1, 2, 0, 3, -1, 9, 3, 3, 10, 6, 32, 3, 25, 25, 3, 1, 1, 36, -1, 4, 36, 4, 2, 6, 13, -1, 33, 4, 0, 11, -1, -1, 11, -1, -1, 4, 9, 9, 0, -1, 0, -1, -1, 15, 0, 1, 34, 10, 2, 15, 24, 22, 32, 10, 30, 1, 7, -1, -1, -1, -1, 1, 20, 7, 7, 30, 1, 25, 24, 17, -1, -1, 20, 14, 17, 0, 0, 0, 17, 1, 21, 3, 23, 3, 34, 35, 0, 22, -1, 0, 2, 0, 3, 0, 0, 0, -1, 0, 17, 30, -1, 1, 2, 3, 3, 0, 5, -1, 13, 13, 2, 2, 3, 3, 0, 0, 24, 7, 25, 24, 24, -1, 0, 27]\n",
            "-------RUN84-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': \"CountVectorizer(ngram_range=(2, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df4a9008410>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[28, 4, 33, 33, -1, 17, -1, 23, -1, -1, -1, -1, -1, -1, 14, 17, 24, 28, -1, 17, -1, 16, 15, 10, 10, 22, 13, -1, 17, -1, -1, -1, 33, 34, -1, -1, -1, 8, -1, -1, 10, 17, 13, 10, 13, 7, 13, 13, 10, 4, 13, 10, -1, -1, -1, 25, -1, 4, 23, 23, -1, 23, 23, 15, 17, 17, -1, -1, 16, -1, 32, 16, 0, 0, 20, -1, 24, -1, -1, 4, 25, -1, -1, 25, 4, 22, 25, 0, 4, 34, 0, 15, 25, -1, 15, 11, 22, 0, -1, -1, -1, 20, -1, -1, -1, 32, -1, -1, -1, -1, -1, 16, 13, 28, 4, -1, 6, 15, -1, -1, 13, 6, 10, 20, 13, 11, 16, 4, 10, 16, 4, 14, 17, 16, 16, 28, -1, 24, 16, 13, -1, 15, 30, -1, 14, 23, 28, -1, 4, -1, -1, -1, -1, 15, -1, 23, 0, 30, -1, 1, -1, 1, 32, 1, 17, -1, -1, 20, -1, -1, 15, 15, -1, -1, 14, 23, -1, -1, 14, -1, 8, 24, 14, -1, 16, 23, -1, 4, 16, 16, 14, 14, 25, 4, -1, 6, 18, 18, 25, 6, 25, 4, 25, -1, 15, -1, 6, 25, 0, -1, 0, -1, 4, 4, 25, 25, 25, 0, 4, -1, 18, -1, 0, 0, 15, 18, 13, 13, -1, 2, -1, -1, -1, 18, 18, -1, 22, -1, -1, 18, -1, 3, 3, -1, 18, -1, -1, -1, 8, 17, -1, 18, 9, -1, -1, 7, 7, -1, 0, 0, 13, 20, 8, -1, -1, 4, 32, 13, -1, 3, 3, 3, -1, 0, 3, -1, 3, 3, 22, -1, 0, 8, 3, 7, 7, 3, 18, 37, 29, 37, 29, 4, 29, 29, 6, -1, -1, -1, 18, 3, 3, 3, 16, 3, -1, 3, -1, 3, -1, -1, 10, 3, 13, 8, -1, -1, -1, -1, 20, -1, 23, -1, -1, 16, 2, 28, -1, 28, 35, 28, -1, -1, 19, 28, 31, -1, 23, 14, 0, 14, 14, 22, -1, 15, -1, -1, 14, 14, -1, 14, -1, 19, -1, 4, 15, -1, -1, 19, 19, 28, 20, 16, -1, -1, 19, -1, 16, 24, 22, 16, -1, 2, 16, 32, -1, 16, -1, 15, 8, -1, 9, 9, 17, 15, 15, 15, 19, 15, -1, 22, 36, -1, -1, 9, 9, 9, 9, 9, 9, 27, -1, 9, 18, -1, 18, -1, 9, 0, -1, -1, 22, -1, 0, 20, -1, 22, -1, -1, 31, 37, -1, -1, -1, -1, -1, 20, 24, 31, 0, 0, 0, 24, 0, 3, 0, -1, 0, 0, 0, 0, 0, 0, -1, 31, -1, 3, -1, 3, 0, -1, 34, 3, 3, 3, 31, 4, 3, -1, 3, 3, 8, 3, 27, 4, 18, 3, 3, 3, 3, 0, 0, 7, -1, 0, -1, 0, 3, -1, -1, 0, -1, 37, 0, 15, -1, -1, 37, 7, 4, 7, -1, 20, -1, 22, 37, -1, 18, -1, 5, 5, 5, -1, 28, 0, -1, 18, 18, -1, 5, -1, 5, 28, 5, 5, 5, 5, 5, 20, 19, 5, 3, 5, 19, 19, 5, 21, 19, 5, -1, -1, 5, 5, 19, -1, 0, 0, -1, 19, 7, -1, 34, 7, -1, -1, 3, 0, 7, -1, 7, 7, 4, 13, 7, 12, 12, 12, -1, -1, 7, 7, 4, 29, 31, 2, 2, 2, -1, 2, 12, -1, 12, 19, 6, 21, 21, 21, 19, 19, 12, 12, 12, 10, -1, 12, 19, 12, 19, 32, 19, -1, -1, 22, -1, 22, 15, 2, -1, 2, 2, -1, -1, 2, 20, 2, 2, -1, 9, 2, 22, 22, 34, -1, 6, 6, 6, 6, 6, 6, 12, 6, 6, 0, 0, 29, -1, 11, 14, -1, 20, -1, 6, -1, 6, 6, -1, 0, 6, 6, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 3, 8, -1, 5, 5, -1, 5, 2, -1, 5, -1, 5, 17, 5, 28, 5, -1, 5, 10, 0, 10, 35, 15, -1, 5, 5, 5, -1, 5, 5, 5, 2, -1, 5, 28, -1, 7, 7, 7, 7, -1, 7, 7, -1, -1, 7, -1, 16, 7, 7, 4, 33, 0, 16, -1, 12, 12, 20, 24, -1, 5, 14, 12, -1, 33, -1, -1, 4, -1, 12, 24, -1, -1, 2, 2, -1, 4, -1, 2, 2, -1, 2, 2, 10, -1, 2, -1, 21, 2, 35, 21, 29, 2, -1, 32, 2, 21, 15, 0, 21, 21, 21, 4, -1, 10, 24, -1, 1, 1, 1, 32, -1, 24, 0, -1, 11, -1, -1, 29, 9, 0, 0, -1, -1, -1, -1, 37, -1, -1, 15, 3, 3, 6, 6, 0, 6, 6, 0, 6, 6, 6, 6, 0, 8, 1, 1, 1, 1, 1, 35, 2, 1, 6, 1, 1, 1, 1, -1, 1, 1, 6, 20, -1, 0, 1, -1, 20, 1, 1, 1, 0, -1, -1, 5, 5, -1, 5, 5, 5, 5, -1, 27, 27, -1, 27, 27, 27, 4, 27, 22, 27, 22, 6, 3, 10, 4, 13, 9, 10, -1, 9, 11, 11, 11, 11, 11, 35, 11, -1, 9, -1, 9, 9, 9, 9, 24, 10, 33, 17, 10, 11, -1, 32, 34, -1, -1, -1, 33, 33, 20, -1, 33, 33, 4, 33, -1, 33, -1, 14, 14, 14, 29, 14, -1, 14, 25, -1, 5, -1, 24, 14, -1, -1, 0, -1, 1, 4, 25, 29, 13, -1, 29, 29, 29, 9, 0, -1, 29, -1, 13, -1, 24, -1, -1, 15, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, -1, 1, -1, 1, 1, 1, 18, -1, -1, 1, 5, -1, 10, 4, 19, 19, 7, -1, 25, 4, -1, 18, -1, 36, 17, 14, 34, 27, 12, 27, 35, 27, 12, 10, 4, -1, 8, -1, 8, 8, 10, 12, 4, 13, 36, -1, 36, -1, 34, -1, 32, 36, 4, 36, 10, 14, 36, -1, 36, -1, 24, -1, 10, 0, 9, -1, 2, 26, 26, -1, 13, -1, 22, 15, 16, -1, 9, -1, -1, 11, 30, 27, -1, 30, 9, 11, 11, 9, -1, -1, 11, 11, 11, -1, 11, 1, 11, 26, -1, 26, 26, 17, -1, 33, 33, -1, 35, 33, 35, -1, -1, 30, 28, -1, 15, 30, -1, 30, -1, 20, 3, 22, 35, 31, 31, 0, -1, -1, -1, 3, -1, 32, 16, 0, 0, -1, -1, 24, 23, 23, 0, 22, 5, 8, 28, 16, 1, 7, -1, 23, 4, 23, -1, -1, 17, 20, -1, 11, -1, 7, 34, 7, 7, 13, 0, 4, 28, 7, 19, 7, 20, 19, 19, -1, 21, 3, 1, -1, 8, 36, 10, 10, 7, 36, 10, 34, 3, 8, -1, 8, 8, 10, 20, 0, -1, -1, 0, 18, -1, 27, 0, 8, -1, -1, 29, -1, 14, 15, 3, 26, 26, 24, 26, -1, 26, 21, 8, 2, -1, -1, 32, -1, -1, 6, 15, 19, 30, 30, 30, 32, -1, 30, 6, 30, 30, 18, -1, 30, 30, 23, 23, 18, 16, 8, 8, 8, 8, 37, 25, 8, 5, 37, -1, 12, 2, 35, 4, 9, 2, 0, 2, 21, 9, 31, 31, 26, 17, -1, -1, -1, 26, 4, -1, -1, 26, 8, -1, 26, 28, -1, 24, 4, 1, 0, -1, 31, -1, 4, 7, 7, -1, 7, -1, 34, 7, 1, 10, 10, -1, 34, 0, 10, 7, 20, 4, 0, 12, 12, 1, 0, 0, -1, 12, 0, 12, 17, 0, 21, 1, -1, 34, 13, 3, -1, 14, 11, 37, 23, 6, -1, -1, 25, 4, 8, 17, 6, 23, -1, 13, 33, 37, 2, -1, 37, 12, 35, 11, 11, 2, 13, 13, 4, 11, 10, 10, 11, 11, 8, 4, 17, 3, 9, 11, 2, 2, 2, 1, -1, -1, 23, -1, -1, 24, 21, -1, -1, -1, 25, 4, 0, -1, 7, 2, -1, 2, 6, 17, 2, 3, 3, 3, 1, 3, 1, 1, 26, 7, 3, 1, 1, 4, 25, 7, 6, 26, 4, -1, 22, 1, 18, 31, 25, 0, -1, 31, 0, 4, 12, 12, 17, 18, 0, 21, 21, -1, 19, -1, 24, 0, 32, -1, 0, -1, 35, 12, -1, -1, 0, 0, -1, 1, -1, 17, -1, 1, 1, 0, -1, 14, 8, 21, 6, 29, -1, 2, 13, 21, 2, 21, 17, 32, 16, 11, 4, 17, 2, 2, 11, 22, -1, 2, 2, 2, 9, 8, 20, 2, 3, -1, 36, 36, -1, 4, -1, 6, 6, 3, 3, 3, 5, 16, -1, -1, 2, 1, -1, 8, 1, 1, 9, 7, 34, 1, 27, 27, 1, 0, 0, 13, -1, 3, 13, 3, 4, 7, 11, -1, 30, 3, 12, 14, -1, -1, 14, -1, -1, 3, 8, 8, 12, -1, 2, 1, 23, 18, 12, 0, -1, 9, 4, 18, 26, 24, 34, 9, 31, 0, 6, 17, -1, -1, -1, 0, -1, 6, 6, 31, 0, 27, 26, 21, 23, -1, 20, 17, 21, 2, 2, 2, 21, 0, 22, 1, 23, 1, -1, 36, 2, 24, -1, 2, -1, -1, 1, 2, 2, 2, -1, 2, 21, 31, -1, 0, 35, 1, 1, 2, 5, -1, 11, 11, 4, 4, 1, 1, 2, 2, 26, 6, 27, 26, 26, -1, -1, 32]\n",
            "Skipping coherence calculation due to error: unable to interpret topic as either a list of tokens or a list of ids\n",
            "-------RUN85-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': \"CountVectorizer(stop_words='english')\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[9, 4, 31, 31, -1, -1, -1, 20, -1, -1, -1, -1, -1, -1, 12, 23, 24, 9, -1, 23, -1, 14, 18, 8, 8, 27, 17, -1, 23, -1, -1, -1, 31, 32, -1, -1, 1, 10, 0, -1, 8, 23, 17, 8, 17, 7, 17, 17, 8, -1, 17, 8, -1, -1, -1, 21, -1, 4, 20, 20, -1, 20, 20, 18, 23, 23, -1, -1, 14, 14, 28, 14, 1, 1, 22, 3, 24, -1, -1, 4, 21, -1, -1, 21, 4, 27, 21, 1, 4, 32, 1, 18, 21, -1, 18, -1, -1, 1, -1, -1, -1, 22, -1, -1, -1, 28, -1, -1, 0, -1, 3, 14, 17, 9, 4, -1, 5, 18, -1, -1, -1, 5, 8, 22, 17, -1, 14, 4, 8, 14, 4, 12, 23, 14, 14, 9, 8, 24, 14, 17, -1, -1, 9, -1, 12, 20, 9, -1, 4, 0, -1, -1, -1, 18, -1, 20, 1, 9, -1, 2, -1, 2, 28, 2, 23, -1, -1, 22, -1, -1, 18, 18, -1, 0, 12, 20, -1, -1, 12, -1, 10, 24, 12, 0, 14, -1, -1, 4, 14, 14, 12, 12, 21, 4, -1, 5, 16, 16, 21, -1, 21, 4, 21, -1, -1, -1, 5, 21, 1, -1, 1, 21, 4, 4, 21, 21, 21, 1, 4, -1, 16, -1, 1, 1, 18, 16, 17, 17, -1, 0, 0, 0, 0, 16, 16, 0, 27, 0, -1, 16, -1, 3, 3, -1, 16, -1, -1, -1, 10, -1, -1, 16, 11, 9, -1, 7, 7, -1, 1, 1, 17, 22, 10, -1, -1, -1, 28, 17, -1, 3, 3, 3, -1, 1, 3, -1, 3, 3, 27, 27, 1, 10, 3, 7, 7, -1, 16, 34, 26, 34, 26, 4, 26, 26, 5, -1, -1, -1, 16, 3, 3, 3, 14, 3, -1, 3, -1, -1, 26, -1, 8, 3, 17, 10, -1, -1, -1, -1, 22, -1, 20, -1, -1, -1, 0, 9, -1, 9, 35, 9, -1, -1, 13, 9, 30, 9, 20, 12, 1, 12, 12, -1, -1, 18, -1, -1, 12, 12, 0, 12, -1, 8, -1, 4, 18, 0, -1, 13, 13, 9, 22, 14, -1, -1, 13, -1, 14, 24, 27, 14, -1, 0, 14, 28, -1, 14, 0, 18, 10, 0, 11, 11, 23, 18, 18, 18, 13, 18, 13, 27, 33, -1, -1, 11, 11, 11, 11, 11, 11, 29, -1, 11, 16, -1, 16, -1, 11, 1, 0, 0, 27, 13, 1, 22, 0, 27, 0, -1, 30, 34, -1, -1, -1, -1, -1, -1, 24, 30, 1, 1, 1, 24, 1, 3, 1, -1, 1, 1, 1, 1, 1, 1, -1, 30, 9, 3, -1, 3, 1, -1, 32, 3, 3, 3, 30, 4, 3, -1, 3, 3, 10, 3, 29, 4, 16, 3, 3, 3, 3, 1, 1, 7, -1, 1, -1, 1, 3, -1, -1, 1, -1, 34, 1, 18, -1, -1, 34, 7, 4, 7, -1, 22, -1, 27, 34, 26, 16, -1, 6, 6, 6, 0, 9, 1, -1, 16, 16, -1, 6, -1, 6, 9, 6, 6, 6, 6, 6, 22, 13, 6, 3, 6, 13, 13, 6, 19, 13, 6, -1, -1, 6, -1, 13, 0, 1, 1, -1, 13, 7, -1, 32, 7, 0, -1, 3, 1, 7, -1, 7, 7, 4, 17, 7, 0, 0, 0, -1, 0, 7, 7, 4, 26, 30, 0, 0, 0, 0, 0, 0, -1, 0, 13, 5, 19, 19, 19, 13, 13, 0, 0, 0, 8, -1, 0, 13, 0, 13, 28, 13, 0, 0, 27, 20, 27, 18, 0, 8, 0, 0, -1, 0, 0, 22, 0, 0, -1, 11, 0, 27, -1, 32, -1, 5, 5, 5, 5, 5, 5, 0, 5, 5, 1, 1, 26, -1, -1, 12, -1, 22, -1, 5, -1, 5, 5, 5, 1, 5, 5, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 35, 3, 10, -1, 6, 6, -1, 6, 0, -1, 6, -1, 6, 23, 6, 9, 6, -1, 6, 8, 1, 8, 35, 18, -1, 6, 6, 6, -1, 6, 6, 6, 0, -1, 6, 9, -1, -1, 7, 7, 7, -1, 7, 7, -1, 1, -1, -1, 14, -1, 7, -1, 31, 1, 14, -1, 0, 0, 22, 24, -1, 6, 12, 0, -1, 31, 0, -1, -1, -1, 0, 24, 8, -1, 0, 0, -1, 4, -1, 0, 0, -1, 0, 0, 8, -1, 0, -1, 19, 0, 35, 19, 26, 0, -1, 28, 0, 19, 18, 1, 19, 19, 19, 4, -1, 8, 24, 0, 2, 2, 2, 28, 8, 24, 1, 0, 15, -1, -1, 26, 11, 1, 1, 0, -1, -1, -1, -1, -1, 4, -1, 3, 3, 5, 5, 1, 5, 5, 1, 5, 5, 5, 5, 1, 10, 2, 2, 2, 2, 2, 35, 0, 2, 5, 2, 2, 2, 2, -1, 2, 2, 5, 22, -1, 1, 2, -1, 22, 2, 2, 2, 1, -1, -1, 6, 6, -1, 6, 6, 6, 6, -1, 29, 29, -1, 29, 29, 29, 4, 29, -1, 29, 27, -1, 3, 8, 4, 17, 11, 8, -1, 11, 15, 15, 15, 15, 15, -1, 15, 20, 11, 8, 11, 11, 11, 11, 24, 8, 31, -1, 8, 15, 28, 28, 32, -1, -1, -1, 31, 31, -1, -1, 31, 31, 4, 31, -1, 31, -1, 12, 12, 12, 26, 12, -1, 12, 21, -1, 6, -1, 24, 12, -1, -1, 1, -1, 2, 4, 21, 26, 17, -1, 26, 26, 26, 11, 1, 0, 26, 4, 17, -1, 24, -1, -1, 18, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, -1, 2, -1, 2, 2, 2, 16, -1, -1, 2, 6, -1, 8, 4, 13, 13, 7, -1, -1, 4, -1, 16, -1, 33, 23, 12, 32, 29, 0, 29, 35, 29, 0, 8, 4, 0, 10, -1, 10, 10, 8, 0, 4, 17, 33, -1, 33, -1, 32, -1, 28, 33, 4, 33, 8, 12, 33, -1, 33, 13, 24, -1, 8, 1, 11, -1, 0, 25, 25, 10, -1, -1, -1, 18, 14, -1, 11, -1, -1, 15, 9, 29, -1, 9, 11, 15, 15, 11, -1, -1, 15, 15, 15, -1, 15, 2, 15, 25, -1, 25, 25, 23, -1, 31, 31, -1, 35, 31, -1, -1, 0, 9, 9, -1, -1, 9, 0, 9, 9, 22, 3, 27, 35, 30, 30, 1, -1, -1, -1, 3, -1, 28, 14, 1, 1, -1, -1, 24, 20, 20, 1, 27, 6, 10, 9, 14, 2, 7, -1, 20, 4, 20, -1, -1, -1, -1, -1, -1, -1, 7, 32, 7, 7, 17, 1, 4, 9, 7, 13, 7, 22, 13, 13, -1, 19, 3, 2, -1, 10, 33, 8, 8, -1, 33, 8, 32, 3, 10, 21, 10, 10, 8, 22, 1, 13, -1, 1, 16, 14, -1, 1, 10, -1, -1, 26, -1, 12, 18, 3, 25, 25, 24, 25, -1, 25, 19, 10, 0, -1, -1, 28, -1, 21, 5, 18, 13, 9, 9, 9, 28, 0, 9, 5, 9, 9, 16, -1, 9, 9, 20, 20, 16, 14, 10, 10, 10, 10, 34, 21, 10, 6, 34, 0, 0, 0, 35, 4, 11, 0, 1, 0, 19, 11, 30, 30, 25, 23, -1, -1, 0, 25, 4, -1, -1, 25, 10, -1, 25, 9, -1, 24, 4, 2, 1, 13, 30, 0, 4, 7, 7, -1, 7, -1, 32, 7, 2, 8, 8, -1, 32, 1, 8, 7, 22, 4, 1, 0, 0, 2, 1, 1, 0, 0, 1, 0, 23, 1, 19, 34, -1, 32, 17, 3, 5, 12, 15, 34, 20, 5, -1, -1, 21, 4, 10, 23, 5, 20, -1, -1, 31, 34, 0, -1, 34, 0, 35, 15, 15, 0, 17, 17, 4, 15, 8, 8, 15, 15, 10, 4, 23, 3, 11, 15, 0, 0, 0, 2, 0, -1, 20, -1, -1, -1, 19, -1, 21, 0, 21, 4, 1, -1, 7, 0, -1, 0, 5, 23, 0, 3, 3, 3, 2, 3, 2, 2, 25, 7, 3, 2, 2, -1, -1, 7, 5, 25, 4, -1, 27, 2, 16, 30, 21, 1, -1, 30, 1, 4, 0, 0, 23, 16, 1, 19, 19, -1, 13, -1, 24, 1, 28, -1, 1, -1, -1, 0, -1, -1, 1, 1, -1, 2, -1, -1, -1, 2, 2, 1, -1, 12, 10, 19, 5, 26, -1, 0, -1, 19, 0, 19, 23, 28, 14, 15, 4, 23, 0, 0, 15, -1, -1, 0, 0, 0, 11, 10, 22, 0, 3, 4, 33, 33, -1, 4, -1, 5, 5, 3, 3, 3, 6, 14, -1, -1, 0, 2, -1, 10, 2, 2, 11, 7, -1, 2, 29, 29, 2, 1, 1, 17, -1, 3, 17, 3, 4, 7, 15, -1, 9, 3, 0, 12, -1, -1, 12, -1, -1, 3, 10, 10, 0, -1, 0, -1, 20, 16, 0, 1, -1, 11, 4, 16, 25, 24, 32, 11, 30, 1, 5, -1, -1, -1, -1, 1, -1, 5, 5, 30, 1, 29, 25, 19, 20, -1, 22, 23, 19, 0, 0, 0, 19, 1, 27, 2, 20, 2, -1, 33, 0, -1, -1, 0, 4, 0, 2, 0, 0, 0, -1, 0, 19, 30, -1, 1, -1, 2, 2, 0, 6, -1, 15, 15, 4, 4, 2, 2, 0, 0, 25, 5, 29, 25, 25, -1, 0, 28]\n",
            "-------RUN86-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': 'CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7df3b00f5c90>)', 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[28, 4, 31, 31, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 12, 22, 15, 28, -1, 22, -1, 13, -1, 9, 9, 21, 11, -1, 22, -1, -1, -1, 31, 32, -1, -1, 1, -1, 0, -1, 9, 22, 11, 9, 11, 6, 11, 11, 9, 4, 11, 9, -1, -1, -1, -1, -1, 4, 23, 23, 1, 23, 23, -1, 22, 22, -1, -1, 13, 3, 29, 13, 1, 1, 14, 3, 15, 0, -1, -1, 25, 4, -1, -1, 4, 21, -1, 1, 4, 32, 1, 19, 25, -1, 19, 8, -1, 1, -1, -1, -1, 14, -1, -1, -1, 29, -1, -1, -1, -1, 3, 13, 11, -1, 4, -1, 7, 19, 14, -1, 11, 7, 9, 14, 11, 8, 13, 4, 9, 13, 4, 12, 22, 13, 13, 28, 9, 15, 13, 11, -1, -1, -1, 8, 12, 23, 28, -1, 4, 0, -1, -1, -1, 19, -1, 23, 1, 26, -1, 2, -1, 2, 29, 2, 22, -1, -1, 14, -1, -1, 19, 19, 35, 0, 12, 23, 14, -1, 12, -1, -1, 15, 12, 0, 13, -1, -1, 4, 13, 13, 12, 12, 25, 4, -1, 25, 16, 16, 25, -1, 25, 4, 25, -1, -1, -1, -1, 25, 1, -1, 1, -1, 4, 4, 25, 25, 25, 1, 4, -1, 16, -1, 1, 1, 19, 16, 11, 11, -1, 0, 0, 0, 0, 16, 16, 0, 21, 0, -1, 16, -1, 3, 3, -1, 16, -1, -1, -1, 17, -1, -1, 16, 10, 26, -1, 6, 6, -1, 1, 1, 11, 14, 17, -1, -1, 4, 29, 11, -1, 3, 3, 3, -1, 1, 3, -1, 3, 3, 21, -1, 1, 17, 3, 6, 6, 3, 16, 33, 36, 33, 36, 4, 36, 36, 7, 11, -1, -1, 16, 3, 3, 3, 13, 3, -1, 3, 1, 3, -1, -1, 9, 3, 11, -1, -1, -1, -1, -1, 14, -1, 23, 8, -1, 3, 0, 28, -1, 28, 35, 28, -1, -1, 20, 28, 30, -1, 23, 12, 1, 12, 12, 21, 21, 19, -1, -1, 12, 12, 0, 12, -1, -1, -1, -1, 19, 0, -1, 20, 20, 28, 14, 13, -1, -1, -1, -1, 13, 15, 21, 13, 15, 0, 13, 29, -1, 13, 0, 19, -1, 0, 10, 10, 22, 19, 19, 19, -1, 19, -1, 21, 34, -1, -1, 10, 10, 10, 10, 10, 10, 27, -1, 10, 16, -1, 16, -1, 10, 1, 0, 0, 21, -1, 1, 14, 0, 21, 0, -1, 30, 33, -1, -1, -1, -1, -1, 14, 15, 30, 1, 1, 1, 15, 1, 3, 1, -1, 1, 1, 1, 1, 1, 1, -1, 30, -1, 3, -1, 3, 1, -1, 32, 3, 3, 3, 30, 4, 3, -1, 3, 3, 17, 3, 27, 4, 16, 3, 3, 3, 3, 1, 1, 6, -1, 1, -1, 1, 3, -1, -1, 1, -1, 33, 1, 19, -1, -1, 33, 6, 4, 6, -1, 14, -1, 21, 33, -1, 16, -1, 5, 5, 5, 0, 28, 1, -1, 16, 16, -1, 5, -1, 5, 28, 5, 5, 5, 5, 5, 14, 20, 5, 3, 5, 20, 20, 5, 18, 20, 5, -1, 14, 5, 5, 20, 0, 1, 1, -1, 20, 6, -1, 32, 6, 0, -1, 3, 1, 6, 15, 6, 6, 4, 11, 6, 0, 0, 0, -1, 0, 6, 6, 4, 5, 30, 0, 0, 0, 0, 0, 0, -1, 0, 20, -1, 18, 18, 18, 20, 20, 0, 0, 0, 9, -1, 0, 20, 0, 20, 29, 20, 0, 0, 21, -1, 21, 19, 0, 9, 0, 0, -1, 0, 0, 14, 0, 0, -1, 10, 0, 21, 21, 32, -1, 7, 7, 7, 7, 7, 7, 0, 7, 7, 1, 1, -1, -1, 8, 12, -1, 14, -1, 7, -1, 7, 7, -1, 1, 7, 7, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 3, -1, -1, 5, 5, -1, 5, 0, -1, 5, -1, 5, 22, 5, 28, 5, -1, 5, 9, 1, 9, -1, 19, -1, 5, 5, 5, -1, 5, 5, 5, 0, -1, 5, 28, -1, 6, 6, 6, 6, -1, 6, 6, -1, 1, 6, -1, 13, -1, 6, -1, 31, 1, 13, -1, 0, 0, 14, 15, -1, -1, 12, 0, -1, 31, 0, -1, 4, -1, 0, 15, -1, -1, 0, 0, -1, -1, -1, 0, 0, 35, 0, 0, 9, -1, 0, 33, 18, 0, 35, 18, 36, 0, 8, 29, 0, 18, 19, 1, 18, 18, 18, 4, -1, 9, 15, 0, 2, 2, 2, 29, 9, 15, 1, 0, 8, -1, 8, 36, 10, 1, 1, 0, -1, 33, -1, 33, 1, -1, 19, 3, 3, 7, 7, 1, 7, 7, 1, 7, 7, 7, 7, 1, -1, 2, 2, 2, 2, 2, 4, 0, 2, 7, 2, 2, 2, 2, -1, 2, 2, 7, 14, -1, 1, 2, -1, 14, 2, 2, 2, 1, -1, -1, 5, 5, -1, 5, 5, 5, 5, -1, 27, 27, -1, 27, 27, 27, 4, 27, 21, 27, 21, -1, 3, 9, 4, 11, 10, 9, -1, 10, 8, 8, 8, 8, 8, 35, 8, -1, 10, -1, 10, 10, 10, 10, 15, 9, 31, 4, 9, 8, -1, 29, 32, -1, -1, -1, 31, 31, 14, -1, 31, 31, 4, 31, -1, 31, -1, 12, 12, 12, -1, 12, -1, 12, 25, -1, 5, -1, 15, 12, 3, -1, 1, -1, 2, 4, 25, 36, 11, -1, 36, 36, 36, 10, 1, 0, 36, 4, 11, 3, 15, -1, -1, 19, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, -1, 2, -1, 2, 2, 2, 16, 2, -1, 2, 5, -1, 9, 4, 20, 20, 6, -1, 25, 4, -1, 16, -1, 34, 22, 12, 32, 27, 0, 27, 35, 27, 0, 9, 4, -1, 17, -1, 17, 17, 9, 0, 4, 11, 34, -1, 34, -1, 32, -1, 29, 34, 4, 34, 9, 12, 34, -1, 34, -1, 15, -1, 9, 1, 10, -1, 0, 24, 24, -1, 11, -1, -1, 19, 13, -1, 10, 35, -1, 8, 26, 27, -1, 26, 10, 8, 8, 10, -1, -1, 8, 8, 8, -1, 8, 2, 8, 24, -1, 24, 24, 22, -1, 31, 31, 17, 35, 31, -1, -1, 0, 26, 28, -1, -1, 26, 0, 26, 26, 14, 3, 21, 35, 30, 30, 1, -1, -1, -1, 3, 3, 29, 13, 1, 1, -1, -1, 15, 23, 23, 1, 21, 5, 17, 28, 13, 2, 6, -1, 23, 4, 23, 15, -1, -1, 14, -1, 8, -1, -1, 32, 6, 6, 11, 1, 4, 28, 6, 20, 6, 14, 20, 20, -1, 18, 3, 2, -1, 17, 34, 9, 9, 6, 34, 9, 32, 3, 17, -1, 17, 17, 9, 14, 1, -1, -1, 1, 16, 13, -1, 1, 17, -1, -1, -1, -1, 12, 19, 3, 24, 24, 15, 24, -1, 24, 18, 17, 0, -1, -1, 29, -1, -1, 7, 19, 20, 26, 26, 26, 29, 0, 26, 7, 26, 26, 16, -1, 26, 26, 23, 23, 16, 13, 17, 17, 17, 17, 33, 25, 17, 5, 33, -1, 0, 0, 35, 4, 10, 0, 1, 0, 18, 10, 30, 30, 24, 22, -1, -1, 0, 24, 4, -1, -1, 24, -1, -1, 24, 28, -1, 15, 4, 2, 1, -1, 30, 0, 4, 6, 6, 1, 6, -1, 32, 6, 2, 9, 9, -1, 32, 1, 9, 6, 14, 4, 1, 0, 0, 2, 1, 1, -1, 0, 1, 0, 22, 1, 18, 2, -1, 32, 11, 3, -1, 12, 8, 33, 23, 7, -1, -1, 25, 4, -1, 22, 7, 23, -1, 11, 31, 33, 0, -1, 33, 0, 35, 8, 8, 0, 11, 11, 4, 8, 9, 9, 8, 8, -1, 4, 22, 3, 10, 8, 0, 0, 0, 2, 0, -1, 23, -1, -1, 15, 18, -1, -1, 0, 25, 4, 1, -1, 6, 0, -1, 0, 7, -1, 0, 3, 3, 3, 2, 3, 2, 2, 24, 6, 3, 2, 2, 4, 25, 6, 7, 24, 4, -1, 21, 2, 16, 30, 25, 1, 11, 30, 1, 4, 0, 0, 22, 16, 1, 18, 18, -1, 20, -1, 15, 1, 29, -1, 1, -1, 35, 0, -1, -1, 1, 1, -1, 2, -1, 22, -1, 2, 2, 1, -1, 12, 17, 18, 7, 36, -1, 0, 11, 18, 0, 18, 22, 29, 13, 8, 4, 22, 0, 0, 8, -1, -1, 0, 0, 0, 10, -1, 14, 0, 3, 4, 34, 34, 3, 4, -1, 7, 7, 3, 3, 3, 5, 13, -1, -1, 0, 2, -1, 17, 2, 2, 10, 6, 32, 2, 27, 27, 2, 1, 1, 11, -1, 3, 11, 3, 4, 6, 8, -1, 26, 3, 0, 12, -1, -1, 12, -1, 15, 3, 17, 17, 0, -1, 0, 2, 23, 16, 0, 1, -1, 10, 4, 16, 24, 15, 32, 10, 30, 1, 7, -1, -1, -1, -1, 1, -1, 7, 7, 30, 1, 27, 24, 18, -1, -1, 14, 22, 18, 0, 0, 0, 18, 1, 21, 2, 23, 2, -1, 34, 0, 15, -1, 0, 4, 0, 2, 0, 0, 0, -1, 0, 18, 30, 8, 1, -1, 2, 2, 0, 5, -1, 8, 8, 4, 4, 2, 2, 0, 0, 24, 7, 27, 24, 24, -1, 0, 29]\n",
            "-------RUN87-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': \"CountVectorizer(stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b02d2c50>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[28, 5, 32, 32, -1, -1, -1, 17, -1, -1, -1, -1, -1, -1, 13, 20, 10, 28, -1, 20, -1, 14, -1, 9, 9, 19, 4, 20, 20, -1, -1, -1, 32, 35, -1, -1, -1, 37, 0, -1, 9, 20, 26, 9, 26, 4, 26, 26, 9, -1, 26, 9, -1, -1, -1, -1, -1, 5, 17, 17, -1, 17, 17, 21, 20, 20, -1, -1, 14, -1, 30, 14, 1, 1, 18, 3, 10, -1, -1, 5, 23, 5, 34, 23, 5, 19, 23, 1, 5, 35, 1, 21, 6, -1, 21, 8, 19, 1, -1, -1, -1, 18, -1, -1, -1, 30, -1, 0, 0, 0, -1, 14, 4, -1, 5, -1, 6, 21, -1, 33, 26, 6, 9, 18, 26, 8, 14, 5, 9, 14, -1, 13, 20, 14, 14, 28, 9, 10, 14, 26, -1, -1, -1, 8, 13, 17, 28, -1, 5, 0, -1, -1, -1, 21, -1, 17, 1, 29, -1, 2, -1, 2, 30, 2, 20, -1, -1, 18, -1, -1, -1, 21, -1, 0, 13, 17, -1, -1, 13, -1, 37, 10, 13, 0, 14, -1, -1, 5, 14, 14, 13, 13, 23, 5, -1, 6, 15, 15, -1, -1, 23, 5, 23, 10, 21, -1, 6, 23, 1, -1, 1, -1, 5, 5, 23, 23, 23, 1, 5, -1, 15, -1, 1, 1, 21, 15, 4, 4, 5, 0, 0, 0, 0, 15, 15, 0, 19, 0, -1, 15, -1, 3, 3, -1, 15, -1, -1, -1, 25, 20, -1, 15, 11, -1, -1, 4, 4, -1, 1, 1, 4, 18, 25, -1, -1, -1, 30, 4, -1, 3, 3, 3, -1, 1, 3, -1, 3, 3, 19, -1, 1, 37, 3, 4, 4, -1, 15, 36, 24, 36, 24, 5, 24, 24, 6, -1, -1, -1, 15, 3, 3, 3, 14, 3, -1, 3, -1, 3, -1, -1, 9, 3, 26, 37, -1, -1, -1, -1, -1, -1, 17, 8, -1, 3, 0, 28, -1, 28, 33, 28, -1, -1, 12, 28, 31, -1, 17, 13, 1, 13, 13, -1, -1, -1, -1, -1, 13, 13, 0, 13, -1, 9, -1, 5, 21, 0, -1, 12, 12, 28, -1, 14, -1, -1, 12, -1, 14, 10, 19, -1, -1, 0, 14, 30, -1, 14, 0, 21, 37, 0, 11, 11, 20, 21, 21, 21, 12, 21, 12, 19, 34, 19, -1, 11, 11, 11, 11, 11, 11, 27, -1, 11, 15, -1, 15, -1, 11, 1, 0, 0, 19, 12, 1, 18, 0, 19, 0, -1, 31, -1, -1, -1, -1, -1, -1, -1, 10, 31, 1, 1, 1, 10, 1, 3, 1, -1, 1, 1, 1, 1, 1, 1, -1, 31, -1, 3, -1, 3, 1, -1, 35, 3, 3, 3, 31, 5, 3, 33, 3, 3, 25, 3, 27, 5, 15, 3, 3, 3, 3, 1, 1, 4, -1, 1, -1, 1, 3, 4, -1, 1, -1, 36, 1, 21, 24, -1, 36, 4, 5, 4, 24, 18, -1, 19, 36, 24, 15, -1, 7, 7, 7, 0, 28, 1, -1, 15, 15, -1, 7, 10, 7, 28, 7, 7, 7, 7, 7, 18, 12, 7, 3, 7, 12, 12, 7, 16, 12, 7, -1, -1, 7, -1, 12, 0, 1, 1, -1, 12, 4, 19, 35, 4, 0, -1, 3, 1, 4, -1, 4, 4, 5, 26, 4, 0, 0, 0, -1, 0, 4, 4, 5, 24, 31, 0, 0, 0, 0, 0, 0, -1, 0, 12, 6, 16, 16, 16, 12, 12, 0, 0, 0, 9, -1, 0, 12, 0, 12, 30, 12, 0, 0, 19, 17, 19, 21, 0, 9, 0, 0, -1, 0, 0, 18, 0, 0, -1, 11, 0, 19, 19, 35, -1, 6, 6, 6, 6, 6, 6, 0, 6, 6, 1, 1, 24, -1, 8, 13, -1, 18, -1, 6, -1, 6, 6, 6, 1, 6, 6, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 33, 3, -1, -1, 7, 7, -1, 7, 0, -1, 7, -1, 7, 20, 7, 28, 7, -1, 7, 9, 1, 9, -1, 21, -1, 7, 7, 7, -1, 7, 7, 7, 0, -1, 7, 28, 33, 4, 4, 4, 4, -1, 4, 4, -1, 1, 4, -1, 14, 4, 4, -1, 32, 1, 14, 4, 0, 0, 18, 10, 33, -1, 13, 0, -1, 32, 0, -1, -1, 4, 0, 10, 9, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 9, 10, 0, -1, 16, 0, 33, 16, 24, 0, 8, 30, 0, 16, 21, 1, 16, 16, 16, 5, -1, 9, 10, 0, 2, 2, 2, 30, 9, 10, 1, 0, 8, -1, 8, 24, 11, 1, 1, 0, -1, -1, -1, -1, 1, -1, -1, 3, 3, 6, 6, 1, 6, 6, 1, 6, 6, 6, 6, 1, 37, 2, 2, 2, 2, 2, 33, 0, 2, 6, 2, 2, 2, 2, 10, 2, 2, 6, 18, -1, 1, 2, -1, 18, 2, 2, 2, 1, 18, -1, 7, 7, -1, 7, 7, 7, 7, -1, 27, 27, -1, 27, 27, 27, 5, 27, -1, 27, 19, 6, 3, 9, 5, 26, 11, 9, -1, 11, 8, 8, 8, 8, 8, 33, 8, 17, 11, 9, 11, 11, 11, 11, -1, 9, 32, -1, 9, 8, 8, 30, 35, -1, -1, 33, 32, 32, 18, -1, 32, 32, 5, 32, -1, 32, -1, 13, 13, 13, 24, 13, -1, 13, 23, -1, 7, -1, 10, 13, 3, 10, 1, -1, 2, 5, 23, 24, 26, -1, 24, 24, 24, 11, 1, 0, 24, -1, 26, 3, 10, -1, -1, -1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, -1, 2, -1, 2, 2, 2, 15, -1, -1, 2, 7, 18, 9, 5, 12, 12, 4, -1, 23, 5, -1, 15, 19, 34, 20, 13, 35, 27, 0, 27, 33, 27, 0, 9, 5, -1, 25, -1, 25, 25, 9, 0, 5, 26, 34, -1, 34, -1, 35, -1, 30, 34, 5, 34, 9, 13, 34, -1, 34, 12, 10, -1, 9, 1, 11, -1, 0, 22, 22, -1, 26, -1, -1, 21, 14, -1, 11, 33, -1, 8, 29, 27, -1, 29, 11, 8, 8, 11, -1, -1, 8, 8, 8, -1, 8, 2, 8, 22, 10, 22, 22, 20, -1, 32, 32, -1, -1, 32, 33, -1, 0, 29, 28, -1, -1, 29, 0, 29, 29, 18, 3, 19, -1, 31, 31, 1, -1, -1, -1, 3, 3, 30, 14, 1, 1, -1, -1, 10, 17, 17, 1, 19, 7, 25, 28, 14, 2, 4, -1, 17, -1, 17, 10, -1, -1, 18, 10, 8, -1, 4, 35, 4, 4, 26, 1, 5, 28, 4, 12, 4, 18, 12, 12, -1, 16, 3, 2, -1, 25, 34, 9, 9, -1, 34, 9, 35, 3, -1, -1, 25, 25, 9, 18, 1, 12, -1, 1, 15, 14, 27, 1, 37, -1, -1, 24, -1, 13, 21, 3, 22, 22, 10, 22, -1, 22, 16, 25, 0, -1, -1, 30, -1, -1, 6, 21, 12, 29, 29, 29, 30, 0, 29, 6, 29, 29, 15, -1, 29, 29, 17, 17, 15, 14, 25, 25, 25, 25, 36, 23, 25, 7, 36, 0, 0, 0, -1, 5, 11, 0, 1, 0, 16, 11, 31, 31, 22, 20, 10, 10, 0, 22, 5, -1, -1, 22, -1, -1, 22, 28, -1, 10, 5, 2, 1, 12, 31, 0, 5, 4, 4, 1, 4, -1, 35, 4, 2, 9, 9, -1, 35, 1, 9, 4, 18, 5, 1, 0, 0, 2, 1, 1, 0, 0, 1, 0, 20, 1, 16, -1, -1, 35, 4, 3, -1, 13, 8, 36, 17, 6, -1, -1, 23, 5, -1, 20, 6, 17, -1, 4, 32, 36, 0, 10, 36, 0, 33, 8, 8, 0, 4, 26, 5, 8, 9, 9, 8, 8, 37, 5, 20, 3, 11, 8, 0, 0, 0, 2, 0, -1, 17, -1, -1, 10, 16, -1, 23, 0, 23, 5, 1, -1, 4, 0, -1, 0, 6, -1, 0, 3, 3, 3, 2, 3, 2, 2, 22, 4, 3, 2, 2, 5, 23, 4, 6, 22, 5, -1, 19, 2, 15, 31, 23, 1, -1, 31, 1, 5, 0, 0, 20, 15, 1, 16, 16, -1, 12, -1, 10, 1, 30, -1, 1, -1, 33, 0, -1, -1, 1, 1, 6, 2, -1, -1, -1, 2, 2, 1, -1, 13, -1, 16, 6, 24, -1, 0, 26, 16, 0, 16, 20, 30, 14, 8, 5, 20, 0, 0, 8, -1, -1, 0, 0, 0, 11, 37, 18, 0, 3, 5, 34, 34, -1, 5, -1, 6, 6, 3, 3, 3, 7, 14, -1, 5, 0, 2, -1, 37, 2, 2, 11, 4, -1, 2, 27, 27, 2, 1, 1, 4, -1, 3, 4, 3, -1, 4, 8, -1, 29, 3, 0, 13, -1, -1, 13, -1, 10, 3, 25, 25, 0, -1, 0, 2, 17, 15, 0, 1, -1, 11, 5, 15, 22, 10, -1, 11, 31, 1, 6, -1, -1, -1, -1, 1, -1, 6, 6, 31, 1, 27, 22, 16, -1, -1, 18, 20, 16, 0, 0, 0, 16, 1, 19, 2, 17, 2, -1, 34, 0, 10, -1, 0, 5, 0, 2, 0, 0, 0, -1, 0, 16, 31, 8, 1, -1, 2, 2, 0, 7, -1, 8, 8, 5, 5, 2, 2, 0, 0, 22, 6, 27, 22, 22, -1, 0, 30]\n",
            "-------RUN88-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': \"CountVectorizer(ngram_range=(1, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b891dd10>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[27, 4, 33, 33, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 12, 20, 26, 27, 8, 20, -1, 14, -1, 8, 8, 19, 5, -1, 20, -1, -1, -1, 33, 31, -1, -1, 1, 11, 0, -1, 8, 20, 30, 8, 30, 5, 30, 30, 8, -1, 30, 8, -1, -1, -1, 6, -1, 4, 23, 23, 1, 23, 23, -1, 20, 20, -1, -1, 14, -1, 34, 14, 1, 1, 18, 3, 26, 0, -1, 4, 17, -1, -1, 17, 4, 19, 17, 1, 4, 31, 1, 21, 17, -1, 21, 9, 19, 1, -1, -1, -1, 18, -1, -1, -1, 34, -1, 0, 0, 0, 3, 14, -1, -1, 4, -1, 6, 21, -1, -1, 30, 6, 8, 18, 30, 9, 14, 4, 8, 14, 4, 12, 20, 14, 14, 27, 8, 26, 14, 30, -1, -1, 29, 9, 12, 23, 27, -1, 4, 0, -1, -1, -1, 21, -1, 23, 1, 29, -1, 2, -1, 2, 34, 2, 20, -1, -1, 18, -1, -1, -1, 21, -1, 0, 12, 23, -1, -1, 12, 14, 11, 26, 12, 0, 14, -1, -1, 4, 14, 14, 12, 12, 17, 4, -1, 6, 15, 15, -1, 6, 17, 4, 17, -1, -1, -1, 6, 17, 1, -1, 1, 17, 4, 4, 17, 17, 17, 1, 4, -1, 15, -1, 1, 1, 21, 15, 5, 5, 4, 0, 0, 0, 0, 15, 15, 0, 19, 0, -1, 15, 24, 3, 3, -1, 15, -1, -1, -1, 11, -1, -1, 15, 10, 27, -1, 5, 5, -1, 1, 1, 5, 18, 11, -1, -1, -1, 34, 5, -1, 3, 3, 3, 1, 1, 3, -1, 3, 3, 19, -1, 1, 11, 3, 5, 5, 3, 15, 36, 25, 36, 25, 4, 25, 25, 6, -1, -1, -1, 15, 3, 3, 3, 14, 3, 1, 3, 1, 3, -1, -1, 8, 3, 30, 11, -1, -1, -1, -1, 18, -1, 23, 9, -1, 3, 0, 27, -1, 27, 28, 27, -1, -1, 13, 27, 32, -1, 23, 12, 1, 12, 12, -1, 19, 21, -1, -1, 12, 12, 0, 12, -1, 8, -1, -1, 21, 0, -1, 13, 13, 27, -1, 14, -1, -1, 13, -1, 14, 26, 19, 14, -1, 0, 14, 34, -1, 14, 0, 21, 11, 0, 10, 10, 20, 21, 21, 21, 13, 21, 13, 19, 35, -1, -1, 10, 10, 10, 10, 10, 10, 24, -1, 10, 15, -1, 15, -1, 10, 1, 0, 0, 19, 13, 1, 18, 0, 19, 0, -1, 32, 36, -1, -1, -1, 28, 24, -1, 26, 32, 1, 1, 1, 26, 1, 3, 1, -1, 1, 1, 1, 1, 1, 1, -1, 32, 27, 3, -1, 3, 1, -1, 31, 3, 3, 3, 32, 4, 3, -1, 3, 3, 11, 3, 24, 4, 15, 3, 3, 3, 3, 1, 1, 5, -1, 1, -1, 1, 3, -1, 1, 1, -1, 36, 1, 21, 25, -1, 36, 5, 4, 5, 25, 18, -1, 19, 36, 25, 15, -1, 7, 7, 7, 0, 27, 1, -1, 15, 15, -1, 7, -1, 7, 27, 7, 7, 7, 7, 7, 18, 13, 7, 3, 7, 13, 13, 7, 16, 13, 7, -1, -1, 7, 7, 13, 0, 1, 1, 4, 13, 5, -1, 31, 5, 0, -1, 3, 1, 5, 26, 5, 5, 4, 30, 5, 0, 0, 0, -1, 0, 5, 5, 4, 25, 32, 0, 0, 0, 0, 0, 0, -1, 0, 13, 6, 16, 16, 16, 13, 13, 0, 0, 0, 8, -1, 0, 13, 0, 13, 34, 13, 0, 0, 19, -1, 19, 21, 0, 8, 0, 0, -1, 0, 0, 18, 0, 0, -1, 10, 0, 19, 19, 31, -1, 6, 6, 6, 6, 6, 6, 0, 6, 6, 1, 1, 25, -1, -1, 12, -1, -1, -1, 6, -1, 6, 6, -1, 1, 6, 6, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 3, -1, -1, 7, 7, -1, 7, 0, -1, 7, -1, 7, 20, 7, 27, 7, -1, 7, 8, 1, 8, 28, 21, -1, 7, 7, 7, -1, 7, 7, 7, 0, -1, 7, 27, -1, 5, 5, 5, 5, -1, 5, 5, -1, 1, 5, -1, 14, -1, 5, -1, 33, 1, 14, 5, 0, 0, 18, -1, 28, 7, 12, 0, -1, 33, 0, -1, 4, -1, 0, 26, 8, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 8, -1, 0, -1, 16, 0, 28, 16, 25, 0, 9, 31, 0, 16, 21, 1, 16, 16, 16, 4, -1, 8, 26, 0, 2, 2, 2, 34, 8, 26, 1, 0, 9, -1, 9, 25, 10, 1, 1, 0, -1, -1, -1, -1, 1, -1, 21, 3, 3, 6, 6, 1, 6, 6, 1, 6, 6, 6, 6, 1, 11, 2, 2, 2, 2, 2, 28, 0, 2, 6, 2, 2, 2, 2, -1, 2, 2, 6, 18, -1, 1, 2, -1, 18, 2, 2, 2, 1, -1, -1, 7, 7, -1, 7, 7, 7, 7, -1, 24, 24, -1, 24, 24, 24, 4, 24, -1, 24, 19, -1, 3, 8, 4, 5, 10, 8, -1, 10, 9, 9, 9, 9, 9, 28, 9, -1, 10, 8, 10, 10, 10, 10, -1, 8, 33, -1, 8, 9, 34, 34, 31, -1, -1, 28, 33, 33, 18, -1, 33, 33, 4, 33, -1, 33, -1, 12, 12, 12, 25, 12, -1, 12, 17, -1, 7, -1, 26, 12, -1, -1, 1, -1, 2, 4, 17, 25, 30, 1, 25, 25, 25, 10, 1, 0, 25, -1, 30, -1, 26, -1, -1, -1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, -1, 2, -1, 2, 2, 2, 15, -1, -1, 2, 7, -1, 8, 4, 13, 13, 5, -1, 17, 4, -1, 15, -1, 35, 20, 12, 31, 24, 0, 24, 28, 24, 0, 8, 4, -1, 11, -1, 11, 11, 8, 0, 4, 30, 35, -1, 35, -1, 31, -1, 34, 35, 4, 35, 8, 12, 35, -1, 35, 13, 26, -1, 8, 1, 10, -1, 0, 22, 22, -1, -1, -1, 19, 21, 14, -1, 10, 28, -1, 9, 29, 24, -1, 29, 10, 9, 9, 10, -1, -1, 9, 9, 9, -1, 9, 2, 9, 22, -1, 22, 22, 20, -1, 33, 33, -1, -1, 33, 28, 28, 0, 29, 27, -1, -1, 29, 0, 29, 29, 18, 3, 19, 28, 32, 32, 1, -1, -1, -1, 3, 3, 34, 14, 1, 1, -1, -1, -1, 23, 23, 1, 19, 7, 11, 27, 14, 2, 5, -1, 23, 4, 23, -1, -1, -1, 18, -1, -1, -1, 5, 31, 5, 5, 30, 1, 4, 27, 5, 13, 5, 18, 13, 13, -1, 16, 3, 2, -1, 11, 35, 8, 8, 5, 35, 8, 31, 3, 11, 17, 11, 11, 8, 18, 1, 13, -1, 1, 15, 14, -1, 1, 11, -1, -1, 25, -1, 12, 21, 3, 22, 22, 26, 22, -1, 22, 16, 11, 0, -1, -1, 34, -1, -1, 6, 21, 13, 29, 29, 29, 34, 0, 29, 6, 29, 29, 15, -1, 29, 29, 23, 23, 15, 14, 11, 11, 11, 11, 36, 17, 11, 7, 36, 0, 0, 0, 28, 4, 10, 0, 1, 0, 16, 10, 32, 32, 22, 20, -1, -1, 0, 22, 4, -1, -1, 22, -1, -1, 22, 27, -1, 26, 4, 2, 1, 13, 32, 0, 4, 5, 5, 1, 5, 5, 31, 5, 2, 8, 8, -1, 31, 1, 8, 5, 18, 4, 1, 0, 0, 2, 1, 1, 0, 0, 1, 0, 20, 1, 16, 2, -1, 31, 5, 3, 6, 12, 9, 36, 23, 6, -1, -1, 17, 4, 11, 20, 6, 23, 6, 5, 33, 36, 0, -1, 36, 0, 28, 9, 9, 0, 5, 30, 4, 9, 8, 8, 9, 9, 11, 4, 20, 3, 10, 9, 0, 0, 0, 2, 0, -1, 23, -1, -1, -1, 16, -1, 17, 0, 17, 4, 1, -1, 5, 0, -1, 0, 6, -1, 0, 3, 3, 3, 2, 3, 2, 2, 22, 5, 3, 2, 2, 4, 17, 5, 6, 22, 4, -1, 19, 2, 15, 32, 17, 1, 5, 32, 1, 4, 0, 0, 20, 15, 1, 16, 16, -1, 13, -1, 26, 1, 34, -1, 1, 18, 28, 0, -1, -1, 1, 1, 6, 2, -1, 20, -1, 2, 2, 1, -1, 12, -1, 16, 6, 25, -1, 0, 30, 16, 0, 16, 20, 34, 14, 9, 4, 20, 0, 0, 9, -1, -1, 0, 0, 0, 10, 11, -1, 0, 3, 4, 35, 35, 3, 4, 28, 6, 6, 3, 3, 3, 7, 14, -1, 4, 0, 2, -1, 11, 2, 2, 10, 5, 31, 2, 24, 24, 2, 1, 1, 5, -1, 3, 5, 3, 4, 5, 9, 35, 29, 3, 0, 12, -1, -1, 12, -1, -1, 3, 11, 11, 0, -1, 0, 2, 23, 15, 0, 1, -1, 10, 4, 15, 22, 26, 31, 10, 32, 1, 6, -1, -1, -1, -1, 1, 18, 6, 6, 32, 1, 24, 22, 16, -1, -1, 18, 20, 16, 0, 0, 0, 16, 1, 19, 2, 23, 2, -1, 35, 0, 26, -1, 0, 4, 0, 2, 0, 0, 0, -1, 0, 16, 32, 9, 1, 28, 2, 2, 0, 7, -1, 9, 9, 4, 4, 2, 2, 0, 0, 22, 6, 24, 22, 22, -1, 0, -1]\n",
            "-------RUN89-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': \"CountVectorizer(ngram_range=(2, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df4a9008410>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[29, 4, 34, 34, -1, -1, -1, 22, -1, -1, -1, -1, -1, -1, 12, 21, 27, 29, 8, 21, -1, 15, -1, 8, 8, 19, 17, -1, 21, -1, -1, -1, 34, 36, -1, -1, -1, 10, -1, -1, 8, 21, 17, 8, 17, 7, 17, 17, 8, -1, 17, 8, -1, -1, -1, -1, -1, 4, 22, 22, -1, 22, 22, 18, 21, 21, -1, -1, -1, -1, 32, 15, 0, 0, 23, 3, 27, -1, -1, 4, 24, -1, 35, 24, 4, 19, 24, 0, 4, 36, 0, 18, -1, -1, 18, 13, 19, 0, -1, -1, -1, 23, -1, -1, -1, 32, -1, -1, -1, -1, -1, 15, -1, -1, 4, -1, 5, 18, -1, 30, -1, 5, 8, 23, 17, -1, 15, 4, 8, 15, 4, 12, 21, 15, 15, 29, 8, 27, 15, 17, -1, -1, 28, -1, 12, 22, 29, -1, 4, -1, -1, -1, -1, 18, -1, 22, 0, 28, -1, 1, -1, 1, 32, 1, 21, -1, -1, 23, 19, -1, 18, 18, -1, -1, 12, 22, -1, -1, 12, -1, 10, 27, 12, -1, 15, 22, -1, 4, 15, 15, 12, 12, 24, 4, -1, 5, 16, 16, 24, -1, 24, 4, 24, -1, 18, -1, 5, 24, 0, -1, 0, 24, 4, 4, 24, 24, 24, 0, 4, -1, 16, -1, 0, 0, 18, 16, 17, 17, -1, 2, -1, -1, 2, 16, 16, -1, 19, -1, -1, 16, -1, 3, 3, -1, 16, -1, -1, -1, 10, 21, -1, 16, 9, 28, -1, 7, 7, -1, 0, 0, 17, 23, 10, -1, -1, 4, 32, 17, -1, 3, 3, 3, -1, 0, 3, -1, 3, 3, 19, -1, 0, 10, 3, 7, 7, 3, 16, 1, 26, 1, 26, 4, 26, 26, 5, -1, -1, -1, 16, 3, 3, 3, 15, 3, -1, 3, -1, 3, 26, -1, 8, 3, 17, 10, -1, -1, -1, -1, 23, -1, 22, -1, -1, 3, 2, 29, -1, 29, 30, 29, -1, -1, 11, 29, 33, -1, 22, 12, 0, 12, 12, 19, 19, 18, -1, -1, 12, 12, -1, 12, -1, 8, 21, 4, 18, -1, 1, 11, 11, 29, -1, 15, -1, -1, 11, -1, 15, 27, 19, 15, -1, -1, 15, 32, 18, 15, -1, 18, 10, -1, 9, 9, 21, 18, 18, 18, 11, 18, 11, 19, 35, -1, -1, 9, 9, 9, 9, 9, 9, 31, -1, 9, 16, -1, 16, -1, 9, 0, 2, -1, 19, 11, 0, 23, -1, 19, -1, -1, 33, 1, -1, -1, -1, 30, -1, 23, 27, 33, 0, 0, 0, 27, 0, 3, 0, -1, 0, 0, 0, 0, 0, 0, -1, 33, 29, 3, -1, 3, 0, -1, 36, 3, 3, 3, 33, 4, 3, 30, 3, 3, 10, 3, 31, 4, 16, 3, 3, 3, 3, 0, 0, 7, -1, 0, -1, 0, 3, -1, -1, 0, -1, 1, 0, 18, -1, -1, 1, 7, 4, 7, -1, 23, -1, 19, 1, -1, 16, -1, 6, 6, 6, -1, 29, 0, -1, 16, 16, -1, 6, -1, 6, 29, 6, 6, 6, 6, 6, 23, 11, 6, 3, 6, 11, 11, 6, 20, 11, 6, -1, -1, 6, 6, 11, -1, 0, 0, -1, 11, 7, -1, 36, 7, 14, -1, 3, 0, 7, -1, 7, 7, 4, 17, 7, 14, 14, 14, -1, 14, 7, 7, 4, 26, 33, 2, 2, 2, -1, 2, -1, -1, 14, 11, 5, 20, 20, 20, 11, 11, 14, 14, 14, 8, -1, 14, 11, 14, 11, 32, 11, -1, -1, 19, 22, 19, 18, 2, 8, 2, 2, -1, -1, 2, 23, 2, 2, -1, 9, 2, 19, 19, 36, -1, 5, 5, 5, 5, 5, 5, 14, 5, 5, 0, 0, 26, -1, -1, 12, -1, 23, 11, 5, -1, 5, 5, -1, 0, 5, 5, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 3, -1, -1, 6, 6, -1, 6, 2, -1, 6, -1, 6, 21, 6, 29, 6, -1, 6, 8, 0, 8, -1, 18, -1, 6, 6, 6, -1, 6, 6, 6, 2, -1, 6, 29, -1, 7, 7, 7, 7, -1, 7, 7, -1, 0, 7, -1, 15, -1, 7, 4, 34, 0, 15, -1, 14, 14, 23, -1, 30, 6, 12, -1, -1, 34, -1, -1, 4, -1, 14, 27, 8, -1, 2, 2, 26, -1, -1, 2, 2, -1, 2, 2, 8, -1, 2, -1, 20, 2, 30, 20, 26, 2, -1, 32, 2, 20, 18, 0, 20, 20, 20, 4, -1, 8, 27, -1, 1, 1, 1, 32, 8, 27, 0, 2, 13, -1, -1, 26, 9, 0, 0, -1, -1, 1, -1, 1, 0, -1, -1, 3, 3, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 0, 10, 1, 1, 1, 1, 1, 30, 2, 1, 5, 1, 1, 1, 1, -1, 1, 1, 5, 23, -1, 0, 1, -1, 23, 1, 1, 1, 0, -1, -1, 6, 6, -1, 6, 6, 6, 6, -1, 31, 31, -1, 31, 31, 31, 4, 31, 19, 31, 19, 5, 3, 8, 4, 17, 9, 8, -1, 9, 13, 13, 13, 13, 13, 30, 13, -1, 9, 8, 9, 9, 9, 9, 27, 8, 34, -1, 8, 13, -1, 32, 36, -1, -1, 30, 34, 34, 23, -1, 34, 34, 4, 34, -1, 34, -1, 12, 12, 12, 26, 12, -1, 12, 24, -1, 6, -1, 27, 12, -1, -1, 0, 26, 1, 4, 24, 26, 17, -1, 26, 26, 26, 9, 0, -1, 26, -1, -1, -1, 27, -1, -1, 18, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, -1, 1, -1, 1, 1, 1, 16, 1, -1, 1, 6, -1, 8, 4, 11, 11, 7, -1, 24, 4, -1, 16, -1, 35, 21, 12, 36, 31, 14, 31, 30, 31, 14, 8, 4, -1, 10, -1, 10, 10, 8, 14, 4, 17, 35, 35, 35, -1, 36, -1, 32, 35, 4, 35, 8, 12, 35, 19, 35, 11, 27, -1, 8, 0, 9, -1, 2, 25, 25, -1, 17, -1, -1, 18, 15, -1, 9, -1, -1, 13, 28, 31, -1, 28, 9, 13, 13, 9, -1, -1, 13, 13, 13, -1, 13, 1, 13, 25, -1, 25, 25, 21, -1, 34, 34, -1, -1, 34, 30, 30, -1, 28, 29, -1, -1, 28, -1, 28, 28, 23, 3, 19, 30, 33, 33, 0, -1, -1, -1, 3, -1, 32, 15, 0, 0, -1, -1, -1, 22, 22, 0, 19, 6, 10, 29, 15, 1, 7, -1, 22, 4, 22, -1, -1, -1, 23, -1, -1, -1, -1, 36, 7, 7, 17, 0, 4, 29, 7, 11, 7, 23, 11, 11, -1, 20, 3, 1, -1, 10, 35, 8, 8, 7, 35, 8, 36, 3, 10, -1, 10, 10, 8, 23, 0, 11, -1, 0, 16, 15, -1, 0, 10, -1, -1, -1, -1, 12, 18, 3, 25, 25, 27, 25, -1, 25, 20, 10, 2, 1, -1, 32, -1, -1, 5, 18, 11, 28, 28, 28, 32, 2, 28, 5, 28, 28, 16, -1, 28, 28, 22, 22, 16, 15, 10, 10, 10, 10, 1, 24, 10, 6, 1, -1, -1, 2, 30, 4, 9, 2, 0, 2, 20, 9, 33, 33, 25, 21, -1, -1, -1, 25, 4, -1, -1, 25, 0, -1, 25, 29, -1, 27, 4, 1, 0, 11, 33, -1, 4, 7, 7, 0, 7, -1, 36, 7, 1, 8, 8, -1, 36, 0, 8, 7, 23, 4, 0, 14, 14, 1, 0, 0, -1, 14, 0, 14, 21, 0, 20, 1, -1, 36, 17, 3, 5, 12, 13, 1, 22, 5, -1, -1, 24, 4, 10, 21, 5, 22, -1, -1, 34, 1, 2, -1, 1, 14, 30, 13, 13, 2, 17, 17, 4, 13, 8, 8, 13, 13, 10, 4, 21, 3, 9, 13, 2, 2, 2, 1, -1, -1, 22, -1, -1, -1, 20, -1, 24, 2, 24, 4, 0, -1, 7, 2, -1, 2, 5, 21, 2, 3, 3, 3, 1, 3, 1, 1, 25, 7, 3, 1, 1, 4, 24, 7, 5, 25, 4, -1, 19, 1, 16, 33, 24, 0, -1, 33, 0, 4, 14, 14, 21, 16, 0, 20, 20, -1, 11, -1, 27, 0, 32, -1, 0, -1, 30, 14, -1, -1, 0, 0, 5, 1, -1, 21, -1, 1, 1, 0, -1, 12, 10, 20, 5, 26, -1, 2, 17, 20, 2, 20, 21, 32, 15, 13, 4, 21, 2, 2, 13, -1, -1, 2, 2, 2, 9, 10, -1, 2, 3, 4, 35, 35, -1, 4, -1, 5, 5, 3, 3, 3, 6, 15, -1, 30, 2, 1, -1, 10, 1, 1, 9, 7, 36, 1, 31, 31, 1, 0, 0, 17, -1, 3, 17, 3, 4, 7, 13, -1, 28, 3, 14, 12, -1, -1, 12, -1, -1, 3, 10, 10, 14, 26, 2, 1, 22, 16, 14, 0, -1, 9, 4, 16, 25, 27, 36, 9, 33, 0, 5, -1, -1, -1, -1, 0, -1, 5, 5, 33, 0, 31, 25, 20, -1, -1, 23, 21, 20, 2, 2, 2, 20, 0, 19, 1, 22, 1, -1, 35, 2, 27, -1, 2, 4, -1, 1, 2, 2, 2, -1, 2, 20, 33, -1, 0, -1, 1, 1, 2, 6, -1, 13, 13, 4, 4, 1, 1, 2, -1, 25, 5, 31, 25, 25, -1, -1, 32]\n",
            "-------RUN90-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': \"CountVectorizer(stop_words='english')\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[29, 1, 33, 33, -1, 54, -1, -1, 38, -1, -1, -1, -1, -1, 12, 10, 24, 29, 48, 10, -1, 15, -1, 13, 13, 21, 37, 10, 10, 41, -1, -1, 33, 34, 41, -1, 0, 46, 49, 41, 13, 10, 30, 13, 30, 7, 30, 30, 13, 1, 30, 13, -1, -1, -1, 19, -1, 1, 20, 20, 0, 20, 20, -1, 10, 10, -1, -1, 15, 2, 31, 15, 0, 0, 18, 2, 24, -1, 41, 1, 19, 1, -1, 19, 1, 21, 19, 0, 1, 34, 0, 22, 19, 45, 22, 8, 21, 0, -1, -1, 41, 18, -1, 56, -1, 31, -1, -1, 49, 49, -1, 15, 37, 25, 1, -1, 5, 22, -1, 11, 30, 5, 13, 18, 30, 8, 15, 1, 13, 15, 1, 12, 10, 15, 15, 29, 48, 24, 15, 30, -1, -1, 25, 8, 12, 20, 29, 54, 1, -1, -1, -1, -1, 22, 57, 20, 0, 25, -1, 47, -1, -1, 31, 47, 10, -1, -1, 18, -1, 10, 22, 22, 11, -1, 12, 20, -1, 53, 12, -1, 46, 24, 12, -1, 15, -1, 54, 1, 15, 15, 12, 12, 19, 1, -1, 5, 16, 16, 19, -1, 19, 1, 19, 39, -1, -1, 5, 19, 0, -1, 0, -1, 1, 1, 19, 19, 19, 0, 1, -1, 16, -1, 0, 0, 22, 16, 37, 37, 1, 43, 43, -1, 43, 16, 16, 43, 21, 43, -1, 16, -1, 2, 2, -1, 16, -1, -1, -1, 26, 10, -1, 16, 9, 25, -1, 7, 7, 41, 0, 0, 37, 18, 26, 57, 10, 1, 31, 37, 45, 2, 2, 2, -1, 0, 2, 38, 2, 2, 21, -1, 0, 46, 2, 7, 7, 2, 16, 40, 28, 40, 28, 1, 28, 28, 5, -1, -1, 38, 16, 2, 2, 2, 15, 2, -1, 2, 0, 2, 51, -1, 13, 2, 30, 46, 53, 38, -1, 57, 18, -1, 20, 8, -1, 2, 4, 29, 10, 29, 11, 29, -1, -1, 42, 29, 32, -1, 20, 12, 0, 12, 12, 21, 21, 22, -1, -1, 12, 12, -1, 12, -1, -1, 10, 1, 22, -1, -1, 42, 42, 29, 11, 15, 54, -1, -1, -1, 15, 24, 21, 15, -1, 4, 15, 31, -1, 15, -1, 22, -1, -1, 9, 9, 10, 22, 22, 22, 50, 22, 50, 21, 35, -1, -1, 9, 9, 9, 9, 9, 9, 27, 45, 9, 16, -1, 16, 52, 9, 0, 43, 43, 21, 50, 0, 18, -1, 21, 43, -1, 32, 40, 56, -1, 45, -1, 27, 18, 24, 32, 0, 0, 0, 24, 0, 2, 0, 38, 0, 0, 0, 0, 0, 0, 38, 32, -1, 2, -1, 2, 0, -1, 34, 2, 2, 2, 32, 1, 2, -1, 2, 2, 26, 2, 27, 1, 16, 2, 2, 2, 2, 0, 0, 7, -1, 0, -1, 0, 2, -1, 0, 0, -1, 40, 0, 22, 28, -1, 40, 7, 1, 7, 28, 18, -1, 21, 40, 28, 16, 41, 6, 6, 6, -1, 29, 0, -1, 16, 16, -1, 6, 39, 6, 29, 6, 6, 6, 6, 6, 18, 42, 6, 2, 6, 36, 42, 6, 17, 36, 6, 11, -1, 6, 6, 36, -1, 0, 0, 1, 36, 7, -1, 34, 7, -1, 0, 2, 0, 7, 56, 7, 7, 1, 30, 7, 14, 14, 14, 41, 55, 7, 7, 1, 28, 32, 4, 4, 4, -1, 4, 55, -1, 14, 36, 5, 17, 17, 17, 36, 36, 14, 14, 14, -1, 45, 14, 36, 14, 42, 31, 42, -1, -1, 21, 20, 21, 22, 4, 48, 4, 4, -1, -1, 4, 18, 4, 4, -1, 9, 4, 21, 21, 34, 45, 5, 5, 5, 5, 5, 5, -1, 5, 5, 0, 0, 28, -1, 8, 12, 11, 18, -1, 5, -1, 5, 5, 5, 0, 5, 5, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 11, 2, -1, -1, 6, 6, 52, 6, -1, 11, 6, 11, 6, 10, 6, 29, 6, 53, 6, 13, 0, 13, 11, 22, 53, 6, 6, 6, -1, 6, -1, 6, -1, 39, 6, 29, 11, 7, 7, 7, 7, -1, 7, 7, 11, 0, 7, 45, 15, -1, 7, 1, 33, 0, 15, -1, 14, 14, 18, 24, 11, 6, 12, 14, -1, 33, 55, 1, 1, -1, 14, 24, 48, 11, 4, 4, 51, 1, -1, 4, 4, 11, 4, 4, 13, 39, 4, -1, 17, 4, 11, 17, 28, -1, 8, 31, 4, 17, 22, 0, 17, 17, 17, 1, -1, 13, 24, 43, 3, 3, -1, 31, 48, 24, 0, -1, 8, -1, 8, 28, 9, 0, 0, -1, 38, 44, 44, 44, 0, -1, 22, 2, 2, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 0, 46, 3, 3, 3, 3, 3, -1, -1, 3, 5, 3, 3, 3, 3, 39, 3, 3, 5, 18, 53, 0, 3, 44, 18, 3, 3, 3, 0, -1, -1, 6, 6, -1, 6, 6, 6, 6, -1, 27, 27, -1, 27, 27, 27, 1, 27, 21, 27, 21, 5, 2, 48, 1, 30, 9, 13, -1, 9, 8, 8, 8, 8, 8, 11, 8, 20, 9, 48, 9, 9, 9, 9, 24, 13, 33, 10, 13, 8, 31, 31, 34, 11, -1, 11, 33, 33, -1, -1, 33, 33, 1, 33, -1, 33, -1, 12, 12, 12, -1, 12, 1, 12, 19, -1, 6, -1, 24, 12, 2, -1, 0, 51, 3, 1, 19, 28, 30, 0, 28, 28, 28, 9, 0, -1, 51, 1, 30, -1, 24, 38, 38, -1, 0, 3, 3, 3, 3, 3, 3, 3, -1, 3, 3, 4, 3, -1, 3, 44, 3, 47, 3, 16, 47, 44, 3, 6, 18, 13, 1, 36, -1, 7, -1, 19, 1, -1, 16, -1, 35, 10, 12, 34, 27, 14, 27, 11, 27, 14, 13, 1, -1, 26, -1, 26, 26, 13, 14, 1, 30, 35, -1, 35, 52, 34, 52, 31, 35, 1, 35, 13, 12, 35, -1, 35, 50, 24, -1, 13, 0, 9, -1, 4, 23, 23, -1, -1, 10, -1, -1, 15, -1, 9, 11, -1, 8, 25, 27, 57, 25, 9, 8, 8, 9, -1, -1, 8, 8, 8, 54, 8, 3, 8, 23, 39, 23, 23, 10, -1, 33, 33, -1, 11, 33, 11, -1, 49, 25, 29, -1, -1, 25, 49, 25, 25, 18, 2, 21, 11, 32, 32, 0, 52, -1, -1, 2, -1, 31, 15, 0, 0, 11, 41, 24, 20, 20, 0, 21, 6, 26, 29, 15, 3, 7, -1, 20, 1, 20, -1, 54, 10, 18, 39, 8, -1, 37, 34, 7, 7, 30, 0, 1, 29, 7, 42, 7, 18, 36, 36, -1, 17, 2, 3, 52, 26, 35, 13, 13, -1, 35, 13, 34, 2, -1, 19, 26, 26, 13, 18, 0, 50, 1, 0, 16, 15, -1, 0, 46, -1, -1, 28, -1, 12, 22, 2, 23, 23, 24, 23, -1, 23, 17, 26, -1, -1, 53, 31, -1, -1, 5, 22, 42, 25, 25, 25, 31, 55, 25, 5, 25, 25, 16, -1, 25, 25, 20, 20, 16, 15, 26, 26, 26, 26, 40, 19, 26, 6, 40, 49, -1, 4, 11, 1, 9, 4, 0, 4, 17, 9, 32, 32, 23, 10, -1, 39, -1, 23, 1, -1, 51, 23, -1, 39, 23, 29, -1, 24, 1, 3, 0, 50, 32, 55, 1, 7, 7, 0, 7, -1, 34, 7, 3, 13, 13, -1, 34, 0, -1, 7, 18, 1, 0, 14, 14, 3, 0, 0, 49, 14, 0, 14, 10, 0, 17, 47, -1, 34, 37, 2, -1, 12, 8, 40, 20, 5, -1, -1, 19, 1, -1, 10, 5, 20, -1, 37, 33, 40, 4, 39, 40, 14, 11, 8, 8, 4, 37, 30, 1, 8, 13, 13, 8, 8, 46, 1, 10, 2, 9, 8, 4, 4, -1, -1, 55, -1, 20, -1, 38, -1, 17, -1, 19, -1, 19, 1, 0, -1, 7, -1, 56, 4, 5, 10, 4, 2, 2, 2, 3, 2, 3, 3, 23, 7, 2, 3, 3, 1, 19, 7, 5, 23, 1, -1, 21, 47, 16, 32, 19, 0, 7, 32, 0, 1, 14, 14, 10, 16, 0, 17, 17, -1, 36, -1, -1, 0, 31, -1, 0, 56, 11, 14, 38, 57, 0, 0, -1, 3, -1, -1, 41, 3, 3, 0, -1, 12, -1, 17, 5, 28, -1, 4, 30, 17, 4, 17, 10, 31, 15, 8, 1, 10, 4, -1, 8, -1, -1, 4, 4, 4, 9, -1, 18, 4, 2, 1, 35, 35, 2, 1, 11, 5, 5, 2, 2, 2, 6, 15, -1, -1, 4, 3, -1, 46, 3, 3, 9, 7, 34, 3, 27, 27, 47, 0, 0, 37, 45, 2, 37, 2, 1, 7, 8, -1, 25, 2, 14, 12, 56, -1, 12, -1, 24, 2, 26, 26, 14, 51, -1, -1, 20, 16, 14, 0, 44, 9, 1, 16, 23, 24, 34, 9, 32, 0, 5, 10, -1, -1, -1, 0, 18, 5, 5, 32, 0, 27, 23, 17, -1, 39, 18, 10, 17, 4, 4, 4, 17, 0, 21, 3, 20, 3, 44, 35, 4, -1, -1, 4, 1, 14, 47, 4, 4, 4, -1, 4, 17, 32, 8, 0, 11, 3, 3, 4, 6, -1, 8, 8, 1, 1, 3, 3, 4, -1, 23, 5, 27, 23, 23, -1, -1, 31]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------RUN91-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': 'CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7df3b00f5c90>)', 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[26, 2, 30, 30, -1, 49, -1, 22, 40, -1, -1, -1, -1, -1, 11, 9, 23, 26, -1, 9, -1, 16, -1, 10, 10, 12, 37, 9, 9, 41, -1, -1, 30, 32, 41, -1, 0, -1, 46, 41, 10, 9, 34, 10, 34, 6, 34, 34, 10, -1, 34, 10, -1, -1, -1, 21, -1, 2, 22, 22, 0, 22, 22, -1, 9, 9, -1, -1, 16, 1, 27, 16, 0, 0, 15, 1, 23, -1, 41, -1, 21, 2, -1, 21, 2, 12, 21, 0, 2, 32, 0, 39, 21, 44, 56, 7, 12, 0, -1, -1, 41, 15, -1, 45, -1, 27, -1, 46, 46, 46, -1, 16, 37, 0, 2, -1, 5, -1, -1, -1, -1, 5, 10, 15, 34, 7, 16, 2, 10, 16, 2, 11, 9, 16, 16, 26, 54, 23, 16, 34, -1, -1, 25, -1, 11, 22, 26, -1, 2, -1, -1, -1, -1, 56, -1, 22, 0, 25, -1, 48, 57, -1, 27, 48, 9, -1, -1, 15, -1, 9, -1, 56, -1, -1, 11, 22, 13, 53, 11, -1, 47, 23, 11, -1, 16, -1, 49, 2, 16, 16, 11, 11, 21, 2, -1, 5, 17, 17, -1, 24, 21, 2, 21, -1, -1, -1, 5, 21, 0, -1, 0, 21, 2, 2, 21, 21, 21, 0, 2, 12, 17, -1, 0, 0, 39, 17, 37, 37, -1, -1, 52, -1, 52, 17, 17, -1, 12, 52, -1, 17, -1, 1, 1, -1, 17, -1, -1, -1, 33, 9, -1, 17, 8, 25, -1, 6, 6, 41, 0, 0, 37, 15, 33, -1, 9, 2, 27, 37, 44, 1, 1, 1, -1, 0, 1, 40, 1, 1, 12, -1, 0, 47, 1, 6, 6, 1, 17, 38, 18, 38, 18, 2, 18, 18, 5, -1, -1, 40, 17, 1, 1, 1, 16, 1, -1, 1, 0, 1, 18, -1, 10, 1, 34, 47, 53, 40, 57, -1, 15, -1, 22, -1, -1, 1, -1, 26, -1, 26, 13, 26, -1, 57, 43, 26, 31, -1, 22, 11, 0, 11, 11, 12, 12, -1, -1, -1, 11, 11, -1, 11, -1, -1, 9, 2, 39, -1, -1, 43, 43, 26, -1, 16, 49, -1, -1, -1, 16, 23, 12, -1, -1, -1, 16, 27, -1, 16, -1, 39, -1, -1, 8, 8, 9, 39, 39, 39, 55, 39, 55, 12, 35, -1, -1, 8, 8, 8, 8, 8, 8, 28, 44, 8, 17, -1, 17, 50, 8, 0, -1, -1, 12, 55, 0, 15, -1, 12, 52, 13, 31, 38, 45, -1, 44, -1, -1, 15, 23, 31, 0, 0, 0, 23, 0, 1, 0, 40, 0, 0, 0, 0, 0, 0, 40, 31, 26, 1, -1, 1, 0, -1, 32, 1, 1, 1, 31, 2, 1, -1, 1, 1, 33, 1, 28, 2, 17, 1, 1, 1, 1, 0, 0, 6, -1, 0, -1, 0, 1, 6, 0, 0, -1, 38, 0, -1, 18, -1, 38, 6, 2, 6, 18, 15, -1, 12, 38, 18, 17, 41, 4, 4, 4, -1, 26, 0, -1, 17, 17, -1, 4, 51, 4, 26, 4, 4, 4, 4, 4, 15, 43, 4, 1, 4, 36, 43, 4, 19, 36, 4, 13, 15, 4, 4, 36, -1, 0, 0, -1, 36, 6, 12, 32, 6, -1, -1, 1, 0, 6, 45, 6, 6, 2, 34, 6, 14, 14, 14, 41, -1, 6, 6, 2, 18, 31, 20, -1, 29, -1, 20, -1, -1, 14, 36, -1, 19, 19, 19, 36, -1, 14, 14, 14, -1, 44, 14, 36, 14, 43, 27, 43, -1, -1, 12, 22, 12, 39, -1, 54, 29, 20, -1, -1, 29, 15, -1, 29, -1, 8, 20, 12, -1, 32, 44, 5, 5, 5, 5, 5, 5, 14, 5, 5, 0, 0, 18, -1, -1, 11, -1, 15, -1, 5, -1, 5, 5, 5, 0, 5, 5, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 13, 1, -1, -1, 4, 4, 50, 4, -1, 13, 4, 13, 4, 9, 4, 26, 4, 53, 4, 10, 0, 10, 13, 56, 53, 4, 4, 4, -1, 4, 4, 4, -1, -1, 4, 26, 13, 6, 6, 6, 6, -1, 6, 6, 13, 0, 6, 44, 16, -1, 6, 2, 30, 0, 16, -1, 14, 14, 15, 23, -1, 4, 11, -1, -1, 30, -1, -1, 2, -1, 14, 23, 54, -1, 20, 20, 18, 2, 18, 29, 20, 13, 29, 29, 10, -1, -1, 42, 19, -1, 13, 19, 18, -1, 7, 27, 20, 19, -1, 0, 19, 19, 19, 2, -1, 10, 23, 52, 3, 3, -1, 27, 54, 23, 0, -1, 7, -1, 7, 18, 8, 0, 0, -1, -1, 42, 42, 42, 0, -1, -1, 1, 1, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 0, 47, 3, 3, 3, 3, 3, 13, -1, 3, 5, 3, 3, 3, 3, 51, 3, 3, 5, 15, 53, 0, 3, 42, 15, 3, 3, 3, 0, 15, -1, 4, 4, 13, 4, 4, 4, 4, -1, 28, 28, -1, 28, 28, 28, 2, 28, 12, 28, 12, 5, 1, 54, 2, 34, 8, 10, -1, 8, 7, 7, 7, 7, 7, 13, 7, 22, 8, 54, 8, 8, 8, 8, 45, 10, 30, -1, 10, 7, 27, 27, 32, 13, -1, 13, 30, 30, 15, -1, 30, 30, 2, 30, 12, 30, -1, 11, 11, 11, 18, 11, 2, 11, 21, -1, 4, -1, 23, 11, 1, -1, 0, 18, 3, 2, 21, 18, 34, 0, 18, 18, 18, 8, 0, -1, 18, 2, -1, -1, 23, 40, 40, -1, 0, 3, 3, 3, 3, 3, 3, 3, -1, 48, 3, 29, 3, -1, 3, 42, 3, 48, 3, 17, -1, 42, 3, 4, 15, 10, 2, 36, -1, 6, -1, 21, 2, -1, 17, 12, 35, 9, 11, 32, 28, 14, 28, 13, 28, 14, 10, 2, -1, 33, -1, 33, 33, 10, 14, 2, 34, 35, -1, 35, 50, 32, 50, 27, 35, 2, 35, 10, 11, 35, 12, 35, 55, 23, -1, 10, 0, 8, 12, 20, 24, 24, -1, -1, 9, 12, -1, 16, -1, 8, 13, -1, 7, 25, 28, -1, 25, 8, 7, 7, 8, -1, -1, 7, 7, 7, 49, 7, 3, 7, 24, 51, 24, 24, 9, 57, 30, 30, -1, 13, 30, 13, -1, 46, 25, 26, -1, -1, 25, 46, 25, 25, 15, 1, 12, 13, 31, 31, 0, 50, -1, -1, 1, -1, 27, 16, 0, 0, -1, 41, 23, 22, 22, 0, 12, 4, -1, 26, 16, 3, 6, -1, -1, 2, 22, -1, 49, 9, 15, -1, 7, -1, 37, 32, 6, 6, 34, 0, 2, 26, 6, 43, 6, 15, 36, 36, -1, 19, 1, 3, 50, 33, 35, 10, 10, -1, 35, 10, 32, 1, -1, 21, 33, 33, 10, 15, 0, 55, -1, 0, 17, 16, -1, 0, 47, -1, -1, 18, -1, 11, 39, 1, 24, -1, 23, 24, -1, 24, 19, 33, -1, -1, 53, 27, -1, -1, 5, 56, 43, 25, 25, 25, 27, -1, 25, 5, 25, 25, 17, -1, 25, 25, 22, 22, 17, 16, 33, 33, -1, 33, 38, 21, -1, 4, 38, -1, -1, 20, 13, 2, 8, 29, 0, -1, 19, 8, 31, 31, 24, 9, 23, 51, -1, 24, 2, -1, 18, 24, -1, 51, 24, 26, -1, 23, 2, 3, 0, -1, 31, -1, 2, 6, 6, 0, 6, 6, 32, 6, 3, 10, 10, -1, 32, 0, 10, 6, 15, 2, 0, 14, 14, 3, 0, 0, 46, 14, 0, 14, 9, 0, 19, -1, -1, 32, 37, 1, -1, 11, 7, 38, 22, 5, -1, -1, 21, 2, -1, 9, 5, 22, -1, -1, 30, 38, 20, -1, 38, 14, 13, 7, 7, 29, 37, 34, 2, 7, 10, 10, 7, 7, 47, 2, 9, 1, 8, 7, 20, -1, -1, 3, -1, -1, 22, -1, 40, -1, 19, -1, 21, -1, 21, 2, 0, -1, 6, -1, 45, 20, 5, 9, 20, 1, 1, 1, 3, 1, 3, 3, 24, 6, 1, 3, 3, 2, 21, 6, 5, 24, 2, -1, 12, 48, 17, 31, 21, 0, -1, 31, 0, 2, 14, 14, 9, 17, 0, 19, 19, 57, 36, -1, 23, 0, 27, -1, 0, 45, 13, 14, 40, -1, 0, 0, -1, 3, 1, 9, -1, 3, 3, 0, 13, 11, -1, 19, 5, 18, -1, -1, -1, 19, 20, 19, 9, 27, 16, 7, 2, 9, 20, 52, 7, -1, -1, 20, 29, 29, 8, -1, 15, 20, 1, 2, 35, 35, -1, 2, -1, 5, 5, 1, 1, 1, 4, 16, -1, 13, -1, 3, -1, 47, 3, 3, 8, 6, 32, 3, 28, 28, 48, 0, 0, 37, 44, 1, 37, 1, 2, 6, 7, -1, 25, 1, 14, 11, 45, 49, 11, -1, -1, 1, 33, 33, 14, 18, 29, -1, 22, 17, 14, 0, 42, 8, 2, 17, 24, 23, 32, 8, 31, 0, 5, 9, -1, -1, -1, 0, 15, 5, 5, 31, 0, 28, 24, 19, -1, 51, 15, 9, 19, 20, 29, 29, 19, 0, 12, 3, 22, 3, 42, 35, 20, 23, -1, 29, 2, -1, 48, 20, 20, -1, -1, -1, 19, 31, 7, 0, 13, 3, 3, 20, 4, 1, 7, 7, 2, 2, 3, 3, 29, -1, 24, 5, 28, 24, 24, -1, -1, 27]\n",
            "-------RUN92-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': \"CountVectorizer(stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b02d2c50>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[26, 31, 29, 29, 9, 56, -1, -1, 42, -1, -1, -1, -1, -1, 10, 18, 22, 26, -1, 18, -1, 9, 19, 11, 11, 12, 39, 49, 18, 43, -1, -1, 29, 30, 43, -1, 1, 47, 0, 43, 11, 18, 24, 11, 24, 6, 24, 24, 11, -1, 24, 11, -1, -1, -1, 15, -1, 31, 19, 19, 1, 19, 19, -1, 18, 18, -1, -1, 9, -1, 27, 9, 1, 1, 14, 2, 22, -1, 43, -1, 15, -1, -1, 15, 34, 12, 15, 1, 34, 30, 1, 16, 4, 44, 16, 7, 12, 1, -1, -1, 43, 14, -1, 52, -1, 27, -1, 0, 0, 0, 2, 9, -1, -1, 34, -1, 4, 16, -1, -1, 24, 4, 11, 14, 24, 7, 9, 32, 11, 9, -1, 10, 18, 9, 9, 26, 58, 22, 9, 24, -1, -1, -1, 7, 10, 19, 26, 56, -1, 0, -1, -1, -1, 16, 51, 19, 1, 25, -1, 57, -1, -1, 27, 57, 18, -1, -1, 14, 12, 49, -1, 16, -1, 0, 10, 19, -1, 50, 10, 9, 47, 22, 10, 0, 9, -1, 56, -1, 9, 9, 10, 10, 15, 32, -1, 4, 13, 13, 15, 4, 15, 32, 15, -1, -1, -1, 4, 15, 1, 44, 1, -1, -1, 48, -1, 15, 15, 1, 32, 12, 13, -1, 1, 1, 16, 13, 39, 39, 31, 0, 0, 0, 0, 13, 13, 0, 12, 0, -1, 13, -1, 2, 2, -1, 13, 51, -1, 51, 20, 49, -1, 13, 8, 25, -1, 6, 6, 43, 1, 1, 39, 14, 20, -1, 49, -1, 27, 39, 44, 2, 2, 2, -1, 1, 2, 42, 2, 2, 12, 12, 1, 47, 2, 6, 6, 2, 13, 37, 21, 37, 21, 31, 21, 21, 4, -1, -1, 42, 13, 2, 2, 2, 9, 2, -1, 2, 1, 2, 53, -1, 11, 2, 24, 47, 50, 42, -1, 51, 14, -1, 19, 7, -1, 9, 0, 26, -1, 26, 45, 26, -1, -1, 38, 26, 28, -1, 19, 10, 1, 10, 10, 12, -1, -1, -1, -1, 10, 10, 0, 10, -1, -1, -1, 48, 16, 0, -1, 38, 38, 26, -1, 9, -1, -1, -1, -1, 9, 22, 12, 9, -1, 0, 9, 27, -1, 9, 0, 16, -1, 0, 8, 8, 18, 16, 16, 16, 54, 16, 54, 12, 33, -1, -1, 8, 8, 8, 8, 8, 8, 23, 44, 8, 13, -1, 13, 55, 8, 1, 0, 0, 12, 54, 1, 14, 0, 12, 0, -1, 28, 37, 52, -1, 44, -1, -1, 14, 22, 28, 1, 1, 1, 22, 1, 2, 1, 42, 1, 1, 1, 1, 1, 1, 42, 28, -1, 2, -1, 2, 1, -1, 30, 2, 2, 2, 28, 31, 2, 59, 2, 2, 20, 2, 23, 31, 13, 2, 2, 2, 2, 1, 1, 6, -1, 1, -1, 1, 2, -1, -1, 1, -1, 37, 1, 16, 21, -1, 37, 6, 34, 6, 21, 14, -1, 12, 37, 21, 13, 43, 5, 5, 5, 0, 26, 1, -1, 13, 13, -1, 5, 46, 5, 26, 5, 5, 5, 5, 5, 14, 38, 5, 2, 5, 36, 38, 5, 35, 36, 5, -1, -1, 5, 5, 36, 0, 1, 1, -1, 36, 6, 12, 30, 6, 0, -1, 2, 1, 6, 52, 6, 6, 34, 24, 6, 0, 0, 0, 43, 0, 6, 6, 32, 21, 28, 0, 0, 0, 0, 0, 0, -1, 0, 36, -1, 40, -1, 40, 36, 38, 0, 0, 0, -1, 44, 0, 36, 0, 38, 27, 38, 0, 0, 12, -1, 12, 16, 0, 58, 0, 0, -1, 0, 0, 14, 0, 0, -1, 8, 0, 12, 12, 30, 44, 4, 4, 4, 4, 4, 4, 0, 4, 4, 1, 1, 21, -1, 7, 10, -1, 14, -1, 4, -1, 4, 4, -1, 1, 4, 4, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 59, 2, -1, -1, 5, 5, 55, 5, 0, -1, 5, -1, 5, 18, 5, 26, 5, 50, 5, 11, 1, 11, 59, 16, 50, 5, 5, 5, -1, 5, -1, 5, 0, 46, 5, 26, 59, 6, 6, 6, 6, -1, 6, 6, -1, 1, 6, 44, 9, -1, 6, 48, 29, 1, 9, -1, 0, 0, 14, 22, -1, 5, 10, 0, -1, 29, 0, -1, -1, -1, 0, 22, -1, -1, 0, 0, 53, 48, 53, 0, 0, -1, 0, 0, 58, -1, 0, 41, 35, 0, 45, 35, 21, 0, -1, 27, 0, -1, 16, 1, 40, 35, 40, 34, -1, 11, 22, 0, 3, 3, -1, 27, 58, 22, 1, 0, 7, -1, 7, 21, 8, 1, 1, 0, -1, 41, 41, 41, 1, 50, 16, 2, 2, 4, 4, 1, 4, 4, 1, 4, 4, 4, 4, 1, 47, 3, 3, 3, 3, 3, -1, 0, 3, 4, 3, 3, 3, 3, 46, 3, 3, 4, 14, 50, 1, 3, 41, 14, 3, 3, 3, 1, 14, -1, 5, 5, -1, 5, 5, 5, 5, -1, 23, 23, -1, 23, 23, 23, 34, 23, 12, 23, 12, -1, 2, 58, 34, 24, 8, 11, -1, 8, 7, 7, 7, 7, 7, 45, 7, -1, 8, 58, 8, 8, 8, 8, 52, 11, 29, -1, 11, 7, 27, 27, 30, -1, -1, -1, 29, 29, 14, -1, 29, 29, 32, 29, -1, 29, -1, 10, 10, 10, 21, 10, -1, 10, 15, -1, 5, -1, 22, 10, 2, 46, 1, 53, 3, -1, 15, 21, 24, -1, 21, 21, 21, 8, 1, 0, 53, 48, 24, -1, 22, 42, -1, -1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, -1, 3, 41, 3, 57, 3, 13, -1, 41, 3, 5, 14, 11, 34, 36, -1, 6, -1, 15, 32, -1, 13, 12, 33, 18, 10, 30, 23, 0, 23, 45, 23, 0, 11, 31, -1, 20, -1, 20, 20, 11, 0, 31, 24, 33, -1, 33, 55, 30, 55, 27, 33, 31, 33, 11, 10, 33, 12, 33, 54, 22, -1, 11, 1, 8, -1, 0, 17, 17, -1, 24, 49, 12, 16, 9, -1, 8, -1, -1, 7, 25, 23, 51, 25, 8, 7, 7, 8, -1, -1, 7, 7, 7, 56, 7, 3, 7, 17, 46, 17, 17, 18, -1, 29, 29, -1, 59, 29, -1, -1, 0, 25, 26, -1, -1, 25, 0, 25, 25, 14, 2, 12, 45, 28, 28, 1, 55, -1, -1, 2, -1, 27, 9, 1, 1, -1, 43, -1, 19, 19, 1, 12, 5, 20, 26, 9, 3, 6, -1, -1, -1, 19, -1, 56, 49, 14, -1, 7, -1, 6, 30, 6, 6, 24, 1, 32, 26, 6, 38, 6, 14, 36, 36, -1, 35, 2, 3, 55, 20, 33, 11, 11, 6, 33, 11, 30, 2, -1, 15, 20, 20, 11, 14, 1, 54, -1, 1, 13, 9, -1, 1, 47, -1, -1, 21, -1, 10, 16, 2, 17, 17, 22, 17, -1, 17, 35, 20, 0, -1, 50, 27, -1, -1, 4, 16, 38, 25, 25, 25, 27, 0, 25, 4, 25, 25, 13, -1, 25, 25, 19, 19, 13, 9, 20, 20, 20, 20, 37, 15, 20, 5, 37, 0, 0, 0, 45, -1, 8, 0, 1, 0, 40, 8, 28, 28, 17, 18, -1, 46, 0, 17, 31, -1, 53, 17, -1, 46, 17, 26, -1, 22, 48, 3, 1, 54, 28, 0, 34, 6, 6, 1, 6, -1, 30, 6, 3, 11, 11, -1, 30, 1, -1, 6, 14, 31, 1, 0, 0, 3, 1, 1, 0, 0, 1, 0, 18, 1, 40, 57, -1, 30, 39, 2, -1, 10, 7, 37, 19, 4, -1, -1, 15, 32, -1, 18, 4, 19, -1, -1, 29, 37, 0, -1, 37, 0, 45, 7, 7, 0, 39, 24, -1, 7, 11, 11, 7, 7, 47, 32, 18, 2, 8, 7, 0, 0, 0, 3, 0, -1, 19, -1, 42, -1, -1, -1, 15, 0, 15, 34, 1, -1, 6, 0, 52, 0, 4, 49, 0, 2, 2, 2, 3, 2, 3, 3, 17, 6, 2, 3, 3, 32, 15, 6, 4, 17, 32, 51, 12, 57, 13, 28, 15, 1, -1, 28, 1, 32, 0, 0, 18, 13, 1, 40, 35, -1, 36, -1, -1, 1, 27, -1, 1, 52, -1, 0, 42, 51, 1, 1, -1, 3, 9, -1, -1, 3, 3, 1, 45, 10, -1, 40, 4, 21, -1, 0, -1, 35, 0, 35, 18, 27, 9, 7, 31, 18, 0, 0, 7, -1, -1, 0, 0, 0, 8, -1, 14, 0, 2, 34, 33, 33, 2, 32, -1, 4, 4, 2, 2, 2, 5, 9, -1, -1, 0, 3, -1, 47, 3, 3, 8, 6, 30, 3, 23, 23, 3, 1, 1, 39, 44, 2, 39, 2, 48, 6, 7, -1, 25, 2, 0, 10, 52, 56, 10, -1, -1, 2, 20, 20, 0, 53, 0, -1, -1, 13, 0, 1, 41, 8, 31, 13, 17, 22, 30, 8, 28, 1, 4, -1, -1, -1, -1, 1, 14, 4, 4, 28, 1, 23, 17, 35, -1, 46, 14, 18, 40, 0, 0, 0, 35, 1, 12, 3, 19, 3, 41, 33, 0, 22, -1, 0, 31, 0, 57, 0, 0, 0, -1, 0, -1, 28, -1, 1, 45, 3, 3, 0, 5, 2, 7, 7, -1, 34, 3, 3, 0, 0, 17, 4, 23, 17, 17, -1, 0, 27]\n",
            "Skipping coherence calculation due to error: unable to interpret topic as either a list of tokens or a list of ids\n",
            "-------RUN93-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': \"CountVectorizer(ngram_range=(1, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b891dd10>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[24, 31, 28, 28, -1, 53, -1, -1, -1, -1, -1, -1, -1, -1, 11, 15, 19, 24, -1, 15, -1, 12, -1, 10, 10, 14, 36, 15, 15, 43, -1, -1, 28, 32, 43, -1, 0, 41, 58, 43, 10, 15, 29, 10, 36, 6, 29, 29, 10, -1, 29, 10, -1, -1, -1, -1, -1, 31, 18, 18, 0, 18, 18, -1, 15, 15, -1, -1, 12, 12, 22, 12, 0, 0, 17, 1, 19, -1, 43, 31, 21, -1, -1, 21, -1, 14, 21, 0, 33, 32, 0, 38, 4, 45, 57, 7, 14, 0, -1, -1, 43, 17, 54, -1, -1, 22, -1, -1, 58, -1, 1, 12, 36, 23, 52, -1, 4, 57, -1, -1, -1, 4, 10, 17, 29, -1, 12, 33, 10, 12, -1, 11, 15, 12, 12, 24, 51, 19, 12, 29, -1, -1, 23, -1, 11, 18, 24, -1, -1, -1, -1, -1, -1, 57, -1, 18, 0, 23, -1, 50, -1, -1, 22, 50, 15, -1, -1, 17, -1, 15, -1, 57, -1, -1, 11, 18, 39, 49, 11, -1, 41, 19, 11, -1, 12, -1, 53, 47, 12, 12, 11, 11, 21, 33, -1, 4, 13, 13, 21, 20, 21, 33, 21, 40, -1, -1, 4, 21, 0, -1, 0, -1, 47, 47, 21, 21, 21, 0, 33, 22, 13, -1, 0, 0, 38, 13, 36, 36, -1, 42, 42, -1, 42, 13, 13, 42, 14, 42, -1, 13, -1, 1, 1, -1, 13, -1, 53, -1, 26, 15, 39, 13, -1, 23, -1, 6, 6, 43, 0, 0, 36, 17, 26, -1, -1, -1, 22, 36, 45, 1, 1, 1, -1, 0, 1, -1, 1, 1, 14, 14, 0, 41, 1, 6, 6, 1, 13, 37, -1, 37, 27, 31, 27, 27, 4, -1, -1, -1, 13, 1, 1, 1, 12, 1, -1, 1, 0, 1, 59, -1, 10, 1, 29, 41, 49, -1, -1, -1, 17, -1, 18, -1, -1, 12, 3, 24, -1, 24, -1, 24, -1, -1, 46, 24, 30, -1, 18, 11, 0, 11, 11, 14, 14, -1, -1, -1, 11, 11, 55, 11, -1, 51, -1, 47, 38, -1, -1, 46, 46, 24, 39, 12, 53, -1, 48, -1, 12, 19, 14, 12, 39, -1, -1, 22, -1, 12, 55, 38, 41, 55, 8, 8, 15, 38, 38, 38, 48, 38, 48, 14, 34, -1, -1, 8, 8, 8, 8, 8, 8, 25, 45, 8, 13, -1, 13, 56, 8, 0, 42, 42, 14, 48, 0, 17, -1, 14, 42, 39, 30, 37, 39, -1, 45, -1, -1, 17, 19, 30, 0, 0, 0, 19, 0, 1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 30, -1, 1, -1, 1, 0, -1, 32, 1, 1, 1, 30, 31, 1, -1, 1, 1, 26, 1, 25, 31, 13, 1, 1, 1, 1, 0, 0, 6, -1, 0, -1, 0, 1, -1, 0, 0, -1, 37, 0, -1, 27, -1, 37, 6, 52, 6, 27, 17, -1, 14, 37, 27, 13, 43, 5, 5, 5, -1, 24, 0, -1, 13, 13, -1, 5, 40, 5, 24, 5, 5, 5, 5, 5, 17, 46, 5, 1, 5, 35, 46, 5, 16, 35, 5, -1, -1, 5, 5, 35, -1, 0, 0, -1, 35, 6, 22, 32, 6, -1, -1, 1, 0, 6, 39, 6, 6, -1, 29, 6, 9, 9, 9, 43, 9, 6, 6, 33, 27, 30, 3, 3, 3, -1, 3, 9, -1, 9, 35, 4, 16, 16, 16, 35, 35, 9, 9, 9, -1, 45, 9, 35, 9, 35, 22, -1, -1, 55, 14, 18, 14, 38, 3, 51, 3, 3, -1, -1, 3, 17, 3, 3, -1, 8, 3, 14, 14, 32, 45, 4, 4, 4, 4, 4, 4, 9, 4, 4, 0, 0, 27, -1, -1, 11, -1, 17, -1, 4, -1, 4, 4, 4, 0, 4, 4, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 54, 1, -1, -1, 5, 5, 56, 5, -1, -1, 5, -1, 5, 15, 5, 24, 5, 49, 5, 10, 0, 10, 54, 57, 49, 5, 5, 5, -1, 5, 5, 5, -1, -1, 5, 24, 54, 6, 6, 6, 6, -1, 6, 6, -1, 0, 6, 45, 12, -1, 6, 47, 28, 0, 12, -1, 9, 9, 17, 19, -1, 5, 11, -1, -1, 28, 55, -1, -1, -1, 9, 19, 51, 54, 3, 3, 59, -1, -1, 3, 3, -1, 3, 3, 10, -1, -1, -1, 16, 3, -1, 16, 27, 3, 7, 22, 3, 16, -1, 0, 16, 16, 16, 52, -1, 10, 19, 42, 2, 2, 2, 22, 51, 19, 0, -1, 7, -1, 7, 27, 8, 0, 0, -1, -1, 44, 44, 44, 0, 49, -1, 1, 1, 4, 4, 0, 4, 4, 0, 4, 4, 4, 4, 0, 41, 2, 2, 2, 2, 2, -1, 3, 2, 4, 2, 2, 2, 2, 40, 2, 2, 4, 17, 49, 0, 2, 44, 17, 2, 2, 2, 0, -1, -1, 5, 5, -1, 5, 5, 5, 5, -1, 25, 25, -1, 25, 25, 25, 52, 25, 14, 25, 14, 4, 1, 51, 52, 29, 8, 10, -1, 8, 7, 7, 7, 7, 7, -1, 7, 18, 8, 51, 8, 8, 8, 8, 39, 10, 28, -1, 10, 7, 22, 22, 32, -1, -1, -1, 28, 28, -1, -1, 28, 28, 33, 28, -1, 28, -1, 11, 11, 11, -1, 11, -1, 11, 21, -1, 5, -1, 19, 11, -1, 40, 0, 59, 2, 33, 21, 27, 29, 0, 27, 27, 27, 8, 0, -1, 27, -1, 29, -1, 19, -1, -1, -1, 0, 2, 2, 2, 2, 2, 2, 2, -1, 2, 2, 3, 2, -1, 2, 44, 2, 50, 2, 13, 50, 44, 2, 5, -1, 10, 52, 35, 48, -1, -1, 21, 33, -1, 13, 22, 34, 15, 11, 32, 25, 9, 25, -1, 25, 9, 10, -1, -1, 26, -1, 26, 26, 10, 9, 31, 29, 34, -1, 34, 56, 32, 56, 22, 34, 31, 34, 10, 11, 34, 14, 34, 48, 19, -1, 10, 0, 8, -1, 3, 20, 20, -1, 29, -1, 14, 38, 12, -1, 8, -1, -1, 7, 23, 25, -1, 23, 8, 7, 7, 8, -1, -1, 7, 7, 7, 53, 7, 2, 7, 20, 40, 20, 20, 15, -1, 28, 28, -1, -1, 28, -1, -1, 58, 23, 24, -1, -1, 23, 58, 23, 24, 17, 1, 14, -1, 30, 30, 0, 56, -1, -1, 1, -1, 22, 12, 0, 0, -1, 43, 19, 18, 18, 0, 14, 5, 26, 24, 12, 2, 6, -1, 18, 31, 18, -1, 53, -1, 17, 40, -1, -1, -1, 32, 6, 6, 29, 0, 33, 24, 6, 46, 6, 17, 35, 35, -1, 16, 1, 2, 56, 26, 34, 10, 10, -1, 34, 10, 32, 1, -1, 21, 26, 26, 10, 17, 0, 48, -1, 0, 13, 12, -1, 0, 41, -1, -1, 27, -1, 11, 38, 1, 20, 20, 19, 20, -1, 20, 16, 26, 3, -1, 49, 22, -1, -1, 4, 57, 46, 23, 23, 23, 22, -1, 23, 4, 23, 23, 13, -1, 23, 23, 18, 18, 13, 12, 26, 26, 26, 26, 37, 21, -1, 5, 37, 58, -1, 3, -1, 31, 8, 3, 0, -1, 16, 8, 30, 30, 20, -1, -1, 40, -1, 20, 31, -1, 59, 20, -1, 40, 20, 24, -1, 19, 47, 2, 0, -1, 30, -1, -1, 6, 6, 0, 6, -1, 32, 6, 2, 10, 10, -1, 32, 0, -1, 6, 17, 31, 0, 9, 9, 2, 0, 0, 58, 9, 0, 9, 15, 0, 16, 50, -1, 32, 36, 1, -1, 11, 7, 37, 18, 4, -1, -1, 21, -1, -1, 15, 4, 18, -1, 36, 28, 37, 3, 40, 37, 9, -1, 7, 7, 3, 36, 29, -1, 7, 10, 10, 7, 7, 41, 33, 15, 1, 8, 7, 3, 3, -1, 2, 9, 54, 18, -1, -1, 19, 16, -1, 21, 55, 21, -1, 0, -1, 6, 3, 39, 3, 4, 15, 3, 1, 1, 1, 2, 1, 2, 2, 20, 6, 1, 2, 2, 33, 21, 6, 4, 20, 33, -1, 14, 50, 13, 30, 21, 0, -1, 30, 0, 33, 9, 9, 15, 13, 0, 16, 16, -1, 35, -1, 19, 0, 22, 0, 0, 39, -1, 9, -1, -1, 0, 0, -1, 2, 1, 15, -1, 2, 2, 0, -1, 11, -1, 16, 4, -1, -1, 3, 29, 16, 3, 16, 15, 22, 12, 7, 31, 15, 3, 3, 7, 54, -1, 3, 3, 3, 8, 41, 17, 3, 1, -1, 34, 34, 1, 33, -1, 4, 4, 1, 1, 1, 5, 12, -1, -1, -1, 2, -1, 41, 2, 2, 8, 6, 32, 2, 25, 25, 2, 0, 0, 36, 45, 1, 36, 1, 47, 6, 7, 34, 23, 1, 9, 11, 39, 53, 11, -1, -1, 1, 26, 26, 9, 59, 3, -1, 18, 13, 9, 0, 44, 8, 31, 13, 20, 19, 32, 8, 30, 0, 4, -1, -1, -1, -1, 0, 17, 4, 4, 30, 0, 25, 20, 16, -1, 40, 17, 15, 16, 3, 3, 3, 16, 0, 14, 2, 18, 2, 44, 34, -1, 19, -1, 3, 31, -1, 50, 3, 3, 3, -1, -1, 16, 30, -1, 0, -1, 2, 2, 3, 5, -1, 7, 7, -1, 52, 2, 2, 3, -1, 20, 4, 25, 20, 20, -1, -1, 22]\n",
            "-------RUN94-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': \"CountVectorizer(ngram_range=(2, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df4a9008410>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[24, 32, 27, 27, -1, 49, -1, 20, 51, -1, -1, -1, -1, -1, 10, 19, 16, 24, -1, 19, -1, 11, -1, 28, 28, 9, 41, -1, 19, 40, -1, -1, 27, 30, 40, -1, 1, 35, 39, 40, 28, 19, 31, 28, 31, 6, 31, 31, 48, -1, 31, 28, -1, -1, -1, 17, -1, 32, 20, 20, 1, 20, 20, -1, 19, 19, -1, -1, 11, 3, 25, 11, 1, 1, 15, 3, 16, -1, 40, -1, 17, -1, -1, 17, -1, 9, 17, 1, 29, 30, 1, -1, 4, 45, 54, 7, 9, 1, -1, -1, 40, 15, -1, 37, -1, 25, 9, 39, 39, 39, 3, 11, -1, 21, 47, -1, 4, -1, -1, -1, -1, 4, 28, 15, 31, 7, 11, 29, 28, 11, -1, 10, 19, 11, 11, 24, 50, 16, 11, 31, -1, -1, -1, 7, 10, 20, 24, 49, -1, 0, -1, -1, -1, 54, -1, 20, 1, 21, -1, 2, -1, 2, 25, 2, 19, -1, -1, 15, -1, -1, -1, 54, -1, 0, 10, 20, 37, -1, 10, -1, 35, 16, 10, 0, 11, -1, 49, -1, 11, 11, 10, 10, 17, 29, -1, 4, 12, 12, 17, 18, 17, -1, 17, 42, -1, -1, -1, 17, 1, -1, 1, 17, 32, 46, 17, 17, 17, 1, 29, -1, 12, -1, 1, 1, 38, 12, 41, 41, -1, 0, 0, 0, 0, 12, 12, 0, 9, 0, -1, 12, 22, 3, 3, -1, 12, -1, 49, -1, 23, -1, -1, 12, 8, 21, -1, 6, 6, 40, 1, 1, 41, 15, 23, -1, -1, -1, 25, 41, 45, 3, 3, 3, -1, 1, 3, -1, 3, 3, 9, 9, 1, 35, 3, 6, 6, 3, 12, 36, 13, 36, 13, -1, 13, 13, 4, -1, -1, -1, 12, 3, 3, 3, 11, 3, -1, 3, 1, 3, 13, -1, 48, 3, 31, 35, -1, 51, -1, -1, 15, -1, 20, 7, -1, 11, 0, 24, -1, 24, -1, 24, -1, -1, 43, 24, 26, -1, 20, 10, 1, 10, 10, 9, 9, -1, -1, -1, 10, 10, 0, 10, -1, 50, -1, 46, 38, 0, -1, 43, 43, 24, 37, 11, 49, -1, -1, -1, 11, 16, 9, 11, 37, 0, 11, 25, -1, 11, 0, 38, 35, 0, 8, 8, 19, 38, 38, 38, -1, 38, -1, 9, 33, -1, -1, 8, 8, 8, 8, 8, 8, 22, 45, 8, 12, -1, 12, 53, 8, 1, 0, 0, 9, -1, 1, 15, 0, 9, 0, -1, 26, 36, 37, -1, 45, -1, -1, -1, 16, 26, 1, 1, 1, 16, 1, 3, 1, 51, 1, 1, 1, 1, 1, 1, 51, 26, -1, 3, -1, 3, 1, -1, 30, 3, 3, 3, 26, 32, 3, -1, 3, 3, 23, 3, 22, 32, 12, 3, 3, 3, 3, 1, 1, 6, -1, 1, -1, 1, 3, -1, 1, 1, -1, 36, 1, -1, 13, -1, 36, 6, 47, 6, 13, 15, -1, 9, 36, 13, 12, 40, 5, 5, 5, 0, 24, 1, -1, 12, 12, -1, 5, 42, 5, 24, 5, 5, 5, 5, 5, 15, 43, 5, 3, 5, 34, 43, 5, 14, 34, 5, -1, -1, 5, 5, 34, 0, 1, 1, -1, 34, 6, -1, 30, 6, 0, -1, 3, 1, 6, 37, 6, 6, 47, 31, 6, 0, 0, 0, 40, 0, 6, 6, 29, 13, 26, 0, 0, 0, 39, 0, 0, -1, 0, 34, 4, 14, 14, 14, 34, 34, 0, 0, 0, -1, 45, 0, 34, 0, 43, 25, -1, 0, 0, 9, -1, 9, 38, 0, 50, 0, 0, 9, 0, 0, 15, 0, 0, -1, 8, 0, 9, 9, 30, 45, 4, 4, 4, 4, 4, 4, 0, 4, 4, 1, 1, 13, -1, 7, 10, -1, 15, -1, 4, -1, 4, 4, 4, 1, 4, 4, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 3, 35, -1, 5, 5, 53, 5, 0, -1, 5, -1, 5, 19, 5, 24, 5, -1, 5, -1, 1, 28, -1, 54, -1, 5, 5, 5, -1, 5, -1, 5, 0, -1, 5, 24, -1, 6, 6, 6, 6, -1, 6, 6, -1, 1, 6, 45, 11, -1, 6, 46, 27, 1, 11, -1, 0, 0, 15, 16, -1, 5, 10, 0, -1, 27, 0, -1, 46, -1, 0, 16, 50, -1, 0, 0, 13, 46, 13, 0, 0, -1, 0, 0, -1, -1, 0, -1, 14, 0, -1, 14, 13, 0, 7, 25, 0, 14, -1, 1, 14, 14, 14, 47, -1, 28, 16, 0, 2, 2, 2, 25, 50, 16, 1, 0, 7, -1, -1, 13, 8, 1, 1, 0, -1, 44, 44, 44, 1, -1, -1, 3, 3, 4, 4, 1, 4, 4, 1, 4, 4, 4, 4, 1, 35, 2, 2, 2, 2, 2, 52, 0, 2, 4, 2, 2, 2, 2, 42, 2, 2, 4, 15, -1, 1, 2, 44, 15, 2, 2, 2, 1, -1, -1, 5, 5, -1, 5, 5, 5, 5, -1, 22, 22, -1, 22, 22, 22, 47, 22, 9, 22, 9, 4, 3, 50, 47, 31, 8, 28, -1, 8, 7, 7, 7, 7, 7, 52, 7, 20, 8, 50, 8, 8, 8, 8, 37, 48, 27, -1, 48, 7, 25, 25, 30, 52, -1, -1, 27, 27, -1, -1, 27, 27, 29, 27, -1, 27, -1, 10, 10, 10, 13, 10, -1, 10, 17, -1, 5, 17, 16, 10, -1, -1, 1, 13, 2, 29, 17, 13, 31, 1, 13, 13, 13, 8, 1, 0, 13, 46, -1, -1, 16, 51, -1, -1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, -1, 2, 44, 2, 2, 2, 12, 2, 44, 2, 5, 15, 48, 47, 34, 43, 6, -1, 17, 29, -1, 12, -1, 33, 19, 10, 30, 22, 0, 22, 52, 22, 0, -1, 32, 39, 23, -1, 23, 23, 28, 0, 32, 31, 33, -1, 33, 53, 30, 53, 25, 33, 32, 33, 48, 10, 33, 9, 33, -1, 16, -1, 28, 1, 8, 9, 0, 18, 18, -1, 31, -1, 9, 38, 11, -1, 8, -1, -1, 7, 21, 22, -1, 21, 8, 7, 7, 8, -1, -1, 7, 7, 7, 49, 7, 2, 7, 18, 42, 18, 18, 19, -1, 27, 27, -1, -1, 27, 52, -1, 39, 21, 24, -1, -1, 21, 39, 21, 21, 15, 3, 9, -1, 26, 26, 1, 53, 9, -1, 3, -1, 25, 11, 1, 1, -1, 40, 16, 20, 20, 1, 9, 5, 23, 24, 11, 2, 6, -1, 20, -1, 20, 16, 49, -1, 15, 42, 7, -1, -1, 30, 6, 6, 31, 1, -1, 24, 6, 43, 6, 15, 34, 34, -1, 14, 3, 2, 53, 23, 33, -1, 28, -1, 33, 48, 30, 3, -1, 17, 23, 23, 28, 15, 1, -1, -1, 1, 12, 11, -1, 1, 35, -1, -1, 13, -1, 10, 38, 3, 18, 18, 16, 18, -1, 18, 14, 23, 0, 2, -1, 25, -1, -1, 4, 54, 43, 21, 21, 21, 25, 0, 21, 4, 21, 21, 12, -1, 21, 21, 20, 20, 12, 11, 23, 23, 23, 23, 36, 17, -1, 5, 36, 0, 0, 0, -1, 32, 8, 0, 1, 0, 14, 8, 26, 26, 18, -1, 16, 42, 0, 18, 32, -1, 13, 18, -1, 42, 18, 24, -1, 16, 46, 2, 1, -1, 26, 0, 47, 6, 6, -1, 6, -1, 30, 6, 2, 28, 28, -1, 30, 1, -1, 6, 15, 32, 1, 0, 0, 2, 1, 1, 39, 0, 1, 0, 19, 1, 14, 2, -1, 30, 41, 3, -1, 10, 7, 36, 20, 4, -1, -1, -1, 29, -1, 19, 4, 20, 4, -1, 27, 36, 0, 42, 36, 0, 52, 7, 7, 0, 41, 31, -1, 7, 48, -1, 7, 7, 35, 29, 19, 3, 8, 7, 0, 0, 0, 2, 0, -1, 20, -1, 51, -1, 14, -1, 17, 0, 17, -1, 1, -1, 6, 0, 37, 0, 4, 19, 0, 3, 3, 3, 2, 3, 2, 2, 18, 6, 3, 2, 2, 29, 17, 6, 4, 18, 29, -1, 9, 2, 12, 26, 17, 1, -1, 26, 1, 29, 0, 0, 19, 12, 1, 14, 14, -1, 34, -1, 16, 1, 25, 1, 1, 37, -1, 0, 51, -1, 1, 1, -1, 2, -1, 19, -1, 2, 2, 1, -1, 10, -1, 14, 4, 13, -1, 0, -1, 14, 0, 14, 19, 25, 11, 7, 32, 19, 0, 0, 7, -1, -1, 0, 0, 0, 8, 35, 15, 0, 3, -1, 33, 33, -1, 29, -1, 4, 4, 3, 3, 3, 5, 11, -1, 52, 0, 2, -1, 35, 2, 2, 8, 6, 30, 2, 22, 22, 2, 1, 1, 41, 45, 3, 41, 3, 46, 6, 7, 33, 21, 3, 0, 10, 37, 49, 10, -1, 16, 3, 23, 23, 0, 13, 0, 2, -1, 12, 0, 1, 44, 8, 32, 12, 18, 16, 30, 8, 26, 1, 4, -1, -1, -1, -1, 1, 15, 4, 4, 26, 1, 22, 18, 14, -1, 42, 15, 19, 14, 0, 0, 0, 14, 1, 9, 2, 20, 2, 44, 33, 0, 16, -1, 0, 32, 0, 2, 0, 0, 0, -1, 0, 14, 26, -1, 1, -1, 2, 2, 0, 5, -1, 7, 7, 32, 29, 2, 2, 0, 0, 18, 4, 22, 18, 18, -1, 0, 25]\n",
            "Skipping coherence calculation due to error: unable to interpret topic as either a list of tokens or a list of ids\n",
            "-------RUN95-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': \"CountVectorizer(stop_words='english')\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[14, 0, 9, 9, 3, 13, 10, 10, 0, 10, 1, 4, 3, 4, 15, 13, 1, 14, 12, 13, 3, 3, 11, 12, 12, 4, 7, 13, 13, 15, 13, 0, 9, 4, 15, 10, 5, 10, 6, 15, 12, 13, 7, 12, 7, 7, 7, 7, 12, 0, 7, 12, 11, 11, 14, 8, 8, 0, 11, 11, 5, 11, 11, 4, 13, 13, 3, 8, 3, 3, 4, 3, 5, 5, 1, 3, 1, 1, 15, 0, 8, 13, 8, 8, 0, 4, 8, 5, 13, 4, 5, 4, 8, 14, 4, 17, 4, 5, 4, 10, 15, 1, 0, 1, 1, 4, 4, 6, 6, 6, 3, 3, 7, 14, 0, 8, 8, 4, 1, 0, 7, 8, 12, 1, 7, 17, 3, 13, 12, 3, 0, 15, 13, 3, 3, 14, 12, 1, 3, 7, 4, 11, 14, 17, 15, 11, 14, 13, 0, 16, 10, 14, 10, 4, 1, 11, 5, 14, 4, 2, 1, 2, 4, 2, 13, 0, 4, 1, 4, 13, 4, 4, 0, 6, 15, 11, 1, 0, 15, 3, 10, 1, 15, 6, 3, 11, 13, 0, 3, 3, 15, 15, 8, 13, 4, 8, 11, 11, 8, 8, 8, 13, 8, 10, 10, 4, 8, 8, 5, 3, 5, 8, 0, 0, 8, 8, 8, 5, 13, 4, 11, 0, 5, 5, 4, 11, 7, 7, 0, 6, 6, 6, 6, 11, 11, 6, 4, 6, 1, 11, 1, 3, 3, 1, 11, 1, 0, 1, 10, 13, 1, 11, 18, 14, 4, 7, 7, 15, 5, 5, 7, 1, 10, 11, 13, 0, 4, 7, 14, 3, 3, 3, 3, 5, 3, 15, 3, 3, 4, 7, 5, 10, 3, 7, 7, 3, 11, 2, 9, 2, 9, 0, 9, 9, 8, 7, 4, 15, 11, 3, 3, 3, 3, 3, 3, 3, 5, 3, 9, 10, 12, 3, 7, 10, 0, 15, 1, 1, 1, 11, 11, 17, 1, 3, 6, 14, 13, 14, 0, 14, 1, 1, 12, 14, 1, 14, 11, 15, 5, 15, 15, 4, 4, 4, 10, 1, 15, 15, 6, 15, 13, 12, 13, 0, 4, 16, 2, 12, 12, 14, 1, 3, 0, 7, 12, 7, 3, 0, 4, 3, 1, 16, 3, 4, 4, 3, 6, 4, 10, 6, 18, 18, 13, 4, 4, 4, 12, 4, 12, 4, 10, 4, 1, 18, 18, 18, 18, 18, 18, 1, 14, 18, 11, 0, 11, 0, 18, 5, 6, 6, 4, 12, 5, 1, 6, 4, 6, 1, 1, 2, 1, 7, 14, 0, 1, 1, 1, 1, 5, 5, 5, 1, 5, 3, 5, 0, 5, 5, 5, 5, 5, 5, 0, 1, 14, 3, 4, 3, 5, 4, 4, 3, 3, 3, 1, 0, 3, 0, 3, 3, 10, 3, 1, 0, 11, 3, 3, 3, 3, 5, 5, 7, 0, 5, 10, 5, 3, 7, 3, 5, 10, 2, 5, 4, 9, 8, 2, 7, 0, 7, 9, 1, 1, 4, 2, 9, 11, 15, 9, 9, 9, 16, 14, 5, 0, 11, 11, 3, 9, 11, 9, 14, 9, 9, 9, 9, 9, 1, 12, 9, 3, 9, 12, 12, 9, 19, 12, 9, 1, 1, 9, 9, 12, 16, 5, 5, 0, 12, 7, 4, 4, 7, 16, 3, 3, 5, 7, 1, 7, 7, 13, 7, 7, 16, 16, 16, 15, 16, 7, 7, 13, 9, 1, 6, 6, 6, 6, 6, 16, 9, 16, 12, 8, 19, 19, 19, 12, 12, 16, 16, 16, 12, 14, 16, 12, 16, 12, 4, 12, 6, 6, 4, 11, 4, 4, 6, 12, 6, 6, 4, 6, 6, 1, 6, 6, 11, 18, 6, 4, 4, 4, 14, 8, 8, 8, 8, 8, 8, 16, 8, 8, 5, 5, 9, 0, 17, 15, 0, 1, 11, 8, 7, 8, 8, 8, 5, 8, 8, 5, 5, 5, 5, 5, 5, 3, 5, 5, 5, 0, 3, 10, 10, 9, 9, 0, 9, 6, 0, 9, 0, 9, 13, 9, 14, 9, 0, 9, 12, 5, 12, 0, 4, 0, 9, 9, 9, 0, 9, 9, 9, 6, 1, 9, 14, 0, 7, 7, 7, 7, 11, 7, 7, 0, 5, 7, 14, 3, 7, 7, 0, 9, 5, 3, 7, 16, 16, 1, 0, 0, 9, 15, 16, 10, 9, 16, 0, 0, 7, 16, 1, 12, 0, 6, 6, 9, 0, 9, 6, 6, 0, 6, 6, 12, 1, 6, 2, 19, 6, 0, 19, 9, 6, 17, 4, 6, 19, 4, 5, 19, 19, 19, 0, 15, 12, 1, 6, 2, 2, 2, 4, 12, 0, 5, 6, 17, 13, 17, 9, 18, 5, 5, 6, 15, 2, 2, 2, 4, 0, 11, 3, 3, 8, 8, 5, 8, 8, 5, 8, 8, 8, 8, 5, 10, 2, 2, 2, 2, 2, 0, 6, 2, 8, 2, 2, 2, 2, 11, 2, 2, 8, 1, 0, 5, 2, 2, 1, 2, 2, 2, 5, 0, 0, 9, 9, 1, 9, 9, 9, 9, 8, 1, 1, 1, 1, 1, 1, 0, 1, 4, 1, 4, 8, 3, 12, 0, 7, 18, 12, 7, 18, 17, 17, 17, 17, 17, 0, 17, 11, 18, 12, 18, 18, 18, 18, 1, 12, 9, 13, 12, 17, 4, 4, 4, 0, 0, 0, 9, 9, 1, 0, 9, 9, 13, 9, 4, 9, 13, 15, 15, 15, 9, 15, 0, 15, 8, 4, 9, 8, 1, 15, 3, 1, 5, 9, 2, 13, 8, 9, 7, 5, 9, 9, 9, 18, 5, 6, 9, 0, 7, 3, 1, 0, 15, 4, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 2, 2, 2, 2, 2, 11, 2, 2, 2, 9, 1, 12, 0, 12, 12, 7, 0, 8, 13, 7, 11, 4, 10, 13, 15, 4, 1, 16, 1, 0, 1, 16, 12, 0, 1, 10, 4, 10, 10, 12, 16, 0, 7, 10, 10, 10, 0, 4, 0, 4, 10, 0, 10, 12, 15, 10, 4, 10, 12, 0, 11, 12, 5, 18, 4, 6, 2, 2, 10, 7, 13, 4, 4, 3, 4, 18, 0, 10, 17, 14, 1, 1, 14, 18, 17, 17, 18, 0, 3, 17, 17, 17, 13, 17, 2, 17, 2, 11, 2, 2, 13, 1, 9, 9, 10, 0, 9, 0, 0, 6, 14, 14, 3, 4, 14, 6, 14, 14, 1, 3, 4, 0, 1, 1, 5, 0, 7, 0, 3, 3, 4, 3, 5, 5, 0, 15, 0, 11, 11, 5, 4, 9, 10, 14, 3, 2, 7, 8, 11, 0, 11, 0, 13, 13, 1, 10, 17, 7, 7, 4, 7, 7, 7, 5, 13, 14, 7, 12, 7, 1, 12, 12, 10, 19, 3, 2, 0, 10, 10, 12, 12, 7, 10, 12, 4, 3, 10, 8, 10, 10, 12, 1, 5, 12, 0, 5, 11, 3, 1, 5, 10, 4, 13, 9, 11, 15, 4, 3, 2, 2, 0, 2, 11, 2, 19, 10, 6, 2, 0, 4, 0, 8, 8, 4, 12, 14, 14, 14, 4, 16, 14, 8, 14, 14, 11, 8, 14, 14, 11, 11, 11, 3, 10, 10, 10, 10, 2, 8, 10, 9, 2, 6, 16, 6, 0, 0, 18, 6, 5, 6, 19, 18, 1, 1, 2, 13, 0, 11, 6, 2, 0, 1, 9, 2, 10, 11, 2, 14, 1, 1, 0, 2, 5, 12, 1, 16, 0, 7, 7, 4, 7, 7, 4, 7, 2, 12, 12, 1, 4, 5, 12, 7, 1, 0, 5, 16, 16, 2, 5, 5, 6, 16, 5, 16, 13, 5, 19, 2, 1, 4, 7, 3, 8, 15, 17, 2, 11, 8, 7, 7, 8, 13, 10, 13, 8, 11, 8, 7, 9, 2, 6, 0, 2, 16, 0, 17, 17, 6, 7, 7, 0, 17, 12, 12, 17, 17, 10, 13, 13, 3, 18, 17, 6, 6, 6, 2, 16, 0, 11, 8, 0, 0, 19, 0, 8, 6, 8, 0, 5, 8, 7, 6, 1, 6, 8, 13, 6, 3, 3, 3, 2, 3, 2, 2, 2, 7, 3, 2, 2, 13, 8, 7, 8, 2, 13, 1, 4, 2, 11, 1, 8, 5, 7, 1, 5, 13, 16, 16, 13, 11, 5, 19, 19, 1, 12, 10, 0, 5, 4, 10, 5, 1, 0, 16, 0, 1, 5, 5, 8, 2, 3, 13, 15, 2, 2, 5, 0, 15, 10, 19, 8, 9, 1, 6, 7, 19, 6, 19, 13, 4, 3, 17, 0, 13, 6, 6, 17, 0, 3, 6, 6, 6, 18, 10, 1, 6, 3, 0, 10, 10, 3, 13, 0, 8, 8, 3, 3, 3, 9, 3, 0, 0, 6, 2, 8, 10, 2, 2, 18, 7, 4, 2, 1, 1, 2, 5, 5, 7, 14, 3, 7, 3, 0, 7, 17, 10, 14, 3, 16, 15, 1, 0, 15, 8, 0, 3, 10, 10, 16, 9, 6, 2, 11, 11, 16, 5, 2, 18, 0, 11, 2, 1, 4, 18, 1, 5, 8, 13, 7, 8, 10, 5, 1, 8, 8, 1, 5, 1, 2, 19, 11, 11, 1, 13, 19, 6, 6, 6, 19, 5, 4, 2, 11, 2, 2, 10, 16, 1, 0, 6, 0, 16, 2, 6, 6, 6, 1, 6, 19, 1, 17, 5, 0, 2, 2, 6, 9, 3, 17, 17, 0, 13, 2, 2, 6, 6, 2, 8, 1, 2, 2, 0, 16, 4]\n",
            "-------RUN96-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': 'CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7df3b00f5c90>)', 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[12, 0, 8, 8, 3, 0, 2, 2, 1, 1, 1, 9, 3, 9, 0, 0, 1, 12, 11, 0, 3, 3, 2, 11, 11, 9, 6, 0, 0, 2, 0, 0, 8, 9, 2, 2, 4, 2, 5, 2, 11, 0, 6, 11, 6, 6, 6, 6, 11, 0, 6, 11, 2, 2, 12, 7, 7, 0, 2, 2, 4, 2, 2, 9, 0, 0, 3, 9, 3, 3, 9, 3, 4, 4, 1, 3, 1, 15, 2, 0, 7, 0, 7, 7, 0, 9, 7, 4, 0, 9, 4, 2, 7, 12, 2, 16, 9, 4, 9, 2, 2, 1, 0, 1, 1, 9, 9, 5, 5, 5, 3, 3, 6, 12, 0, 7, 7, 2, 1, 1, 6, 7, 11, 1, 6, 16, 3, 0, 11, 3, 0, 0, 0, 3, 3, 12, 11, 1, 3, 6, 9, 2, 12, 16, 0, 2, 12, 0, 0, 14, 2, 2, 2, 2, 1, 2, 4, 12, 2, 10, 1, 10, 9, 10, 0, 0, 9, 1, 9, 0, 9, 2, 1, 5, 0, 2, 1, 0, 0, 3, 2, 1, 0, 5, 3, 2, 0, 0, 3, 3, 0, 0, 7, 0, 9, 7, 18, 18, 7, 7, 7, 0, 7, 1, 2, 9, 7, 7, 4, 3, 4, 7, 0, 0, 7, 7, 7, 4, 0, 9, 18, 1, 4, 4, 2, 18, 6, 6, 0, 5, 5, 5, 5, 18, 18, 5, 9, 5, 1, 18, 15, 3, 3, 1, 18, 1, 0, 1, 2, 0, 1, 18, 17, 12, 9, 6, 6, 2, 4, 4, 6, 1, 2, 2, 0, 0, 9, 6, 12, 3, 3, 3, 3, 4, 3, 0, 3, 3, 9, 6, 4, 2, 3, 6, 6, 3, 18, 13, 8, 13, 8, 0, 8, 8, 7, 6, 9, 0, 18, 3, 3, 3, 3, 3, 3, 3, 4, 3, 8, 2, 11, 3, 6, 2, 0, 1, 1, 1, 1, 2, 2, 16, 1, 3, 5, 12, 0, 12, 1, 12, 1, 1, 11, 12, 15, 12, 2, 0, 4, 0, 0, 9, 9, 2, 2, 1, 0, 0, 5, 0, 0, 11, 0, 0, 2, 14, 10, 11, 11, 12, 1, 3, 0, 6, 11, 6, 3, 1, 9, 3, 1, 14, 3, 9, 9, 3, 5, 2, 13, 5, 17, 17, 0, 2, 2, 2, 11, 2, 11, 9, 2, 9, 1, 17, 17, 17, 17, 17, 17, 15, 12, 17, 18, 1, 18, 1, 17, 4, 5, 5, 9, 11, 4, 1, 5, 9, 5, 1, 15, 13, 1, 6, 12, 1, 15, 1, 1, 15, 4, 4, 4, 1, 4, 3, 4, 1, 4, 4, 4, 4, 4, 4, 1, 15, 12, 3, 9, 3, 4, 9, 9, 3, 3, 3, 15, 0, 3, 1, 3, 3, 2, 3, 15, 0, 18, 3, 3, 3, 3, 4, 4, 6, 0, 4, 2, 4, 3, 6, 4, 4, 2, 13, 4, 2, 8, 7, 13, 6, 0, 6, 8, 1, 1, 9, 13, 8, 18, 2, 8, 8, 8, 14, 12, 4, 0, 18, 18, 3, 8, 1, 8, 12, 8, 8, 8, 8, 8, 1, 11, 8, 3, 8, 11, 11, 8, 19, 11, 8, 1, 1, 8, 8, 11, 14, 4, 4, 0, 11, 6, 9, 9, 6, 14, 3, 3, 4, 6, 1, 6, 6, 0, 6, 6, 14, 14, 14, 2, 14, 6, 6, 0, 8, 15, 5, 5, 5, 5, 5, 14, 8, 14, 11, 7, 19, 19, 19, 11, 11, 14, 14, 14, 11, 12, 14, 11, 14, 11, 9, 11, 5, 5, 9, 2, 9, 2, 5, 11, 5, 5, 9, 5, 5, 1, 5, 5, 2, 17, 5, 9, 9, 9, 12, 7, 7, 7, 7, 7, 7, 14, 7, 7, 4, 4, 8, 0, 16, 0, 0, 1, 11, 7, 7, 7, 7, 7, 4, 7, 7, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 1, 3, 13, 2, 8, 8, 1, 8, 5, 1, 8, 1, 8, 0, 8, 12, 8, 0, 8, 11, 4, 11, 1, 2, 0, 8, 8, 8, 0, 8, 8, 8, 5, 1, 8, 12, 1, 6, 6, 6, 6, 2, 6, 6, 1, 4, 6, 12, 3, 6, 6, 0, 8, 4, 3, 6, 14, 14, 1, 1, 0, 8, 0, 14, 2, 8, 14, 0, 0, 6, 14, 1, 11, 1, 5, 5, 8, 0, 8, 5, 5, 1, 5, 5, 11, 1, 5, 13, 19, 5, 1, 19, 8, 5, 16, 9, 5, 19, 2, 4, 19, 19, 19, 0, 2, 11, 1, 5, 10, 10, 10, 9, 11, 1, 4, 5, 16, 0, 16, 8, 17, 4, 4, 5, 0, 13, 13, 13, 9, 0, 2, 3, 3, 7, 7, 4, 7, 7, 4, 7, 7, 7, 7, 4, 2, 10, 10, 10, 10, 10, 0, 5, 10, 7, 10, 10, 10, 10, 1, 10, 10, 7, 1, 0, 4, 10, 13, 1, 10, 10, 10, 4, 1, 1, 8, 8, 1, 8, 8, 8, 8, 7, 15, 15, 1, 15, 15, 15, 0, 15, 9, 15, 9, 7, 3, 11, 0, 6, 17, 11, 6, 17, 16, 16, 16, 16, 16, 1, 16, 2, 17, 11, 17, 17, 17, 17, 1, 11, 8, 0, 11, 16, 16, 9, 9, 0, 1, 1, 8, 8, 1, 1, 8, 8, 0, 8, 9, 8, 0, 0, 0, 0, 8, 0, 0, 0, 7, 9, 8, 7, 1, 0, 3, 1, 4, 8, 10, 0, 7, 8, 6, 4, 8, 8, 8, 17, 4, 5, 8, 0, 6, 3, 1, 1, 2, 9, 4, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 5, 10, 1, 10, 13, 10, 10, 10, 18, 10, 13, 10, 8, 1, 11, 0, 11, 11, 6, 1, 7, 0, 6, 18, 9, 2, 0, 0, 9, 15, 14, 15, 1, 15, 14, 11, 0, 15, 2, 9, 2, 2, 11, 14, 0, 6, 2, 2, 2, 1, 9, 1, 9, 2, 0, 2, 11, 0, 2, 9, 2, 11, 1, 2, 11, 4, 17, 9, 5, 13, 13, 13, 6, 0, 9, 2, 3, 2, 17, 0, 2, 16, 12, 15, 1, 12, 17, 16, 16, 17, 0, 3, 16, 16, 16, 0, 16, 10, 16, 13, 1, 13, 13, 0, 1, 8, 8, 1, 1, 8, 0, 0, 5, 12, 12, 3, 2, 12, 5, 12, 12, 1, 3, 9, 1, 15, 15, 4, 1, 9, 1, 3, 3, 9, 3, 4, 4, 1, 2, 1, 2, 2, 4, 9, 8, 2, 12, 3, 10, 6, 7, 2, 0, 2, 1, 0, 0, 1, 1, 16, 6, 6, 9, 6, 6, 6, 4, 0, 12, 6, 11, 6, 1, 11, 11, 2, 19, 3, 10, 1, 2, 2, 11, 11, 6, 2, 11, 9, 3, 1, 7, 2, 2, 11, 1, 4, 11, 0, 4, 18, 3, 15, 4, 2, 9, 0, 8, 2, 0, 2, 3, 13, 13, 1, 13, 2, 13, 19, 2, 5, 10, 0, 9, 0, 7, 7, 2, 11, 12, 12, 12, 9, 14, 12, 7, 12, 12, 18, 7, 12, 12, 2, 2, 18, 3, 2, 2, 2, 2, 13, 7, 2, 8, 13, 5, 14, 5, 1, 0, 17, 5, 4, 5, 19, 17, 15, 15, 13, 0, 1, 1, 5, 13, 0, 2, 8, 13, 2, 1, 13, 12, 15, 1, 0, 10, 4, 11, 15, 14, 0, 6, 6, 9, 6, 6, 9, 6, 10, 11, 11, 15, 9, 4, 11, 6, 1, 0, 4, 14, 14, 10, 4, 4, 5, 14, 4, 14, 0, 4, 19, 10, 1, 9, 6, 3, 7, 0, 16, 13, 2, 7, 6, 6, 7, 0, 13, 0, 7, 2, 7, 6, 8, 13, 5, 1, 13, 14, 0, 16, 16, 5, 6, 6, 0, 16, 11, 11, 16, 16, 2, 0, 0, 3, 17, 16, 5, 5, 5, 10, 14, 1, 2, 7, 1, 1, 19, 0, 7, 14, 7, 0, 4, 7, 6, 5, 1, 5, 7, 0, 5, 3, 3, 3, 10, 3, 10, 10, 13, 6, 3, 10, 10, 0, 7, 6, 7, 13, 0, 1, 9, 10, 18, 15, 7, 4, 6, 15, 4, 0, 14, 14, 0, 18, 4, 19, 19, 1, 11, 2, 1, 4, 9, 4, 4, 1, 0, 14, 1, 1, 4, 4, 7, 10, 3, 0, 0, 10, 10, 4, 0, 0, 2, 19, 7, 8, 1, 5, 6, 19, 5, 19, 0, 9, 3, 16, 0, 0, 5, 5, 16, 1, 3, 5, 5, 5, 17, 13, 1, 5, 3, 0, 2, 2, 3, 0, 0, 7, 7, 3, 3, 3, 8, 3, 1, 1, 5, 10, 7, 2, 10, 10, 17, 6, 9, 10, 15, 15, 10, 4, 4, 6, 12, 3, 6, 3, 0, 6, 16, 2, 12, 3, 14, 0, 1, 0, 0, 7, 1, 3, 2, 2, 14, 8, 5, 10, 2, 18, 14, 4, 13, 17, 0, 18, 13, 1, 9, 17, 15, 4, 7, 0, 6, 7, 2, 4, 1, 7, 7, 15, 4, 15, 13, 19, 2, 1, 1, 0, 19, 5, 5, 5, 19, 4, 9, 10, 2, 10, 13, 2, 5, 1, 0, 5, 0, 14, 10, 5, 5, 5, 1, 5, 19, 15, 16, 4, 0, 10, 10, 5, 8, 3, 16, 16, 0, 0, 10, 10, 5, 5, 13, 7, 15, 13, 13, 1, 14, 9]\n",
            "-------RUN97-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': \"CountVectorizer(stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b02d2c50>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[13, 1, 9, 9, 3, 1, 11, 5, 11, 0, 0, 4, 3, 4, 11, 1, 0, 13, 12, 1, 3, 3, 5, 12, 12, 4, 7, 1, 1, 11, 1, 0, 9, 4, 11, 11, 2, 5, 6, 11, 12, 1, 7, 12, 7, 7, 7, 7, 12, 1, 7, 12, 5, 5, 13, 8, 8, 1, 5, 5, 2, 5, 5, 4, 1, 1, 3, 8, 3, 3, 4, 3, 2, 2, 0, 3, 0, 16, 11, 1, 8, 1, 8, 8, 1, 7, 8, 2, 1, 4, 2, 4, 8, 13, 4, 17, 4, 2, 4, 4, 11, 0, 1, 0, 0, 4, 4, 6, 6, 6, 3, 3, 7, 2, 1, 8, 8, 4, 0, 0, 7, 8, 12, 0, 7, 6, 3, 1, 12, 3, 1, 11, 1, 3, 3, 13, 12, 0, 3, 7, 4, 5, 13, 17, 11, 5, 13, 1, 1, 14, 11, 5, 11, 4, 0, 5, 2, 13, 4, 10, 0, 10, 4, 10, 1, 1, 4, 0, 4, 1, 4, 4, 0, 6, 11, 5, 0, 1, 11, 3, 5, 0, 11, 6, 3, 5, 1, 1, 3, 3, 11, 11, 8, 1, 4, 8, 5, 5, 8, 8, 8, 1, 8, 0, 5, 4, 8, 8, 2, 13, 2, 8, 1, 1, 8, 8, 8, 2, 1, 4, 5, 0, 2, 2, 4, 5, 7, 7, 1, 6, 6, 6, 6, 5, 5, 6, 4, 6, 0, 5, 16, 3, 3, 0, 5, 0, 1, 0, 5, 1, 0, 5, 18, 13, 4, 7, 7, 11, 2, 2, 7, 0, 5, 5, 1, 1, 4, 7, 13, 3, 3, 3, 3, 2, 3, 11, 3, 3, 4, 7, 2, 5, 3, 7, 7, 3, 5, 15, 9, 15, 9, 1, 9, 9, 8, 7, 4, 11, 5, 3, 3, 3, 3, 3, 3, 3, 2, 3, 9, 11, 12, 3, 7, 5, 1, 11, 0, 0, 0, 0, 5, 17, 0, 3, 6, 13, 1, 13, 0, 13, 0, 0, 12, 13, 16, 13, 5, 11, 2, 11, 11, 4, 4, 4, 5, 0, 11, 11, 6, 11, 1, 12, 1, 1, 4, 14, 10, 12, 12, 13, 0, 3, 1, 7, 12, 7, 3, 0, 4, 3, 0, 14, 3, 4, 4, 3, 6, 4, 5, 6, 18, 18, 1, 4, 4, 4, 12, 4, 12, 4, 11, 4, 0, 18, 18, 18, 18, 18, 18, 16, 13, 18, 5, 0, 5, 0, 18, 2, 6, 6, 4, 12, 2, 0, 6, 4, 6, 0, 16, 15, 0, 7, 13, 0, 16, 0, 0, 16, 2, 2, 2, 0, 2, 3, 2, 11, 2, 2, 2, 2, 2, 2, 11, 16, 13, 3, 4, 3, 2, 4, 4, 3, 3, 3, 16, 1, 3, 0, 3, 3, 5, 3, 16, 1, 5, 3, 3, 3, 3, 2, 2, 7, 1, 2, 5, 2, 3, 7, 2, 2, 5, 15, 2, 4, 9, 8, 15, 7, 1, 7, 9, 0, 0, 4, 15, 9, 5, 11, 9, 9, 9, 14, 13, 2, 1, 5, 5, 3, 9, 0, 9, 13, 9, 9, 9, 9, 9, 0, 12, 9, 3, 9, 12, 12, 9, 19, 12, 9, 0, 0, 9, 9, 12, 14, 2, 2, 1, 12, 7, 4, 4, 7, 14, 3, 3, 2, 7, 0, 7, 7, 1, 7, 7, 14, 14, 14, 11, 14, 7, 7, 1, 9, 16, 6, 6, 6, 6, 6, 14, 9, 14, 12, 8, 19, 19, 19, 12, 12, 14, 14, 14, 12, 13, 14, 12, 14, 12, 4, 12, 6, 6, 4, 5, 4, 4, 6, 12, 6, 6, 4, 6, 6, 0, 6, 6, 5, 18, 6, 4, 4, 4, 13, 8, 8, 8, 8, 8, 8, 14, 8, 8, 2, 2, 9, 1, 17, 11, 1, 0, 12, 8, 8, 8, 8, 8, 2, 8, 8, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 0, 3, 5, 11, 9, 9, 0, 9, 6, 0, 9, 0, 9, 1, 9, 13, 9, 1, 9, 12, 2, 12, 0, 4, 1, 9, 9, 9, 1, 9, 9, 9, 6, 0, 9, 13, 0, 7, 7, 7, 7, 5, 7, 7, 0, 2, 7, 13, 3, 7, 7, 1, 9, 2, 3, 7, 14, 14, 0, 0, 0, 9, 11, 14, 11, 9, 14, 0, 1, 7, 14, 0, 12, 0, 6, 6, 9, 1, 9, 6, 6, 0, 6, 6, 12, 0, 6, 15, 19, 6, 0, 19, 9, 6, 17, 4, 6, 19, 4, 2, 19, 19, 19, 1, 11, 12, 0, 6, 10, 10, 10, 4, 12, 0, 2, 6, 17, 1, 17, 9, 18, 2, 2, 6, 11, 15, 15, 15, 2, 1, 5, 3, 3, 8, 8, 2, 8, 8, 2, 8, 8, 8, 8, 2, 5, 10, 10, 10, 10, 10, 1, 6, 10, 8, 10, 10, 10, 10, 0, 10, 10, 8, 0, 1, 2, 10, 15, 0, 10, 10, 10, 2, 0, 0, 9, 9, 0, 9, 9, 9, 9, 8, 16, 16, 0, 16, 16, 16, 1, 16, 4, 16, 4, 8, 3, 12, 1, 7, 18, 12, 7, 18, 17, 17, 17, 17, 17, 0, 17, 5, 18, 12, 18, 18, 18, 18, 0, 12, 9, 1, 12, 17, 4, 4, 4, 1, 0, 0, 9, 9, 0, 0, 9, 9, 1, 9, 4, 9, 1, 11, 11, 11, 9, 11, 1, 11, 8, 4, 9, 8, 0, 11, 3, 0, 2, 9, 10, 1, 8, 9, 7, 2, 9, 9, 9, 18, 2, 6, 9, 1, 7, 3, 0, 11, 11, 4, 2, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 6, 10, 0, 10, 15, 10, 10, 10, 5, 10, 15, 10, 9, 0, 12, 1, 12, 12, 7, 0, 8, 1, 7, 5, 4, 11, 1, 11, 4, 16, 14, 16, 1, 16, 14, 12, 1, 16, 5, 4, 5, 5, 12, 14, 1, 7, 11, 4, 11, 0, 4, 0, 4, 11, 1, 11, 12, 11, 11, 4, 11, 12, 0, 5, 12, 2, 18, 4, 6, 15, 15, 15, 7, 1, 4, 4, 3, 4, 18, 0, 0, 17, 13, 16, 0, 13, 18, 17, 17, 18, 1, 3, 17, 17, 17, 1, 17, 10, 17, 15, 0, 15, 15, 1, 0, 9, 9, 0, 0, 9, 1, 1, 6, 13, 13, 3, 4, 13, 6, 13, 13, 0, 3, 4, 0, 16, 16, 2, 0, 4, 0, 3, 3, 4, 3, 2, 2, 0, 11, 0, 5, 5, 2, 4, 9, 5, 13, 3, 10, 7, 8, 5, 1, 5, 0, 1, 1, 0, 0, 17, 7, 7, 4, 7, 7, 7, 2, 1, 13, 7, 12, 7, 0, 12, 12, 5, 19, 3, 10, 0, 5, 11, 12, 12, 7, 11, 12, 4, 3, 0, 8, 0, 5, 12, 0, 2, 12, 1, 2, 5, 3, 16, 2, 5, 4, 1, 9, 5, 11, 4, 3, 15, 15, 0, 15, 5, 15, 19, 5, 6, 10, 1, 4, 1, 8, 8, 4, 12, 13, 13, 13, 4, 14, 13, 8, 13, 13, 5, 11, 13, 13, 5, 5, 5, 3, 5, 5, 5, 5, 15, 8, 5, 9, 15, 6, 14, 6, 0, 1, 18, 6, 2, 6, 19, 18, 16, 16, 15, 1, 0, 0, 6, 15, 1, 5, 9, 15, 5, 0, 15, 13, 0, 0, 1, 10, 2, 12, 16, 14, 1, 7, 7, 2, 7, 7, 4, 7, 10, 12, 12, 16, 4, 2, 12, 7, 0, 1, 2, 14, 14, 10, 2, 2, 6, 14, 2, 14, 1, 2, 19, 10, 0, 4, 7, 3, 8, 11, 17, 15, 5, 8, 7, 7, 8, 1, 5, 1, 8, 5, 8, 7, 9, 15, 6, 0, 15, 14, 1, 17, 17, 6, 7, 7, 1, 17, 12, 12, 17, 17, 5, 1, 1, 3, 18, 17, 6, 6, 6, 10, 14, 0, 5, 8, 11, 0, 19, 1, 8, 6, 8, 1, 2, 8, 7, 6, 0, 6, 8, 1, 6, 3, 3, 3, 10, 3, 10, 10, 15, 7, 3, 10, 10, 1, 8, 7, 8, 15, 1, 0, 7, 10, 5, 16, 8, 2, 7, 16, 2, 1, 14, 14, 1, 5, 2, 19, 19, 0, 12, 5, 0, 2, 4, 2, 2, 0, 1, 14, 11, 0, 2, 2, 8, 10, 3, 1, 11, 10, 10, 2, 0, 11, 0, 19, 8, 9, 0, 6, 7, 19, 6, 19, 1, 4, 3, 17, 1, 1, 6, 6, 17, 0, 3, 6, 6, 6, 18, 5, 0, 6, 3, 1, 11, 11, 3, 1, 1, 8, 8, 3, 3, 3, 9, 3, 0, 1, 6, 10, 8, 5, 10, 10, 18, 7, 4, 10, 16, 16, 10, 2, 2, 7, 13, 3, 7, 3, 1, 7, 17, 5, 13, 3, 14, 11, 0, 1, 11, 8, 0, 3, 5, 5, 14, 9, 6, 10, 5, 5, 14, 2, 15, 18, 1, 5, 15, 0, 4, 18, 16, 2, 8, 1, 7, 8, 5, 2, 0, 8, 8, 16, 2, 16, 15, 19, 5, 0, 0, 1, 19, 6, 6, 6, 19, 2, 4, 10, 5, 10, 15, 11, 6, 0, 1, 6, 1, 14, 10, 6, 6, 6, 0, 6, 19, 16, 6, 2, 1, 10, 10, 6, 9, 3, 17, 17, 1, 1, 10, 10, 6, 6, 15, 8, 16, 15, 15, 0, 14, 4]\n",
            "-------RUN98-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': \"CountVectorizer(ngram_range=(1, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b891dd10>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[14, 4, 8, 8, 3, 4, 7, 7, 13, 0, 0, 10, 3, 10, 13, 15, 0, 14, 12, 15, 3, 3, 7, 12, 12, 10, 6, 15, 15, 13, 15, 0, 8, 10, 13, 7, 5, 16, 1, 13, 12, 15, 6, 12, 6, 6, 6, 6, 12, 4, 6, 12, 7, 7, 14, 9, 9, 4, 7, 7, 5, 7, 7, 7, 15, 15, 3, 6, 3, 3, 10, 3, 5, 5, 0, 3, 0, 17, 13, 4, 9, 4, 9, 9, 4, 10, 9, 5, 4, 10, 5, 7, 9, 14, 7, 11, 10, 5, 10, 16, 13, 0, 4, 0, 0, 10, 10, 1, 1, 1, 3, 3, 6, 14, 4, 9, 9, 7, 0, 4, 6, 9, 12, 0, 6, 11, 3, 15, 12, 3, 4, 13, 15, 3, 3, 14, 12, 0, 3, 6, 10, 7, 14, 11, 13, 7, 14, 4, 4, 1, 7, 7, 7, 7, 0, 7, 5, 14, 7, 2, 0, 2, 10, 2, 15, 4, 10, 0, 10, 15, 7, 7, 0, 1, 13, 7, 0, 4, 13, 3, 16, 0, 13, 1, 3, 7, 15, 4, 3, 3, 13, 13, 9, 4, 10, 9, 18, 18, 9, 9, 9, 4, 9, 0, 7, 10, 9, 9, 5, 14, 5, 9, 4, 4, 9, 9, 9, 5, 15, 10, 18, 4, 5, 5, 7, 18, 6, 6, 4, 1, 1, 1, 1, 18, 18, 1, 10, 1, 0, 18, 17, 3, 3, 0, 18, 0, 0, 0, 16, 15, 0, 18, 11, 14, 10, 6, 6, 13, 5, 5, 6, 0, 16, 7, 15, 4, 10, 6, 14, 3, 3, 3, 3, 5, 3, 13, 3, 3, 10, 6, 5, 16, 3, 6, 6, 3, 18, 2, 8, 2, 8, 4, 8, 8, 9, 6, 10, 13, 18, 3, 3, 3, 3, 3, 3, 3, 5, 3, 8, 13, 12, 3, 6, 16, 4, 13, 0, 0, 0, 7, 7, 11, 0, 3, 1, 14, 15, 14, 4, 14, 0, 0, 12, 14, 17, 14, 7, 13, 5, 13, 13, 10, 10, 7, 7, 0, 13, 13, 1, 13, 15, 12, 15, 4, 7, 1, 2, 12, 12, 14, 0, 3, 4, 6, 12, 6, 3, 0, 10, 3, 0, 1, 3, 10, 7, 3, 1, 7, 16, 1, 11, 11, 15, 7, 7, 7, 12, 7, 12, 10, 7, 10, 0, 11, 11, 11, 11, 11, 11, 17, 14, 11, 18, 0, 18, 0, 11, 5, 1, 1, 10, 12, 5, 0, 1, 10, 1, 0, 17, 2, 0, 6, 14, 0, 17, 0, 0, 17, 5, 5, 5, 0, 5, 3, 5, 13, 5, 5, 5, 5, 5, 5, 13, 17, 14, 3, 10, 3, 5, 10, 10, 3, 3, 3, 17, 4, 3, 0, 3, 3, 16, 3, 17, 4, 18, 3, 3, 3, 3, 5, 5, 6, 4, 5, 16, 5, 3, 6, 16, 5, 16, 2, 5, 7, 8, 9, 2, 6, 4, 6, 8, 0, 0, 10, 2, 8, 18, 13, 8, 8, 8, 1, 14, 5, 4, 18, 18, 3, 8, 0, 8, 14, 8, 8, 8, 8, 8, 0, 12, 8, 3, 8, 12, 12, 8, 19, 12, 8, 0, 0, 8, 8, 12, 1, 5, 5, 4, 12, 6, 10, 10, 6, 1, 3, 3, 5, 6, 0, 6, 6, 4, 6, 6, 1, 1, 1, 13, 1, 6, 6, 15, 8, 17, 1, 1, 1, 1, 1, 1, 8, 1, 12, 9, 19, 19, 19, 12, 12, 1, 1, 1, 12, 14, 1, 12, 1, 12, 10, 12, 1, 1, 10, 7, 10, 7, 1, 12, 1, 1, 10, 1, 1, 0, 1, 1, 7, 11, 1, 10, 10, 10, 14, 9, 9, 9, 9, 9, 9, 1, 9, 9, 5, 5, 8, 4, 11, 13, 4, 0, 12, 9, 6, 9, 9, 9, 5, 9, 9, 5, 5, 5, 5, 5, 5, 3, 5, 5, 5, 0, 3, 16, 13, 8, 8, 0, 8, 1, 4, 8, 0, 8, 15, 8, 14, 8, 4, 8, 12, 5, 12, 0, 7, 4, 8, 8, 8, 4, 8, 8, 8, 1, 0, 8, 14, 0, 6, 6, 6, 6, 7, 6, 6, 0, 5, 6, 14, 3, 6, 6, 4, 8, 5, 3, 6, 1, 1, 0, 0, 4, 8, 13, 1, 7, 8, 1, 4, 4, 6, 1, 0, 12, 0, 1, 1, 8, 4, 8, 1, 1, 0, 1, 1, 12, 0, 1, 2, 19, 1, 4, 19, 8, 1, 11, 10, 1, 19, 7, 5, 19, 19, 19, 4, 13, 12, 0, 1, 2, 2, 2, 10, 12, 0, 5, 1, 11, 4, 11, 8, 11, 5, 5, 0, 13, 2, 2, 2, 5, 4, 7, 3, 3, 9, 9, 5, 9, 9, 5, 9, 9, 9, 9, 5, 16, 2, 2, 2, 2, 2, 4, 1, 2, 9, 2, 2, 2, 2, 0, 2, 2, 9, 0, 4, 5, 2, 2, 0, 2, 2, 2, 5, 0, 0, 8, 8, 0, 8, 8, 8, 8, 9, 17, 17, 0, 17, 17, 17, 4, 17, 10, 17, 10, 9, 3, 12, 4, 6, 11, 12, 6, 11, 11, 11, 11, 11, 11, 4, 11, 7, 11, 12, 11, 11, 11, 11, 0, 12, 8, 15, 12, 11, 10, 10, 10, 4, 0, 4, 8, 8, 0, 4, 8, 8, 15, 8, 10, 8, 15, 13, 13, 13, 8, 13, 4, 13, 9, 10, 8, 9, 0, 13, 3, 0, 5, 8, 2, 4, 9, 8, 6, 16, 8, 8, 8, 11, 5, 1, 8, 4, 6, 3, 0, 13, 13, 7, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 18, 2, 2, 2, 8, 0, 12, 4, 12, 12, 6, 0, 9, 4, 6, 18, 10, 7, 15, 13, 10, 17, 1, 17, 4, 17, 1, 12, 4, 0, 16, 10, 16, 16, 12, 1, 4, 6, 7, 7, 7, 0, 10, 0, 10, 7, 4, 7, 12, 13, 7, 10, 7, 7, 0, 7, 12, 5, 11, 10, 1, 2, 2, 16, 6, 15, 10, 7, 3, 10, 11, 4, 0, 11, 14, 17, 0, 14, 11, 11, 11, 11, 4, 3, 11, 11, 11, 4, 11, 2, 11, 2, 0, 2, 2, 15, 0, 8, 8, 16, 0, 8, 4, 4, 1, 14, 14, 3, 7, 14, 1, 14, 14, 0, 3, 10, 0, 17, 17, 5, 0, 6, 13, 3, 3, 10, 3, 5, 5, 0, 13, 0, 7, 7, 5, 10, 8, 16, 14, 3, 2, 6, 9, 7, 4, 7, 0, 4, 15, 0, 0, 11, 6, 6, 10, 6, 6, 6, 5, 4, 14, 6, 12, 6, 0, 12, 12, 7, 19, 3, 2, 0, 16, 7, 12, 12, 6, 7, 12, 10, 3, 16, 9, 16, 16, 12, 0, 5, 7, 4, 5, 18, 3, 0, 5, 16, 10, 15, 8, 0, 13, 7, 3, 2, 2, 0, 2, 0, 2, 19, 16, 1, 2, 4, 10, 4, 9, 9, 7, 12, 14, 14, 14, 10, 1, 14, 9, 14, 14, 18, 9, 14, 14, 7, 7, 18, 3, 16, 16, 16, 16, 2, 9, 16, 8, 2, 1, 1, 1, 4, 4, 11, 1, 5, 1, 19, 11, 17, 17, 2, 15, 0, 0, 1, 2, 4, 0, 8, 2, 16, 0, 2, 14, 0, 0, 4, 2, 5, 7, 17, 1, 4, 6, 6, 5, 6, 6, 10, 6, 2, 12, 12, 17, 10, 5, 12, 6, 0, 4, 5, 1, 1, 2, 5, 5, 1, 1, 5, 1, 15, 5, 19, 2, 0, 10, 6, 3, 9, 13, 11, 2, 7, 9, 6, 6, 9, 4, 16, 15, 9, 7, 9, 6, 8, 2, 1, 0, 2, 1, 4, 11, 11, 1, 6, 6, 4, 11, 12, 12, 11, 11, 16, 4, 15, 3, 11, 11, 1, 1, 1, 2, 1, 0, 7, 9, 13, 0, 19, 4, 9, 1, 9, 4, 5, 9, 6, 1, 0, 1, 9, 15, 1, 3, 3, 3, 2, 3, 2, 2, 2, 6, 3, 2, 2, 15, 9, 6, 9, 2, 15, 0, 10, 2, 18, 17, 9, 5, 6, 17, 5, 15, 1, 1, 15, 18, 5, 19, 19, 0, 12, 16, 0, 5, 10, 16, 5, 0, 4, 1, 13, 0, 5, 5, 9, 2, 3, 15, 4, 2, 2, 5, 4, 13, 16, 19, 9, 8, 0, 1, 6, 19, 1, 19, 15, 10, 3, 11, 4, 15, 1, 1, 11, 0, 3, 1, 1, 1, 11, 16, 0, 1, 3, 4, 7, 7, 3, 15, 4, 9, 9, 3, 3, 3, 8, 3, 4, 4, 1, 2, 9, 16, 2, 2, 11, 6, 10, 2, 17, 17, 2, 5, 5, 6, 14, 3, 6, 3, 4, 6, 11, 7, 14, 3, 1, 13, 0, 4, 13, 7, 0, 3, 16, 16, 1, 8, 1, 2, 7, 18, 1, 5, 2, 11, 4, 18, 2, 0, 10, 11, 17, 5, 9, 15, 6, 9, 16, 5, 0, 9, 9, 17, 5, 17, 2, 19, 7, 0, 0, 15, 19, 1, 1, 1, 19, 5, 10, 2, 7, 2, 2, 7, 1, 0, 13, 1, 4, 1, 2, 1, 1, 1, 0, 1, 19, 17, 11, 5, 4, 2, 2, 1, 8, 3, 11, 11, 4, 4, 2, 2, 1, 1, 2, 9, 17, 2, 2, 0, 1, 10]\n",
            "-------RUN99-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': \"UMAP(metric='cosine', min_dist=0.0, n_components=10)\", 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': \"CountVectorizer(ngram_range=(2, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df4a9008410>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[12, 1, 6, 6, 16, 1, 5, 5, 1, 2, 2, 7, 16, 7, 1, 11, 2, 12, 10, 11, 16, 16, 5, 10, 10, 7, 0, 11, 11, 5, 11, 1, 6, 7, 5, 5, 4, 2, 3, 5, 10, 11, 0, 10, 0, 0, 0, 0, 10, 1, 0, 10, 5, 5, 12, 0, 0, 1, 5, 5, 4, 5, 5, 5, 11, 11, 16, 0, 16, 8, 7, 16, 4, 4, 2, 8, 2, 14, 1, 1, 0, 11, 0, 0, 1, 7, 0, 4, 11, 7, 4, 5, 0, 12, 5, 15, 7, 4, 7, 5, 5, 2, 1, 2, 2, 7, 7, 3, 3, 3, 8, 16, 0, 12, 1, 0, 0, 5, 2, 1, 0, 0, 10, 2, 0, 15, 16, 11, 10, 16, 1, 1, 11, 16, 16, 12, 10, 2, 16, 0, 5, 5, 12, 15, 1, 5, 12, 1, 1, 3, 5, 12, 5, 5, 2, 5, 4, 12, 5, 9, 2, 9, 7, 9, 11, 1, 7, 2, 7, 11, 5, 5, 1, 3, 1, 5, 2, 1, 1, 16, 2, 2, 1, 3, 16, 5, 1, 1, 16, 16, 1, 1, 0, 11, 7, 0, 18, 18, 0, 0, 0, 11, 0, 2, 5, 7, 0, 0, 4, 8, 4, 0, 1, 1, 0, 0, 0, 4, 11, 7, 18, 1, 4, 4, 5, 18, 0, 0, 1, 3, 3, 3, 3, 18, 18, 3, 7, 3, 2, 18, 14, 8, 8, 2, 18, 2, 1, 2, 2, 11, 2, 18, 2, 12, 7, 0, 0, 5, 4, 4, 0, 2, 2, 2, 11, 1, 7, 0, 12, 8, 8, 8, 8, 4, 8, 1, 8, 8, 7, 7, 4, 2, 8, 0, 0, 8, 18, 13, 6, 13, 6, 1, 6, 6, 0, 0, 7, 1, 18, 8, 8, 8, 16, 8, 8, 8, 4, 8, 6, 5, 10, 8, 0, 2, 1, 1, 2, 2, 2, 2, 5, 15, 2, 16, 3, 12, 11, 12, 1, 12, 2, 2, 10, 12, 14, 12, 5, 1, 4, 1, 1, 7, 7, 5, 5, 2, 1, 1, 3, 1, 11, 10, 11, 1, 5, 3, 9, 10, 10, 12, 2, 16, 1, 0, 10, 0, 16, 2, 7, 16, 2, 3, 16, 7, 5, 16, 3, 5, 2, 3, 17, 17, 11, 5, 5, 5, 10, 5, 10, 7, 5, 7, 2, 17, 17, 17, 17, 17, 17, 14, 12, 17, 18, 2, 18, 1, 17, 4, 3, 3, 7, 10, 4, 2, 3, 7, 3, 2, 14, 13, 2, 0, 12, 1, 14, 2, 2, 14, 4, 4, 4, 2, 4, 8, 4, 1, 4, 4, 4, 4, 4, 4, 1, 14, 12, 8, 7, 8, 4, 7, 7, 8, 8, 8, 14, 1, 8, 1, 8, 8, 2, 8, 14, 1, 18, 8, 8, 8, 8, 4, 4, 0, 1, 4, 5, 4, 8, 0, 4, 4, 5, 13, 4, 5, 6, 0, 13, 0, 1, 0, 6, 2, 2, 7, 13, 6, 18, 5, 6, 6, 6, 3, 12, 4, 1, 18, 18, 16, 6, 2, 6, 12, 6, 6, 6, 6, 6, 2, 10, 6, 8, 6, 10, 10, 6, 19, 10, 6, 2, 2, 6, 6, 10, 3, 4, 4, 1, 10, 0, 7, 7, 0, 3, 16, 8, 4, 0, 2, 0, 0, 11, 0, 0, 3, 3, 3, 5, 3, 0, 0, 11, 6, 14, 3, 3, 3, 3, 3, 3, 6, 3, 10, 0, 19, 19, 19, 10, 10, 3, 3, 3, 10, 12, 3, 10, 3, 10, 7, 10, 3, 3, 7, 5, 7, 5, 3, 10, 3, 3, 7, 3, 3, 2, 3, 3, 5, 17, 3, 7, 7, 7, 12, 0, 0, 0, 0, 0, 0, 3, 0, 0, 4, 4, 6, 1, 15, 1, 1, 2, 5, 0, 0, 0, 0, 0, 4, 0, 0, 4, 4, 4, 4, 4, 4, 16, 4, 4, 4, 1, 8, 2, 5, 6, 6, 1, 6, 3, 1, 6, 1, 6, 11, 6, 12, 6, 1, 6, 10, 4, 10, 1, 5, 1, 6, 6, 6, 1, 6, 6, 6, 3, 2, 6, 12, 1, 0, 0, 0, 0, 2, 0, 0, 1, 4, 0, 12, 16, 0, 0, 1, 6, 4, 16, 0, 3, 3, 2, 2, 1, 6, 1, 3, 5, 6, 3, 1, 1, 0, 3, 2, 10, 1, 3, 3, 6, 1, 6, 3, 3, 1, 3, 3, 10, 2, 3, 13, 19, 3, 1, 19, 6, 3, 15, 7, 3, 19, 5, 4, 19, 19, 19, 1, 5, 10, 2, 3, 9, 9, 9, 7, 10, 2, 4, 3, 15, 1, 15, 6, 17, 4, 4, 3, 1, 13, 13, 13, 4, 1, 5, 8, 8, 0, 0, 4, 0, 0, 4, 0, 0, 0, 0, 4, 2, 9, 9, 9, 9, 9, 1, 3, 9, 0, 9, 9, 9, 9, 2, 9, 9, 0, 2, 1, 4, 9, 13, 2, 9, 9, 9, 4, 2, 2, 6, 6, 2, 6, 6, 6, 6, 0, 14, 14, 1, 14, 14, 14, 1, 14, 7, 14, 7, 0, 8, 10, 1, 0, 17, 10, 0, 17, 15, 15, 15, 15, 15, 1, 15, 5, 17, 10, 17, 17, 17, 17, 2, 10, 6, 11, 10, 15, 7, 7, 7, 1, 1, 1, 6, 6, 2, 1, 6, 6, 11, 6, 7, 6, 11, 1, 1, 1, 6, 1, 1, 1, 0, 7, 6, 0, 2, 1, 8, 2, 4, 6, 9, 11, 0, 6, 0, 4, 6, 6, 6, 17, 4, 3, 6, 1, 0, 8, 2, 1, 5, 5, 4, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 3, 9, 2, 9, 13, 9, 9, 9, 18, 9, 13, 9, 6, 2, 10, 1, 10, 10, 0, 1, 0, 11, 0, 18, 7, 5, 11, 1, 7, 14, 3, 14, 1, 14, 3, 10, 1, 14, 2, 7, 2, 2, 10, 3, 1, 0, 5, 5, 5, 1, 7, 1, 7, 5, 1, 5, 10, 1, 5, 7, 5, 10, 2, 5, 10, 4, 17, 7, 3, 13, 13, 2, 0, 11, 7, 5, 16, 7, 17, 1, 2, 15, 12, 14, 2, 12, 17, 15, 15, 17, 1, 8, 15, 15, 15, 1, 15, 9, 15, 13, 2, 13, 13, 11, 2, 6, 6, 2, 1, 6, 1, 1, 3, 12, 12, 8, 5, 12, 3, 12, 12, 2, 8, 7, 1, 14, 14, 4, 1, 7, 2, 8, 8, 7, 16, 4, 4, 2, 5, 2, 5, 5, 4, 7, 6, 2, 12, 16, 9, 0, 0, 5, 1, 5, 1, 1, 11, 2, 2, 15, 0, 0, 7, 0, 0, 0, 4, 11, 12, 0, 10, 0, 2, 10, 10, 5, 19, 8, 9, 1, 2, 5, 10, 10, 0, 5, 10, 7, 8, 2, 0, 2, 2, 10, 2, 4, 10, 1, 4, 18, 16, 14, 4, 2, 7, 11, 6, 2, 1, 5, 8, 13, 13, 2, 13, 2, 13, 19, 2, 3, 9, 1, 7, 1, 0, 0, 5, 10, 12, 12, 12, 7, 3, 12, 0, 12, 12, 18, 5, 12, 12, 5, 5, 18, 16, 2, 2, 2, 2, 13, 0, 5, 6, 13, 3, 3, 3, 1, 1, 17, 3, 4, 3, 19, 17, 14, 14, 13, 11, 2, 2, 3, 13, 1, 2, 6, 13, 4, 2, 13, 12, 2, 2, 1, 9, 4, 10, 14, 3, 1, 0, 0, 4, 0, 0, 7, 0, 9, 10, 10, 14, 7, 4, 10, 0, 2, 1, 4, 3, 3, 9, 4, 4, 3, 3, 4, 3, 11, 4, 19, 9, 2, 7, 0, 8, 0, 1, 15, 13, 5, 0, 0, 0, 0, 11, 2, 11, 0, 5, 0, 0, 6, 13, 3, 2, 13, 3, 1, 15, 15, 3, 0, 0, 1, 15, 10, 10, 15, 15, 2, 11, 11, 8, 17, 15, 3, 3, 3, 9, 3, 1, 5, 0, 1, 2, 19, 1, 0, 3, 0, 1, 4, 0, 0, 3, 2, 3, 0, 11, 3, 8, 8, 8, 9, 8, 9, 9, 13, 0, 8, 9, 9, 11, 0, 0, 0, 13, 11, 2, 7, 9, 18, 14, 0, 4, 0, 14, 4, 11, 3, 3, 11, 18, 4, 19, 19, 2, 10, 5, 2, 4, 7, 2, 4, 2, 1, 3, 1, 2, 4, 4, 0, 9, 8, 11, 7, 9, 9, 4, 1, 1, 2, 19, 0, 6, 2, 3, 0, 19, 3, 19, 11, 7, 16, 15, 1, 11, 3, 3, 15, 1, 16, 3, 3, 3, 17, 2, 2, 3, 8, 1, 5, 5, 8, 11, 1, 0, 0, 8, 8, 8, 6, 16, 1, 1, 3, 9, 0, 2, 9, 9, 17, 0, 7, 9, 14, 14, 9, 4, 4, 0, 12, 8, 0, 8, 1, 0, 15, 5, 12, 8, 3, 1, 2, 1, 1, 5, 2, 8, 2, 2, 3, 6, 3, 9, 5, 18, 3, 4, 13, 17, 1, 18, 13, 2, 7, 17, 14, 4, 0, 11, 0, 0, 5, 4, 2, 0, 0, 14, 4, 14, 13, 19, 2, 2, 2, 11, 19, 3, 3, 3, 19, 4, 7, 9, 5, 9, 13, 5, 3, 2, 1, 3, 1, 3, 9, 3, 3, 3, 2, 3, 19, 14, 15, 4, 1, 9, 9, 3, 6, 8, 15, 15, 1, 11, 9, 9, 3, 3, 13, 0, 14, 13, 13, 2, 3, 7]\n",
            "-------RUN100-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': \"CountVectorizer(stop_words='english')\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[27, 4, 28, 28, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 10, 12, 25, 27, -1, 12, -1, 13, 17, 8, 8, 20, 5, 12, 12, -1, 4, -1, 28, 33, -1, -1, 1, 19, -1, -1, 8, 12, 31, 8, 31, 5, 31, 31, 8, -1, 31, 8, -1, -1, -1, 6, -1, 4, 21, 21, 1, 21, 21, 17, 12, 12, -1, -1, 13, -1, 30, 13, 1, 1, 16, 3, 25, -1, -1, 4, 23, 4, -1, 23, 4, 20, 23, 1, 4, 33, 1, 17, 23, -1, 17, 11, 20, 1, -1, -1, -1, 16, -1, -1, -1, 30, -1, -1, -1, -1, -1, 13, -1, -1, 4, -1, 6, 17, -1, -1, 16, 6, 8, 16, 31, 0, 13, 4, 8, 13, 4, 10, 12, 13, 13, 27, 8, 25, 13, 31, -1, -1, -1, -1, 10, 21, 27, -1, 4, 0, -1, -1, -1, 17, -1, 21, 1, -1, -1, 2, -1, -1, 30, 2, 12, -1, -1, 16, -1, -1, -1, 17, -1, 0, 10, 21, -1, -1, 10, -1, 19, 25, 10, 0, 13, -1, -1, 4, 13, 13, 10, 10, 23, 4, -1, 6, 15, 15, 23, -1, 23, 4, 23, -1, 17, -1, 6, -1, 1, -1, 1, -1, 4, 4, 23, 23, 23, 1, 4, -1, 15, -1, 1, 1, 17, 15, 5, 5, -1, 0, 0, 0, 0, 15, 15, 0, 20, 0, -1, 15, -1, 3, 3, -1, 15, -1, -1, -1, 19, 12, -1, 15, 9, 32, -1, 5, 5, -1, 1, 1, 5, 16, -1, -1, -1, 4, 30, 5, -1, 3, 3, 3, -1, 1, 3, -1, 3, 3, -1, 20, 1, 19, 3, 5, 5, 13, 15, -1, 22, -1, 22, 4, 22, 22, 6, -1, -1, -1, 15, 3, 3, 3, 13, 3, -1, 3, 1, 3, 22, -1, 8, 3, 31, -1, -1, -1, -1, -1, 16, -1, 21, -1, -1, -1, 0, 27, 12, 27, 34, 27, -1, -1, 14, 27, 29, -1, 21, 10, 1, 10, 10, 20, 20, 17, -1, -1, 10, 10, 0, 10, -1, 8, -1, 4, 17, -1, -1, 14, 14, 27, 25, 13, -1, -1, 14, -1, 13, 25, 20, 13, -1, 0, 13, 30, 17, 13, 0, 17, 19, 0, 9, 9, 12, 17, 17, 17, 14, 17, 14, 20, -1, -1, -1, 9, 9, 9, 9, 9, 9, 26, -1, 9, 15, -1, 15, -1, 9, 1, 0, 0, 20, 14, 1, 16, 0, 20, 0, -1, 29, -1, -1, -1, -1, -1, -1, 16, 25, 29, 1, 1, 1, 25, 1, 3, 1, -1, 1, 1, 1, 1, 1, 1, -1, 29, -1, 3, -1, 3, 1, -1, 33, 3, 3, 3, 29, 4, 3, -1, 3, 3, 19, 3, 26, 4, 15, 3, 3, 3, 3, 1, 1, 5, -1, 1, 1, 1, 3, -1, -1, 1, -1, -1, 1, 17, -1, -1, -1, 5, 4, 5, 22, 16, -1, 20, -1, 22, 15, -1, 7, 7, 7, 0, 27, 1, -1, 15, 15, -1, 7, -1, 7, 27, 7, 7, 7, 7, 7, -1, 14, 7, 3, 7, 14, 14, 7, 18, 14, 7, -1, -1, 7, 7, 14, 0, 1, 1, -1, 14, 5, -1, 33, 5, 0, -1, 3, 1, 5, -1, 5, 5, 4, 31, 5, 0, 0, 0, -1, 0, 5, 5, 4, 22, 29, 0, 0, 0, -1, 0, 0, -1, 0, 14, -1, 18, 18, 18, 14, 14, 0, 0, 0, 8, -1, 0, 14, 0, 14, 30, 14, 0, 0, 20, -1, 20, 17, 0, 8, 0, 0, 20, 0, 0, 16, 0, 0, -1, 9, 0, 20, 20, 33, -1, 6, 6, 6, 6, 6, 6, 0, 6, 6, 1, 1, 22, -1, -1, 10, -1, 16, -1, 6, -1, 6, 6, 6, 1, 6, 6, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 3, -1, -1, 7, 7, -1, 7, 0, -1, 7, -1, 7, 12, 7, 27, 7, -1, 7, 8, 1, 8, 34, 17, -1, 7, 7, 7, -1, 7, 7, 7, 0, -1, 7, 27, -1, -1, 5, 5, 5, -1, 5, 5, -1, 1, 5, -1, 13, -1, 5, 4, 28, 1, 13, 5, 0, 0, 16, 25, -1, 7, 10, 0, -1, 28, 0, -1, 4, -1, 0, -1, 8, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 8, -1, 0, -1, 18, 0, 34, 18, 22, 0, -1, 30, 0, 18, 17, 1, 18, 18, 18, 4, -1, 8, 25, 0, 2, 2, 2, 30, 8, 25, 1, 0, 11, -1, 9, 22, 9, 1, 1, 0, -1, -1, -1, -1, -1, 4, -1, 3, 3, 6, 6, 1, 6, 6, 1, 6, 6, 6, 6, 1, 19, 2, 2, 2, 2, 2, 34, 0, 2, 6, 2, 2, 2, 2, -1, 2, 2, 6, 16, -1, 1, 2, -1, 16, 2, 2, 2, 1, -1, -1, 7, 7, -1, 7, 7, 7, 7, -1, 26, 26, -1, 26, 26, 26, 4, 26, 20, 26, 20, -1, 3, 8, 4, 31, 9, 8, -1, 9, 11, 11, 11, 11, 11, 34, 11, -1, 9, 8, 9, 9, 9, 9, 25, 8, 28, -1, 8, 11, 30, 30, 33, -1, -1, 34, 28, 28, 16, -1, 28, 28, 4, 28, -1, 28, -1, 10, 10, 10, 22, 10, 4, 10, 23, -1, 7, -1, 25, 10, -1, -1, 1, 22, 2, 4, 23, 22, 31, -1, 22, 22, 22, 9, 1, 0, 22, -1, -1, 3, -1, -1, -1, -1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, -1, 2, -1, 2, 2, 2, 15, 2, -1, 2, 7, 16, 8, 4, 14, 14, 5, 16, 23, 4, -1, 15, -1, 35, 12, 10, 33, 26, 0, 26, 34, 26, 0, 8, 4, -1, 19, -1, 19, 19, 8, 0, 4, 31, 35, -1, 35, -1, 33, -1, 30, 35, 4, 35, 8, 10, 35, -1, 35, 14, 25, -1, 8, 1, 9, -1, 0, 24, 24, -1, 31, 12, -1, 17, 13, -1, 9, -1, -1, 11, 32, 26, -1, 32, 9, 11, 11, 9, -1, -1, 11, 11, 11, -1, 11, 2, 11, 24, -1, 24, 24, 12, -1, 28, 28, -1, 34, 28, -1, 28, -1, 32, 27, -1, -1, 32, -1, 32, 32, 16, 3, 20, -1, 29, 29, 1, -1, -1, -1, 3, 3, 30, 13, 1, 1, -1, -1, -1, 21, 21, 1, 20, 7, -1, 27, 13, 2, 5, -1, 21, 4, 21, -1, -1, 12, 16, -1, -1, -1, 5, 33, 5, 5, 31, 1, 4, 27, 5, 14, 5, 16, 14, 14, -1, 18, 3, 2, -1, 19, 35, 8, 8, -1, 35, 8, 33, 3, -1, -1, 19, 19, 8, 16, 1, -1, -1, 1, 15, 13, -1, 1, 19, -1, -1, 22, -1, 10, 17, 3, 24, 24, 25, 24, -1, 24, 18, 19, 0, 2, -1, 30, -1, -1, 6, 17, 14, 32, 32, 32, 30, 0, 32, 6, 32, 32, 15, -1, 32, -1, 21, 21, 15, 13, 19, 19, 19, 19, -1, 23, -1, 7, -1, 0, 0, 0, 34, 4, 9, 0, 1, 0, 18, 9, 29, 29, 24, 12, -1, -1, 0, 24, 4, -1, -1, 24, -1, -1, 24, 27, -1, 25, 4, 2, 1, -1, 29, 0, 4, 5, 5, -1, 5, -1, 33, 5, 2, 8, 8, -1, 33, 1, 8, 5, 16, 4, 1, 0, 0, 2, 1, 1, -1, 0, 1, 0, 12, 1, 18, 2, -1, 33, 5, 3, 6, 10, 11, -1, 21, 6, -1, -1, 23, 4, -1, 12, 6, 21, 6, -1, 28, -1, 0, -1, -1, 0, 34, 11, 11, 0, 5, 31, 4, 11, 8, 8, 11, 11, -1, 4, 12, 3, 9, 11, 0, 0, 0, 2, 0, -1, 21, -1, -1, -1, 18, -1, 23, 0, 23, 4, 1, 0, 5, 0, -1, 0, 6, 12, 0, 3, 3, 3, 2, 3, 2, 2, 24, 5, 3, 2, 2, 4, 23, 5, 6, 24, 4, -1, 20, 2, 15, 29, 23, 1, -1, 29, 1, 4, 0, 0, 12, 15, 1, 18, 18, -1, 14, -1, -1, 1, 30, -1, 1, -1, 34, 0, -1, -1, 1, 1, 6, 2, -1, 12, -1, 2, 2, 1, -1, 10, -1, 18, 6, 22, -1, 0, -1, 18, 0, 18, 12, 30, 13, 11, 4, 12, 0, 0, 11, -1, -1, 0, 0, 0, 9, -1, 16, 0, 3, 4, 35, 35, 3, 4, -1, 6, 6, 3, 3, 3, 7, 13, 34, -1, 0, 2, -1, 19, 2, 2, 9, 5, -1, 2, 26, 26, 2, 1, 1, 5, -1, 3, 5, 3, 4, 5, 11, -1, -1, 3, 0, 10, -1, -1, 10, -1, -1, 3, 19, 19, 0, 22, 0, 0, 21, 15, 0, 1, -1, 9, 4, 15, 24, 25, -1, 9, 29, 1, 6, 12, -1, -1, -1, 1, 16, 6, 6, 29, 1, 26, 24, 18, 21, -1, 16, 12, 18, 0, 0, 0, 18, 1, 20, 2, 21, 2, -1, 35, 0, 25, -1, 0, 4, 0, 2, 0, 0, 0, -1, 0, 18, 29, 0, 1, -1, 2, 2, 0, 7, 3, 11, 11, 4, 4, 2, 2, 0, 0, 24, 6, 26, 24, 24, 19, 0, -1]\n",
            "-------RUN101-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': 'CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7df3b00f5c90>)', 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[26, 3, 31, 31, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 12, 19, 23, 26, -1, 19, -1, 16, -1, 8, 8, 22, 5, 19, 19, -1, -1, -1, 31, 35, -1, -1, -1, 10, -1, -1, 8, 19, 32, 8, 32, 5, 32, 32, 8, -1, -1, 8, -1, -1, -1, -1, -1, 3, 18, 18, -1, 18, 18, 15, 19, 19, -1, -1, 16, 4, 29, 16, 1, 1, 17, -1, 23, -1, -1, 3, 28, 3, -1, -1, 3, 22, 28, 1, 3, 35, 1, 15, 28, -1, 15, 13, -1, 1, -1, -1, -1, 17, -1, -1, -1, 29, -1, -1, -1, -1, -1, 16, -1, -1, 3, -1, 7, 15, 17, 24, 17, 7, 8, 17, 32, -1, 16, 3, 8, 16, 3, 12, 19, 16, 16, 26, 8, 23, 16, 32, -1, -1, -1, -1, 12, 18, 26, -1, 3, -1, -1, -1, -1, 15, -1, 18, 1, 27, -1, 2, -1, 2, 29, 2, 19, -1, -1, 17, -1, -1, 15, 15, 24, 0, 12, 18, -1, -1, 12, -1, 10, 23, 12, 0, 16, -1, -1, 3, 16, 16, 12, 12, 28, 3, -1, 7, 14, 14, 28, 7, 28, 3, -1, -1, -1, -1, 7, -1, 1, -1, 1, 28, 3, 3, -1, 28, 28, 1, 3, -1, 14, -1, 1, 1, 15, 14, -1, 5, -1, 0, 0, 0, 0, 14, 14, 0, 22, 0, -1, 14, -1, 4, 4, -1, 14, -1, -1, -1, 10, 19, -1, 14, 9, 26, -1, 5, 5, -1, 1, 1, 5, 17, 10, 18, -1, 3, 29, -1, -1, 4, 4, 4, -1, 1, 4, -1, 4, 4, -1, 22, 1, 10, 4, 5, 5, 4, 14, -1, 33, -1, 33, 3, 33, 33, 7, -1, -1, -1, 14, 4, 4, 4, 16, 4, -1, 4, 1, 4, -1, -1, 8, 4, 32, 10, -1, -1, -1, -1, 17, -1, 18, -1, -1, -1, 0, 26, -1, 26, 24, 26, -1, -1, 11, 26, 30, -1, 18, 12, 1, 12, 12, 22, -1, 15, -1, -1, 12, 12, 0, 12, -1, 8, -1, 3, 15, -1, 2, 11, 11, 26, -1, 16, -1, -1, 11, -1, 16, 23, 22, -1, -1, 0, 16, 29, 15, 16, 0, 15, 10, 0, 9, 9, 19, 15, 15, 15, 11, 15, 11, 22, 34, -1, -1, 9, 9, 9, 9, 9, 9, 25, -1, 9, 14, -1, 14, -1, 9, 1, 0, 0, 22, 11, 1, 17, 0, 22, 0, -1, 30, -1, -1, -1, -1, -1, 25, -1, 23, 30, 1, 1, 1, 23, 1, 4, 1, -1, 1, 1, 1, 1, 1, 1, -1, 30, -1, 4, -1, 4, 1, -1, 35, 4, 4, 4, 30, 3, 4, -1, 4, 4, 10, 4, 25, 3, 14, 4, 4, 4, 4, 1, 1, 5, -1, 1, -1, 1, 4, -1, -1, 1, -1, -1, 1, 15, -1, -1, -1, 5, 3, 5, -1, 17, -1, 22, -1, -1, 14, -1, 6, 6, 6, -1, 26, 1, -1, 14, 14, -1, 6, -1, 6, 26, 6, 6, 6, 6, 6, 17, 11, 6, 4, 6, 11, 11, 6, 20, 11, 6, -1, -1, 6, 6, 11, -1, 1, 1, 3, 11, 5, -1, 35, 5, 0, -1, 4, 1, 5, 23, 5, 5, 3, 32, 5, 0, 0, 0, -1, 0, 5, 5, 3, 33, 30, 0, 0, 0, -1, 0, 0, -1, 0, 11, 7, 20, 20, 20, 11, 11, 0, 0, 0, 8, -1, 0, 11, 0, 11, 29, 11, 0, 0, 22, 18, 22, 15, 0, 8, 0, 0, -1, 0, 0, 17, 0, 0, -1, 9, 0, 22, 22, 35, -1, 7, 7, 7, 7, 7, 7, 0, 7, 7, 1, 1, -1, 3, -1, 12, -1, 17, -1, 7, -1, 7, 7, -1, 1, 7, 7, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 4, 10, -1, 6, 6, -1, 6, 0, -1, 6, 24, 6, 19, 6, 26, 6, -1, 6, 8, 1, 8, 24, 15, -1, 6, 6, 6, -1, 6, 6, 6, 0, -1, 6, 26, -1, -1, 5, 5, 5, -1, 5, 5, -1, 1, 5, -1, 16, -1, 5, 3, 31, 1, 16, 5, 0, 0, 17, -1, -1, 6, 12, 0, -1, 31, 0, -1, -1, -1, 0, 23, 8, -1, 0, 0, -1, 3, -1, 0, 0, 24, 0, 0, 8, -1, 0, -1, 20, 0, 24, 20, 33, 0, -1, -1, 0, 20, 15, 1, 20, 20, 20, 3, -1, 8, 23, 0, 2, 2, 2, 29, 8, -1, 1, 0, 13, -1, -1, 33, 9, 1, 1, 0, -1, -1, -1, -1, -1, -1, 15, 4, 4, 7, 7, 1, 7, 7, 1, 7, 7, 7, 7, 1, 10, 2, 2, 2, 2, 2, 24, 0, 2, 7, 2, 2, 2, 2, -1, 2, 2, 7, 17, -1, 1, 2, -1, 17, 2, 2, 2, 1, -1, -1, 6, 6, -1, 6, 6, 6, 6, -1, 25, 25, -1, 25, 25, 25, 3, 25, 22, 25, 22, -1, 4, 8, 3, 32, 9, 8, -1, 9, 13, 13, 13, 13, 13, 24, 13, 18, 9, 8, 9, 9, 9, 9, 23, 8, 31, -1, 8, 13, 29, 29, 35, -1, -1, 24, 31, 31, 17, -1, 31, 31, 3, 31, -1, 31, -1, 12, 12, 12, 33, 12, 3, 12, 28, -1, 6, -1, 23, 12, -1, -1, 1, -1, 2, 3, 28, 33, 32, -1, 33, 33, 33, 9, 1, 0, 33, -1, -1, 4, 23, -1, -1, 15, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, -1, 2, -1, 2, 2, 2, 14, 2, -1, 2, 6, 17, 8, 3, 11, 11, -1, -1, -1, 3, -1, 14, -1, 34, 19, 12, 35, 25, 0, 25, 24, 25, 0, 8, 3, -1, 10, -1, 10, 10, 8, 0, 3, 32, 34, -1, 34, -1, 35, -1, 29, 34, 3, 34, 8, 12, 34, -1, 34, 11, 23, -1, 8, 1, 9, -1, 0, 21, 21, -1, 32, -1, -1, 15, 16, -1, 9, 24, -1, 13, 27, 25, -1, 27, 9, 13, 13, 9, -1, -1, 13, 13, 13, -1, 13, 2, 13, 21, -1, 21, 21, 19, -1, 31, 31, -1, 24, 31, 24, -1, -1, 27, 26, -1, 15, 27, -1, 27, 27, 17, 4, 22, 24, 30, 30, 1, -1, -1, -1, 4, 4, 29, 16, 1, 1, -1, -1, -1, 18, 18, 1, 22, 6, -1, 26, 16, 2, 5, -1, 18, 3, 18, -1, -1, 19, 17, -1, -1, -1, -1, 35, 5, 5, 32, 1, 3, 26, 5, 11, 5, 17, 11, 11, -1, 20, 4, 2, -1, 10, 34, 8, 8, -1, 34, 8, -1, 4, 10, -1, 10, 10, 8, 17, 1, 11, -1, 1, 14, 16, -1, 1, 10, -1, -1, -1, -1, 12, 15, 4, 21, 21, 23, 21, -1, 21, 20, 10, 0, 2, -1, 29, -1, -1, 7, 15, 11, 27, 27, 27, 29, 0, 27, 7, 27, 27, 14, -1, 27, 27, 18, 18, 14, 16, 10, 10, 10, 10, -1, 28, 10, 6, -1, 0, 0, 0, 24, 3, 9, 0, 1, 0, 20, 9, 30, 30, 21, 19, -1, -1, 0, 21, 3, -1, -1, 21, -1, -1, 21, 26, -1, 23, 3, 2, 1, 11, 30, 0, 3, 5, 5, -1, 5, -1, 35, 5, 2, 8, 8, -1, 35, 1, 8, 5, 17, 3, 1, 0, 0, 2, 1, 1, -1, 0, 1, 0, 19, 1, 20, 2, -1, 35, -1, 4, -1, 12, 13, -1, 18, 7, -1, -1, -1, 3, 10, 19, 7, 18, -1, -1, 31, -1, 0, -1, -1, 0, 24, 13, 13, 0, 5, 32, 3, 13, 8, 8, 13, 13, 10, 3, 19, 4, 9, 13, 0, 0, 0, 2, 0, -1, 18, -1, -1, -1, 20, -1, 28, 0, 28, 3, 1, 0, 5, 0, -1, 0, 7, -1, 0, 4, 4, 4, 2, -1, 2, 2, 21, 5, 4, 2, 2, 3, 28, 5, 7, 21, 3, -1, 22, 2, 14, 30, 28, 1, 5, 30, 1, 3, 0, 0, 19, 14, 1, 20, 20, -1, 11, -1, -1, 1, 29, -1, 1, 23, 24, 0, -1, -1, 1, 1, -1, 2, -1, -1, -1, 2, 2, 1, -1, 12, -1, 20, 7, 33, -1, 0, -1, 20, 0, 20, 19, 29, 16, 13, 3, 19, 0, 0, 13, -1, -1, 0, 0, 0, 9, 10, 17, 0, 4, 3, 34, 34, -1, 3, -1, 7, 7, 4, 4, 4, 6, 16, -1, -1, 0, 2, -1, 10, 2, 2, 9, 5, -1, 2, 25, 25, 2, 1, 1, 5, -1, 4, -1, 4, 3, 5, 13, -1, 27, 4, 0, 12, -1, -1, 12, 34, -1, 4, 10, 10, 0, -1, 0, -1, 18, 14, 0, 1, -1, 9, 3, 14, 21, 23, -1, 9, 30, 1, 7, 19, -1, -1, -1, 1, 17, 7, 7, 30, 1, 25, 21, 20, 18, -1, 17, 19, 20, 0, 0, 0, 20, 1, 22, 2, 18, 2, -1, 34, 0, 23, -1, 0, 3, 0, 2, 0, 0, 0, -1, 0, 20, 30, -1, 1, -1, 2, 2, 0, 6, -1, 13, 13, 3, 3, 2, 2, 0, 0, 21, 7, 25, 21, 21, 10, 0, 29]\n",
            "-------RUN102-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': \"CountVectorizer(stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b02d2c50>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[28, 3, 32, 32, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 13, 15, 21, 28, -1, 15, -1, 16, -1, 7, 7, 29, 22, 15, 15, -1, 15, -1, 32, 35, -1, -1, 1, 14, -1, -1, 7, 15, 22, 7, 22, 8, 22, 22, 7, -1, 22, 7, -1, -1, -1, -1, -1, 3, 24, 24, -1, 24, 24, 18, 15, 15, -1, -1, 16, -1, 30, 16, 1, 1, 20, -1, 21, -1, -1, -1, 23, 3, -1, 23, 3, 29, 23, 1, 3, 35, 1, 18, -1, -1, 18, 11, -1, 1, -1, -1, -1, 20, -1, -1, -1, 30, -1, -1, -1, -1, -1, 16, 22, 1, 3, -1, 5, 18, -1, -1, -1, 5, 7, 20, 22, 0, 16, 3, 7, 16, 3, 13, 15, 16, 16, 28, 7, 21, 16, 22, -1, -1, 25, -1, 13, 24, 28, -1, 3, 0, -1, -1, -1, 18, -1, 24, 1, 25, -1, 2, -1, -1, 30, 2, 15, -1, -1, 20, -1, -1, 18, 18, 33, 0, 13, 24, -1, -1, 13, -1, 14, 21, 13, 0, 16, -1, -1, 3, 16, 16, 13, 13, 23, 3, -1, 5, 17, 17, -1, -1, 23, 3, 23, -1, -1, -1, 5, 23, 1, -1, 1, 23, 3, 3, 23, 23, 23, 1, 3, -1, 17, -1, 1, 1, 18, 17, 22, 22, 3, 0, 0, 0, 0, 17, 17, 0, 29, 0, -1, 17, -1, 4, 4, -1, 17, -1, -1, -1, 14, 15, -1, 17, 9, 25, -1, 8, 8, -1, 1, 1, 22, 20, 14, -1, -1, 3, 30, 22, -1, 4, 4, 4, -1, 1, 4, -1, 4, 4, -1, -1, 1, 14, 4, 8, 8, 4, 17, 10, 27, 10, 27, 3, 27, 27, 5, -1, -1, -1, 17, 4, 4, 4, 16, 4, -1, 4, 1, 4, 27, -1, 7, 4, 22, 14, -1, -1, -1, -1, 20, -1, 24, -1, -1, -1, 0, 28, 15, 28, 33, 28, -1, -1, 12, 28, 31, -1, 24, 13, 1, 13, 13, -1, -1, 18, -1, -1, 13, 13, 0, 13, -1, 7, -1, 3, 18, -1, 2, 12, 12, 28, -1, 16, -1, -1, 12, -1, 16, 21, 29, 16, 21, 0, 16, 30, 18, 16, 0, 18, 14, 0, 9, 9, 15, 18, 18, 18, 12, 18, 12, 29, -1, -1, -1, 9, 9, 9, 9, 9, 9, 26, -1, 9, 17, -1, 17, -1, 9, 1, 0, 0, 29, 12, 1, 20, 0, 29, 0, 20, 31, 10, -1, -1, -1, -1, 26, -1, 21, 31, 1, 1, 1, 21, 1, 4, 1, -1, 1, 1, 1, 1, 1, 1, -1, 31, -1, 4, -1, 4, 1, -1, -1, 4, 4, 4, 31, 3, 4, -1, 4, 4, 14, 4, 26, 3, 17, 4, 4, 4, 4, 1, 1, 8, -1, 1, -1, 1, 4, -1, -1, 1, -1, 10, 1, 18, -1, -1, 10, 8, 3, 8, -1, 20, -1, 29, 10, -1, 17, -1, 6, 6, 6, -1, 28, 1, -1, 17, 17, -1, 6, -1, 6, 28, 6, 6, 6, 6, 6, 20, 12, 6, 4, 6, 12, 12, 6, 19, 12, 6, -1, -1, 6, 6, 12, -1, 1, 1, 3, 12, 8, -1, 35, 8, 0, -1, 4, 1, 8, 21, 8, 8, 3, 22, 8, 0, 0, 0, -1, 0, 8, 8, 3, 27, 31, 0, 0, 0, -1, 0, 0, -1, 0, 12, -1, 19, 19, 19, 12, 12, 0, 0, 0, 7, -1, 0, 12, 0, 12, 30, 12, 0, 0, 29, -1, 29, 18, 0, 7, 0, 0, -1, 0, 0, 20, 0, 0, -1, 9, 0, 29, -1, 35, -1, 5, 5, 5, 5, 5, 5, 0, 5, 5, 1, 1, -1, 3, -1, 13, -1, 20, 12, 5, -1, 5, 5, 5, 1, 5, 5, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 4, 14, -1, 6, 6, -1, 6, 0, 33, 6, -1, 6, 15, 6, 28, 6, -1, 6, 7, 1, 7, 33, 18, -1, 6, 6, 6, -1, 6, -1, 6, 0, -1, 6, 28, -1, -1, 8, 8, -1, -1, 8, 8, -1, 1, 8, -1, 16, -1, 8, 3, 32, 1, 16, -1, 0, 0, 20, 21, -1, 6, 13, 0, -1, 32, 0, -1, 3, -1, 0, 21, 7, -1, 0, 0, -1, 3, -1, 0, 0, 33, 0, 0, 7, -1, 0, -1, 19, 0, 33, 19, 27, 0, -1, 30, 0, 19, 18, 1, 19, 19, 19, 3, -1, 7, 21, 0, 2, 2, 2, 30, 7, 21, 1, 0, 11, -1, 11, 27, 9, 1, 1, 0, -1, -1, -1, -1, -1, -1, 18, 4, 4, 5, 5, 1, 5, 5, 1, 5, 5, 5, 5, 1, 14, 2, 2, 2, 2, 2, 3, 0, 2, 5, 2, 2, 2, 2, -1, 2, 2, 5, 20, -1, 1, 2, -1, 20, 2, 2, 2, 1, -1, -1, 6, 6, -1, 6, 6, 6, 6, 23, 26, 26, -1, 26, 26, 26, 3, 26, 29, 26, 29, 5, 4, 7, 3, 22, 9, 7, -1, 9, 11, 11, 11, 11, 11, 33, 11, 24, 9, 7, 9, 9, 9, 9, 21, 7, 32, -1, 7, 11, 30, 30, 35, -1, -1, 33, 32, 32, 20, -1, 32, 32, 3, 32, -1, 32, -1, 13, 13, 13, 27, 13, 3, 13, 23, -1, 6, -1, 21, 13, -1, 21, 1, 27, 2, 3, 23, 27, 22, 1, 27, 27, 27, 9, 1, 0, 27, 3, -1, -1, 21, -1, -1, 18, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, -1, 2, -1, 2, 2, 2, 17, 2, -1, 2, 6, 20, 7, 3, 12, 12, -1, -1, 23, 3, -1, 17, -1, 34, 15, 13, 35, 26, 0, 26, 33, 26, 0, 7, 3, -1, 14, -1, 14, 14, 7, 0, 3, 22, 34, -1, 34, -1, 35, -1, 30, 34, 3, 34, 7, 13, 34, -1, 34, 12, 21, -1, 7, 1, 9, -1, 0, 10, 10, -1, -1, -1, -1, 18, 16, -1, 9, -1, -1, 11, 25, 26, -1, 25, 9, 11, 11, 9, -1, -1, 11, 11, 11, -1, 11, 2, 11, 10, -1, 10, 10, 15, -1, 32, 32, -1, 33, 32, -1, -1, -1, 25, 28, -1, -1, 25, -1, 25, 25, 20, 4, 29, 33, 31, 31, 1, -1, -1, -1, 4, 4, 30, 16, 1, 1, -1, -1, 21, 24, 24, 1, 29, 6, -1, 28, 16, 2, 8, -1, 24, 3, 24, -1, -1, 15, 20, -1, -1, -1, 22, 35, 8, 8, 22, 1, 3, 28, 8, 12, 8, 20, 12, 12, -1, 19, 4, 2, -1, 14, 34, 7, 7, -1, 34, 7, 35, 4, -1, -1, 14, 14, 7, 20, 1, 12, -1, 1, 17, 16, -1, 1, 14, -1, -1, -1, -1, 13, 18, 4, 10, 10, 21, 10, -1, 10, 19, 14, 0, 2, -1, 30, -1, -1, 5, 18, 12, 25, 25, 25, 30, 0, 25, 5, 25, 25, 17, -1, 25, 25, 24, 24, 17, 16, 14, 14, 14, 14, 10, 23, -1, 6, 10, 0, 0, 0, 33, 3, 9, 0, 1, 0, 19, 9, 31, 31, 10, 15, -1, -1, 0, 10, 3, -1, -1, 10, -1, -1, 10, 28, -1, 21, 3, 2, 1, 12, 31, 0, 3, 8, 8, -1, 8, -1, 35, 8, 2, 7, 7, -1, 35, 1, 7, 8, 20, 3, 1, 0, 0, 2, 1, 1, -1, 0, 1, 0, 15, 1, 19, 2, -1, 35, 22, 4, -1, 13, 11, 10, 24, 5, -1, -1, 23, 3, 14, 15, 5, 24, -1, -1, 32, 10, 0, -1, 10, 0, 33, 11, 11, 0, -1, 22, 3, 11, 7, 7, 11, 11, 14, 3, 15, 4, 9, 11, 0, 0, 0, 2, 0, -1, 24, -1, -1, -1, 19, -1, 23, 0, 23, 3, 1, -1, 8, 0, -1, 0, 5, 15, 0, 4, 4, 4, 2, 4, 2, 2, 10, 8, 4, 2, 2, 3, 23, 8, 5, 10, 3, -1, 29, 2, 17, 31, 23, 1, -1, 31, 1, 3, 0, 0, 15, 17, 1, 19, 19, -1, 12, -1, 21, 1, 30, -1, 1, -1, 33, 0, -1, -1, 1, 1, -1, 2, -1, -1, -1, 2, 2, 1, -1, 13, -1, 19, 5, 27, -1, 0, -1, 19, 0, 19, 15, 30, 16, 11, 3, 15, 0, 0, 11, -1, -1, 0, 0, 0, 9, 14, -1, 0, 4, 3, 34, 34, -1, 3, -1, 5, 5, 4, 4, 4, 6, 16, -1, 33, 0, 2, -1, 14, 2, 2, 9, 8, -1, 2, 26, 26, 2, 1, 1, 22, -1, 4, 22, 4, 3, 8, 11, -1, 25, 4, 0, 13, -1, -1, 13, 34, -1, 4, 14, 14, 0, 27, 0, 0, 24, 17, 0, 1, -1, 9, 3, 17, 10, 21, -1, 9, 31, 1, 5, 15, -1, -1, 1, 1, 20, 5, 5, 31, 1, 26, 10, 19, -1, -1, 20, 15, 19, 0, 0, 0, 19, 1, 29, 2, 24, 2, -1, 34, 0, 21, -1, 0, 3, 0, 2, 0, 0, 0, -1, 0, 19, 31, -1, 1, 3, 2, 2, 0, 6, 4, 11, 11, 3, 3, 2, 2, 0, 0, 10, 5, 26, 10, 10, -1, 0, -1]\n",
            "-------RUN103-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': \"CountVectorizer(ngram_range=(1, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b891dd10>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[27, 4, 31, 31, -1, -1, -1, 19, -1, -1, -1, -1, -1, -1, 13, 18, 21, 27, -1, 18, -1, 14, -1, 8, 8, 23, -1, 18, 18, -1, -1, -1, 31, 36, -1, -1, 0, 16, -1, -1, 8, 18, 33, 8, 33, 7, 33, 33, 8, -1, 33, 8, -1, -1, -1, 25, -1, 4, 19, 19, -1, 19, 19, 20, 18, 18, -1, -1, 14, -1, 29, 14, 0, 0, 22, 3, 21, -1, -1, -1, 25, 4, -1, -1, 4, 23, 25, 0, 4, 36, 0, 20, -1, -1, 20, 10, -1, 0, -1, -1, -1, 22, -1, -1, -1, 29, -1, -1, -1, -1, -1, 14, -1, -1, 4, -1, 5, 20, -1, -1, -1, 5, 8, 22, 33, -1, 14, 4, 8, 14, 4, 13, 18, 14, 14, 27, 8, 21, 14, 33, -1, -1, -1, -1, 13, 19, 27, -1, 4, -1, -1, -1, -1, 20, -1, 19, 0, 28, -1, 1, -1, 1, 29, 1, 18, -1, -1, 22, -1, 4, -1, 20, 32, -1, 13, 19, -1, -1, 13, -1, 16, 21, 13, -1, 14, -1, -1, 4, 14, 14, 13, 13, 25, 4, -1, 5, 15, 15, 25, -1, 25, 4, 25, -1, 20, -1, 5, -1, 0, -1, 0, 25, 4, 4, 25, 25, 25, 0, 4, -1, 15, -1, 0, 0, 20, 15, -1, -1, 4, 2, -1, -1, -1, 15, 15, -1, 23, -1, -1, 15, -1, 3, 3, -1, 15, -1, -1, -1, 16, 18, -1, 15, 9, -1, -1, 7, 7, -1, 0, 0, 7, 22, 16, -1, -1, 4, 29, -1, -1, 3, 3, 3, -1, 0, 3, -1, 3, 3, 23, -1, 0, 16, 3, 7, 7, 14, 15, 35, -1, 35, 34, 4, 34, 34, 5, -1, -1, -1, 15, 3, 3, 3, 14, 3, -1, 3, -1, 3, -1, -1, 8, 3, 33, -1, -1, -1, -1, -1, 22, -1, 19, -1, -1, -1, 2, 27, -1, 27, 32, 27, -1, -1, 12, 27, 30, -1, 19, 13, 0, 13, 13, 23, 23, 20, -1, -1, 13, 13, -1, 13, -1, 8, -1, 4, 20, -1, 1, 12, 12, 27, 21, 14, -1, -1, 12, -1, 14, 21, 23, 14, -1, -1, 14, 29, -1, 14, -1, 20, 16, -1, 9, 9, 18, 20, 20, 20, 12, 20, 12, 23, -1, -1, -1, 9, 9, 9, 9, 9, 9, 26, -1, 9, 15, -1, 15, -1, 9, 0, -1, -1, 23, 12, 0, 22, -1, 23, -1, -1, 30, 35, -1, -1, -1, -1, -1, 22, 21, 30, 0, 0, 0, 21, 0, 3, 0, -1, 0, 0, 0, 0, 0, 0, -1, 30, -1, 3, -1, 3, 0, -1, 36, 3, 3, 3, 30, 4, 3, -1, 3, 3, 16, 3, 26, 4, 15, 3, 3, 3, 3, 0, 0, 7, -1, 0, -1, 0, 3, -1, -1, 0, -1, 35, 0, 20, -1, -1, 35, 7, 4, 7, -1, 22, -1, 23, 35, -1, 15, -1, 6, 6, 6, -1, 27, 0, -1, 15, 15, -1, 6, -1, 6, 27, 6, 6, 6, 6, 6, 22, 12, 6, 3, 6, 12, 12, 6, 17, 12, 6, -1, -1, 6, -1, 12, -1, 0, 0, -1, 12, 7, -1, 36, 7, 11, -1, 3, 0, 7, -1, 7, 7, 4, 33, 7, 11, 11, 11, -1, -1, 7, 7, 4, 34, 30, 2, 2, 2, -1, 2, 11, -1, 11, 12, 5, 17, 17, 17, 12, 12, 11, 11, 11, 8, -1, 11, 12, 11, 12, 29, 12, -1, -1, 23, -1, 23, 20, 2, 8, 2, 2, 23, -1, 2, 22, 2, 2, -1, 9, 2, 23, -1, 36, -1, 5, 5, 5, 5, 5, 5, 11, 5, 5, 0, 0, -1, 4, -1, 13, -1, 22, -1, 5, -1, 5, 5, -1, 0, 5, 5, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 3, -1, -1, 6, 6, -1, 6, 2, -1, 6, -1, 6, 18, 6, 27, 6, -1, 6, 8, 0, 8, -1, 20, -1, 6, 6, 6, -1, 6, -1, 6, -1, 21, 6, 27, -1, -1, 7, 7, 7, -1, -1, 7, -1, 0, 7, -1, 14, -1, 7, -1, 31, 0, 14, -1, 11, 11, 22, 21, -1, -1, 13, -1, -1, 31, -1, -1, 4, -1, 11, 21, 8, -1, 2, 2, -1, -1, -1, 2, 2, 32, 2, 2, 8, -1, 2, -1, 17, 2, 32, 17, 34, 2, 10, 29, 2, 17, 20, 0, 17, 17, 17, 4, -1, 8, 21, -1, 1, 1, 1, 29, 8, 21, 0, 2, 10, -1, 10, 34, 9, 0, 0, -1, -1, -1, -1, -1, -1, -1, 19, 3, 3, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 0, -1, 1, 1, 1, 1, 1, 32, -1, 1, 5, 1, 1, 1, 1, -1, 1, 1, 5, 22, -1, 0, 1, -1, 22, 1, 1, 1, 0, -1, -1, 6, 6, -1, 6, 6, 6, 6, -1, 26, 26, -1, 26, 26, 26, 4, 26, 23, 26, 23, -1, 3, 8, 4, 33, 9, 8, -1, 9, 10, 10, 10, 10, 10, 32, 10, -1, 9, 8, 9, 9, 9, 9, -1, 8, 31, -1, 8, 10, -1, 29, 36, -1, -1, 32, 31, 31, 22, -1, 31, 31, 4, 31, -1, 31, -1, 13, 13, 13, 34, 13, -1, 13, 25, -1, 6, -1, 21, 13, -1, 21, 0, -1, 1, 4, 25, 34, 33, -1, 34, 34, 34, 9, 0, -1, 34, -1, -1, 3, 21, -1, -1, 20, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, -1, 1, -1, 1, 1, 1, 15, 1, -1, 1, 6, -1, 8, 4, 12, 12, -1, -1, -1, 4, -1, 15, -1, 37, 18, 13, 36, 26, 11, 26, 32, 26, 11, 8, 4, -1, 16, -1, 16, 16, 8, 11, 4, 33, 37, -1, 37, -1, 36, -1, 29, 37, 4, 37, 8, 13, 37, -1, 37, 12, 21, -1, 8, 0, 9, -1, 2, 24, 24, -1, -1, -1, -1, 20, 14, -1, 9, -1, -1, 10, 28, 26, -1, 28, 9, 10, 10, 9, -1, -1, 10, 10, 10, -1, 10, 1, 10, 24, -1, 24, 24, 18, -1, 31, 31, -1, -1, 31, 32, -1, -1, 28, 27, -1, -1, 28, -1, 28, 28, 22, 3, 23, 32, 30, 30, 0, -1, -1, -1, 3, 3, 29, 14, 0, 0, -1, -1, -1, 19, 19, 0, 23, 6, 16, 27, 14, 1, 7, -1, 19, 4, 19, -1, -1, -1, 22, -1, -1, -1, -1, 36, -1, 7, 33, 0, 4, 27, 7, 12, 7, 22, 12, 12, -1, 17, 3, 1, -1, 16, 37, 8, 8, -1, 37, 8, 36, 3, 16, -1, 16, 16, 8, 22, 0, 12, -1, 0, 15, 14, -1, 0, 16, -1, -1, -1, -1, 13, 20, 3, 24, 24, 21, 24, -1, 24, 17, 16, -1, 1, -1, 29, -1, -1, 5, 20, 12, 28, 28, 28, 29, -1, 28, 5, 28, 28, 15, -1, 28, 28, 19, 19, 15, 14, 16, 16, 16, 16, 35, 25, 16, 6, 35, -1, -1, 2, 32, 4, 9, 2, 0, 2, 17, 9, 30, 30, 24, 18, -1, -1, 2, 24, 4, -1, -1, 24, -1, -1, 24, 27, -1, 21, 4, 1, 0, 12, 30, -1, 4, 7, 7, -1, 7, -1, 36, 7, 1, 8, 8, -1, -1, 0, 8, 7, 22, 4, 0, 11, 11, 1, 0, 0, -1, 11, 0, 11, 18, 0, 17, 1, -1, 36, -1, 3, -1, 13, 10, 35, 19, 5, -1, -1, -1, 4, -1, 18, 5, 19, -1, -1, 31, 35, 2, -1, 35, 11, 32, 10, 10, 2, -1, 33, 4, 10, 8, 8, 10, 10, -1, 4, 18, 3, 9, 10, 2, 2, 2, 1, -1, -1, 19, -1, -1, -1, 17, -1, 25, 2, 25, 4, 0, -1, 7, 2, -1, 2, 5, -1, 2, 3, 3, 3, 1, 3, 1, 1, 24, 7, 3, 1, 1, 4, 25, 7, 5, 24, 4, -1, 23, 1, 15, 30, 25, 0, -1, 30, 0, 4, 11, 11, 18, 15, 0, 17, 17, -1, 12, -1, 21, 0, 29, -1, 0, 21, 32, 11, -1, -1, 0, 0, -1, 1, -1, 18, -1, 1, 1, 0, -1, 13, -1, 17, 5, 34, -1, 2, -1, 17, 2, 17, 18, 29, 14, 10, 4, 18, 2, 2, 10, -1, -1, 2, 2, 2, 9, -1, -1, 2, 3, 4, 37, 37, 3, 4, -1, 5, 5, 3, 3, 3, 6, 14, -1, 32, 2, 1, -1, 16, 1, 1, 9, 7, -1, 1, 26, 26, 1, 0, 0, 7, -1, 3, -1, 3, 4, 7, 10, -1, 28, 3, 11, 13, -1, -1, 13, -1, -1, 3, 16, 16, 11, -1, 2, -1, 19, 15, 11, 0, -1, 9, 4, 15, 24, 21, -1, 9, 30, 0, 5, 18, -1, -1, -1, 0, 22, 5, 5, 30, 0, 26, 24, 17, 19, -1, 22, 18, 17, 2, 2, 2, 17, 0, 23, 1, 19, 1, 35, 37, 2, 21, -1, 2, 4, 11, 1, 2, 2, 2, -1, 2, 17, 30, -1, 0, -1, 1, 1, 2, 6, -1, 10, 10, 4, 4, 1, 1, 2, -1, 24, 5, 26, 24, 24, -1, -1, 29]\n",
            "-------RUN104-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': \"CountVectorizer(ngram_range=(2, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df4a9008410>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[24, 3, 28, 28, -1, -1, -1, 17, -1, -1, -1, -1, -1, -1, 13, 22, 21, 24, 8, 22, -1, 14, 16, 8, 8, 31, 23, -1, 22, -1, -1, -1, 28, 32, -1, -1, -1, 10, -1, -1, 8, 22, 23, 8, 23, 7, 23, 23, 8, -1, 23, 8, -1, -1, -1, -1, -1, 3, 17, 17, 1, 17, 17, 16, 22, 22, -1, -1, 14, -1, 29, 14, 1, 1, 18, -1, 21, -1, -1, 3, 26, 3, -1, -1, 3, 31, 26, 1, 3, 32, 1, 16, -1, -1, 16, 12, -1, 1, -1, -1, -1, 18, -1, -1, -1, 29, -1, -1, -1, -1, -1, 14, -1, -1, 3, -1, 5, 16, -1, -1, 18, 5, 8, 18, 23, 0, 14, 3, 8, 14, 3, 13, 22, 14, 14, 24, 8, 21, 14, 23, -1, -1, 27, -1, 13, 17, 24, -1, 3, -1, -1, -1, -1, 16, -1, 17, 1, 27, -1, 2, -1, 2, 29, 2, 22, -1, -1, 18, -1, -1, 16, 16, -1, 0, 13, 17, -1, -1, 13, -1, 10, 21, 13, 0, 14, -1, -1, 3, 14, 14, 13, 13, 26, 3, -1, 5, 15, 15, 26, 20, 26, 3, 26, -1, 16, -1, 5, -1, 1, -1, 1, 26, 3, 3, -1, 26, 26, 1, 3, -1, 15, -1, 1, 1, 16, 15, 23, 23, -1, 0, 0, 0, 0, 15, 15, 0, 31, 0, -1, 15, -1, 4, 4, -1, 15, -1, -1, -1, 10, 22, -1, 15, 9, 24, -1, 7, 7, -1, 1, 1, 23, 18, 10, 17, -1, 3, 29, -1, -1, 4, 4, 4, -1, 1, 4, -1, 4, 4, 31, -1, 1, 10, 4, 7, 7, -1, 15, 34, -1, 34, 6, 3, 35, 35, 5, -1, -1, -1, 15, 4, 4, 4, 14, 4, -1, 4, 1, 4, -1, -1, 8, 4, 23, -1, -1, -1, -1, -1, 18, -1, 17, -1, -1, -1, 0, 24, -1, 24, 36, 24, -1, -1, 11, 24, 30, -1, 17, 13, 1, 13, 13, -1, -1, 16, -1, -1, 13, 13, 0, 13, -1, 8, -1, 3, 16, -1, 2, 11, 11, 24, -1, 14, -1, -1, 11, -1, 14, 21, 31, 14, -1, 0, 14, 29, -1, 14, 0, 16, 10, 0, 9, 9, 22, 16, 16, 16, 11, 16, 11, 31, -1, -1, -1, 9, 9, 9, 9, 9, 9, 25, -1, 9, 15, -1, 15, -1, 9, 1, 0, 0, 31, 11, 1, 18, 0, 31, 0, -1, 30, 34, -1, -1, -1, -1, -1, 18, 21, 30, 1, 1, 1, 21, 1, 4, 1, -1, 1, 1, 1, 1, 1, 1, -1, 30, -1, 4, -1, 4, 1, -1, 32, 4, 4, 4, 30, 3, 4, -1, 4, 4, 10, 4, 25, 3, 15, 4, 4, 4, 4, 1, 1, 7, -1, 1, -1, 1, 4, -1, -1, 1, -1, 34, 1, 16, -1, -1, 34, 7, 3, 7, -1, 18, -1, 31, 34, -1, 15, -1, 6, 6, 6, -1, 24, 1, -1, 15, 15, -1, 6, -1, 6, 24, 6, 6, 6, 6, 6, 18, 11, 6, 4, 6, 11, 11, 6, 19, 11, 6, -1, -1, 6, 6, 11, -1, 1, 1, -1, 11, 7, -1, 32, 7, 0, -1, 4, 1, 7, -1, 7, 7, 3, 23, 7, 0, 0, 0, -1, 0, 7, 7, 3, 35, 30, 0, 0, 0, -1, 0, 0, -1, 0, 11, 5, 19, 19, 19, 11, 11, 0, 0, 0, 8, -1, 0, 11, 0, 11, 29, 11, 0, 0, 31, 17, 31, 16, 0, 8, 0, 0, -1, 0, 0, 18, 0, 0, -1, 9, 0, 31, -1, 32, -1, 5, 5, 5, 5, 5, 5, 0, 5, 5, 1, 1, -1, -1, -1, 13, -1, 18, -1, 5, -1, 5, 5, 5, 1, 5, 5, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 4, 10, -1, 6, 6, -1, 6, 0, -1, 6, -1, 6, 22, 6, 24, 6, -1, 6, 8, 1, 8, 36, 16, -1, 6, 6, 6, -1, 6, 6, 6, 0, 21, 6, 24, -1, 7, 7, 7, 7, -1, 7, 7, -1, 1, 7, -1, 14, -1, 7, -1, 28, 1, 14, -1, 0, 0, 18, 21, 36, 6, 13, 0, -1, 28, 0, -1, -1, -1, 0, 21, 8, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 8, -1, 0, -1, 19, 0, -1, 19, 35, 0, -1, 29, 0, 19, 16, 1, 19, 19, 19, 3, -1, 8, 21, 0, 2, 2, 2, 29, 8, 21, 1, 0, 12, -1, -1, 35, 9, 1, 1, 0, -1, -1, -1, -1, -1, -1, 16, 4, 4, 5, 5, 1, 5, 5, 1, 5, 5, 5, 5, 1, 10, 2, 2, 2, 2, 2, 36, 0, 2, 5, 2, 2, 2, 2, -1, 2, 2, 5, 18, -1, 1, 2, -1, 18, 2, 2, 2, 1, -1, -1, 6, 6, -1, 6, 6, 6, 6, -1, 25, 25, -1, 25, 25, 25, 3, 25, -1, 25, 31, 5, 4, 8, 3, 23, 9, 8, -1, 9, 12, 12, 12, 12, 12, 36, 12, 17, 9, 8, 9, 9, 9, 9, -1, 8, 28, -1, 8, 12, 29, 29, 32, -1, -1, -1, 28, 28, -1, -1, 28, 28, 3, 28, -1, 28, -1, 13, 13, 13, -1, 13, 3, 13, 26, -1, 6, -1, 21, 13, -1, 21, 1, -1, 2, 3, 26, 35, 23, -1, 35, 35, 35, 9, 1, 0, 35, -1, -1, -1, 21, -1, -1, 16, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, -1, 2, -1, 2, 2, 2, 15, 2, -1, 2, 6, 18, 8, 3, 11, 11, -1, -1, 26, 3, -1, 15, -1, 33, 22, 13, 32, 25, 0, 25, 36, 25, 0, 8, 3, -1, 10, -1, 10, 10, 8, 0, 3, 23, 33, -1, 33, -1, 32, -1, 29, 33, 3, 33, 8, 13, 33, -1, 33, 11, 21, -1, 8, 1, 9, -1, 0, 20, 20, -1, -1, -1, -1, 16, 14, -1, 9, 36, -1, 12, 27, 25, -1, 27, 9, 12, 12, 9, -1, -1, 12, 12, 12, -1, 12, 2, 12, 20, -1, 20, 20, 22, -1, 28, 28, -1, -1, 28, -1, 28, -1, 27, 24, -1, -1, 27, -1, 27, 27, 18, 4, 31, 36, 30, 30, 1, -1, -1, -1, 4, -1, 29, 14, 1, 1, -1, -1, 21, 17, 17, 1, 31, 6, 10, 24, 14, 2, 7, -1, 17, 3, 17, -1, -1, -1, 18, -1, -1, -1, -1, 32, 7, 7, 23, 1, 3, 24, 7, 11, 7, 18, 11, 11, -1, 19, 4, 2, -1, 10, 33, 8, 8, -1, 33, 8, 32, 4, 10, -1, 10, 10, 8, 18, 1, 11, -1, 1, 15, -1, -1, 1, 10, -1, -1, -1, -1, 13, 16, 4, 20, 20, 21, 20, -1, 20, 19, 10, 0, 2, -1, 29, -1, -1, 5, 16, 11, 27, 27, 27, 29, 0, 27, 5, 27, 27, 15, -1, 27, 27, 17, 17, 15, 14, 10, 10, 10, 10, 34, 26, 10, 6, -1, 0, 0, 0, -1, 3, 9, 0, 1, 0, 19, 9, 30, 30, 20, 22, -1, -1, 0, 20, 3, -1, -1, 20, -1, -1, 20, 24, -1, 21, 3, 2, 1, 11, 30, 0, 3, 7, 7, -1, 7, -1, 32, 7, 2, 8, 8, -1, 32, 1, 8, 7, 18, 3, 1, 0, 0, 2, 1, 1, -1, 0, 1, 0, 22, 1, 19, 2, -1, 32, 23, 4, 5, 13, 12, 34, 17, 5, -1, -1, -1, 3, 10, 22, 5, 17, 5, -1, 28, 34, 0, -1, 34, 0, 36, 12, 12, 0, -1, 23, 3, 12, 8, 8, 12, 12, 10, 3, 22, 4, 9, 12, 0, 0, 0, 2, 0, -1, 17, -1, -1, -1, 19, -1, 26, 0, 26, 3, 1, -1, 7, 0, -1, 0, 5, -1, 0, 4, 4, 4, 2, -1, 2, 2, 20, 7, 4, 2, 2, 3, 26, 7, 5, 20, 3, -1, -1, 2, 15, 30, 26, 1, -1, 30, 1, 3, 0, 0, 22, 15, 1, 19, 19, -1, 11, -1, 21, 1, 29, -1, 1, -1, 36, 0, -1, -1, 1, 1, 5, 2, 14, -1, -1, 2, 2, 1, -1, 13, -1, 19, 5, -1, -1, 0, -1, 19, 0, 19, 22, 29, 14, 12, 3, 22, 0, 0, 12, -1, -1, 0, 0, 0, 9, 10, 18, 0, 4, 3, 33, 33, -1, 3, -1, 5, 5, 4, 4, 4, 6, 14, -1, -1, 0, 2, -1, 10, 2, 2, 9, 7, -1, 2, 25, 25, 2, 1, 1, 23, -1, 4, -1, 4, 3, 7, 12, -1, 27, 4, 0, 13, -1, -1, 13, 33, -1, 4, 10, 10, 0, -1, 0, 0, 17, 15, 0, 1, -1, 9, 3, 15, 20, 21, -1, 9, 30, 1, 5, -1, -1, -1, -1, 1, -1, 5, 5, 30, 1, 25, 20, 19, -1, -1, 18, 22, 19, 0, 0, 0, 19, 1, 31, 2, 17, 2, -1, 33, 0, -1, -1, 0, 3, 0, 2, 0, 0, 0, -1, 0, 19, 30, 0, 1, -1, 2, 2, 0, 6, -1, 12, 12, 3, 3, 2, 2, 0, 0, 20, 5, 25, 20, 20, -1, 0, -1]\n",
            "-------RUN105-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': \"CountVectorizer(stop_words='english')\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[27, 4, 29, 29, -1, 10, -1, -1, -1, -1, -1, -1, -1, -1, 13, 10, 21, 27, -1, 10, -1, 17, -1, 8, 8, 23, -1, 10, 10, -1, -1, -1, 29, 33, -1, -1, 1, 16, -1, -1, 8, 10, 25, 8, 25, 7, 25, 25, 8, -1, -1, 8, -1, -1, -1, -1, -1, 4, 15, 15, -1, 15, 15, 20, 10, 10, -1, -1, 3, -1, 30, 17, 1, 1, 19, 3, 21, -1, -1, -1, 24, 4, -1, -1, 4, 23, 24, 1, 4, 33, 1, 20, -1, -1, 20, 12, -1, 1, -1, -1, -1, 19, -1, -1, -1, 30, -1, -1, -1, -1, -1, 17, -1, -1, 4, -1, 5, 20, -1, -1, -1, 5, 8, 19, 25, -1, 17, 4, 8, 17, 4, 13, 10, 17, 17, 27, -1, 21, 17, 25, -1, -1, -1, -1, 13, 15, 27, -1, 4, 0, -1, -1, -1, 20, -1, 15, 1, 32, -1, 2, -1, -1, 30, 2, 10, -1, -1, 19, -1, -1, -1, 20, -1, 0, 13, 15, -1, -1, 13, -1, 16, 21, 13, 0, 17, -1, -1, 4, 17, 17, 13, 13, 24, 4, -1, 5, 14, 14, 24, 5, 24, 4, -1, -1, -1, -1, -1, 24, 1, -1, 1, -1, 4, 4, -1, 24, 24, 1, 4, -1, 14, -1, 1, 1, 20, 14, 25, 25, -1, 0, 0, 0, 0, 14, 14, 0, 23, 0, -1, 14, 26, 3, 3, -1, 14, -1, -1, -1, 16, 10, -1, 14, 9, 32, -1, 7, 7, -1, 1, 1, 25, 19, 16, 15, -1, -1, 30, -1, -1, 3, 3, 3, -1, 1, 3, -1, 3, 3, 23, 23, 1, 16, 3, 7, 7, 3, 14, 2, 31, 2, 31, 4, 31, 31, 5, -1, -1, -1, 14, 3, 3, 3, 17, 3, -1, 3, 1, 3, -1, -1, 8, 3, 25, -1, -1, -1, -1, -1, 19, -1, 15, -1, -1, 3, 0, 27, 10, 27, 35, 27, -1, -1, 11, 27, 28, -1, 15, 13, 1, 13, 13, -1, -1, 20, -1, -1, 13, 13, 0, 13, -1, 11, 10, -1, 20, -1, -1, 11, 11, 27, -1, 17, -1, -1, 11, -1, 17, 21, 23, -1, 19, 0, 17, 30, -1, 17, 0, 20, 16, 0, 9, 9, 10, 20, 20, 20, 11, 20, 11, 23, -1, -1, -1, 9, 9, 9, 9, 9, 9, 26, -1, 9, 14, -1, 14, -1, 9, 1, 0, 0, 23, 11, 1, 19, 0, 23, 0, -1, 28, -1, -1, -1, -1, -1, -1, -1, 21, 28, 1, 1, 1, 21, 1, 3, 1, -1, 1, 1, 1, 1, 1, 1, -1, 28, -1, 3, -1, 3, 1, -1, 33, 3, 3, 3, 28, 4, 3, -1, 3, 3, 16, 3, 26, 4, 14, 3, 3, 3, 3, 1, 1, 7, -1, 1, -1, 1, 3, -1, -1, 1, -1, 2, 1, 20, -1, -1, 2, 7, 4, 7, -1, 19, 19, 23, 2, -1, 14, -1, 6, 6, 6, 0, 27, 1, -1, 14, 14, -1, 6, -1, 6, 27, 6, 6, 6, 6, 6, 19, 11, 6, 3, 6, 11, 11, 6, 18, 11, 6, -1, 19, 6, -1, 11, 0, 1, 1, -1, 11, 7, -1, 33, 7, 0, -1, 3, 1, 7, 21, 7, 7, 4, 25, 7, 0, 0, 0, -1, 0, 7, 7, 4, 31, 28, 0, 0, 0, -1, 0, 0, -1, 0, 11, 5, 18, 18, 18, 11, 11, 0, 0, 0, 8, -1, 0, 11, 0, 11, 30, 11, 0, 0, 23, 15, 23, 20, 0, -1, 0, 0, -1, 0, 0, 19, 0, 0, 15, 9, 0, 23, 23, 33, -1, 5, 5, 5, 5, 5, 5, 0, 5, 5, 1, 1, -1, -1, -1, 13, -1, 19, -1, 5, -1, 5, 5, 5, 1, 5, 5, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 3, -1, -1, 6, 6, -1, 6, 0, -1, 6, -1, 6, 10, 6, 27, 6, -1, 6, 8, 1, 8, 35, 20, -1, 6, 6, 6, -1, 6, 6, 6, 0, -1, 6, 27, -1, 7, 7, 7, 7, 15, 7, 7, -1, 1, -1, -1, 17, -1, 7, 4, 29, 1, 17, 7, 0, 0, 19, 21, -1, -1, 13, 0, -1, 29, 0, -1, 4, -1, 0, 21, -1, -1, 0, 0, -1, 4, -1, 0, 0, 35, 0, 0, 8, -1, 0, -1, 18, 0, -1, 18, 31, 0, -1, 30, 0, 18, 20, 1, 18, 18, 18, 4, -1, 8, 21, 0, 2, 2, 2, 30, -1, 21, 1, 0, 12, -1, 12, 31, 9, 1, 1, 0, -1, -1, -1, -1, -1, -1, 15, 3, 3, 5, 5, 1, 5, 5, 1, 5, 5, 5, 5, 1, -1, 2, 2, 2, 2, 2, 35, 0, 2, 5, 2, 2, 2, 2, -1, 2, 2, 5, 19, -1, 1, 2, -1, 19, 2, 2, 2, 1, -1, -1, 6, 6, -1, 6, 6, 6, 6, -1, 26, 26, -1, 26, 26, 26, 4, 26, -1, 26, 23, -1, 3, 8, 4, 25, 9, 8, -1, 9, 12, 12, 12, 12, 12, 35, 12, -1, 9, -1, 9, 9, 9, 9, 21, 8, 29, -1, 8, 12, 30, 30, 33, -1, -1, -1, 29, 29, -1, -1, 29, 29, 4, 29, -1, 29, -1, 13, 13, 13, 31, 13, 4, 13, 24, -1, 6, -1, 21, 13, -1, -1, 1, -1, 2, 4, 24, 31, 25, -1, 31, 31, 31, 9, 1, 0, 31, -1, -1, 3, 21, -1, -1, 20, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, -1, 2, -1, 2, 2, 2, 14, 2, -1, 2, 6, -1, 8, 4, 11, 11, -1, -1, 24, 4, -1, 14, -1, 34, 10, 13, 33, 26, 0, 26, 35, 26, 0, 8, 4, -1, 16, -1, 16, 16, 8, 0, 4, 25, 34, -1, 34, -1, 33, -1, 30, 34, 4, 34, 8, 13, 34, -1, 34, 11, 21, -1, 8, 1, 9, -1, 0, 22, 22, -1, 25, -1, -1, 20, 17, -1, 9, -1, -1, 12, 32, 26, -1, 32, 9, 12, 12, 9, 10, -1, 12, 12, 12, -1, 12, 2, 12, 22, -1, 22, 22, 10, -1, 29, 29, -1, 35, 29, -1, -1, -1, 32, 27, -1, -1, 32, -1, 32, -1, 19, 3, 23, 35, 28, 28, 1, -1, -1, -1, 3, 3, 30, 17, 1, 1, -1, -1, 21, 15, 15, 1, 23, 6, -1, 27, 17, 2, 7, -1, 15, 4, 15, -1, -1, 10, 19, -1, -1, -1, -1, 33, 7, 7, 25, 1, 4, 27, 7, 11, 7, 19, 11, 11, -1, 18, 3, 2, -1, 16, 34, 8, 8, -1, 34, 8, 33, 3, -1, 24, 16, 16, 8, 19, 1, 11, -1, 1, 14, -1, -1, 1, 16, -1, -1, -1, -1, 13, 20, 3, 22, 22, 21, 22, -1, 22, 18, 16, 0, 2, -1, -1, -1, -1, 5, 20, 11, 32, 32, 32, 30, 0, 32, 5, 32, 32, 14, -1, -1, -1, 15, 15, 14, 17, 16, 16, 16, 16, 2, 24, 16, 6, 2, 0, 0, 0, 35, 4, 9, 0, 1, 0, 18, 9, 28, 28, 22, 10, -1, -1, 0, 22, 4, -1, -1, 22, -1, -1, 22, 27, -1, 21, 4, 2, 1, 11, 28, 0, 4, 7, 7, -1, 7, -1, 33, 7, 2, 8, 8, -1, 33, 1, 8, 7, -1, 4, 1, 0, 0, 2, 1, 1, -1, 0, 1, 0, 10, 1, 18, 2, -1, 33, -1, 3, -1, 13, 12, 2, 15, 5, -1, -1, 24, 4, -1, 10, 5, 15, -1, -1, 29, 2, 0, -1, 2, 0, 35, 12, 12, 0, -1, 25, 4, 12, 8, 8, 12, 12, 16, 4, 10, 3, 9, 12, 0, 0, 0, 2, 0, -1, 15, -1, -1, -1, 18, -1, 24, 0, 24, 4, 1, 0, 7, 0, -1, 0, 5, 10, 0, 3, 3, 3, 2, 3, 2, 2, 22, 7, 3, 2, 2, 4, 24, 7, 5, 22, 4, -1, 23, 2, 14, 28, 24, 1, -1, 28, 1, 4, 0, 0, 10, 14, 1, 18, 18, -1, 11, -1, 21, 1, 30, -1, 1, 19, 35, 0, -1, -1, 1, 1, 5, 2, -1, 10, -1, 2, 2, 1, -1, 13, -1, 18, 5, 31, -1, 0, -1, 18, 0, 18, 10, 30, 17, 12, 4, 10, 0, 0, 12, -1, -1, 0, 0, 0, 9, 16, -1, 0, 3, 4, 34, 34, 3, 4, -1, 5, 5, 3, 3, 3, 6, 17, -1, -1, 0, 2, -1, 16, 2, 2, 9, 7, -1, 2, 26, 26, 2, 1, 1, 25, -1, 3, -1, 3, 4, 7, 12, -1, 32, 3, 0, 13, -1, -1, 13, 34, -1, 3, 16, 16, 0, -1, 0, -1, 15, 14, 0, 1, -1, 9, 4, 14, 22, 21, 33, 9, 28, 1, 5, 10, -1, -1, -1, 1, -1, 5, 5, 28, 1, 26, 22, 18, 15, -1, 19, 10, 18, 0, 0, 0, 18, 1, 23, 2, 15, 2, -1, 34, 0, -1, -1, 0, 4, 0, 2, 0, 0, 0, -1, 0, 18, 28, -1, 1, -1, 2, 2, 0, 6, -1, 12, 12, 4, 4, 2, 2, 0, 0, 22, 5, 26, 22, 22, -1, 0, -1]\n",
            "-------RUN106-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': 'CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7df3b00f5c90>)', 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[27, 3, 29, 29, -1, -1, -1, 22, -1, -1, -1, -1, -1, -1, 12, 15, 18, 27, -1, 15, -1, 16, -1, 8, 8, 26, 25, 15, 15, -1, -1, -1, 29, 9, -1, -1, 1, 11, -1, -1, 8, 15, 25, 8, 25, 7, 25, 25, 8, -1, 25, 8, -1, -1, -1, 32, -1, 3, 22, 22, -1, 22, 22, 20, 15, 15, -1, -1, 16, 4, 9, 16, 1, 1, 21, 4, 18, 0, -1, 31, -1, 3, -1, -1, 3, 26, 32, 1, 3, 9, 1, 20, -1, -1, 20, -1, -1, 1, 9, -1, -1, 21, -1, -1, -1, 9, -1, -1, -1, -1, -1, 16, 7, -1, 3, -1, 6, 20, -1, -1, -1, 6, 8, 21, 25, -1, 16, 3, 8, 16, 3, 12, 15, 16, 16, 27, 8, 18, 16, 25, -1, -1, -1, -1, 12, 22, 27, -1, 3, 0, -1, -1, -1, 20, -1, 22, 1, 30, -1, 2, -1, -1, 9, 2, 15, -1, -1, 21, 9, 15, 20, 20, -1, 0, 12, 22, -1, -1, 12, -1, 11, 18, 12, 0, 16, -1, -1, 3, 16, 16, 12, 12, 32, 3, -1, 6, 17, 17, 32, -1, -1, 3, -1, -1, 20, -1, 6, 32, 1, -1, 1, 32, 3, 3, -1, 32, -1, 1, 3, -1, 17, -1, 1, 1, 20, 17, 25, 25, 3, 0, 0, 0, 0, 17, 17, 0, 26, 0, -1, 17, -1, 4, 4, -1, 17, -1, -1, -1, 11, 15, -1, 17, 10, 27, -1, 7, 7, -1, 1, 1, 25, 21, 11, -1, -1, 3, 9, -1, -1, 4, 4, 4, -1, 1, 4, -1, 4, 4, 26, -1, 1, 11, 4, 7, 7, -1, 17, 2, 23, 2, 23, 3, 23, 23, 6, -1, -1, -1, 17, 4, 4, 4, 16, 4, -1, 4, -1, 4, 23, -1, 8, 4, 25, 11, -1, -1, -1, -1, 21, -1, 22, -1, -1, -1, 0, 27, -1, 27, 31, 27, -1, -1, 13, 27, 33, -1, 22, 12, 1, 12, 12, -1, 9, 20, -1, -1, 12, 12, 0, 12, -1, 8, 15, 3, 20, 0, -1, 13, 13, 27, 18, 16, -1, -1, 13, -1, 16, 18, 26, 16, -1, 0, 16, 9, -1, 16, 0, 20, 11, 0, 10, 10, 15, 20, 20, 20, 13, 20, 13, 26, -1, -1, -1, 10, 10, 10, 10, 10, 10, 28, -1, 10, 17, -1, 17, -1, 10, 1, 0, 0, 26, 13, 1, 21, 0, 26, 0, -1, 33, 2, -1, -1, -1, -1, -1, 21, 18, 33, 1, 1, 1, 18, 1, 4, 1, -1, 1, 1, 1, 1, 1, 1, -1, 33, -1, 4, -1, 4, 1, -1, 9, 4, 4, 4, 33, 3, 4, -1, 4, 4, 11, 4, 28, 3, 17, 4, 4, 4, 4, 1, 1, 7, -1, 1, -1, 1, 4, -1, -1, 1, -1, 2, 1, 20, 23, -1, 2, 7, 3, 7, 23, 21, -1, 26, 2, 23, 17, -1, 5, 5, 5, 0, 27, 1, -1, 17, 17, -1, 5, -1, 5, 27, 5, 5, 5, 5, 5, 21, 13, 5, 4, 5, 13, 13, 5, 19, 13, 5, -1, -1, 5, 5, 13, 0, 1, -1, 3, 13, 7, 9, 9, 7, 0, -1, 4, 1, 7, 18, 7, 7, 3, 25, 7, 0, 0, 0, -1, 0, 7, 7, 3, 23, 33, 0, 0, 0, -1, 0, 0, -1, 0, 13, 6, 19, 19, 19, 13, 13, 0, 0, 0, 8, -1, 0, 13, 0, 13, 9, 13, 0, 0, 26, -1, 26, 20, 0, 8, 0, 0, 26, 0, 0, 21, 0, 0, -1, 10, 0, 26, 26, 9, -1, 6, 6, 6, 6, 6, 6, 0, 6, 6, 1, 1, 23, 3, -1, 12, -1, 21, -1, 6, -1, 6, 6, -1, 1, 6, 6, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 31, 4, 11, -1, 5, 5, -1, 5, 0, -1, 5, 31, 5, 15, 5, 27, 5, -1, 5, 8, 1, 8, 31, 20, -1, 5, 5, 5, -1, 5, 5, 5, 0, -1, 5, 27, 31, 7, 7, 7, -1, -1, 7, 7, -1, -1, 7, -1, 16, -1, 7, -1, 29, 1, 16, -1, 0, 0, 21, 18, -1, 5, 12, 0, -1, 29, 0, -1, 3, -1, 0, 18, 8, -1, 0, 0, -1, 3, -1, 0, 0, 31, 0, 0, 8, 11, 0, -1, 19, 0, 31, 19, 23, 0, -1, 9, 0, 19, 20, 1, 19, 19, 19, 3, -1, 8, 18, 0, 2, 2, 2, 9, 8, 18, 1, 0, 14, -1, -1, 23, 10, 1, 1, 0, 3, -1, -1, -1, -1, -1, 22, 4, 4, 6, 6, 1, 6, 6, 1, 6, 6, 6, 6, 1, 11, 2, 2, 2, 2, 2, -1, 0, 2, 6, 2, 2, 2, 2, -1, 2, 2, 6, 21, -1, 1, 2, -1, 21, 2, 2, 2, 1, -1, -1, 5, 5, -1, 5, 5, 5, 5, -1, 28, 28, -1, 28, 28, 28, 3, 28, 26, 28, 26, 6, 4, 8, 3, 25, 10, 8, -1, 10, 14, 14, 14, 14, 14, 31, 14, -1, 10, 8, 10, 10, 10, 10, 18, 8, 29, -1, 8, 14, 14, 9, 9, -1, -1, 31, 29, 29, 21, -1, 29, 29, 3, 29, -1, 29, -1, 12, 12, 12, 23, 12, 3, 12, 32, -1, 5, -1, 18, 12, -1, -1, 1, 23, 2, 3, 32, 23, 25, -1, 23, 23, 23, 10, 1, 0, 23, -1, -1, -1, 18, -1, -1, 20, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, -1, 2, -1, 2, 2, 2, 17, 2, -1, 2, 5, -1, 8, 3, 13, 13, 7, -1, 32, 3, -1, 17, -1, 34, 15, 12, 9, 28, 0, 28, 31, 28, 0, 8, 3, -1, 11, -1, 11, 11, 8, 0, 3, 25, 34, -1, 34, -1, 9, -1, 9, 34, 3, 34, 8, 12, 34, -1, 34, 13, 18, -1, 8, 1, 10, -1, 0, 24, 24, -1, -1, 15, -1, 20, 16, -1, 10, -1, -1, 14, 30, 28, -1, 30, 10, 14, 14, 10, -1, -1, 14, 14, 14, -1, 14, 2, 14, 24, -1, 24, 24, 15, -1, 29, 29, -1, 31, 29, -1, 29, 0, 30, 27, -1, -1, 30, -1, 30, 30, 21, 4, 26, 31, 33, 33, 1, -1, -1, -1, 4, -1, 9, 16, 1, 1, -1, -1, 18, 22, 22, 1, 26, 5, -1, 27, 16, 2, 7, -1, 22, 3, 22, 18, -1, 15, 21, -1, -1, -1, -1, 9, 7, 7, 25, 1, 3, 27, 7, 13, 7, 21, 13, 13, -1, 19, 4, 2, -1, 11, 34, 8, 8, -1, 34, 8, 9, 4, 11, -1, 11, 11, 8, 21, 1, 13, -1, 1, 17, 16, -1, 1, 11, -1, -1, 23, -1, 12, 20, 4, 24, 24, 18, 24, -1, 24, 19, 11, 0, 2, -1, 9, -1, -1, 6, 20, 13, 30, 30, 30, 9, 0, 30, 6, 30, 30, 17, -1, 30, 30, 22, 22, 17, 16, 11, 11, 11, 11, 2, 32, -1, 5, 2, 0, 0, 0, 31, 3, 10, 0, -1, 0, 19, 10, 33, 33, 24, 15, -1, -1, 0, 24, 3, -1, -1, 24, -1, -1, 24, 27, -1, 18, 3, 2, 1, 13, 33, 0, 3, 7, 7, -1, 7, 7, 9, 7, 2, 8, 8, -1, 9, 1, 8, 7, 21, 3, 1, 0, 0, 2, 1, 1, -1, 0, 1, 0, 15, 1, 19, 2, -1, 9, 25, 4, -1, 12, 14, 2, -1, 6, -1, -1, 32, 3, 11, 15, 6, 22, 6, -1, 29, 2, 0, -1, 2, 0, 31, 14, 14, 0, -1, 25, 3, 14, 8, 8, 14, 14, 11, 3, 15, 4, 10, 14, 0, 0, 0, 2, 0, -1, 22, -1, -1, 18, 19, -1, 32, 0, 32, 3, -1, 0, 7, 0, -1, 0, 6, 15, 0, 4, 4, 4, 2, 4, 2, 2, 24, 7, 4, 2, 2, 3, 32, 7, 6, 24, 3, -1, 26, 2, 17, 33, 32, 1, 7, 33, 1, 3, 0, 0, 15, 17, 1, 19, 19, -1, 13, -1, 18, 1, 9, -1, 1, -1, -1, 0, -1, -1, 1, 1, 6, 2, -1, 15, -1, 2, 2, 1, -1, 12, -1, 19, 6, 23, -1, 0, -1, 19, 0, 19, 15, 9, 16, 14, 3, 15, 0, 0, 14, -1, -1, 0, 0, 0, 10, 11, 21, 0, 4, 3, 34, 34, -1, 3, -1, 6, 6, 4, 4, 4, 5, 16, -1, 31, 0, 2, -1, 11, 2, 2, 10, 7, 9, 2, 28, 28, 2, 1, 1, 25, -1, 4, -1, 4, 3, 7, 14, -1, 30, 4, 0, 12, -1, -1, 12, -1, 18, 4, 11, 11, 0, 23, 0, 0, 22, 17, 0, 1, -1, 10, 3, 17, 24, 18, 9, 10, 33, 1, 6, 15, -1, -1, -1, 1, 21, -1, 6, 33, 1, 28, 24, 19, 22, -1, 21, 15, 19, 0, 0, 0, 19, 1, 26, 2, 22, 2, -1, 34, 0, 18, -1, 0, 3, 0, 2, 0, 0, 0, -1, 0, 19, 33, -1, 1, -1, 2, 2, 0, 5, -1, 14, 14, 3, 3, 2, 2, 0, 0, 24, 6, 28, 24, 24, 11, 0, 9]\n",
            "-------RUN107-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': \"CountVectorizer(stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b02d2c50>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[27, 4, 28, 28, 13, -1, -1, -1, -1, -1, -1, -1, -1, -1, 12, 17, 15, 27, -1, 17, -1, 13, 19, 8, 8, 24, 22, 17, 17, -1, -1, -1, 28, 33, -1, -1, 1, 9, 0, -1, 8, 17, 22, 8, 22, 7, 22, 22, 8, -1, -1, 8, -1, -1, -1, 23, -1, 4, 21, 21, -1, 21, 21, 19, 17, 17, -1, -1, 13, 3, 30, 13, 1, 1, 18, 3, 15, -1, -1, 4, 23, 4, -1, -1, 4, 24, 23, 1, 4, 33, 1, 19, 23, -1, 19, 11, 24, 1, -1, -1, -1, 18, -1, -1, -1, 30, -1, -1, -1, -1, -1, 13, -1, -1, 4, -1, 5, 19, 15, -1, 18, 5, 8, 18, 22, -1, 13, 4, 8, 13, 4, 12, 17, 13, 13, 27, 8, 15, 13, 22, -1, -1, -1, -1, 12, 21, 27, -1, 4, 0, -1, -1, -1, 19, -1, 21, 1, 32, -1, 2, -1, 2, 30, 2, 17, -1, -1, 18, -1, -1, 19, 19, 18, 0, 12, 21, 15, -1, 12, -1, 9, 15, 12, 0, 13, -1, -1, 4, 13, 13, 12, 12, 23, 4, 0, 5, 16, 16, 23, -1, 23, 4, 23, -1, -1, -1, 5, 23, -1, -1, 1, 23, 4, 4, -1, 23, 23, 1, 4, -1, 16, -1, 1, 1, 19, 16, 22, 22, -1, 0, 0, 0, 0, 16, 16, 0, 24, 0, -1, 16, 26, 3, 3, -1, 16, -1, -1, -1, 9, 17, -1, 16, 10, -1, -1, 7, 7, -1, 1, 1, 22, 18, 9, -1, 17, 4, -1, 22, -1, 3, 3, 3, -1, 1, 3, -1, 3, 3, 24, -1, 1, 9, 3, 7, 7, 3, 16, 35, -1, 35, 29, 4, 29, 29, 5, -1, -1, -1, 16, 3, 3, 3, 13, 3, -1, 3, 1, 3, -1, -1, 8, 3, 22, 9, -1, -1, -1, -1, 18, -1, 21, -1, -1, -1, 0, 27, -1, 27, 36, 27, -1, -1, 14, 27, 31, -1, 21, 12, 1, 12, 12, 24, -1, 19, -1, -1, 12, 12, 0, 12, -1, 8, -1, 4, 19, -1, -1, 14, 14, 27, 15, 13, -1, -1, 14, -1, 13, 15, 24, 13, -1, 0, 13, 30, -1, 13, 0, 19, 9, 0, 10, 10, 17, 19, 19, 19, 14, 19, 14, 24, -1, -1, -1, 10, 10, 10, 10, 10, 10, 26, -1, 10, 16, -1, 16, -1, 10, 1, 0, 0, 24, 14, 1, 18, 0, 24, 0, -1, 31, 35, -1, -1, -1, -1, -1, -1, 15, 31, 1, 1, 1, 15, 1, 3, 1, -1, 1, 1, 1, 1, 1, 1, -1, 31, -1, 3, -1, 3, 1, -1, 33, 3, 3, 3, 31, 4, 3, -1, 3, 3, 9, 3, 26, 4, 16, 3, 3, 3, 3, 1, 1, 7, -1, 1, -1, 1, 3, -1, -1, 1, -1, 35, 1, 19, -1, -1, 35, 7, 4, 7, 29, 18, -1, 24, 35, -1, 16, -1, 6, 6, 6, -1, 27, 1, -1, 16, 16, -1, 6, -1, 6, 27, 6, 6, 6, 6, 6, 18, 14, 6, 3, 6, 14, 14, 6, 20, 14, 6, -1, -1, 6, -1, 14, -1, 1, 1, -1, 14, 7, -1, 33, 7, 0, -1, 3, 1, 7, 15, 7, 7, 4, 22, 7, 0, 0, 0, -1, 0, 7, 7, 4, 29, 31, 0, 0, 0, -1, 0, 0, 28, 0, 14, -1, 20, 20, 20, 14, 14, 0, 0, 0, 8, -1, 0, 14, 0, 14, 30, 14, 0, 0, 24, 21, 24, 19, 0, 8, 0, 0, -1, 0, 0, 18, 0, 0, -1, 10, 0, 24, 24, 33, -1, 5, 5, 5, 5, 5, 5, 0, 5, 5, 1, 1, 29, 4, -1, 12, -1, 18, -1, 5, -1, 5, 5, 5, 1, 5, 5, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 36, 3, 9, -1, 6, 6, -1, 6, 0, -1, 6, -1, 6, 17, 6, 27, 6, -1, 6, 8, 1, 8, 36, 19, -1, 6, 6, 6, -1, 6, 6, 6, 0, 15, 6, 27, -1, 7, 7, 7, 7, -1, 7, 7, -1, 1, 7, -1, 13, -1, 7, 4, 28, 1, 13, -1, 0, 0, 18, 15, -1, 6, 12, 0, -1, 28, 0, -1, 4, -1, 0, 15, 8, -1, 0, 0, -1, 4, -1, 0, 0, -1, 0, 0, 8, -1, 0, -1, 20, 0, 36, 20, 29, 0, 11, 30, 0, 20, 19, 1, 20, 20, 20, 4, -1, 8, 15, 0, 2, 2, 2, 30, 8, 15, 1, 0, 11, -1, 11, 29, 10, 1, 1, 0, -1, -1, -1, -1, -1, -1, 21, 3, 3, 5, 5, 1, 5, 5, 1, 5, 5, 5, 5, 1, 9, 2, 2, 2, 2, 2, 4, 0, 2, 5, 2, 2, 2, 2, -1, 2, 2, 5, 18, -1, 1, 2, -1, 18, 2, 2, 2, 1, -1, -1, 6, 6, -1, 6, 6, 6, 6, -1, 26, 26, -1, 26, 26, 26, 4, 26, 24, 26, 24, -1, 3, 8, 4, 22, 10, 8, -1, 10, 11, 11, 11, 11, 11, 36, 11, 21, 10, 8, 10, 10, 10, 10, 15, 8, 28, -1, 8, 11, 30, 30, 33, -1, -1, 36, 28, 28, 18, -1, 28, 28, 4, 28, -1, 28, -1, 12, 12, 12, 29, 12, 4, 12, 23, -1, 6, 23, 15, 12, -1, 15, 1, -1, 2, 4, 23, 29, 22, -1, 29, 29, 29, 10, 1, 0, 29, -1, -1, 3, 15, -1, -1, 19, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, -1, 2, -1, 2, 2, 2, 16, 2, -1, 2, 6, -1, 8, 4, 14, 14, 7, -1, 23, 4, -1, 16, -1, 34, 17, 12, 33, 26, 0, 26, 36, 26, 0, 8, 4, -1, 9, -1, 9, 9, 8, 0, 4, -1, 34, -1, 34, -1, 33, -1, 30, 34, 4, 34, 8, 12, 34, -1, 34, 14, 15, -1, 8, 1, 10, -1, 0, 25, 25, 9, 22, -1, -1, 19, 13, -1, 10, -1, -1, 11, 32, 26, -1, 32, 10, 11, 11, 10, -1, -1, 11, 11, 11, -1, 11, 2, 11, 25, -1, 25, 25, 17, -1, 28, 28, -1, 36, 28, -1, -1, -1, 32, 27, -1, -1, 32, -1, 32, 27, 18, 3, 24, -1, 31, 31, 1, -1, -1, -1, 3, 3, 30, 13, 1, 1, -1, -1, 15, 21, 21, 1, 24, 6, 9, 27, 13, 2, 7, -1, 21, 4, 21, -1, -1, 17, 18, -1, -1, -1, 22, 33, -1, 7, 22, 1, 4, 27, 7, 14, 7, 18, 14, 14, -1, 20, 3, 2, -1, 9, 34, 8, 8, -1, 34, 8, 33, 3, 9, -1, 9, 9, 8, 18, 1, -1, -1, 1, 16, 13, -1, 1, 9, -1, -1, 29, -1, 12, 19, 3, 25, 25, 15, 25, -1, 25, 20, 9, 0, 2, -1, 30, -1, -1, 5, 19, 14, 32, 32, 32, 30, 0, 32, 5, 32, 32, 16, -1, 32, -1, 21, 21, 16, 13, 9, 9, 9, 9, 35, 23, 9, 6, 35, 0, 0, 0, 36, 4, 10, 0, 1, 0, 20, 10, 31, 31, 25, -1, -1, -1, 0, 25, 4, -1, -1, 25, -1, -1, 25, 27, -1, 15, 4, 2, 1, -1, 31, 0, 4, 7, 7, -1, 7, -1, 33, 7, 2, 8, 8, -1, 33, 1, 8, 7, 18, 4, 1, 0, 0, 2, 1, 1, -1, 0, 1, 0, 17, 1, 20, 2, -1, 33, 22, 3, -1, 12, 11, 35, 21, 5, -1, -1, 23, 4, 9, 17, 5, 21, -1, -1, 28, 35, 0, -1, 35, 0, 36, 11, 11, 0, 22, 22, 4, 11, 8, 8, 11, 11, 9, 4, 17, 3, 10, 11, 0, 0, 0, 2, 0, -1, 21, -1, -1, -1, 20, -1, 23, 0, 23, 4, 1, -1, 7, 0, -1, 0, 5, 17, 0, 3, 3, 3, 2, 3, 2, 2, 25, 7, 3, 2, 2, 4, 23, 7, 5, 25, 4, -1, 24, 2, 16, 31, 23, 1, -1, 31, 1, 4, 0, 0, 17, 16, 1, 20, 20, -1, 14, -1, 15, 1, 30, -1, 1, -1, 36, 0, -1, -1, 1, 1, 5, 2, -1, 17, -1, 2, 2, 1, -1, 12, -1, 20, 5, 29, -1, 0, -1, 20, 0, 20, 17, 30, 13, 11, 4, 17, 0, 0, 11, -1, -1, 0, 0, 0, 10, 9, -1, 0, 3, 4, 34, 34, 3, 4, -1, 5, 5, 3, 3, 3, 6, 13, -1, -1, 0, 2, -1, 9, 2, 2, 10, 7, -1, 2, 26, 26, 2, 1, 1, 22, -1, 3, -1, 3, 4, 7, 11, -1, 32, 3, 0, 12, -1, -1, 12, -1, -1, 3, 9, 9, 0, -1, 0, 0, 21, 16, 0, 1, -1, 10, 4, 16, 25, 15, -1, 10, 31, 1, 5, 17, -1, -1, -1, 1, 18, 5, 5, 31, 1, 26, 25, 20, -1, -1, 18, 17, 20, 0, 0, 0, 20, 1, 24, 2, 21, 2, -1, 34, 0, 15, -1, 0, 4, 0, 2, 0, 0, 0, -1, 0, 20, 31, -1, 1, -1, 2, 2, 0, 6, -1, 11, 11, 4, 4, 2, 2, 0, 0, 25, 5, 26, 25, 25, -1, 0, 30]\n",
            "-------RUN108-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': \"CountVectorizer(ngram_range=(1, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b891dd10>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[28, 4, 31, 31, -1, -1, -1, 18, -1, -1, -1, -1, -1, -1, 11, 16, 21, 28, -1, 16, -1, 15, 14, 8, 8, 22, 24, 16, 16, -1, -1, -1, 31, 34, -1, -1, -1, -1, -1, -1, 8, 16, 24, 8, 24, 7, 24, 24, 8, -1, 24, 8, -1, -1, -1, -1, -1, 4, 18, 18, 1, 18, 18, 14, 16, 16, -1, -1, -1, 3, 29, 15, 1, 1, 13, 3, 21, -1, -1, 36, 27, 4, -1, -1, 4, 22, 27, 1, 4, 34, 1, 14, 27, -1, 14, 10, -1, 1, -1, -1, -1, 13, -1, -1, -1, 29, -1, -1, -1, -1, -1, 15, -1, -1, 4, -1, 5, 14, -1, -1, -1, 5, 8, 13, 24, 0, 15, 4, 8, 15, 4, 11, 16, 15, 15, 28, 8, 21, 15, 24, -1, -1, 25, -1, 11, 18, 28, -1, -1, -1, -1, -1, -1, 14, -1, 18, 1, 25, -1, 2, -1, 2, 29, 2, 16, -1, -1, 13, -1, 16, 14, 14, -1, 0, 11, 18, 13, -1, 11, -1, 20, 21, 11, 0, 15, -1, -1, 4, 15, 15, 11, 11, 27, 4, -1, 5, 17, 17, 27, -1, 27, 4, -1, -1, 14, -1, 5, -1, 1, -1, 1, 27, 4, -1, -1, 27, 27, 1, 4, -1, 17, -1, 1, 1, 14, 17, 24, 24, -1, 0, 0, 0, 0, 17, 17, 0, 22, 0, -1, 17, -1, 3, 3, -1, 17, -1, -1, -1, 20, 16, -1, 17, 9, 25, -1, 7, 7, -1, 1, 1, 24, 13, 20, 18, -1, 4, 29, -1, -1, 3, 3, 3, -1, 1, 3, 11, 3, 3, -1, -1, 1, 20, 3, 7, 7, 3, 17, 33, 32, 33, 32, 4, 32, 32, -1, -1, -1, -1, 17, 3, 3, 3, 15, 3, -1, 3, 1, 3, -1, -1, 8, 3, 24, -1, -1, -1, -1, -1, 13, -1, 18, -1, -1, 15, 0, 28, -1, 28, 36, 28, -1, -1, 12, 28, 30, -1, 18, 11, 1, 11, 11, 22, -1, 14, -1, -1, 11, 11, 0, 11, -1, 8, -1, 4, 14, -1, -1, 12, 12, 28, 13, 15, -1, -1, 12, -1, 15, 21, 22, 15, -1, 0, 15, 29, 14, 15, 0, 14, -1, 0, 9, 9, 16, 14, 14, 14, 12, 14, 12, 22, -1, -1, -1, 9, 9, 9, 9, 9, 9, 26, -1, 9, 17, -1, 17, -1, 9, 1, 0, 0, 22, 12, 1, 13, 0, 22, 0, -1, 30, 33, -1, -1, -1, -1, -1, 13, 21, 30, 1, 1, 1, 21, 1, 3, 1, -1, 1, 1, 1, 1, 1, 1, -1, 30, -1, 3, -1, 3, 1, -1, 34, 3, 3, 3, 30, 4, 3, -1, 3, 3, 20, 3, 26, 4, 17, 3, 3, 3, 3, 1, 1, 7, -1, 1, -1, 1, 3, -1, -1, 1, -1, 33, 1, 14, -1, -1, 33, 7, 4, 7, -1, 13, -1, 22, 33, -1, 17, -1, 6, 6, 6, -1, 28, 1, -1, 17, 17, -1, 6, -1, 6, 28, 6, 6, 6, 6, 6, 13, 12, 6, 3, 6, 12, 12, 6, 19, 12, 6, -1, -1, 6, 6, 12, -1, 1, 1, -1, 12, 7, -1, 34, 7, 0, -1, 3, 1, 7, -1, 7, 7, 4, 24, 7, 0, 0, 0, -1, 0, 7, 7, 4, 32, 30, 0, 0, 0, -1, 0, 0, -1, 0, 12, 5, 19, 19, 19, 12, 12, 0, 0, 0, 8, -1, 0, 12, 0, 12, 29, 12, 0, 0, 22, 18, 22, 14, 0, 8, 0, 0, 22, 0, 0, 13, 0, 0, -1, 9, 0, 22, 22, 34, -1, 5, 5, 5, 5, 5, 5, 0, 5, 5, 1, 1, -1, -1, -1, 11, -1, 13, -1, 5, -1, 5, 5, -1, 1, 5, 5, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 3, -1, -1, 6, 6, -1, 6, 0, -1, 6, -1, 6, 16, 6, 28, 6, -1, 6, 8, 1, 8, -1, 14, -1, 6, 6, 6, -1, 6, -1, 6, 0, -1, 6, 28, -1, 7, 7, 7, -1, -1, 7, 7, -1, 1, 7, -1, 15, -1, 7, -1, 31, 1, 15, 24, 0, 0, 13, 21, -1, 6, 11, 0, -1, 31, 0, -1, -1, -1, 0, -1, 8, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 8, -1, 0, -1, 19, 0, 36, 19, 32, 0, 10, 29, 0, 19, 14, 1, 19, 19, 19, 4, -1, 8, 21, 0, 2, 2, 2, 29, 8, 21, 1, 0, 10, 4, 10, 32, 9, 1, 1, 0, -1, 33, -1, 33, -1, -1, 18, 3, 3, 5, 5, 1, 5, 5, 1, 5, 5, 5, 5, 1, -1, 2, 2, 2, 2, 2, 36, 0, 2, 5, 2, 2, 2, 2, -1, 2, 2, 5, 13, -1, 1, 2, -1, 13, 2, 2, 2, 1, -1, -1, 6, 6, -1, 6, 6, 6, 6, -1, 26, 26, -1, 26, 26, 26, 4, 26, 22, 26, 22, -1, 3, 8, 4, 24, 9, 8, -1, 9, 10, 10, 10, 10, 10, 36, 10, 18, 9, 8, 9, 9, 9, 9, 13, 8, 31, 16, 8, 10, 29, 29, 34, -1, -1, -1, 31, 31, -1, -1, 31, 31, 4, 31, -1, 31, -1, 11, 11, 11, 32, 11, 4, 11, 27, -1, 6, 27, 21, 11, -1, 21, 1, -1, 2, 4, 27, 32, 24, -1, 32, 32, 32, 9, 1, 0, 32, -1, -1, -1, 21, -1, -1, 14, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, -1, 2, -1, 2, 2, 2, 17, 2, -1, 2, 6, -1, 8, 4, 12, 12, 7, -1, 27, 4, -1, 17, -1, 35, 16, 11, 34, 26, 0, 26, 36, 26, 0, 8, 4, -1, 20, -1, 20, 20, 8, 0, 4, 24, 35, -1, 35, -1, 34, -1, 29, 35, 4, 35, 8, 11, 35, -1, 35, 12, 21, -1, 8, 1, 9, -1, 0, 23, 23, -1, -1, -1, -1, 14, 15, -1, 9, 36, -1, 10, 25, 26, -1, 25, 9, 10, 10, 9, -1, -1, 10, 10, 10, -1, 10, 2, 10, 23, -1, 23, 23, 16, 13, 31, 31, -1, -1, 31, -1, -1, -1, 25, 28, -1, 14, 25, -1, 25, 25, 13, 3, 22, -1, 30, 30, 1, -1, -1, -1, 3, -1, 29, 15, 1, 1, -1, -1, 21, 18, 18, 1, 22, 6, -1, 28, 15, 2, 7, -1, 18, 4, 18, -1, -1, 16, 13, -1, -1, -1, -1, 34, 7, 7, 24, 1, 4, 28, 7, 12, 7, 13, 12, 12, -1, 19, 3, 2, -1, 20, 35, 8, 8, -1, 35, 8, 34, 3, -1, -1, 20, 20, 8, 13, 1, 12, -1, 1, 17, 15, 26, 1, 20, -1, -1, -1, -1, 11, 14, 3, 23, 23, 21, 23, -1, 23, 19, 20, 0, 2, -1, 29, -1, -1, 5, 14, 12, 25, 25, 25, 29, 0, 25, 5, 25, 25, 17, -1, 25, 25, 18, 18, 17, 15, 20, 20, 20, 20, 33, 27, 20, 6, 33, 0, 0, 0, -1, 4, 9, 0, 1, 0, 19, 9, 30, 30, 23, 16, -1, -1, 0, 23, 4, -1, -1, 23, -1, -1, 23, 28, -1, 21, 4, 2, 1, 12, 30, 0, 4, 7, 7, -1, 7, -1, 34, 7, 2, 8, 8, -1, 34, 1, 8, 7, 13, 4, 1, 0, 0, 2, 1, 1, -1, 0, 1, 0, 16, 1, 19, 2, -1, 34, -1, 3, -1, 11, 10, 33, 18, 5, -1, -1, -1, 4, -1, 16, 5, 18, 5, -1, 31, 33, 0, -1, 33, 0, 36, 10, 10, 0, -1, 24, 4, 10, 8, 8, 10, 10, 20, 4, 16, 3, 9, 10, 0, 0, 0, 2, 0, -1, 18, -1, -1, 21, 19, -1, 27, 0, 27, 4, 1, -1, 7, 0, -1, 0, 5, -1, 0, 3, 3, 3, 2, 3, 2, 2, 23, 7, 3, 2, 2, 4, 27, 7, 5, 23, 4, -1, 22, 2, 17, 30, 27, 1, -1, 30, 1, 4, 0, 0, 16, 17, 1, 19, 19, -1, 12, -1, 21, 1, 29, -1, 1, 13, 36, 0, -1, -1, 1, 1, 5, 2, -1, 16, -1, 2, 2, 1, -1, 11, -1, 19, 5, 32, -1, 0, -1, 19, 0, 19, 16, 29, 15, 10, 4, 16, 0, 0, 10, -1, -1, 0, 0, 0, 9, 20, 13, 0, 3, 4, 35, 35, -1, 4, -1, 5, 5, 3, 3, 3, 6, 15, -1, 36, 0, 2, -1, 20, 2, 2, 9, 7, -1, 2, 26, 26, 2, 1, 1, 24, -1, 3, -1, 3, 4, 7, 10, -1, 25, 3, 0, 11, -1, -1, 11, -1, 21, 3, 20, 20, 0, 32, 0, 0, 18, 17, 0, 1, 33, 9, 4, 17, 23, 21, -1, 9, 30, 1, 5, 16, -1, -1, -1, 1, 13, 5, 5, 30, 1, 26, 23, 19, 18, -1, 13, 16, 19, 0, 0, 0, 19, 1, 22, 2, 18, 2, 33, 35, 0, 21, -1, 0, -1, 0, 2, 0, 0, 0, -1, 0, 19, 30, -1, 1, -1, 2, 2, 0, 6, 3, 10, 10, 4, 4, 2, 2, 0, 0, 23, 5, 26, 23, 23, 20, 0, 29]\n",
            "-------RUN109-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': \"CountVectorizer(ngram_range=(2, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df4a9008410>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[27, 4, 28, 28, -1, -1, -1, 18, -1, -1, -1, -1, -1, -1, 11, 16, 22, 27, -1, 16, -1, 15, -1, 8, 8, 26, 23, 16, 16, -1, -1, -1, 28, 30, -1, -1, -1, -1, -1, -1, 8, 16, 23, 8, 23, 7, 23, 23, 8, -1, 23, 8, -1, -1, -1, 35, -1, 4, 18, 18, -1, 18, 18, 17, 16, 16, -1, -1, 15, 2, 30, 15, 0, 0, 13, 2, 22, -1, -1, 4, -1, 4, -1, -1, 4, 26, 35, 0, 4, 36, 0, 17, -1, -1, 17, 10, -1, 0, -1, -1, -1, 13, -1, -1, -1, 30, -1, -1, -1, -1, -1, 15, -1, -1, 4, -1, 6, 17, -1, 25, 13, 6, 8, 13, 23, -1, 15, 4, 8, 15, 4, 11, 16, 15, 15, 27, 8, 22, 15, 23, -1, -1, 17, -1, 11, 18, 27, -1, 4, -1, -1, -1, -1, 17, -1, 18, 0, 31, -1, 1, -1, -1, 30, 1, 16, -1, -1, 13, -1, -1, 17, 17, -1, -1, 11, 18, 13, -1, 11, -1, 20, 22, 11, -1, 15, -1, -1, 4, 15, 15, 11, 11, 35, 4, -1, 6, 14, 14, 35, -1, 35, 4, 35, -1, 17, -1, -1, -1, 0, -1, 0, -1, 4, 4, -1, 35, 33, 0, 4, -1, 14, -1, 0, 0, 17, 14, 23, 23, 4, 3, -1, -1, -1, 14, 14, -1, 26, -1, -1, 14, -1, 2, 2, -1, 14, -1, -1, -1, 20, 16, -1, 14, 9, 27, -1, 7, 7, -1, 0, 0, 23, 13, 20, -1, -1, 4, 30, -1, -1, 2, 2, 2, -1, 0, 2, -1, 2, 2, -1, -1, 0, 20, 2, 7, 7, 2, 14, 1, 34, 1, 34, 4, 34, 34, 6, -1, -1, -1, 14, 2, 2, 2, 15, 2, -1, 2, 0, 2, -1, -1, 8, 2, 23, -1, -1, -1, -1, -1, 13, -1, 18, -1, -1, -1, 3, 27, -1, 27, 25, 27, -1, -1, 21, 27, 32, 31, 18, 11, 0, 11, 11, 26, -1, 17, -1, -1, 11, 11, -1, 11, -1, -1, -1, 4, 17, -1, -1, 21, 21, 27, -1, 15, -1, -1, 21, -1, 15, 22, 26, 15, -1, -1, 15, 30, 17, 15, -1, 17, 20, -1, 9, 9, 16, 17, 17, 17, 21, 17, -1, 26, -1, -1, -1, 9, 9, 9, 9, 9, 9, 29, -1, 9, 14, -1, 14, -1, 9, 0, -1, -1, 26, -1, 0, 13, -1, 26, -1, -1, 32, 1, -1, -1, -1, -1, -1, 13, 22, 32, 0, 0, 0, 22, 0, 2, 0, -1, 0, 0, 0, 0, 0, 0, -1, 32, -1, 2, -1, 2, 0, -1, 36, 2, 2, 2, 32, 4, 2, -1, 2, 2, 20, 2, 29, 4, 14, 2, 2, 2, 2, 0, 0, 7, -1, 0, -1, 0, 2, -1, -1, 0, -1, 1, 0, 17, -1, -1, 1, 7, 4, 7, -1, 13, -1, 26, 1, -1, 14, -1, 5, 5, 5, -1, 27, 0, -1, 14, 14, -1, 5, -1, 5, 27, 5, 5, 5, 5, 5, 13, 21, 5, 2, 5, 21, 21, 5, 19, 21, 5, -1, -1, 5, -1, 21, -1, 0, 0, 4, 21, 7, -1, 36, 7, 12, -1, 2, 0, 7, -1, 7, 7, 4, 23, 7, 12, 12, 12, -1, -1, 7, 7, 4, 5, 32, 3, 3, 3, -1, 3, -1, 28, 12, 21, -1, 19, 19, 19, 21, 21, 12, 12, 12, 8, -1, 12, 21, 12, 21, 30, 21, -1, -1, 26, 18, 26, 17, 3, -1, 3, 3, -1, 3, 3, 13, 3, 3, 18, 9, 3, 26, 26, 36, -1, 6, 6, 6, -1, 6, 6, 12, 6, 6, 0, 0, -1, -1, -1, 11, -1, 13, -1, 6, -1, 6, 6, 6, 0, 6, 6, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 2, 20, -1, 5, 5, -1, 5, 3, -1, 5, -1, 5, 16, 5, 27, 5, -1, 5, 8, 0, 8, 25, 17, -1, 5, 5, 5, -1, 5, -1, 5, 3, -1, 5, 27, -1, -1, 7, 7, 7, -1, 7, 7, -1, 0, 7, -1, 15, -1, 7, -1, 28, 0, 15, 7, 12, 12, 13, 22, -1, 5, 11, -1, -1, 28, -1, -1, 4, -1, 12, 22, -1, -1, 3, 3, -1, -1, -1, 3, 3, 25, 3, 3, 8, -1, 3, -1, 19, 3, 25, 19, 34, 3, -1, 30, 3, 19, 17, 0, 19, 19, 19, 4, -1, 8, 22, -1, 1, 1, 1, 30, -1, 22, 0, 3, 10, -1, 10, 34, 9, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, 2, 2, 6, 6, 0, 6, 6, 0, 6, 6, 6, 6, 0, -1, 1, 1, 1, 1, 1, 25, -1, 1, 6, 1, 1, 1, 1, -1, 1, 1, 6, 13, -1, 0, 1, -1, 13, 1, 1, 1, 0, 13, -1, 5, 5, 13, 5, 5, 5, 5, -1, 29, 29, -1, 29, 29, 29, 4, 29, 26, 29, 26, -1, 2, 8, 4, 23, 9, 8, -1, 9, 10, 10, 10, 10, 10, 25, 10, 18, 9, -1, 9, 9, 9, 9, 22, 8, 28, 16, 8, 10, 30, -1, 36, 25, -1, 25, 28, 28, 13, -1, 28, 28, 4, 28, -1, 28, -1, 11, 11, 11, 34, 11, 4, 11, 35, -1, 5, -1, 22, 11, -1, -1, 0, -1, 1, 4, 35, 34, 23, -1, 34, 34, 34, 9, 0, -1, 34, -1, -1, 2, 22, -1, -1, 17, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, -1, 1, -1, 1, 1, 1, 14, 1, -1, 1, 5, 13, 8, 4, 21, 21, 7, -1, -1, 4, -1, 14, -1, 33, 16, 11, 36, 29, 12, 29, 25, 29, 12, 8, 4, -1, 20, -1, 20, 20, 8, 12, 4, 23, 33, -1, 33, -1, 36, -1, 30, 33, 4, 33, 8, 11, 33, -1, 33, -1, 22, -1, 8, 0, 9, -1, 3, 24, 24, -1, 23, -1, -1, 17, 15, -1, 9, 25, -1, 10, 31, 29, -1, 31, 9, 10, 10, 9, -1, -1, 10, 10, 10, -1, 10, 1, 10, 24, -1, 24, 24, 16, -1, 28, 28, -1, 25, 28, -1, 28, -1, 31, 27, -1, -1, 31, -1, 31, -1, 13, 2, 26, 25, 32, 32, 0, -1, -1, -1, 2, 2, 30, 15, 0, 0, -1, -1, 22, 18, 18, 0, 26, 5, -1, 27, 15, 1, 7, -1, 18, 4, 18, -1, -1, 16, 13, -1, -1, -1, -1, 36, -1, 7, 23, 0, 4, 27, 7, 21, 7, 13, 21, 21, -1, 19, 2, 1, -1, 20, 33, 8, 8, -1, 33, 8, 36, 2, -1, -1, -1, 20, 8, 13, 0, -1, -1, 0, 14, 15, -1, 0, 20, -1, -1, -1, -1, 11, 17, 2, 24, 24, 22, 24, 14, 24, 19, 20, 3, 1, -1, 30, -1, -1, 6, 17, 21, 31, 31, 31, 30, -1, 31, 6, 31, 31, 14, -1, 31, 31, 18, 18, 14, 15, 20, 20, 20, 20, 1, 35, -1, 5, 1, -1, 12, 3, 25, 4, 9, 3, 0, 3, 19, 9, 32, 32, 24, 16, -1, -1, -1, 24, 4, -1, -1, 24, -1, -1, 24, 27, -1, 22, 4, 1, 0, -1, 32, -1, 4, 7, 7, 0, 7, -1, 36, 7, 1, 8, 8, -1, 36, 0, 8, 7, 13, 4, 0, 12, 12, 1, 0, 0, -1, 12, 0, 12, 16, 0, 19, 1, -1, 36, 23, 2, -1, 11, 10, 1, 18, 6, -1, -1, -1, 4, 20, 16, 6, 18, 6, -1, 28, 1, 3, -1, 1, 12, 25, 10, 10, 3, -1, 23, 4, 10, 8, 8, 10, 10, 20, 4, 16, 2, 9, 10, 3, 3, 3, 1, -1, -1, 18, -1, -1, -1, 19, -1, 35, -1, -1, 4, 0, -1, 7, -1, -1, 3, 6, 16, 3, 2, 2, 2, 1, 2, 1, 1, 24, 7, 2, 1, 1, 4, -1, 7, 6, 24, 4, -1, 26, 1, 14, 32, 35, 0, -1, 32, 0, 4, 12, 12, 16, 14, 0, 19, 19, -1, 21, -1, 22, 0, 30, -1, 0, -1, 25, 12, -1, -1, 0, 0, -1, 1, -1, 16, -1, 1, 1, 0, 25, 11, -1, 19, 6, 34, -1, 3, -1, 19, 3, 19, 16, 30, 15, 10, 4, 16, 3, 3, 10, -1, -1, 3, 3, 3, 9, 20, 13, 3, 2, 4, 33, 33, -1, 4, -1, 6, 6, 2, 2, 2, 5, 15, -1, 25, 3, 1, -1, 20, 1, 1, 9, 7, -1, 1, 29, 29, 1, 0, 0, 23, -1, 2, -1, 2, 4, 7, 10, -1, 31, 2, 12, 11, -1, -1, 11, -1, 22, 2, 20, 20, 12, -1, -1, -1, 18, 14, 12, 0, -1, 9, 4, 14, 24, 22, -1, 9, 32, 0, 6, 16, -1, -1, -1, 0, 13, 6, 6, 32, 0, 29, 24, 19, 18, -1, 13, 16, 19, 3, 3, 3, 19, 0, 26, 1, 18, 1, -1, 33, 3, 22, -1, 3, 4, -1, 1, 3, 3, 3, -1, 3, 19, 32, -1, 0, 25, 1, 1, 3, 5, -1, 10, 10, -1, 4, 1, 1, 3, 3, 24, 6, 29, 24, 24, -1, -1, 30]\n",
            "-------RUN110-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': \"CountVectorizer(stop_words='english')\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[24, -1, 26, 26, -1, 53, -1, -1, 46, -1, -1, -1, -1, -1, 11, 15, 9, 24, 42, 15, -1, 13, 50, 8, 8, 23, 54, -1, 15, 44, -1, -1, 26, 18, 44, -1, 1, -1, 48, 44, 8, 15, 32, 8, -1, 5, 32, 32, 8, -1, 32, 8, -1, -1, -1, -1, -1, 33, 19, 19, -1, 19, 19, -1, 15, 15, -1, -1, 13, -1, 25, 13, 1, 1, 10, 2, 9, -1, -1, -1, 22, -1, -1, 22, 36, 23, 22, 1, 31, 18, 1, -1, -1, 43, 50, 7, 18, 1, -1, -1, 44, 10, 16, -1, -1, 25, -1, 48, 48, 48, -1, 13, -1, -1, 36, -1, -1, -1, -1, 16, 10, 49, 8, 10, 32, 0, 13, -1, 8, 13, -1, 11, 15, 13, 13, 24, 42, 9, 13, 32, -1, -1, -1, -1, 11, 19, 24, 53, 33, 0, -1, -1, -1, 50, -1, 19, 1, 30, -1, 47, -1, -1, 25, 47, 15, -1, -1, 10, 18, -1, -1, 50, -1, 0, 11, 19, 10, -1, 11, -1, 38, 9, 11, 0, 13, -1, -1, -1, 13, 13, 11, 11, 22, 31, -1, 49, 14, 14, 22, -1, 22, 31, -1, -1, -1, -1, -1, 22, 1, -1, 1, 22, -1, -1, 22, 22, 22, 1, 31, 18, 14, -1, 1, 1, 37, 14, 54, 54, -1, 0, 0, 0, 0, 14, 14, 0, 23, 0, -1, 14, -1, 2, 2, -1, 14, -1, -1, -1, 29, 15, -1, 14, 6, 24, -1, 5, 5, 44, 1, 1, -1, 10, -1, -1, -1, -1, 25, -1, 43, 2, 2, 2, -1, 1, 2, -1, 2, 2, 23, -1, 1, 38, 2, 5, 5, 2, 14, 35, 20, 35, 20, 33, 20, 20, -1, -1, -1, -1, 14, 2, 2, 2, 13, 2, -1, 2, 1, 2, 20, -1, 8, 2, 32, 38, -1, 46, -1, -1, 10, -1, 19, -1, -1, 13, 0, 24, 53, 24, 16, 24, -1, -1, 41, 24, 28, -1, 19, 11, 1, 11, 11, -1, -1, -1, -1, -1, 11, 11, 0, 11, -1, 42, 15, -1, 37, -1, -1, 41, 41, 24, 10, 13, 53, -1, 51, -1, 13, 9, 23, -1, -1, 0, 13, 25, -1, 13, 0, 37, 38, 0, 6, 6, 15, 37, 37, 37, 51, 37, 51, 23, -1, -1, -1, 6, 6, 6, 6, 6, 6, 27, 43, 6, 14, -1, 14, 52, 6, 1, 0, 0, 23, 51, 1, 10, 0, 23, 0, 10, 28, 35, 9, -1, 43, -1, -1, 10, 9, 28, 1, 1, 1, 9, 1, 2, 1, 46, 1, 1, 1, 1, 1, 1, 46, 28, -1, 2, -1, 2, 1, -1, 18, 2, 2, 2, 28, -1, 2, 16, 2, 2, 29, 2, 27, 33, 14, 2, 2, 2, 2, 1, 1, 5, -1, 1, -1, 1, 2, -1, 1, 1, -1, 35, 1, -1, -1, -1, 35, 5, 36, 5, -1, 10, -1, 23, 35, -1, 14, 44, 4, 4, 4, -1, 24, 1, -1, 14, 14, -1, 4, 45, 4, 24, 4, 4, 4, 4, 4, 10, 41, 4, 2, 4, 39, 41, 4, 17, 39, 4, -1, -1, 4, -1, 41, 0, 1, 1, -1, 39, 5, 18, 18, 5, 0, -1, 2, 1, 5, 9, 5, 5, 31, 32, 5, 0, 0, 0, 44, 0, 5, 5, 31, 20, 28, 0, 0, 0, -1, 0, 0, 26, 0, 39, -1, 17, 17, 17, 39, -1, 0, 0, 0, 8, 43, 0, -1, 0, 41, 25, -1, 0, 0, 23, 19, 23, 37, 0, 42, 0, 0, 23, 0, 0, 10, 0, 0, -1, 6, 0, 23, 23, 18, 43, 12, 12, 12, 12, 12, 12, 0, 49, 12, 1, 1, 20, -1, -1, 11, -1, 10, -1, 12, -1, 12, 12, -1, 1, 12, 12, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 16, 2, -1, -1, 4, 4, 52, 4, 0, -1, 4, 16, 4, 15, 4, 24, 4, -1, 4, 8, 1, 8, 16, 50, -1, 4, 4, 4, -1, 4, 4, 4, 0, -1, 4, 24, 16, 5, 5, 5, 5, -1, 5, 5, -1, 1, 5, 43, 13, -1, 5, -1, 26, 1, 13, -1, 0, 0, 10, 9, 16, 4, 11, 0, -1, 26, 0, -1, -1, -1, 0, 9, 42, 16, 0, 0, 20, -1, -1, 0, 0, 16, 0, 0, 8, -1, 0, 40, 17, 0, 16, 17, 20, 0, 7, 25, 0, 17, 50, 1, 17, 17, 17, 36, -1, 8, 9, 0, 3, 3, -1, 25, 42, 9, 1, 0, 7, -1, 7, 20, 6, 1, 1, 0, -1, 40, 40, 40, -1, -1, -1, 2, 2, 12, 12, 1, 49, 12, 1, 12, 12, 49, 12, 1, 38, 3, 3, 3, 3, 3, 16, 0, 3, 12, 3, 3, 3, 3, 45, 3, 3, 12, 10, -1, 1, 3, 40, 10, 3, 3, 3, 1, -1, -1, 4, 4, 10, 4, 4, 4, 4, -1, 27, 27, -1, 27, 27, 27, 36, 27, 23, 27, 23, -1, 2, 42, 36, -1, 6, 8, -1, 6, 7, 7, 7, 7, 7, 16, 7, 19, 6, 42, 6, 6, 6, 6, 9, 8, 26, -1, 8, 7, 25, 25, 18, 16, -1, 16, 26, 26, 10, -1, 26, 26, 31, 26, 18, 26, -1, 11, 11, 11, 20, 11, -1, 11, 22, -1, 4, -1, 9, 11, 2, 45, 1, 20, 3, 31, 22, 20, 32, 1, 20, 20, 20, 6, 1, 0, 20, -1, -1, -1, 9, 46, -1, -1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, -1, 3, 40, 3, 47, 3, 14, 47, 40, 3, 4, -1, 8, 36, 39, -1, -1, -1, 22, 31, -1, 14, 18, 34, 15, 11, 18, 27, 0, 27, 16, 27, 0, 8, -1, -1, 29, -1, 29, 29, 8, 0, 33, 32, 34, -1, 34, 52, 18, 52, 25, 34, 33, 34, 8, 11, 34, 18, 34, 51, 9, -1, 8, 1, 6, -1, 0, 21, 21, -1, -1, -1, 18, 37, 13, -1, 6, 16, -1, 7, 30, 27, -1, 30, 6, 7, 7, 6, -1, -1, 7, 7, 7, 53, 7, 3, 7, 21, 45, 21, 21, 15, -1, 26, 26, -1, -1, 26, -1, 16, 48, 30, 24, -1, -1, 30, 48, 30, 30, 10, 2, 23, 16, 28, 28, 1, 52, -1, -1, 2, -1, 25, 13, 1, 1, -1, 44, 9, 19, 19, 1, 23, 4, -1, 24, 13, -1, 5, -1, 19, -1, 19, 9, 53, -1, 10, -1, -1, -1, -1, 18, 5, 5, 32, 1, 31, 24, 5, 41, 5, 10, 39, 39, -1, 17, 2, 3, 52, 29, 34, 8, 8, 5, 34, 8, 18, 2, 38, -1, 29, 29, 8, 10, 1, 51, -1, 1, 14, 13, -1, 1, 38, -1, -1, 20, -1, 11, 37, 2, 21, 21, 9, 21, -1, 21, 17, 29, 0, -1, -1, 25, -1, -1, -1, 50, 41, 30, 30, 30, 25, 0, 30, 12, 30, 30, 14, -1, 30, 30, 19, 19, 14, 13, 29, 29, 29, 29, 35, 22, 29, 4, 35, 0, 0, 0, -1, 33, 6, 0, 1, 0, 17, 6, 28, 28, 21, 15, 9, 45, 0, 21, 33, -1, 20, 21, -1, 45, 21, 24, -1, 9, -1, 3, 1, -1, 28, 0, 36, 5, 5, -1, 5, -1, 18, 5, 3, 8, 8, -1, 18, 1, 8, 5, 10, 33, 1, 0, 0, 3, 1, 1, 48, 0, 1, 0, 15, 1, 17, 47, -1, 18, 54, 2, -1, 11, 7, 35, 19, 12, -1, -1, 22, 31, -1, 15, -1, 19, -1, -1, 26, 35, 0, -1, 35, 0, 16, 7, 7, 0, -1, 32, 33, 7, 8, 8, 7, 7, 38, 31, 15, 2, 6, 7, 0, 0, 0, 3, 0, 16, 19, -1, 46, 9, 17, -1, 22, 0, 22, -1, 1, -1, 5, 0, 9, 0, 12, 15, 0, 2, 2, 2, 3, 2, 3, 3, 21, 5, 2, 3, 3, -1, 22, 5, 49, 21, 31, -1, 23, 47, 14, 28, 22, 1, 5, 28, 1, 31, 0, 0, 15, 14, 1, 17, 17, -1, 39, -1, 9, 1, 25, -1, 1, 9, 16, 0, 46, -1, 1, 1, -1, 3, 2, 15, -1, 3, 3, 1, -1, 11, -1, 17, 12, 20, -1, 0, -1, 17, 0, 17, 15, 25, 13, 7, 33, 15, 0, 0, 7, -1, -1, 0, 0, 0, 6, 38, 10, 0, 2, 36, 34, 34, -1, -1, -1, 12, 49, 2, 2, 2, 4, 13, -1, 16, 0, 3, -1, 38, 3, 3, 6, 5, 18, 3, 27, 27, 3, 1, 1, 32, 43, 2, 54, 2, -1, 5, 7, -1, -1, 2, 0, 11, 9, -1, 11, -1, 9, 2, 29, 29, 0, 20, 0, -1, 19, 14, 0, 1, 40, 6, 33, 14, 21, 9, 18, 6, 28, 1, 12, -1, -1, -1, -1, 1, 10, -1, 12, 28, 1, 27, 21, 17, 19, 45, 10, 15, 17, 0, 0, 0, 17, 1, 23, 3, 19, 3, 40, 34, 0, 9, -1, 0, -1, 0, 47, 0, 0, 0, -1, 0, 17, 28, -1, 1, -1, 3, 3, 0, 4, -1, 7, 7, -1, 36, 3, 3, 0, 0, 21, 12, 27, 21, 21, -1, 0, 25]\n",
            "Skipping coherence calculation due to error: unable to interpret topic as either a list of tokens or a list of ids\n",
            "-------RUN111-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': 'CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7df3b00f5c90>)', 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[27, 37, 26, 26, 15, 6, -1, 19, 52, -1, -1, -1, -1, -1, 12, 18, 11, 27, 8, 18, -1, 15, 16, 8, 8, 24, 45, 18, 18, 46, -1, -1, 26, 32, -1, -1, 0, 41, 50, 46, 8, 18, 34, 8, 34, 7, 34, 34, 8, -1, 34, 8, 16, -1, -1, -1, -1, 37, 19, 19, -1, 19, 19, 16, 18, 18, -1, -1, 15, 2, 28, 15, 0, 0, 13, 2, 11, -1, 46, -1, 22, -1, -1, -1, 38, 24, 22, 0, 31, 32, 0, 16, 22, 42, 16, 14, -1, 0, -1, -1, 46, 13, 6, 11, 48, 28, -1, 50, 50, 50, -1, 15, -1, 0, 38, -1, 5, 16, -1, -1, 13, 5, 8, 13, 34, -1, 15, 31, 8, 15, 38, 12, 18, 15, 15, 27, 8, 11, 15, 34, -1, 16, 25, -1, 12, 19, 27, 6, -1, -1, -1, -1, -1, 16, 48, 19, 0, 25, -1, 1, -1, 1, 28, 1, 18, -1, -1, 13, -1, 18, 16, 16, -1, -1, 12, 19, 13, 49, 12, -1, 41, 11, 12, -1, 15, -1, 6, 38, 15, 15, 12, 12, 22, 31, -1, 5, 17, 17, 22, 23, 22, 31, 22, -1, -1, -1, 5, 22, 0, 42, 0, 22, -1, -1, 22, 22, 22, 0, 31, -1, 17, -1, 0, 0, 16, 17, 45, 45, -1, 44, 44, -1, 44, 17, 17, -1, 24, 44, -1, 17, -1, 2, 2, -1, 17, 48, -1, -1, 33, 18, -1, 17, 10, -1, -1, 7, 7, 46, 0, 0, 45, 13, -1, -1, -1, -1, 28, 45, 42, 2, 2, 2, -1, 0, 2, -1, 2, 2, -1, -1, 0, 41, 2, 7, 7, 2, 17, 39, 20, 39, 20, -1, 20, 20, 5, -1, -1, -1, 17, 2, 2, 2, 15, 2, -1, 2, 0, 2, 20, -1, 8, 2, 34, 41, 49, -1, -1, 48, 13, -1, 19, -1, -1, 15, -1, 27, -1, 27, 6, 27, -1, -1, 40, 27, 30, -1, 19, 12, 0, 12, 12, 24, -1, 16, -1, -1, 12, 12, 53, 12, -1, 8, -1, -1, 16, -1, 1, 40, 40, 27, 13, 15, 6, -1, 40, -1, 15, 11, 24, 15, 13, 9, 15, 28, 25, 15, 53, 16, 41, 53, 10, 10, 18, 16, 16, 16, -1, 16, 40, 24, -1, -1, -1, 10, 10, 10, 10, 10, 10, 29, 42, 10, 17, 6, 17, 51, 10, 0, 44, -1, 24, 40, 0, 13, -1, 24, 44, 6, 30, 39, 11, -1, 42, 6, -1, -1, 11, 30, 0, 0, 0, 11, 0, 2, 0, 52, 0, 0, 0, 0, 0, 0, 52, 30, -1, 2, -1, 2, 0, -1, 32, 2, 2, 2, 30, -1, 2, 6, 2, 2, 33, 2, 29, 37, 17, 2, 2, 2, 2, 0, 0, 7, 6, 0, -1, 0, 2, -1, -1, 0, -1, 39, 0, 16, 20, -1, 39, 7, 38, 7, 20, 13, -1, 24, 39, 20, 17, 46, 4, 4, 4, -1, 27, 0, -1, 17, 17, -1, 4, 47, 4, 27, 4, 4, 4, 4, 4, 13, 40, 4, 2, 4, 36, 40, 4, 21, 36, 4, -1, 13, 4, 4, 36, -1, 0, 0, -1, 36, 7, -1, 32, 7, -1, -1, 2, 0, 7, 11, 7, 7, 31, 34, 7, 9, 9, 9, 46, 9, 7, 7, 31, 20, 30, 3, 3, 3, -1, 3, 9, 26, 9, 36, -1, 21, 21, 21, 36, 36, 9, 9, 9, 8, 42, 9, 36, 9, 36, 28, 40, 53, 53, 24, 19, 24, 16, 3, 8, 3, 3, -1, 3, 3, 13, 3, 3, -1, 10, 3, 24, 24, 32, 42, 5, 5, 5, 5, 5, 5, 9, 5, 5, 0, 0, 20, -1, -1, 12, 6, 13, -1, 5, -1, 5, 5, 5, 0, 5, 5, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 6, 2, -1, -1, 4, 4, 51, 4, -1, -1, 4, 6, 4, 18, 4, 27, 4, 49, 4, 8, 0, 8, 6, 16, 49, 4, 4, 4, -1, 4, 4, 4, 3, -1, 4, 27, 6, 7, 7, 7, 7, -1, 7, 7, 6, 0, 7, 42, 15, -1, 7, -1, 26, 0, 15, -1, 9, 9, 13, 11, 6, 4, 12, 9, -1, 26, 9, -1, -1, -1, 9, 11, 8, 6, 3, 3, 20, -1, 20, 3, 3, 6, 3, 3, 8, -1, -1, 43, 21, 3, 6, 21, 20, 3, -1, 28, 3, 21, 16, 0, 21, 21, 21, 38, -1, 8, 11, 44, 1, 1, 1, 28, 8, 11, 0, -1, 14, -1, -1, 20, 10, 0, 0, -1, -1, 43, 43, 43, 0, 49, 16, 2, 2, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 0, 41, 1, 1, 1, 1, 1, 6, -1, 1, 5, 1, 1, 1, 1, 47, 1, 1, 5, 13, 49, 0, 1, 43, 13, 1, 1, 1, 0, -1, -1, 4, 4, 13, 4, 4, 4, 4, -1, 29, 29, -1, 29, 29, 29, 38, 29, -1, 29, 24, -1, 2, 8, 38, 34, 10, 8, -1, 10, 14, 14, 14, 14, 14, 6, 14, 19, 10, 8, 10, 10, 10, 10, 11, 8, 26, 18, 8, 14, 28, 28, 32, 6, -1, 6, 26, 26, 13, -1, 26, 26, 31, 26, -1, 26, -1, 12, 12, 12, 20, 12, -1, 12, 22, -1, 4, -1, 11, 12, -1, 47, 0, 20, 1, 31, 22, 20, 34, -1, 20, 20, 20, 10, 0, -1, 20, -1, -1, 2, 11, 52, -1, 16, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, -1, 1, 43, 1, 1, 1, 17, 1, 43, 1, 4, -1, 8, 38, 36, -1, -1, 6, 22, 31, -1, 17, -1, 35, 18, 12, 32, 29, 9, 29, 6, 29, 9, 8, -1, -1, 33, -1, 33, 33, 8, 9, -1, 34, 35, -1, 35, 51, 32, 51, 28, 35, -1, 35, 8, 12, 35, 24, 35, -1, 11, -1, 8, 0, 10, -1, 3, 23, 23, -1, -1, -1, 24, 16, 15, -1, 10, 6, -1, 14, 25, 29, 48, 25, 10, 14, 14, 10, -1, -1, 14, 14, 14, 6, 14, 1, 14, 23, 47, 23, 23, 18, 11, 26, 26, -1, -1, 26, 6, 26, 50, 25, 27, -1, -1, 25, 50, 25, 25, 13, 2, 24, 6, 30, 30, 0, 51, -1, -1, 2, 2, 28, 15, 0, 0, -1, 46, 11, 19, 19, 0, 24, 4, -1, 27, 15, 1, 7, -1, 19, 37, 19, 11, 6, -1, 13, -1, -1, -1, 7, 32, 7, 7, 34, 0, 31, 27, 7, 40, 7, 13, 36, 36, -1, 21, 2, 1, 51, 33, 35, 8, 8, -1, 35, 8, 32, 2, -1, -1, -1, 33, 8, 13, 0, -1, -1, 0, 17, 15, -1, 0, 41, -1, -1, 20, -1, 12, 16, 2, 23, 23, 11, 23, -1, 23, 21, 33, 3, 1, 49, 28, -1, -1, 5, 16, 40, 25, 25, 25, 28, 9, 25, 5, 25, 25, 17, -1, 25, 25, 19, 19, 17, 15, 33, 33, 33, 33, 39, 22, -1, 4, 39, -1, -1, 3, 6, 37, 10, 3, 0, -1, 21, 10, 30, 30, 23, -1, 11, 47, 3, 23, 37, 19, 20, 23, -1, 47, 23, 27, -1, 11, -1, 1, 0, -1, 30, 9, 38, 7, 7, -1, 7, -1, 32, 7, 1, 8, 8, -1, 32, 0, 8, 7, 13, 37, 0, 9, 9, 1, 0, 0, 50, 9, 0, 9, 18, 0, 21, 1, -1, 32, 45, 2, -1, 12, 14, 39, 19, 5, -1, -1, 22, 31, -1, 18, 5, 19, -1, -1, 26, 39, 3, -1, 39, 9, 6, 14, 14, 3, 45, 34, 37, 14, 8, 8, 14, 14, 41, 31, 18, 2, 10, 14, 3, 3, 3, 1, 9, 6, 19, -1, 52, -1, 21, 6, 22, -1, 22, -1, 0, -1, 7, 3, 11, 3, 5, 18, 3, 2, 2, 2, 1, 2, 1, 1, 23, 7, 2, 1, 1, -1, 22, 7, 5, 23, 31, 48, 24, 1, 17, 30, 22, 0, -1, 30, 0, 31, 9, 9, 18, 17, 0, 21, 21, -1, 36, -1, 11, 0, 28, -1, 0, 11, 6, 9, 52, 48, 0, 0, 5, 1, 2, -1, -1, 1, 1, 0, 6, 12, -1, 21, 5, 20, -1, 3, -1, 21, 3, 21, 18, 28, 15, 14, 37, 18, 3, 44, 14, -1, -1, 3, 3, 3, 10, 41, 13, 3, 2, 38, 35, 35, 2, 31, 6, 5, 5, 2, 2, 2, 4, 15, -1, 6, 3, 1, -1, 41, 1, 1, 10, 7, 32, 1, 29, 29, 1, 0, 0, -1, 42, 2, 45, 2, -1, 7, 14, -1, 25, 2, 9, 12, 11, 6, 12, 35, 11, 2, 33, 33, 9, 20, 3, -1, 19, 17, 9, 0, 43, 10, 37, 17, 23, 11, 32, 10, 30, 0, 5, 18, -1, -1, -1, 0, 13, -1, 5, 30, 0, 29, 23, 21, 19, 47, 13, 18, 21, 3, 3, 3, 21, 0, 24, 1, 19, 1, 43, 35, 3, 11, -1, 3, 37, 9, 1, 3, 3, 3, -1, 3, 21, 30, -1, 0, 6, 1, 1, 3, 4, -1, 14, 14, -1, -1, 1, 1, 3, -1, 23, 5, 29, 23, 23, -1, 9, 28]\n",
            "Skipping coherence calculation due to error: unable to interpret topic as either a list of tokens or a list of ids\n",
            "-------RUN112-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': \"CountVectorizer(stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b02d2c50>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[26, 38, 27, 27, -1, -1, -1, -1, 56, -1, -1, -1, -1, -1, 12, 20, 23, 26, 7, 20, -1, 9, 10, 7, 7, 13, 39, -1, 20, 45, -1, -1, 27, 35, 45, -1, 0, -1, 52, 45, 7, 20, 33, 7, 33, 5, 33, 33, 7, -1, 33, 7, 10, 10, -1, -1, -1, 38, 21, 21, -1, 21, 21, 10, 20, 20, -1, -1, 9, 1, 28, 9, 0, 0, 15, 1, 23, -1, -1, -1, 19, -1, 32, 19, -1, 13, 19, 0, 29, 35, 0, 10, 19, 44, 10, 11, 13, 0, -1, -1, 45, 15, 14, 50, -1, 28, -1, 52, 52, 52, -1, 9, -1, 24, 40, -1, 6, 10, -1, 14, 15, 6, 7, 15, 33, -1, 9, 29, 7, 9, -1, 12, 20, 9, 9, 26, 7, 23, 9, 33, -1, 10, -1, -1, 12, 21, 26, -1, 38, -1, -1, -1, -1, 10, 54, 21, 0, 24, -1, 48, -1, -1, 28, 48, 20, -1, -1, 15, 13, -1, 10, 10, 14, -1, 12, 21, -1, -1, 12, 9, 46, 23, 12, -1, 9, -1, -1, -1, 9, 9, 12, 12, 19, 29, 13, 6, 16, 16, 19, 22, 19, -1, 19, -1, -1, -1, 6, 19, 0, -1, 0, 19, 40, -1, 19, 19, 19, 0, 29, 13, 16, -1, 0, 0, 10, 16, 39, 39, -1, 43, 43, -1, 43, 16, 16, -1, 13, 43, -1, 16, -1, 1, 1, -1, 16, -1, -1, -1, 31, 20, -1, 16, 8, 24, -1, 5, 5, 45, 0, 0, 39, 15, -1, -1, -1, -1, 28, 39, 44, 1, 1, 1, -1, 0, 1, -1, 1, 1, 13, -1, 0, 46, 1, 5, 5, 1, 16, 37, 17, 37, 17, -1, 17, 17, 6, -1, -1, -1, 16, 1, 1, 1, 9, 1, -1, 1, 0, 1, 17, -1, 7, 1, 33, 46, -1, -1, -1, 54, 15, -1, 21, -1, -1, 9, 3, 26, -1, 26, 14, 26, -1, -1, 41, 26, 30, -1, 21, 12, 0, 12, 12, 13, 13, 10, -1, -1, 12, 12, 53, 12, -1, 7, -1, -1, 10, -1, -1, 41, 41, 26, 50, 9, -1, -1, -1, -1, 9, 23, 13, 9, 15, -1, 9, 28, 10, 9, 53, 10, -1, 53, 8, 8, 20, 10, 10, 10, 47, 10, 47, 13, -1, -1, -1, 8, 8, 8, 8, 8, 8, 25, 44, 8, 16, -1, 16, 51, 8, 0, 43, -1, 13, 47, 0, 15, -1, 13, -1, 15, 30, 37, -1, -1, 44, 14, -1, -1, 23, 30, 0, 0, 0, 23, 0, 1, 0, 56, 0, 0, 0, 0, 0, 0, 56, 30, -1, 1, -1, 1, 0, -1, 35, 1, 1, 1, 30, -1, 1, 14, 1, 1, -1, 1, 25, 38, 16, 1, 1, 1, 1, 0, 0, 5, -1, 0, -1, 0, 1, -1, -1, 0, -1, 37, 0, 10, 17, -1, 37, 5, -1, 5, 17, 15, -1, 13, 37, 17, 16, 45, 4, 4, 4, -1, 26, 0, -1, 16, 16, -1, 4, 49, 4, 26, 4, 4, 4, 4, 4, 15, 41, 4, 1, 4, 34, 41, 4, 18, 34, 4, -1, -1, 4, 4, 34, -1, 0, 0, -1, 34, 5, 13, 35, 5, -1, -1, 1, 0, 5, 50, 5, 5, 29, 33, 5, 36, -1, 36, 45, -1, 5, 5, 29, 17, 30, 3, 3, 3, -1, 3, -1, 27, 36, 34, -1, 18, 18, 18, 34, 34, -1, -1, 55, 7, 44, 36, 34, 36, 34, 28, 41, -1, 53, 13, -1, 13, 10, 3, 7, 3, 3, -1, -1, 3, 15, 3, 3, -1, 8, 3, 13, -1, 35, 44, 6, 6, 6, 6, 6, 6, -1, 6, 6, 0, 0, 17, -1, -1, 12, -1, 15, -1, 6, -1, 6, 6, 6, 0, 6, 6, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 1, -1, -1, 4, 4, 51, 4, -1, -1, 4, 14, 4, 20, 4, 26, 4, -1, 4, 7, 0, 7, 14, 10, -1, 4, 4, 4, -1, 4, 4, 4, -1, -1, 4, 26, 14, 5, 5, 5, 5, -1, 5, 5, -1, 0, 5, 44, 9, -1, 5, -1, 27, 0, 9, 5, 36, 36, 15, 23, 14, 4, 12, -1, -1, 27, -1, -1, -1, -1, 36, 23, 7, 14, 3, 3, 17, -1, 17, 3, 3, 14, 3, 3, 7, -1, 3, -1, 18, 3, 14, 18, 17, 3, -1, 28, 3, 18, 10, 0, 18, 18, 18, 40, -1, 7, 23, 43, 2, 2, 2, 28, 7, 23, 0, -1, 11, -1, -1, 17, 8, 0, 0, -1, -1, 42, 42, 42, -1, -1, 10, 1, 1, 6, 6, 0, 6, 6, 0, 6, 6, 6, 6, 0, 46, 2, 2, 2, 2, 2, 14, -1, 2, 6, 2, 2, 2, 2, 49, 2, 2, 6, 15, -1, 0, 2, 42, 15, 2, 2, 2, 0, -1, -1, 4, 4, -1, 4, 4, 4, 4, 19, 25, 25, -1, 25, 25, 25, 40, 25, 13, 25, 13, -1, 1, 7, 40, 33, 8, 7, -1, 8, 11, 11, 11, 11, 11, 14, 11, 21, 8, 7, 8, 8, 8, 8, 50, 7, 27, -1, 7, 11, 28, 28, 35, 14, -1, 14, 27, 27, -1, -1, 27, 27, 29, 27, -1, 27, -1, 12, 12, 12, 17, 12, -1, 12, 19, -1, 4, -1, 23, 12, -1, 49, 0, 17, 2, 29, 19, 17, 33, 0, 17, 17, 17, 8, 0, -1, 17, -1, -1, -1, 23, 56, -1, 10, 0, 2, 2, 2, 2, 2, 2, 2, -1, -1, 2, 3, 2, -1, 2, 42, 2, 48, 2, 16, 48, 42, 2, 4, 15, 7, 40, 34, 47, -1, -1, 19, 29, -1, 16, 13, 32, 20, 12, 35, 25, 55, 25, 14, 25, 36, 7, 38, -1, 31, -1, 31, 31, 7, -1, -1, 33, 32, -1, 32, 51, 35, 51, 28, 32, -1, 32, 7, 12, 32, 13, 32, 47, 23, -1, 7, 0, 8, -1, 3, 22, 22, -1, -1, -1, 13, 10, 9, -1, 8, 14, -1, 11, 24, 25, 54, 24, 8, 11, 11, 8, -1, -1, 11, 11, 11, -1, 11, 2, 11, 22, 49, 22, 22, 20, -1, 27, 27, -1, 14, 27, 14, -1, -1, 24, 26, -1, -1, 24, 52, 24, 24, 15, 1, 13, 14, 30, 30, 0, 51, -1, -1, 1, 1, 28, 9, 0, 0, -1, 45, 23, 21, 21, 0, 13, 4, -1, 26, 9, -1, 5, -1, 21, -1, 21, -1, -1, 20, 15, -1, -1, -1, 39, 35, 5, 5, 33, 0, 29, 26, 5, 41, 5, 15, 34, 34, -1, 18, 1, 2, 51, 31, 32, 7, 7, -1, 32, 7, 35, 1, 31, 19, 31, 31, 7, 15, 0, 47, -1, 0, 16, 9, -1, 0, 46, -1, -1, 17, -1, 12, 10, 1, 22, 22, 23, 22, -1, 22, 18, 31, -1, -1, -1, 28, -1, -1, 6, 10, 41, 24, 24, 24, 28, -1, 24, 6, 24, 24, 16, -1, 24, 24, 21, 21, 16, 9, 31, 31, 31, 31, 37, 19, -1, 4, 37, -1, -1, 3, 14, 38, 8, 3, 0, 3, 18, 8, 30, 30, 22, 20, -1, 49, -1, 22, 38, -1, 17, 22, -1, 49, 22, 26, -1, 23, -1, 2, 0, 47, 30, -1, 40, 5, 5, -1, 5, 5, 35, 5, 2, 7, 7, -1, 35, 0, 7, 5, 15, -1, 0, -1, 36, 2, 0, 0, 52, 36, 0, -1, 20, 0, 18, 48, -1, 35, 39, 1, -1, 12, 11, 37, 21, 6, 5, -1, 19, 29, -1, 20, 6, 21, -1, -1, 27, 37, 3, -1, 37, -1, 14, 11, 11, 3, 39, 33, 38, 11, 7, 7, 11, 11, 46, 29, 20, 1, 8, 11, 3, -1, 43, 2, -1, -1, 21, -1, 56, -1, 18, -1, 19, 53, 19, -1, 0, -1, 5, 3, 50, 3, 6, -1, 3, 1, 1, 1, 2, 1, 2, 2, 22, 5, 1, 2, 2, 29, 19, 5, 6, 22, 29, 54, 13, 48, 16, 30, 19, 0, -1, 30, 0, 29, 36, 36, 20, 16, 0, 18, 18, -1, 34, -1, 23, 0, 28, -1, 0, 50, 14, -1, -1, 54, 0, 0, -1, 2, -1, 20, -1, 2, 2, 0, 14, 12, -1, 18, 6, 17, -1, 3, -1, 18, 3, 18, 20, 28, 9, 11, 38, 20, 3, 43, 11, -1, -1, 3, 3, 3, 8, -1, 15, 3, 1, 40, 32, 32, 1, 29, -1, 6, 6, 1, 1, 1, 4, 9, -1, 14, 3, 2, -1, 46, 2, 2, 8, 5, -1, 2, 25, 25, 2, 0, 0, 39, 44, 1, 39, 1, -1, 5, 11, -1, 24, 1, 55, 12, 50, -1, 12, 32, 23, 1, 31, 31, 55, 17, -1, -1, 21, 16, 55, 0, 42, 8, 38, 16, 22, 23, -1, 8, 30, 0, 6, 20, -1, -1, -1, 0, 15, 6, 6, 30, 0, 25, 22, 18, 21, 49, 15, 20, 18, 3, 3, 3, 18, 0, 13, 2, 21, 2, 42, 32, 3, 23, -1, 3, -1, -1, 48, 3, 3, 3, -1, 3, 18, 30, -1, 0, 14, 2, 2, 3, 4, -1, 11, 11, -1, 40, 2, 2, 3, -1, 22, 6, 25, 22, 22, -1, -1, 28]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------RUN113-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': \"CountVectorizer(ngram_range=(1, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b891dd10>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[22, 2, 28, 28, -1, 50, -1, 16, 44, -1, -1, -1, -1, -1, 11, 25, 15, 22, 7, 25, -1, 14, 9, 7, 7, 24, 47, -1, 25, 42, -1, -1, 28, 31, 42, -1, 1, 39, 46, 42, 7, 25, 33, 7, 33, 6, 33, 33, 7, -1, 33, 7, 9, -1, 38, 13, 4, 2, 16, 16, 1, 16, 16, 9, 25, 25, -1, -1, 14, -1, 29, 14, 1, 1, 19, -1, 15, -1, -1, -1, 13, 2, -1, 13, 2, 24, 13, 1, 2, 31, 1, 9, 13, 38, 9, -1, -1, 1, -1, -1, 42, 19, 52, 15, 48, 29, -1, 46, 46, 46, -1, 14, -1, -1, 2, 4, 4, 9, -1, 26, 19, 4, 7, 19, 33, -1, 14, 2, 7, 14, 2, 11, 25, 14, 14, 22, 7, 15, 14, 33, 9, 9, -1, -1, 11, 16, 22, 50, 2, 0, -1, -1, -1, 9, 48, 16, 1, 34, -1, 51, -1, -1, 29, 51, 25, -1, -1, 19, -1, 2, 9, 9, -1, 0, 11, 16, -1, -1, 11, -1, 39, 15, 11, 0, 14, -1, -1, 2, 14, 14, 11, 11, 13, 2, -1, 4, 17, 17, 13, 4, 13, 2, 13, -1, 9, -1, 4, 13, -1, -1, 1, 13, 2, 2, 13, 13, 13, 1, 2, -1, 17, -1, 1, 1, 9, 17, 47, 47, 2, 0, 0, 0, 0, 17, 17, 0, 24, 0, -1, 17, -1, 12, 12, -1, 17, -1, -1, -1, 32, 2, -1, 17, 8, 22, -1, 6, 6, 42, 1, 1, 47, 19, -1, 16, 2, 2, 29, 47, 38, -1, 12, 12, -1, 1, 12, -1, 12, 12, 24, -1, 1, 39, 12, 6, 6, -1, 17, 37, 18, 37, 18, 2, 18, 18, 4, -1, -1, -1, 17, 21, 21, 21, 14, 21, 1, 21, 1, -1, 18, -1, 7, 21, 33, 39, -1, -1, -1, 48, 19, -1, 16, -1, -1, 14, 0, 22, 50, 22, 26, 22, -1, -1, 40, 22, 30, 38, 16, 11, 1, 11, 11, 24, -1, 9, -1, -1, 11, 11, 0, 11, -1, 7, -1, 2, 9, 0, -1, 40, 40, 22, -1, 14, 50, -1, -1, -1, 14, 15, 24, -1, -1, 0, 14, 29, 9, 14, 0, 9, 39, 0, 8, 8, 25, 9, 9, 9, 45, 9, 45, 24, -1, -1, -1, 8, 8, 8, 8, 8, 8, 27, 38, 8, 17, -1, 17, 49, 8, 1, 0, 0, 24, 45, 1, 19, 0, 24, 0, -1, 30, 37, 15, -1, 38, -1, -1, -1, 15, 30, 1, 1, 1, 15, 1, 21, 1, 44, 1, 1, 1, 1, 1, 1, 44, 30, -1, 12, -1, 12, 1, -1, 31, 12, 12, 12, 30, 2, 12, -1, 12, -1, 32, 12, 27, 2, 17, 12, -1, 12, 12, 1, 1, 6, -1, 1, -1, 1, 12, -1, 1, 1, -1, 37, 1, 9, 18, -1, 37, 6, 2, 6, 18, 19, -1, 24, 37, 18, 17, 42, 5, 5, 5, 0, 22, 1, -1, 17, 17, -1, 5, 41, 5, 22, 5, 5, 5, 5, 5, 19, 40, 5, -1, 5, 36, 40, 5, 20, 36, 5, -1, 19, 5, 5, 36, 0, 1, 1, 2, 36, 6, -1, 31, 6, 0, -1, 21, 1, 6, 15, 6, 6, 2, 33, 6, 0, 0, 0, 42, 0, 6, 6, 2, 18, 30, 0, 0, 0, 0, 0, 0, 28, 0, 36, 4, 20, 20, 20, 36, 36, 0, 0, 0, 7, 38, 0, 36, 0, 40, 29, 40, 0, 0, 24, 16, 24, 9, 0, 7, 0, 0, 24, 0, 0, 19, 0, 0, 16, 8, 0, 24, -1, 31, 38, 4, 4, 4, 4, 4, 4, 0, 4, 4, 1, 1, 18, -1, -1, 11, -1, 19, -1, 4, -1, 4, 4, 4, 1, 4, 4, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 52, 12, -1, 44, 5, 5, 49, 5, 0, -1, 5, 26, 5, 25, 5, 22, 5, -1, 5, 7, 1, 7, 52, 9, -1, 5, 5, 5, -1, 5, 5, 5, 0, -1, 5, 22, 52, 6, 6, 6, 6, 16, 6, 6, -1, 1, 6, 38, 14, -1, 6, 2, 28, 1, 14, -1, 0, 0, 19, 15, 26, 5, 11, 0, -1, 28, 0, -1, 2, -1, 0, 15, 7, 52, 0, 0, 18, 2, 18, 0, 0, 26, 0, 0, 7, -1, 0, -1, 20, 0, 26, 20, 18, 0, -1, 29, 0, 20, 9, 1, 20, 20, 20, 2, -1, 7, 15, 0, 3, 3, -1, 29, 7, 15, 1, 0, 10, -1, 10, 18, 8, 1, 1, 0, -1, 43, 43, 43, -1, -1, 9, 12, 12, 4, 4, 1, 4, 4, 1, 4, 4, 4, 4, 1, 39, 3, 3, 3, 3, 3, 26, 0, 3, 4, 3, 3, 3, 3, 41, 3, 3, 4, 19, -1, 1, 3, 43, 19, 3, -1, 3, 1, -1, -1, 5, 5, -1, 5, 5, 5, 5, 13, 27, 27, -1, 27, 27, 27, 2, 27, 24, 27, 24, 4, 12, 7, 2, 33, 8, 7, -1, 8, 10, 10, 10, 10, 10, 26, 10, 16, 8, 7, 8, 8, 8, 8, 15, 7, 28, 2, 7, 10, 29, 29, 31, 26, -1, 26, 28, 28, 19, -1, 28, 28, 2, 28, -1, 28, -1, 11, 11, 11, 18, 11, 2, 11, 13, -1, 5, 13, 15, 11, -1, 41, 1, 18, 3, 2, 13, 18, 33, 1, 18, 18, 18, 8, 1, 0, 18, 2, -1, -1, 15, 44, -1, 9, 1, 3, 3, 3, 3, 3, 3, 3, -1, 3, 3, 0, 3, -1, 3, 43, 3, 51, 3, 17, -1, 43, 3, 5, -1, 7, 2, 36, 45, 6, -1, 13, 2, -1, 17, -1, 35, 25, 11, 31, 27, 0, 27, 26, 27, 0, 7, 2, -1, 32, -1, 32, 32, 7, 0, 2, 33, 35, -1, 35, 49, 31, 49, 29, 35, 2, 35, 7, 11, 35, -1, 35, 45, 15, -1, 7, 1, 8, -1, 0, 23, 23, -1, -1, 2, -1, 9, 14, -1, 8, 26, -1, 10, 34, 27, 48, 34, 8, 10, 10, 8, -1, -1, 10, 10, 10, 50, 10, 3, 10, 23, 41, 23, 23, 25, -1, 28, 28, -1, -1, 28, 26, -1, 46, 34, 22, -1, 9, 34, 46, 34, -1, 19, 21, 24, 26, 30, 30, 1, 49, -1, -1, 12, -1, 29, 14, 1, 1, -1, 42, 15, 16, 16, 1, 24, 5, -1, 22, 14, -1, 6, -1, 16, 2, 16, -1, 50, 2, 19, -1, -1, -1, 6, 31, -1, 6, 33, 1, 2, 22, 6, 40, 6, 19, 36, 36, -1, 20, 21, 3, 49, 32, 35, 7, 7, -1, 35, 7, 31, -1, -1, 13, 32, 32, 7, 19, 1, 45, -1, 1, 17, 14, -1, 1, 39, -1, -1, 18, -1, 11, 9, 21, 23, 23, 15, 23, -1, 23, 20, 32, 0, -1, -1, 29, -1, 13, 4, 9, 40, 34, 34, 34, 29, 0, 34, 4, 34, 34, 17, -1, 22, -1, 16, 16, 17, 14, 32, 32, 32, 32, 37, 13, -1, 5, 37, 0, 0, 0, -1, 2, 8, 0, 1, 0, 20, 8, 30, 30, 23, -1, -1, 41, 0, 23, 2, -1, 18, 23, -1, 41, 23, 22, -1, 15, 2, 3, 1, 45, 30, 0, 2, 6, 6, -1, 6, -1, 31, 6, 3, 7, 7, -1, 31, 1, 7, 6, 19, 2, 1, 0, 0, 3, 1, 1, 46, 0, 1, 0, 25, 1, 20, 51, -1, 31, 47, 12, 4, 11, 10, 37, 16, 4, -1, -1, 13, 2, -1, 25, 4, 16, 4, -1, 28, 37, 0, 41, 37, 0, 26, 10, 10, 0, 6, 33, 2, 10, 7, 7, 10, 10, 39, 2, 25, 21, 8, 10, 0, 0, 0, -1, 0, -1, 16, -1, 44, -1, 20, -1, 13, 0, 13, 2, 1, -1, -1, 0, 15, 0, 4, -1, 0, 21, 21, 21, 3, -1, 3, 3, 23, 6, 21, 3, 3, 2, 13, 6, 4, 23, 2, 48, 24, 51, 17, 30, 13, 1, -1, 30, 1, 2, 0, 0, 25, 17, 1, 20, 20, -1, 36, -1, 15, 1, 29, -1, 1, 15, 26, 0, 44, 48, 1, 1, 4, 3, -1, 25, -1, 3, 3, 1, -1, 11, -1, 20, 4, 18, -1, 0, -1, 20, 0, 20, 25, 29, 14, 10, 2, 25, 0, 0, 10, -1, -1, 0, 0, 0, 8, -1, 19, 0, 12, 2, 35, 35, -1, 2, -1, 4, 4, 21, 12, 21, 5, 14, 26, 26, 0, 3, -1, 39, -1, 3, 8, 6, 31, 3, 27, 27, -1, 1, 1, 33, 38, -1, 47, -1, 2, 6, 10, -1, 22, 21, 0, 11, 15, -1, 11, 13, -1, 21, 32, 32, 0, 18, 0, 0, 16, 17, 0, 1, 43, 8, 2, 17, 23, 15, 31, 8, 30, 1, 4, -1, -1, -1, -1, 1, 19, 4, 4, 30, 1, 27, 23, 20, 16, 41, 19, 25, 20, 0, 0, 0, 20, 1, 24, 3, 16, 3, 43, 35, 0, 15, -1, 0, 2, 0, 51, 0, 0, 0, -1, 0, 20, 30, -1, 1, 26, 3, 3, 0, 5, -1, 10, 10, -1, 2, 3, 3, 0, 0, 23, 4, 27, 23, 23, -1, 0, 29]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------RUN114-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': \"CountVectorizer(ngram_range=(2, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df4a9008410>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[25, -1, 23, 23, -1, 41, -1, -1, 52, -1, -1, -1, -1, -1, 11, 21, 17, 25, -1, 21, -1, 13, -1, 10, 10, 22, 42, -1, 21, 43, -1, -1, 23, 33, -1, -1, 1, -1, 45, 43, 10, 21, 31, 10, 31, 6, 31, 31, 10, -1, 31, 10, -1, -1, -1, 4, -1, 30, 18, 18, -1, 18, 18, -1, -1, 21, -1, -1, 13, 2, 27, 13, 1, 1, 12, 2, 17, 45, 43, 19, 15, -1, -1, 15, 7, 22, 15, 1, 7, 33, 1, 37, 15, 44, 48, 9, -1, 1, -1, 54, 43, 12, -1, -1, -1, 27, -1, 45, 45, 45, 2, 13, -1, 24, 7, -1, 4, 48, -1, -1, 12, 4, 10, 12, 31, -1, 13, 7, 10, 13, -1, 11, 21, 13, 13, 25, 49, 17, 13, 31, -1, -1, -1, -1, 11, 18, 25, 41, 30, -1, -1, -1, -1, 48, -1, 18, 1, 24, -1, 50, -1, -1, 27, 50, 21, -1, -1, 12, -1, 53, -1, 48, 19, 0, 11, 18, 19, -1, 11, -1, 36, 17, 11, 0, 13, -1, 41, 7, 13, 13, 11, 11, 15, 7, -1, 4, 14, 14, 15, 20, 15, 7, 15, -1, -1, -1, -1, 15, 1, -1, 1, 15, 7, -1, 15, 15, 15, 1, 7, -1, 14, -1, 1, 1, 37, 14, 42, 42, -1, 0, 0, 0, 0, 14, 14, 0, 22, 0, -1, 14, -1, 2, 2, -1, 14, -1, -1, -1, 29, 53, -1, 14, 8, 24, -1, 6, 6, 43, 1, 1, 42, 12, -1, -1, 53, 7, -1, -1, 44, 2, 2, 2, -1, 1, 2, -1, 2, 2, -1, -1, 1, 36, 2, 6, 6, 2, 14, 38, -1, 38, 35, 30, 35, 35, 4, -1, -1, -1, 14, 2, 2, 2, 13, 2, 1, 2, 1, 2, 46, -1, 10, 2, 31, 36, -1, -1, -1, -1, 12, -1, 18, -1, -1, 2, 0, 25, -1, 25, 19, 25, -1, -1, 39, 25, 28, -1, 18, 11, 1, 11, 11, 22, -1, -1, 1, -1, 11, 11, 0, 11, -1, 49, -1, -1, 37, -1, -1, 39, 39, 25, 17, 13, 41, -1, -1, -1, 13, 17, 22, -1, -1, 0, 13, 27, -1, 13, 0, 37, 36, 0, 8, 8, 21, 37, 37, 37, -1, 37, -1, 22, -1, -1, -1, 8, 8, 8, 8, 8, 8, 26, 44, 8, 14, -1, 14, 51, 8, 1, 0, 0, 22, -1, 1, 12, 0, 22, 0, 19, 28, 38, -1, -1, 44, -1, -1, -1, 17, 28, 1, 1, 1, 17, 1, 2, 1, 52, 1, 1, 1, 1, 1, 1, 52, 28, -1, 2, -1, 2, 1, -1, 33, 2, 2, 2, 28, 30, 2, -1, 2, 2, 29, 2, 26, 30, 14, 2, 2, 2, 2, 1, 1, 6, -1, 1, 54, 1, 2, -1, 1, 1, 54, 38, 1, -1, -1, -1, 38, 6, 7, 6, -1, 12, -1, 22, 38, -1, 14, 43, 5, 5, 5, -1, 25, 1, -1, 14, 14, -1, 5, 47, 5, 25, 5, 5, 5, 5, 5, 12, 39, 5, 2, 5, 34, 39, 5, 16, 34, 5, -1, -1, 5, 5, 34, -1, 1, 1, -1, 34, 6, -1, 33, 6, 0, -1, 2, 1, 6, 17, 6, 6, 7, 31, 6, 0, 0, 0, 43, 0, 6, 6, 7, 35, 28, 0, 0, 0, -1, 0, 0, 23, 0, 34, -1, 16, 16, 16, 34, 34, 0, 0, 0, 10, 44, 0, 34, 0, 34, 27, -1, 0, 0, 22, 18, 22, 37, 0, 49, 0, 0, -1, 0, 0, 12, 0, 0, -1, 8, 0, 22, -1, 33, 44, 4, 4, 4, 4, 4, 4, 0, 4, 4, 1, 1, 35, -1, -1, 11, 41, 12, -1, 4, -1, 4, 4, 4, 1, 4, 4, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 2, -1, -1, 5, 5, 51, 5, 0, -1, 5, 19, 5, 21, 5, 25, 5, -1, 5, 10, 1, 10, 19, 48, -1, 5, 5, 5, -1, 5, 5, 5, 0, -1, 5, 25, -1, -1, 6, 6, -1, -1, -1, 6, -1, 1, 6, 44, 13, -1, 6, 7, 23, 1, 13, 6, 0, 0, 12, 17, 19, 5, 11, 0, -1, 23, 0, -1, 7, -1, 0, 17, 49, -1, 0, 0, 46, -1, 46, 0, 0, 19, 0, 0, 10, -1, 0, -1, 16, 0, 19, 16, 35, 0, 9, 27, 0, 16, 48, 1, 16, 16, 16, 7, -1, 10, 17, 0, 3, 3, 3, 27, 49, 17, 1, 0, 9, -1, 9, 35, 8, 1, 1, 0, -1, 40, 40, 40, 1, -1, 18, 2, 2, 4, 4, 1, 4, 4, 1, 4, 4, 4, 4, 1, 36, 3, 3, 3, 3, 3, 19, 0, 3, 4, 3, 3, 3, 3, 47, 3, 3, 4, 12, -1, 1, 3, 40, 12, 3, 3, 3, 1, 12, -1, 5, 5, -1, 5, 5, 5, 5, 15, 26, 26, -1, 26, 26, 26, 7, 26, 22, 26, 22, 4, 2, 49, 7, 31, 8, 10, -1, 8, 9, 9, 9, 9, 9, 19, 9, -1, 8, 49, 8, 8, 8, 8, 17, 10, 23, 53, 10, 9, 27, 27, 33, 19, -1, -1, 23, 23, 12, -1, 23, 23, 7, 23, -1, 23, -1, 11, 11, 11, 35, 11, 7, 11, 15, -1, 5, 32, 17, 11, -1, -1, 1, 46, 3, 7, 15, 35, 31, 1, 35, 35, 35, 8, 1, 0, 46, -1, -1, -1, 17, 52, -1, -1, 1, 3, 3, 3, 3, 3, 3, 3, -1, 3, 3, 0, 3, -1, 3, 40, 3, 50, 3, 14, -1, 40, 3, 5, 12, 10, 7, 34, 39, -1, -1, 15, 7, -1, 14, -1, 32, 21, 11, 33, 26, 0, 26, 19, 26, 0, 10, 30, -1, 29, -1, 29, 29, 10, 0, 30, 31, 32, -1, 32, 51, 33, 51, 27, 32, 30, 32, 10, 11, 32, -1, 32, -1, 17, -1, 10, 1, 8, -1, 0, 20, 20, -1, 31, -1, -1, -1, 13, -1, 8, 19, -1, 9, 24, 26, -1, 24, 8, 9, 9, 8, -1, -1, 9, 9, 9, 41, 9, 3, 9, 20, 47, 20, 20, 21, -1, 23, 23, -1, 19, 23, -1, 23, 45, 24, 25, -1, -1, 24, 45, 24, 24, 12, 2, 22, 19, 28, 28, 1, 51, -1, -1, 2, -1, 27, 13, 1, 1, -1, 43, 17, 18, 18, 1, 22, 5, -1, 25, 13, 3, 6, -1, 18, -1, 18, -1, 41, -1, 12, -1, -1, -1, -1, 33, -1, 6, 31, 1, 7, 25, 6, 39, 6, 12, 34, 34, -1, 16, 2, 3, 51, 29, 32, 10, 10, -1, 32, 10, 33, 2, -1, 15, 29, 29, 10, 12, 1, -1, -1, 1, 14, 13, -1, 1, 36, -1, -1, -1, -1, 11, 37, 2, 20, 20, 17, 20, -1, 20, 16, 29, 0, -1, -1, 27, -1, -1, 4, 48, 39, 24, 24, 24, 27, 0, 24, 4, 24, 24, 14, -1, 24, 24, 18, 18, 14, 13, 29, 29, 29, 29, -1, 15, 54, 5, -1, 0, 0, 0, 19, 30, 8, 0, 1, 0, 16, 8, 28, 28, 20, 21, -1, 47, 0, 20, 30, -1, 46, 20, -1, 47, 20, 25, -1, 17, 7, 3, 1, -1, 28, 0, 7, 6, 6, 1, 6, -1, 33, 6, 3, 10, 10, -1, 33, 1, 10, 6, 12, 30, 1, 0, 0, 3, 1, 1, 45, 0, 1, 0, 21, 1, 16, 50, -1, 33, 42, 2, 4, 11, 9, 38, 18, 4, -1, -1, 15, 7, 36, 21, 4, 18, 4, -1, 23, 38, 0, 47, 38, 0, 19, 9, 9, 0, 42, 31, 30, 9, 10, 10, 9, 9, 36, 7, 21, 2, 8, 9, 0, 0, 0, 3, 0, -1, 18, -1, 52, -1, 16, -1, 15, 0, 15, 7, 1, -1, -1, 0, -1, 0, 4, -1, 0, 2, 2, 2, 3, 2, 3, 3, 20, 6, 2, 3, 3, -1, 15, 6, 4, 20, 7, -1, 22, 50, 14, 28, 15, 1, -1, 28, 1, 7, 0, 0, 21, 14, 1, 16, 16, -1, 34, -1, 17, 1, 27, -1, 1, 17, 19, 0, 52, -1, 1, 1, -1, 3, 2, -1, -1, 3, 3, 1, -1, 11, -1, 16, 4, -1, -1, 0, -1, 16, 0, 16, 21, 27, 13, 9, 30, 21, 0, 0, 9, -1, -1, 0, 0, 0, 8, 36, 12, 0, 2, 7, 32, 32, -1, 7, -1, 4, 4, 2, 2, 2, 5, 13, -1, 19, 0, 3, -1, 36, 3, 3, 8, 6, -1, 3, 26, 26, 50, 1, 1, 42, 44, 2, 42, 2, 7, 6, 9, -1, 24, 2, 0, 11, -1, 41, 11, 32, -1, 2, 29, 29, 0, 46, 0, 0, 18, 14, 0, 1, 40, 8, 30, 14, 20, 17, 33, 8, 28, 1, 4, 53, -1, -1, 54, 1, 12, 4, 4, 28, 1, 26, 20, 16, 18, 47, 12, 21, 16, 0, 0, 0, 16, 1, 22, 3, 18, 3, 40, 32, 0, -1, -1, 0, -1, 0, 50, 0, 0, 0, -1, 0, 16, 28, -1, 1, 19, 3, 3, 0, 5, -1, 9, 9, 30, 7, 3, 3, 0, 0, 20, 4, 26, 20, 20, -1, 0, 27]\n",
            "Skipping coherence calculation due to error: unable to interpret topic as either a list of tokens or a list of ids\n",
            "-------RUN115-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': \"CountVectorizer(stop_words='english')\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[14, 2, 9, 9, 4, 2, 6, 6, 12, 6, 0, 5, 4, 5, 12, 2, 0, 14, 11, 2, 4, 4, 6, 11, 11, 5, 8, 2, 2, 12, 2, 0, 9, 5, 5, 6, 3, 6, 13, 12, 11, 2, 8, 11, 8, 8, 8, 8, 11, 2, 8, 11, 6, 6, 14, 7, 7, 2, 6, 6, 6, 6, 6, 5, 2, 2, 4, 8, 4, 4, 5, 4, 3, 3, 0, 4, 0, 13, 12, 0, 7, 2, 7, 7, 2, 8, 7, 3, 2, 5, 3, 5, 7, 14, 5, 16, 5, 3, 5, 5, 12, 0, 0, 0, 6, 5, 5, 13, 13, 13, 4, 4, 8, 14, 2, 7, 7, 5, 0, 0, 0, 7, 11, 0, 8, 16, 4, 2, 11, 4, 2, 12, 2, 4, 4, 14, 11, 0, 4, 8, 5, 6, 14, 16, 12, 6, 14, 2, 12, 1, 6, 6, 6, 5, 13, 6, 3, 14, 6, 10, 13, 10, 5, 10, 2, 2, 5, 0, 5, 2, 5, 5, 0, 1, 12, 6, 0, 2, 12, 4, 6, 0, 12, 1, 4, 6, 2, 2, 4, 4, 12, 12, 7, 2, 5, 7, 18, 18, 7, 15, 7, 2, 7, 6, 6, 5, 7, 7, 3, 9, 3, 7, 2, 2, 7, 7, 7, 3, 2, 5, 18, 12, 3, 3, 5, 18, 8, 8, 0, 1, 1, 1, 1, 18, 18, 1, 5, 1, 6, 18, 13, 4, 4, 0, 18, 13, 0, 0, 6, 2, 0, 18, 17, 14, 6, 8, 8, 12, 3, 3, 8, 0, 6, 6, 2, 2, 5, 8, 14, 4, 4, 4, 4, 3, 4, 12, 4, 4, 5, 8, 3, 6, 4, 8, 8, 4, 18, 15, 9, 15, 9, 2, 9, 9, 7, 8, 5, 12, 18, 4, 4, 4, 4, 4, 4, 4, 3, 4, 9, 6, 11, 4, 8, 6, 2, 12, 13, 6, 0, 6, 6, 16, 6, 4, 1, 14, 2, 14, 0, 14, 0, 13, 11, 14, 13, 14, 6, 12, 3, 12, 12, 5, 5, 5, 3, 0, 12, 12, 1, 12, 2, 11, 2, 2, 5, 1, 10, 11, 11, 14, 0, 4, 2, 8, 7, 8, 4, 0, 5, 4, 0, 1, 4, 5, 5, 4, 1, 5, 6, 1, 17, 17, 2, 5, 5, 5, 7, 5, 7, 5, 6, 5, 6, 17, 17, 17, 17, 17, 17, 13, 14, 17, 18, 0, 18, 0, 17, 3, 1, 1, 5, 7, 3, 0, 1, 5, 1, 0, 13, 15, 0, 8, 14, 0, 13, 0, 0, 13, 3, 3, 3, 0, 3, 4, 3, 12, 3, 3, 3, 3, 3, 3, 12, 13, 14, 4, 5, 4, 3, 5, 5, 4, 4, 4, 13, 2, 4, 0, 4, 4, 6, 4, 13, 2, 18, 4, 4, 4, 4, 3, 3, 8, 2, 3, 6, 3, 4, 8, 3, 3, 6, 15, 3, 5, 9, 7, 15, 8, 2, 8, 9, 0, 0, 5, 15, 9, 18, 12, 9, 9, 9, 1, 14, 3, 12, 18, 18, 4, 9, 6, 9, 14, 9, 9, 9, 9, 9, 0, 11, 9, 4, 9, 11, 11, 9, 19, 11, 9, 0, 0, 9, 9, 11, 1, 3, 3, 2, 11, 8, 5, 5, 8, 1, 4, 4, 3, 8, 0, 8, 8, 2, 8, 8, 1, 1, 1, 12, 1, 8, 8, 2, 9, 13, 1, 1, 1, 13, 1, 1, 9, 1, 11, 7, 19, 19, 19, 11, 11, 1, 1, 1, 11, 14, 1, 11, 1, 11, 5, 11, 1, 1, 5, 6, 5, 5, 1, 11, 1, 1, 5, 1, 1, 0, 1, 1, 6, 17, 1, 5, 5, 5, 14, 7, 7, 7, 7, 7, 7, 1, 7, 7, 3, 3, 9, 0, 16, 12, 0, 0, 6, 7, 7, 7, 7, 7, 3, 7, 7, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 0, 4, 6, 12, 9, 9, 0, 9, 1, 0, 9, 0, 9, 2, 9, 14, 9, 2, 9, 11, 3, 11, 0, 5, 2, 9, 9, 9, 12, 9, 9, 9, 1, 0, 9, 14, 0, 8, 8, 8, 8, 6, 8, 8, 0, 3, 8, 14, 4, 8, 8, 12, 9, 3, 4, 8, 1, 1, 0, 0, 0, 9, 12, 1, 6, 9, 1, 12, 12, 8, 1, 0, 11, 0, 1, 1, 9, 2, 9, 1, 1, 0, 1, 1, 11, 0, 1, 15, 19, 1, 0, 19, 9, 1, 16, 5, 1, 19, 5, 3, 19, 19, 19, 2, 5, 11, 0, 1, 10, 10, 10, 5, 11, 0, 3, 1, 16, 2, 16, 9, 17, 3, 3, 1, 12, 15, 15, 15, 3, 2, 6, 4, 4, 7, 7, 3, 7, 7, 3, 7, 7, 7, 7, 3, 6, 10, 10, 10, 10, 10, 0, 1, 10, 7, 10, 10, 10, 10, 0, 10, 10, 7, 0, 2, 3, 10, 15, 0, 10, 10, 10, 3, 0, 0, 9, 9, 0, 9, 9, 9, 9, 7, 13, 13, 0, 13, 13, 13, 2, 13, 5, 13, 5, 7, 4, 11, 2, 8, 17, 11, 8, 17, 16, 16, 16, 16, 16, 0, 16, 6, 17, 11, 17, 17, 17, 17, 0, 11, 9, 2, 11, 16, 5, 5, 5, 12, 0, 0, 9, 9, 0, 0, 9, 9, 2, 9, 5, 9, 2, 12, 12, 12, 9, 12, 2, 12, 7, 5, 9, 7, 0, 12, 4, 0, 3, 9, 10, 2, 7, 9, 8, 3, 9, 9, 9, 17, 3, 16, 9, 12, 8, 4, 0, 12, 6, 5, 3, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1, 10, 13, 10, 15, 10, 10, 10, 18, 10, 15, 10, 9, 0, 11, 2, 11, 7, 8, 0, 7, 2, 8, 18, 5, 7, 2, 12, 5, 13, 1, 13, 0, 13, 1, 11, 2, 13, 6, 5, 6, 6, 11, 1, 2, 8, 7, 6, 7, 0, 5, 0, 5, 7, 2, 7, 11, 12, 7, 5, 7, 7, 0, 6, 11, 3, 17, 5, 1, 15, 15, 15, 8, 2, 5, 5, 4, 5, 17, 0, 6, 16, 14, 13, 6, 14, 17, 16, 16, 17, 2, 4, 16, 16, 16, 2, 16, 10, 16, 15, 6, 15, 15, 2, 0, 9, 9, 0, 0, 9, 0, 0, 13, 14, 14, 4, 5, 14, 13, 14, 14, 0, 4, 5, 0, 13, 13, 3, 0, 8, 6, 4, 4, 5, 4, 3, 3, 0, 12, 0, 6, 6, 3, 5, 9, 6, 14, 4, 10, 8, 7, 6, 2, 6, 0, 2, 2, 0, 6, 16, 8, 8, 5, 8, 8, 8, 3, 2, 14, 8, 11, 8, 0, 11, 11, 6, 19, 4, 10, 0, 6, 7, 11, 11, 8, 7, 11, 5, 4, 6, 7, 6, 6, 11, 0, 3, 7, 12, 3, 18, 4, 13, 3, 6, 5, 2, 9, 6, 12, 5, 4, 15, 15, 0, 15, 6, 15, 19, 6, 1, 10, 2, 5, 2, 7, 7, 5, 11, 14, 14, 14, 5, 1, 14, 7, 14, 14, 18, 7, 14, 14, 6, 6, 18, 4, 6, 6, 6, 6, 15, 7, 6, 9, 15, 1, 1, 1, 0, 2, 17, 1, 3, 1, 19, 17, 13, 13, 15, 2, 0, 0, 1, 15, 2, 6, 9, 15, 6, 0, 15, 14, 13, 0, 2, 10, 3, 7, 13, 1, 2, 8, 8, 3, 8, 8, 5, 8, 10, 11, 11, 13, 5, 3, 11, 8, 0, 2, 3, 1, 1, 10, 3, 3, 13, 1, 3, 1, 2, 3, 19, 10, 0, 5, 8, 4, 7, 12, 16, 15, 6, 7, 8, 8, 7, 2, 6, 2, 7, 6, 7, 8, 9, 15, 1, 0, 15, 1, 0, 16, 16, 1, 8, 8, 2, 16, 11, 11, 16, 16, 6, 2, 2, 4, 17, 16, 1, 1, 1, 10, 1, 0, 6, 5, 12, 0, 19, 2, 7, 1, 7, 2, 3, 1, 8, 1, 0, 1, 7, 2, 1, 4, 4, 4, 10, 4, 10, 10, 15, 8, 4, 10, 10, 2, 7, 8, 7, 15, 2, 13, 8, 10, 18, 13, 7, 3, 8, 13, 3, 2, 1, 1, 2, 18, 3, 19, 19, 13, 11, 6, 0, 3, 5, 6, 3, 0, 0, 1, 12, 13, 3, 3, 7, 10, 4, 2, 5, 10, 10, 3, 0, 12, 0, 19, 7, 9, 6, 1, 8, 19, 1, 19, 2, 5, 4, 16, 2, 2, 1, 1, 16, 5, 4, 1, 1, 1, 17, 6, 0, 1, 4, 2, 7, 7, 4, 2, 0, 7, 7, 4, 4, 4, 9, 4, 0, 0, 1, 10, 7, 6, 10, 10, 17, 8, 5, 10, 13, 13, 10, 3, 3, 8, 14, 4, 8, 4, 2, 8, 16, 6, 14, 4, 1, 12, 0, 2, 12, 7, 0, 4, 6, 6, 1, 9, 1, 1, 6, 18, 1, 3, 15, 17, 2, 18, 15, 0, 5, 17, 13, 3, 7, 2, 8, 7, 6, 3, 0, 7, 7, 13, 3, 13, 15, 19, 6, 6, 0, 2, 19, 1, 1, 1, 19, 3, 5, 10, 6, 10, 15, 7, 1, 0, 12, 1, 2, 1, 10, 1, 1, 1, 13, 1, 19, 13, 16, 3, 0, 10, 10, 1, 9, 4, 16, 16, 2, 2, 10, 10, 1, 1, 15, 7, 13, 15, 15, 0, 1, 5]\n",
            "-------RUN116-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': 'CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7df3b00f5c90>)', 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[3, 1, 3, 3, 2, 10, 13, 13, 1, 13, 0, 11, 2, 11, 1, 10, 0, 3, 12, 10, 2, 2, 8, 12, 12, 6, 6, 10, 10, 1, 10, 1, 3, 11, 13, 13, 5, 0, 15, 1, 12, 10, 6, 12, 6, 6, 6, 6, 12, 1, 6, 12, 8, 8, 8, 7, 7, 1, 8, 8, 13, 8, 8, 8, 10, 10, 2, 6, 2, 2, 11, 2, 5, 5, 0, 2, 0, 15, 1, 1, 7, 10, 13, 7, 10, 6, 7, 5, 10, 11, 5, 8, 7, 8, 8, 16, 11, 5, 11, 13, 1, 0, 1, 0, 0, 11, 13, 15, 15, 15, 2, 2, 6, 8, 10, 7, 7, 8, 0, 1, 0, 7, 12, 0, 6, 16, 2, 10, 12, 2, 10, 1, 10, 2, 2, 3, 12, 0, 2, 6, 8, 8, 8, 16, 1, 8, 3, 10, 1, 14, 13, 8, 13, 8, 0, 8, 5, 8, 8, 4, 0, 4, 11, 4, 10, 1, 11, 0, 11, 10, 8, 8, 1, 9, 1, 8, 0, 10, 1, 2, 0, 0, 1, 9, 2, 8, 10, 1, 2, 2, 1, 1, 7, 10, 11, 7, 18, 18, 7, 7, 7, 10, 7, 0, 13, 11, 7, 7, 5, 3, 5, 7, 10, 1, 7, 7, 7, 5, 10, 11, 18, 1, 5, 5, 8, 18, 6, 6, 1, 9, 9, 9, 9, 18, 18, 9, 11, 9, 0, 18, 0, 2, 2, 0, 18, 0, 1, 0, 0, 10, 0, 18, 17, 8, 11, 6, 6, 1, 5, 5, 6, 0, 0, 0, 10, 1, 11, 6, 8, 2, 2, 2, 2, 5, 2, 1, 2, 2, 11, 6, 5, 0, 2, 6, 6, 2, 18, 4, 3, 4, 3, 1, 3, 3, 7, 6, 11, 1, 18, 2, 2, 2, 2, 2, 2, 2, 5, 2, 3, 13, 12, 2, 6, 0, 10, 1, 0, 0, 0, 0, 8, 16, 0, 2, 9, 3, 10, 3, 1, 3, 0, 0, 12, 3, 0, 8, 8, 1, 5, 1, 1, 11, 11, 8, 13, 0, 1, 1, 9, 1, 10, 12, 10, 1, 8, 14, 4, 12, 12, 3, 0, 2, 10, 6, 7, 13, 2, 0, 6, 2, 0, 14, 2, 11, 8, 2, 9, 8, 0, 9, 17, 17, 10, 8, 8, 8, 7, 8, 7, 11, 13, 11, 0, 17, 17, 17, 17, 17, 17, 15, 8, 17, 18, 1, 18, 1, 17, 5, 9, 9, 6, 7, 5, 0, 9, 6, 9, 0, 0, 4, 0, 6, 8, 1, 0, 0, 0, 0, 5, 5, 5, 0, 5, 2, 5, 1, 5, 5, 5, 5, 5, 5, 1, 0, 3, 2, 11, 2, 5, 13, 11, 2, 2, 2, 0, 1, 2, 1, 2, 2, 0, 2, 15, 1, 18, 2, 2, 2, 2, 5, 5, 6, 1, 5, 13, 5, 2, 6, 13, 5, 13, 4, 5, 8, 3, 7, 4, 6, 10, 6, 3, 0, 0, 6, 4, 3, 18, 1, 3, 3, 3, 14, 3, 5, 1, 18, 18, 2, 3, 0, 3, 3, 3, 3, 3, 3, 3, 0, 12, 3, 2, 3, 12, 12, 3, 19, 12, 3, 0, 0, 3, 3, 12, 14, 5, 5, 1, 12, 6, 11, 11, 6, 14, 2, 2, 5, 6, 0, 6, 6, 10, 6, 6, 14, 14, 14, 1, 14, 6, 6, 10, 3, 0, 9, 9, 9, 15, 9, 14, 3, 14, 12, 7, 19, 19, 19, 12, 12, 14, 14, 14, 12, 8, 14, 12, 14, 12, 11, 12, 9, 9, 6, 8, 6, 8, 9, 12, 9, 9, 11, 9, 9, 0, 9, 9, 8, 17, 9, 11, 11, 11, 8, 7, 7, 7, 7, 7, 7, 14, 7, 7, 5, 5, 3, 1, 15, 1, 1, 0, 13, 7, 7, 7, 7, 7, 5, 7, 7, 5, 5, 5, 5, 5, 5, 2, 5, 5, 5, 1, 2, 0, 13, 3, 3, 1, 3, 9, 1, 3, 1, 3, 10, 3, 3, 3, 1, 3, 12, 5, 12, 1, 8, 10, 3, 3, 3, 1, 3, 3, 3, 9, 0, 3, 3, 1, 6, 6, 6, 6, 0, 6, 6, 1, 5, 6, 8, 2, 6, 6, 1, 3, 5, 2, 6, 14, 14, 0, 0, 1, 3, 1, 14, 13, 3, 14, 1, 1, 6, 14, 0, 12, 1, 9, 9, 3, 1, 3, 9, 9, 1, 9, 9, 12, 0, 9, 4, 19, 9, 1, 19, 3, 9, 16, 11, 9, 19, 8, 5, 19, 19, 19, 10, 13, 12, 0, 9, 4, 4, 4, 11, 12, 0, 5, 9, 16, 10, 16, 3, 17, 5, 5, 9, 1, 15, 15, 15, 13, 1, 8, 2, 2, 7, 7, 5, 7, 7, 5, 7, 7, 7, 7, 5, 0, 4, 4, 4, 4, 4, 1, 9, 4, 7, 4, 4, 4, 4, 0, 4, 4, 7, 0, 1, 5, 4, 15, 0, 4, 4, 4, 5, 0, 0, 3, 3, 0, 3, 3, 3, 3, 7, 15, 15, 0, 15, 15, 15, 10, 15, 11, 15, 11, 7, 2, 12, 10, 6, 17, 12, 6, 17, 16, 16, 16, 16, 16, 1, 16, 18, 17, 12, 17, 17, 17, 17, 0, 12, 3, 10, 12, 16, 11, 11, 11, 1, 1, 1, 3, 3, 0, 1, 3, 3, 10, 3, 11, 3, 10, 1, 1, 1, 3, 1, 1, 1, 7, 11, 3, 7, 0, 1, 2, 0, 5, 3, 4, 10, 7, 3, 6, 13, 3, 3, 3, 17, 5, 9, 3, 1, 6, 2, 0, 1, 13, 8, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 9, 4, 0, 4, 15, 4, 4, 4, 18, 4, 15, 4, 3, 0, 12, 10, 12, 12, 6, 1, 7, 10, 6, 18, 11, 13, 10, 1, 11, 15, 14, 15, 1, 15, 14, 12, 1, 0, 0, 11, 0, 0, 12, 14, 1, 6, 13, 13, 13, 1, 11, 1, 11, 13, 1, 13, 12, 1, 13, 11, 13, 7, 0, 13, 12, 5, 17, 11, 9, 4, 4, 0, 6, 10, 11, 8, 2, 13, 17, 1, 0, 16, 8, 15, 0, 8, 17, 16, 16, 17, 10, 2, 16, 16, 16, 10, 16, 4, 16, 4, 0, 4, 4, 10, 0, 3, 3, 0, 1, 3, 1, 1, 15, 8, 3, 3, 8, 8, 15, 8, 8, 0, 2, 6, 1, 0, 0, 5, 1, 6, 1, 2, 2, 11, 2, 5, 5, 0, 1, 0, 8, 8, 5, 11, 3, 0, 3, 2, 4, 6, 7, 8, 1, 8, 1, 10, 10, 0, 0, 15, 6, 6, 11, 6, 6, 6, 5, 10, 3, 6, 12, 6, 0, 12, 12, 18, 19, 2, 4, 1, 0, 13, 12, 12, 6, 13, 12, 11, 2, 0, 7, 0, 0, 12, 0, 5, 7, 1, 5, 18, 2, 0, 5, 0, 13, 10, 3, 0, 1, 8, 2, 4, 4, 0, 4, 18, 4, 19, 0, 9, 4, 1, 11, 1, 7, 7, 8, 12, 8, 8, 8, 11, 14, 8, 7, 8, 8, 18, 7, 8, 8, 8, 8, 18, 2, 0, 0, 0, 0, 4, 7, 13, 3, 4, 14, 14, 9, 1, 1, 17, 9, 5, 9, 19, 17, 0, 0, 4, 10, 0, 0, 9, 4, 1, 0, 3, 4, 13, 0, 4, 3, 0, 0, 1, 4, 5, 7, 0, 14, 10, 6, 6, 11, 6, 6, 11, 6, 4, 12, 12, 15, 11, 5, 12, 6, 0, 1, 5, 14, 14, 4, 5, 5, 15, 14, 5, 14, 10, 5, 19, 4, 0, 11, 6, 2, 7, 1, 16, 4, 8, 7, 6, 6, 7, 10, 0, 10, 7, 8, 7, 6, 3, 4, 9, 0, 4, 14, 1, 16, 16, 9, 6, 6, 1, 16, 12, 12, 16, 16, 0, 10, 10, 2, 17, 16, 9, 14, 9, 4, 14, 1, 8, 8, 1, 0, 19, 10, 7, 9, 7, 1, 5, 7, 6, 9, 0, 9, 7, 10, 9, 2, 2, 2, 4, 2, 4, 4, 4, 6, 2, 4, 4, 10, 7, 6, 7, 4, 10, 0, 6, 4, 18, 0, 7, 5, 6, 0, 5, 10, 14, 14, 10, 18, 5, 19, 19, 0, 12, 13, 0, 5, 11, 0, 5, 0, 1, 14, 1, 0, 5, 5, 7, 4, 2, 10, 13, 4, 4, 5, 1, 1, 0, 19, 7, 3, 0, 9, 6, 19, 9, 19, 10, 11, 2, 16, 1, 10, 9, 9, 16, 1, 2, 9, 9, 9, 17, 0, 0, 9, 2, 10, 13, 13, 2, 10, 1, 7, 7, 2, 2, 2, 3, 2, 1, 1, 9, 4, 7, 0, 4, 4, 17, 6, 11, 4, 15, 15, 4, 5, 5, 6, 8, 2, 6, 2, 1, 6, 16, 18, 8, 2, 14, 1, 0, 10, 1, 13, 0, 2, 0, 0, 14, 3, 9, 9, 8, 18, 14, 5, 4, 17, 1, 18, 4, 0, 11, 17, 0, 5, 7, 10, 13, 7, 13, 5, 0, 7, 7, 0, 5, 15, 4, 19, 8, 0, 0, 10, 19, 9, 9, 9, 19, 5, 11, 4, 8, 4, 4, 13, 14, 0, 1, 9, 1, 14, 4, 9, 9, 9, 0, 9, 19, 0, 16, 5, 1, 4, 4, 9, 3, 2, 16, 16, 1, 10, 4, 4, 9, 9, 4, 7, 15, 4, 4, 0, 14, 11]\n",
            "-------RUN117-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': \"CountVectorizer(stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b02d2c50>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[13, 1, 16, 16, 3, 10, 5, 5, 1, 5, 0, 9, 3, 9, 1, 10, 0, 13, 11, 10, 3, 3, 5, 11, 11, 9, 7, 10, 10, 1, 10, 1, 16, 9, 5, 5, 4, 15, 17, 1, 11, 10, 7, 11, 7, 7, 7, 7, 11, 1, 7, 11, 5, 5, 13, 8, 8, 1, 5, 5, 15, 5, 5, 5, 10, 10, 3, 9, 3, 3, 9, 3, 4, 4, 0, 3, 0, 0, 1, 1, 8, 10, 8, 8, 10, 9, 8, 4, 10, 9, 4, 5, 8, 13, 5, 12, 9, 4, 9, 9, 1, 0, 1, 0, 0, 9, 9, 17, 17, 17, 3, 3, 7, 13, 10, 8, 8, 5, 0, 1, 0, 8, 11, 0, 7, 17, 3, 10, 11, 3, 10, 1, 10, 3, 3, 13, 11, 0, 3, 7, 9, 5, 13, 17, 1, 5, 13, 10, 1, 2, 5, 5, 5, 5, 0, 5, 4, 13, 5, 6, 0, 6, 9, 6, 10, 1, 9, 0, 9, 10, 5, 5, 1, 2, 1, 5, 0, 10, 1, 3, 15, 0, 1, 2, 3, 5, 10, 10, 3, 3, 1, 1, 8, 10, 9, 8, 18, 18, 8, 8, 8, 10, 8, 0, 5, 9, 8, 8, 4, 13, 4, 8, 10, 10, 8, 8, 8, 4, 10, 9, 18, 1, 4, 4, 5, 18, 7, 7, 1, 2, 2, 2, 2, 18, 18, 2, 9, 2, 0, 18, 0, 3, 3, 0, 18, 0, 1, 0, 15, 10, 0, 18, 12, 13, 9, 7, 7, 1, 4, 4, 7, 0, 15, 0, 10, 1, 9, 7, 13, 3, 3, 3, 13, 4, 3, 1, 3, 3, 9, 7, 4, 15, 3, 7, 7, 3, 18, 17, 16, 17, 16, 1, 16, 16, 8, 7, 9, 1, 18, 3, 3, 3, 3, 3, 3, 3, 15, 3, 16, 15, 11, 3, 7, 15, 10, 1, 0, 0, 0, 5, 5, 17, 0, 3, 2, 13, 10, 13, 1, 13, 0, 0, 11, 13, 0, 13, 5, 1, 4, 1, 1, 9, 9, 5, 15, 0, 1, 1, 2, 1, 10, 11, 10, 1, 5, 2, 6, 11, 11, 13, 0, 3, 10, 7, 11, 7, 3, 0, 9, 3, 0, 2, 3, 9, 5, 3, 2, 5, 15, 2, 12, 12, 10, 5, 5, 5, 11, 5, 8, 9, 5, 9, 0, 12, 12, 12, 12, 12, 12, 0, 13, 12, 18, 1, 18, 1, 12, 4, 2, 2, 9, 8, 4, 0, 2, 9, 2, 0, 0, 17, 0, 7, 13, 1, 0, 0, 0, 0, 4, 4, 4, 0, 4, 3, 4, 1, 4, 4, 4, 4, 4, 4, 1, 0, 16, 3, 9, 3, 4, 9, 9, 3, 3, 3, 0, 1, 3, 1, 3, 3, 15, 3, 0, 1, 18, 3, 3, 3, 3, 4, 4, 7, 1, 4, 15, 4, 3, 7, 15, 4, 15, 17, 4, 5, 16, 8, 17, 7, 10, 7, 16, 0, 0, 9, 17, 16, 18, 1, 14, 14, 14, 2, 13, 4, 1, 18, 18, 3, 14, 0, 14, 13, 14, 14, 14, 14, 14, 0, 11, 14, 3, 14, 11, 11, 14, 19, 11, 14, 0, 0, 14, 14, 11, 2, 4, 4, 1, 11, 7, 9, 9, 7, 2, 3, 3, 4, 7, 0, 7, 7, 10, 7, 7, 2, 2, 2, 1, 2, 7, 7, 10, 16, 0, 2, 2, 2, 0, 2, 2, 16, 2, 11, 8, 19, 19, 19, 11, 11, 2, 2, 2, 11, 13, 2, 11, 2, 11, 9, 11, 2, 2, 9, 5, 9, 5, 2, 11, 2, 2, 9, 2, 2, 0, 2, 2, 5, 12, 2, 9, 9, 9, 13, 8, 8, 8, 8, 8, 8, 2, 8, 8, 4, 4, 16, 1, 17, 1, 1, 0, 5, 8, 8, 8, 8, 8, 4, 8, 8, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 1, 3, 15, 1, 14, 14, 1, 14, 2, 1, 14, 1, 14, 10, 14, 13, 14, 1, 14, 11, 4, 11, 1, 5, 10, 14, 14, 14, 1, 14, 14, 14, 2, 0, 14, 13, 1, 7, 7, 7, 7, 5, 7, 7, 1, 4, 7, 13, 3, 7, 7, 1, 16, 4, 3, 7, 2, 2, 0, 0, 1, 14, 1, 2, 5, 16, 2, 1, 1, 7, 2, 0, 11, 1, 2, 2, 14, 1, 14, 2, 2, 1, 2, 2, 11, 0, 2, 17, 19, 2, 1, 19, 16, 2, 12, 9, 2, 19, 5, 4, 19, 19, 19, 10, 5, 11, 0, 2, 6, 6, 6, 9, 11, 1, 4, 2, 12, 10, 12, 16, 12, 4, 4, 2, 1, 17, 17, 17, 9, 1, 5, 3, 3, 8, 8, 4, 8, 8, 4, 8, 8, 8, 8, 4, 15, 6, 6, 6, 6, 6, 1, 2, 6, 8, 6, 6, 6, 6, 0, 6, 6, 8, 0, 1, 4, 6, 17, 0, 6, 6, 6, 4, 0, 0, 14, 14, 0, 14, 14, 14, 14, 8, 0, 0, 0, 0, 0, 0, 10, 0, 9, 0, 9, 8, 3, 11, 10, 7, 12, 11, 7, 12, 12, 12, 12, 12, 12, 1, 12, 5, 12, 11, 12, 12, 12, 12, 0, 11, 16, 10, 11, 12, 9, 9, 9, 1, 0, 1, 16, 16, 0, 1, 16, 16, 10, 16, 9, 16, 10, 1, 1, 1, 16, 1, 1, 1, 8, 9, 14, 8, 0, 1, 3, 0, 4, 16, 6, 10, 8, 16, 7, 15, 16, 16, 16, 12, 4, 2, 16, 1, 7, 3, 0, 1, 5, 5, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 2, 6, 0, 6, 17, 6, 6, 6, 18, 6, 17, 6, 14, 0, 11, 10, 11, 11, 7, 0, 8, 10, 7, 18, 9, 5, 10, 1, 9, 0, 2, 0, 1, 0, 2, 11, 1, 0, 15, 9, 15, 15, 11, 2, 1, 7, 5, 5, 5, 1, 9, 1, 9, 5, 1, 5, 11, 1, 5, 9, 5, 8, 0, 5, 11, 4, 12, 9, 2, 6, 6, 17, 7, 10, 9, 5, 3, 9, 12, 1, 5, 12, 13, 0, 0, 13, 12, 12, 12, 12, 10, 13, 12, 12, 12, 10, 12, 6, 12, 6, 0, 6, 6, 10, 0, 16, 16, 0, 1, 16, 1, 1, 17, 13, 13, 13, 5, 13, 17, 13, 13, 0, 3, 9, 1, 0, 0, 4, 1, 9, 1, 3, 3, 9, 3, 4, 4, 0, 1, 1, 5, 5, 4, 9, 14, 15, 13, 3, 6, 7, 8, 5, 1, 5, 1, 10, 10, 0, 0, 17, 7, 7, 9, 7, 7, 7, 4, 10, 13, 7, 11, 7, 0, 11, 11, 5, 19, 3, 6, 1, 15, 5, 11, 11, 7, 5, 11, 9, 3, 15, 8, 15, 15, 11, 0, 4, 5, 1, 4, 18, 3, 0, 4, 15, 9, 10, 16, 0, 1, 5, 3, 6, 6, 0, 6, 5, 6, 19, 15, 2, 6, 1, 9, 1, 8, 8, 5, 11, 13, 13, 13, 9, 2, 13, 8, 13, 13, 18, 8, 13, 13, 5, 5, 18, 3, 15, 15, 15, 15, 17, 8, 15, 14, 17, 2, 2, 2, 1, 10, 12, 2, 4, 2, 19, 12, 0, 0, 6, 10, 1, 0, 2, 6, 1, 5, 16, 6, 15, 0, 6, 13, 0, 0, 1, 6, 4, 5, 0, 2, 10, 7, 7, 9, 7, 7, 9, 7, 6, 11, 11, 0, 9, 4, 11, 7, 0, 1, 4, 2, 2, 6, 4, 4, 17, 2, 4, 2, 10, 4, 19, 6, 0, 9, 7, 3, 8, 1, 12, 17, 5, 8, 7, 7, 8, 10, 15, 10, 8, 5, 8, 7, 16, 17, 2, 0, 17, 2, 1, 12, 12, 2, 7, 7, 1, 12, 11, 11, 12, 12, 15, 10, 10, 3, 12, 12, 2, 2, 2, 6, 2, 1, 5, 5, 1, 1, 19, 10, 8, 2, 8, 10, 4, 2, 7, 2, 0, 2, 8, 10, 2, 3, 3, 3, 6, 3, 6, 6, 6, 7, 3, 6, 6, 10, 8, 7, 8, 6, 10, 0, 7, 6, 18, 0, 8, 4, 7, 0, 4, 10, 2, 2, 10, 18, 4, 19, 19, 0, 11, 15, 0, 4, 9, 15, 4, 0, 1, 2, 1, 0, 4, 4, 8, 6, 3, 10, 9, 6, 6, 4, 1, 1, 15, 19, 8, 16, 0, 2, 7, 19, 2, 19, 10, 9, 3, 12, 1, 10, 2, 2, 12, 1, 3, 2, 2, 2, 12, 15, 0, 2, 3, 10, 5, 5, 3, 10, 1, 8, 8, 3, 3, 3, 14, 3, 1, 1, 2, 6, 8, 15, 6, 6, 12, 7, 9, 6, 0, 0, 6, 4, 4, 7, 13, 3, 7, 3, 1, 7, 12, 15, 13, 3, 2, 1, 0, 1, 1, 5, 1, 3, 15, 15, 2, 14, 2, 2, 5, 18, 2, 4, 17, 12, 1, 18, 6, 0, 9, 12, 0, 4, 8, 10, 7, 8, 15, 4, 0, 8, 8, 0, 4, 0, 6, 19, 5, 0, 0, 10, 19, 2, 2, 2, 19, 4, 9, 6, 5, 6, 17, 5, 2, 0, 1, 2, 1, 2, 6, 2, 2, 2, 0, 2, 19, 0, 12, 4, 1, 6, 6, 2, 14, 3, 12, 16, 1, 10, 6, 6, 2, 2, 6, 8, 0, 6, 6, 0, 2, 9]\n",
            "-------RUN118-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': \"CountVectorizer(ngram_range=(1, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b891dd10>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[14, 2, 6, 6, 16, 13, 9, 9, 2, 7, 3, 8, 16, 8, 17, 13, 3, 14, 10, 13, 16, 16, 9, 10, 10, 8, 0, 13, 13, 17, 13, 2, 6, 8, 9, 9, 5, 7, 15, 17, 10, 13, 0, 10, 0, 0, 0, 0, 10, 2, 0, 10, 9, 9, 14, 0, 0, 2, 7, 7, 7, 9, 7, 9, 13, 13, 16, 0, 16, 16, 8, 16, 5, 5, 3, 12, 3, 15, 17, 2, 0, 13, 9, 0, 2, 8, 0, 5, 13, 8, 5, 9, 0, 14, 9, 11, 8, 5, 8, 9, 17, 3, 2, 3, 3, 8, 8, 15, 15, 15, 12, 16, 0, 14, 2, 0, 0, 9, 3, 2, 3, 0, 10, 3, 0, 11, 16, 13, 10, 16, 2, 17, 13, 16, 16, 14, 10, 3, 16, 0, 8, 9, 14, 11, 17, 7, 14, 13, 2, 1, 9, 9, 9, 9, 7, 7, 5, 14, 9, 4, 3, 4, 8, 4, 13, 2, 8, 3, 8, 13, 9, 9, 3, 1, 17, 7, 3, 2, 17, 16, 7, 3, 17, 1, 16, 9, 13, 2, 16, 16, 17, 17, 0, 13, 8, 0, 18, 18, 0, 0, 0, 13, 0, 3, 9, 8, 0, 0, 8, 14, 5, 0, 2, 2, 0, 0, 0, 5, 13, 8, 18, 2, 5, 5, 9, 18, 0, 0, 2, 1, 1, 1, 1, 18, 18, 1, 8, 1, 7, 18, 15, 12, 12, 3, 18, 15, 3, 3, 7, 13, 3, 18, 11, 14, 7, 0, 0, 17, 5, 5, 0, 3, 7, 7, 13, 2, 8, 0, 14, 12, 12, 12, 16, 5, 12, 2, 12, 12, 8, 8, 5, 7, 12, 0, 0, 16, 18, 4, 6, 4, 6, 2, 6, 6, 0, 0, 8, 2, 18, 12, 12, 12, 16, 12, 16, 12, 5, 12, 6, 7, 10, 12, 0, 7, 2, 17, 3, 7, 3, 7, 7, 11, 7, 16, 1, 14, 13, 14, 2, 14, 3, 3, 10, 14, 15, 14, 7, 17, 5, 17, 17, 8, 8, 9, 9, 3, 17, 17, 1, 17, 13, 10, 13, 2, 9, 1, 4, 10, 10, 14, 3, 16, 2, 0, 10, 9, 16, 3, 8, 16, 3, 1, 16, 8, 9, 16, 1, 9, 7, 1, 11, 11, 13, 9, 9, 9, 10, 9, 10, 8, 9, 8, 3, 11, 11, 11, 11, 11, 11, 15, 14, 11, 18, 3, 18, 2, 11, 5, 1, 1, 8, 10, 5, 3, 1, 8, 1, 3, 15, 4, 3, 0, 14, 3, 15, 3, 3, 15, 5, 5, 5, 3, 5, 12, 5, 2, 5, 5, 5, 5, 5, 5, 2, 15, 14, 12, 8, 12, 5, 9, 8, 12, 12, 12, 15, 2, 12, 3, 12, 12, 7, 12, 15, 2, 18, 12, 12, 12, 12, 5, 5, 0, 2, 5, 9, 5, 12, 0, 5, 5, 9, 4, 5, 9, 6, 0, 4, 0, 2, 0, 6, 3, 3, 8, 4, 6, 18, 17, 6, 6, 6, 1, 14, 5, 2, 18, 18, 16, 6, 3, 6, 14, 6, 6, 6, 6, 6, 3, 10, 6, 12, 6, 10, 10, 6, 19, 10, 6, 3, 3, 6, 6, 10, 1, 5, 5, 2, 10, 0, 8, 8, 0, 1, 16, 12, 5, 0, 3, 0, 0, 13, 0, 0, 1, 1, 1, 17, 1, 0, 0, 13, 6, 15, 1, 1, 1, 15, 1, 1, 6, 1, 10, 0, 19, 19, 19, 10, 10, 1, 1, 1, 10, 14, 1, 10, 1, 10, 8, 10, 1, 1, 8, 7, 8, 9, 1, 10, 1, 1, 8, 1, 1, 3, 1, 1, 7, 11, 1, 8, 8, 8, 14, 0, 0, 0, 0, 0, 0, 1, 0, 0, 5, 5, 6, 2, 15, 17, 2, 3, 9, 0, 0, 0, 0, 0, 5, 0, 0, 5, 5, 5, 5, 5, 5, 16, 5, 5, 5, 2, 12, 7, 17, 6, 6, 2, 6, 1, 2, 6, 3, 6, 13, 6, 14, 6, 2, 6, 10, 5, 10, 3, 9, 2, 6, 6, 6, 2, 6, 6, 6, 1, 3, 6, 14, 2, 0, 0, 0, 0, 7, 0, 0, 3, 5, 0, 14, 16, 0, 0, 2, 6, 5, 16, 0, 1, 1, 3, 3, 2, 6, 17, 1, 9, 6, 1, 2, 2, 0, 1, 3, 10, 2, 1, 1, 6, 2, 6, 1, 1, 2, 1, 1, 10, 3, 1, 4, 19, 1, 2, 19, 6, 1, 11, 8, 1, 19, 9, 5, 19, 19, 19, 2, 9, 10, 3, 1, 4, 4, 4, 8, 10, 3, 5, 1, 11, 2, 11, 6, 11, 5, 5, 1, 2, 7, 7, 7, 8, 2, 9, 12, 12, 0, 0, 5, 0, 0, 5, 0, 0, 0, 0, 5, 7, 4, 4, 4, 4, 4, 2, 1, 4, 0, 4, 4, 4, 4, 3, 4, 4, 0, 3, 2, 5, 4, 14, 3, 4, 4, 4, 5, 3, 3, 6, 6, 3, 6, 6, 6, 6, 0, 15, 15, 3, 15, 15, 15, 2, 15, 8, 15, 8, 0, 12, 10, 2, 0, 11, 10, 0, 11, 11, 11, 11, 11, 11, 2, 11, 7, 11, 10, 11, 11, 11, 11, 3, 10, 6, 13, 10, 11, 8, 8, 8, 2, 3, 2, 6, 6, 3, 2, 6, 6, 13, 6, 8, 6, 13, 17, 17, 17, 6, 17, 2, 17, 0, 17, 6, 0, 3, 17, 12, 3, 5, 6, 4, 13, 0, 6, 0, 5, 6, 6, 6, 11, 5, 1, 6, 2, 0, 12, 3, 2, 9, 9, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 7, 4, 14, 4, 4, 4, 18, 4, 14, 4, 6, 3, 10, 2, 10, 10, 0, 3, 0, 13, 0, 18, 8, 9, 13, 17, 8, 15, 1, 15, 2, 15, 1, 10, 2, 15, 7, 8, 7, 7, 10, 1, 2, 0, 9, 9, 9, 2, 8, 2, 8, 9, 2, 9, 10, 17, 9, 8, 9, 10, 3, 9, 10, 5, 11, 8, 1, 4, 4, 7, 0, 13, 8, 9, 16, 9, 11, 2, 7, 11, 14, 15, 7, 14, 11, 11, 11, 11, 2, 16, 11, 11, 11, 13, 11, 4, 11, 4, 3, 4, 4, 13, 3, 6, 6, 3, 2, 6, 2, 2, 15, 14, 14, 14, 9, 14, 15, 14, 14, 3, 12, 8, 2, 15, 15, 5, 2, 8, 7, 12, 12, 8, 16, 5, 5, 3, 17, 3, 7, 7, 5, 8, 6, 7, 14, 16, 4, 0, 0, 7, 2, 7, 3, 13, 13, 3, 3, 15, 0, 0, 8, 0, 0, 0, 5, 13, 14, 0, 10, 0, 3, 10, 10, 7, 19, 12, 4, 2, 7, 9, 10, 10, 0, 9, 10, 8, 12, 7, 9, 7, 7, 10, 3, 5, 10, 2, 5, 18, 16, 15, 5, 7, 9, 13, 6, 7, 17, 9, 12, 4, 4, 3, 4, 7, 4, 19, 7, 1, 4, 2, 8, 2, 0, 0, 9, 10, 14, 14, 14, 8, 1, 14, 0, 14, 14, 18, 0, 14, 14, 7, 7, 18, 16, 7, 7, 7, 7, 4, 0, 7, 6, 4, 1, 1, 1, 2, 2, 11, 1, 5, 1, 19, 11, 15, 15, 4, 13, 3, 3, 1, 4, 2, 7, 6, 4, 7, 3, 4, 14, 15, 3, 2, 4, 5, 10, 15, 1, 2, 0, 0, 8, 0, 0, 8, 0, 4, 10, 10, 15, 8, 5, 10, 0, 3, 2, 5, 1, 1, 4, 5, 5, 15, 1, 5, 1, 13, 5, 19, 4, 3, 8, 0, 12, 0, 17, 11, 4, 7, 0, 0, 0, 0, 13, 7, 13, 0, 7, 0, 0, 6, 4, 1, 3, 4, 1, 2, 11, 11, 1, 0, 0, 2, 11, 10, 10, 11, 11, 7, 13, 13, 12, 11, 11, 1, 1, 1, 4, 1, 2, 7, 9, 2, 3, 19, 2, 0, 1, 0, 2, 5, 0, 0, 1, 3, 1, 0, 13, 1, 12, 12, 12, 4, 12, 4, 4, 4, 0, 12, 4, 4, 13, 0, 0, 0, 4, 13, 3, 8, 4, 18, 15, 0, 5, 0, 15, 5, 13, 1, 1, 13, 18, 5, 19, 19, 3, 10, 9, 3, 5, 8, 7, 5, 3, 2, 1, 2, 7, 5, 5, 0, 4, 16, 13, 17, 4, 4, 5, 2, 17, 3, 19, 0, 6, 7, 1, 0, 19, 1, 19, 13, 8, 16, 11, 2, 13, 1, 1, 11, 2, 16, 1, 1, 1, 11, 7, 3, 1, 12, 2, 9, 9, 12, 13, 2, 0, 0, 12, 12, 12, 6, 16, 2, 2, 1, 4, 0, 7, 4, 4, 11, 0, 8, 4, 15, 15, 4, 5, 5, 0, 14, 12, 0, 12, 2, 0, 11, 7, 14, 12, 1, 17, 3, 2, 17, 9, 3, 12, 7, 7, 1, 6, 1, 1, 7, 18, 1, 5, 7, 11, 2, 18, 4, 3, 8, 11, 15, 5, 0, 13, 9, 0, 9, 5, 3, 0, 0, 15, 5, 15, 4, 19, 7, 3, 3, 13, 19, 1, 1, 1, 19, 5, 8, 4, 7, 4, 4, 9, 1, 3, 2, 1, 2, 1, 4, 1, 1, 1, 3, 1, 19, 15, 11, 5, 2, 4, 4, 1, 6, 12, 11, 11, 2, 13, 4, 4, 1, 1, 4, 0, 15, 4, 4, 3, 1, 8]\n",
            "-------RUN119-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'UMAP(min_dist=0.0, n_components=15, n_neighbors=20)', 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': \"CountVectorizer(ngram_range=(2, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df4a9008410>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[15, 1, 2, 2, 18, 1, 9, 9, 14, 0, 0, 8, 18, 8, 14, 1, 0, 15, 13, 1, 18, 5, 9, 13, 13, 8, 7, 1, 1, 14, 1, 14, 2, 8, 8, 9, 3, 10, 10, 14, 13, 1, 7, 13, 7, 7, 7, 7, 13, 1, 7, 13, 9, 9, 15, 6, 6, 1, 9, 9, 10, 9, 9, 9, 1, 1, 5, 7, 5, 5, 8, 5, 3, 3, 0, 5, 0, 0, 14, 1, 6, 1, 6, 6, 1, 7, 6, 3, 1, 8, 3, 9, 6, 18, 9, 2, 8, 3, 8, 8, 14, 0, 0, 0, 0, 8, 8, 10, 10, 10, 5, 5, 7, 15, 1, 6, 6, 9, 0, 14, 0, 6, 13, 0, 7, 12, 5, 1, 13, 5, 1, 14, 1, 5, 5, 15, 13, 0, 5, 7, 8, 9, 15, 12, 14, 9, 15, 1, 1, 11, 9, 9, 9, 9, 0, 9, 3, 15, 9, 4, 0, 4, 8, 4, 1, 1, 8, 0, 8, 1, 9, 9, 0, 12, 14, 9, 0, 1, 14, 5, 10, 0, 14, 12, 5, 9, 1, 1, 5, 5, 14, 14, 6, 1, 8, 6, 10, 10, 6, 6, 6, 1, 6, 0, 9, 8, 6, 6, 3, 18, 3, 6, 1, 1, 6, 6, 6, 3, 1, 8, 10, 14, 3, 3, 9, 10, 7, 7, 1, 12, 12, 12, 12, 10, 10, 12, 8, 12, 0, 10, 0, 5, 5, 0, 10, 0, 0, 0, 10, 1, 0, 10, 16, 15, 8, 7, 7, 14, 3, 3, 7, 0, 10, 10, 1, 1, 8, 7, 18, 5, 5, 5, 18, 3, 5, 14, 5, 5, 8, 7, 3, 10, 5, 7, 7, 5, 10, 4, 2, 4, 2, 1, 2, 2, 6, 7, 8, 14, 10, 5, 5, 5, 5, 5, 18, 5, 3, 5, 2, 10, 13, 5, 7, 10, 1, 14, 0, 0, 0, 0, 9, 12, 0, 5, 12, 15, 1, 15, 0, 15, 0, 0, 13, 15, 19, 15, 9, 14, 3, 14, 14, 8, 8, 9, 8, 0, 14, 14, 11, 14, 1, 13, 1, 1, 9, 11, 4, 13, 13, 15, 0, 5, 1, 7, 6, 7, 5, 0, 8, 5, 0, 11, 5, 8, 9, 5, 11, 9, 10, 11, 16, 16, 1, 9, 9, 9, 6, 9, 6, 8, 9, 8, 0, 16, 16, 16, 16, 16, 16, 12, 18, 16, 10, 0, 10, 1, 16, 3, 12, 12, 8, 6, 3, 0, 12, 8, 12, 0, 19, 4, 0, 7, 18, 0, 0, 0, 0, 19, 3, 3, 3, 0, 3, 5, 3, 14, 3, 3, 3, 3, 3, 3, 14, 19, 2, 5, 8, 5, 3, 8, 8, 5, 5, 5, 19, 1, 5, 0, 5, 5, 10, 5, 0, 1, 10, 5, 5, 5, 5, 3, 3, 7, 1, 3, 8, 3, 5, 7, 18, 3, 8, 4, 3, 9, 2, 6, 4, 7, 1, 7, 2, 0, 0, 8, 4, 2, 10, 14, 2, 2, 2, 11, 15, 3, 1, 10, 10, 18, 2, 0, 2, 15, 2, 2, 2, 2, 2, 0, 13, 2, 5, 2, 13, 13, 2, 17, 13, 2, 0, 0, 2, 2, 13, 11, 3, 3, 1, 13, 7, 8, 8, 7, 11, 18, 5, 3, 7, 0, 7, 7, 1, 7, 7, 11, 11, 11, 14, 11, 7, 7, 1, 2, 19, 12, 12, 12, 12, 11, 11, 2, 11, 13, 6, 17, 17, 17, 13, 13, 11, 11, 11, 13, 18, 11, 13, 11, 13, 8, 13, 11, 11, 8, 9, 8, 9, 12, 13, 12, 12, 8, 11, 12, 0, 12, 12, 9, 16, 12, 8, 8, 8, 18, 6, 6, 6, 6, 6, 6, 11, 6, 6, 3, 3, 2, 1, 12, 14, 1, 0, 9, 6, 6, 6, 6, 6, 3, 6, 6, 3, 3, 3, 3, 3, 3, 18, 3, 3, 3, 0, 5, 10, 14, 2, 2, 1, 2, 12, 0, 2, 0, 2, 1, 2, 15, 2, 1, 2, 13, 3, 13, 0, 9, 1, 2, 2, 2, 14, 2, 2, 2, 12, 0, 2, 15, 0, 7, 7, 7, 7, 0, 7, 7, 0, 3, 7, 18, 5, 7, 7, 1, 2, 3, 5, 7, 11, 11, 0, 0, 1, 2, 14, 11, 9, 2, 11, 14, 1, 7, 11, 0, 13, 0, 11, 12, 2, 1, 2, 12, 11, 0, 12, 12, 13, 0, 12, 4, 17, 11, 0, 17, 2, 12, 16, 8, 11, 17, 9, 3, 17, 17, 17, 1, 9, 13, 0, 12, 4, 4, 4, 8, 13, 0, 3, 11, 2, 1, 16, 2, 16, 3, 3, 12, 14, 10, 10, 10, 8, 1, 9, 5, 5, 6, 6, 3, 6, 6, 3, 6, 6, 6, 6, 3, 10, 4, 4, 4, 4, 4, 1, 12, 4, 6, 4, 4, 4, 4, 0, 4, 4, 6, 0, 1, 3, 4, 10, 0, 4, 4, 4, 3, 0, 0, 2, 2, 0, 2, 2, 2, 2, 6, 0, 0, 0, 0, 0, 0, 1, 0, 8, 0, 8, 6, 5, 13, 1, 7, 16, 13, 7, 16, 2, 2, 2, 2, 2, 0, 2, 9, 16, 13, 16, 16, 16, 16, 0, 13, 2, 1, 13, 2, 8, 8, 8, 1, 0, 0, 2, 2, 0, 0, 2, 2, 1, 2, 8, 2, 1, 14, 14, 14, 2, 14, 1, 14, 6, 8, 2, 6, 0, 14, 5, 0, 3, 2, 4, 1, 6, 2, 7, 3, 2, 2, 2, 16, 3, 12, 2, 14, 7, 5, 0, 14, 9, 9, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 12, 4, 0, 4, 10, 4, 4, 4, 10, 4, 10, 4, 2, 0, 13, 1, 13, 13, 7, 0, 6, 1, 7, 10, 8, 9, 1, 14, 8, 0, 11, 0, 0, 12, 11, 13, 1, 0, 10, 8, 10, 10, 13, 11, 1, 7, 9, 9, 9, 1, 8, 1, 8, 9, 1, 9, 13, 14, 9, 8, 9, 6, 0, 9, 13, 3, 16, 8, 11, 4, 4, 10, 7, 1, 8, 9, 5, 8, 16, 14, 9, 2, 15, 0, 0, 15, 16, 2, 2, 16, 1, 18, 2, 2, 2, 1, 2, 4, 2, 4, 0, 4, 4, 1, 0, 2, 2, 0, 0, 2, 1, 2, 10, 15, 15, 18, 9, 15, 10, 15, 15, 0, 5, 8, 0, 19, 19, 3, 1, 7, 14, 5, 5, 8, 5, 3, 3, 0, 14, 0, 9, 9, 3, 8, 2, 10, 15, 5, 4, 7, 6, 9, 1, 9, 0, 1, 1, 0, 0, 12, 7, 7, 8, 7, 7, 7, 3, 1, 15, 7, 13, 7, 0, 13, 13, 10, 17, 5, 4, 1, 10, 9, 13, 13, 7, 9, 13, 8, 5, 0, 6, 10, 10, 13, 0, 3, 6, 1, 3, 10, 5, 0, 3, 10, 8, 1, 2, 10, 14, 9, 5, 4, 4, 0, 4, 10, 4, 17, 10, 12, 4, 1, 8, 1, 6, 6, 9, 13, 15, 15, 15, 8, 11, 15, 6, 15, 15, 10, 6, 15, 15, 9, 9, 10, 5, 10, 10, 10, 10, 4, 6, 10, 2, 4, 11, 11, 11, 1, 1, 16, 12, 3, 11, 17, 16, 19, 19, 4, 1, 0, 0, 12, 4, 1, 0, 2, 4, 10, 0, 4, 15, 0, 0, 1, 4, 3, 6, 19, 11, 1, 7, 7, 8, 7, 7, 8, 7, 4, 13, 13, 0, 8, 3, 13, 7, 0, 1, 3, 11, 11, 4, 3, 3, 10, 11, 3, 11, 1, 3, 17, 4, 0, 8, 7, 5, 6, 14, 2, 4, 9, 6, 7, 7, 6, 1, 10, 1, 6, 9, 6, 7, 2, 4, 11, 0, 4, 11, 1, 2, 2, 12, 7, 7, 1, 2, 13, 13, 2, 2, 10, 1, 1, 5, 16, 16, 11, 11, 12, 4, 11, 0, 9, 9, 14, 0, 17, 1, 6, 11, 6, 1, 3, 6, 7, 12, 0, 11, 6, 1, 11, 5, 5, 5, 4, 5, 4, 4, 4, 7, 5, 4, 4, 1, 6, 7, 6, 4, 1, 0, 8, 4, 10, 19, 6, 3, 7, 19, 3, 1, 11, 11, 1, 10, 3, 17, 17, 0, 13, 8, 0, 3, 8, 10, 3, 0, 1, 11, 14, 0, 3, 3, 6, 4, 5, 1, 8, 4, 4, 3, 1, 14, 14, 17, 6, 2, 0, 12, 7, 17, 11, 17, 1, 8, 5, 2, 1, 1, 11, 12, 2, 14, 18, 12, 12, 12, 16, 10, 0, 11, 5, 1, 9, 9, 5, 1, 1, 6, 6, 5, 5, 5, 2, 5, 14, 1, 12, 4, 6, 10, 4, 4, 16, 7, 8, 4, 0, 0, 4, 3, 3, 7, 18, 5, 7, 5, 1, 7, 2, 10, 15, 5, 11, 14, 0, 1, 14, 9, 0, 5, 10, 10, 11, 2, 12, 11, 9, 10, 11, 3, 10, 16, 1, 10, 4, 0, 8, 16, 19, 3, 6, 1, 7, 6, 8, 3, 0, 6, 6, 19, 3, 12, 4, 17, 9, 0, 0, 1, 17, 11, 12, 12, 17, 3, 8, 4, 9, 4, 10, 9, 11, 0, 14, 12, 1, 11, 4, 11, 11, 12, 0, 12, 17, 19, 12, 3, 1, 4, 4, 12, 2, 5, 2, 2, 1, 1, 4, 4, 12, 12, 4, 6, 12, 4, 4, 0, 11, 8]\n",
            "-------RUN120-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': \"CountVectorizer(stop_words='english')\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, 1, 1, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, 1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, 1, -1, -1, -1, -1, 0, 1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 2, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, 1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 2, -1, 2, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, 0, 1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 1, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, 0, 2, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, 2, -1, -1, 0, -1, 2, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
            "-------RUN121-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': 'CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7df3b00f5c90>)', 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, 1, 1, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, 1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, 1, -1, -1, -1, -1, 0, 1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, 1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 2, -1, 2, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, 0, 1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 1, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 2, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, 0, -1, -1, 0, -1, 2, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
            "-------RUN122-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': \"CountVectorizer(stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b02d2c50>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, 1, 1, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, 1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, 1, -1, -1, -1, -1, 0, 1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, 1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 2, -1, 2, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, 0, 1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 2, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, 0, -1, -1, 0, -1, 2, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
            "-------RUN123-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': \"CountVectorizer(ngram_range=(1, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b891dd10>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, 2, 2, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, 2, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, 2, -1, -1, -1, -1, 0, 2, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, 0, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, 2, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 1, -1, 1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, 0, 2, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, 2, -1, 0, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 2, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 0, 1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, 0, -1, -1, 0, -1, 1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
            "-------RUN124-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': \"CountVectorizer(ngram_range=(2, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df4a9008410>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, 1, 1, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, 1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, 1, -1, -1, -1, -1, 0, 1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, 1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 2, -1, 2, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, 0, 1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, -1, 0, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 1, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, 0, 2, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, 2, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
            "-------RUN125-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': \"CountVectorizer(stop_words='english')\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, 2, 2, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 2, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 2, -1, -1, -1, -1, 0, 2, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, 2, -1, -1, 1, -1, -1, -1, -1, -1, -1, 1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 1, -1, 0, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 2, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, 0, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, 2, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 2, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, 1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1]\n",
            "-------RUN126-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': 'CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7df3b00f5c90>)', 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, 1, 1, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 1, -1, -1, -1, -1, 0, 1, -1, 0, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, 1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, 2, -1, 0, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 1, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, 2, -1, -1, 0, -1, 2, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1]\n",
            "-------RUN127-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': \"CountVectorizer(stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b02d2c50>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 2, 2, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, 2, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 2, -1, -1, -1, -1, 0, 2, -1, 0, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, 1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, 2, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, -1, 1, -1, 0, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 2, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, 0, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, 2, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 2, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, 1, -1, -1, 0, -1, 1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
            "-------RUN128-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': \"CountVectorizer(ngram_range=(1, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b891dd10>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[-1, -1, -1, -1, -1, -1, -1, -1, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 1, 1, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 1, -1, -1, -1, -1, 0, 1, -1, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, 2, -1, 2, -1, 0, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 3, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 3, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, 2, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1]\n",
            "-------RUN129-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': \"CountVectorizer(ngram_range=(2, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df4a9008410>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[-1, -1, -1, -1, -1, -1, -1, -1, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 2, 2, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 2, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 2, -1, -1, -1, -1, 0, 2, -1, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, -1, 1, -1, 0, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 3, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 3, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, -1, 2, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 2, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, 1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 3, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1]\n",
            "-------RUN130-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': \"CountVectorizer(stop_words='english')\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, -1, -1, 2, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, 5, -1, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 4, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 1, 0, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, -1, -1, 1, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, 1, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, 2, -1, 2, -1, -1, 2, -1, 0, -1, -1, 0, -1, 0, 2, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, 1, -1, 0, -1, 0, -1, -1, -1, -1, 1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, 4, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, 3, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 3, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 3, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, 5, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, 3, 0, -1, 0, 3, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, 5, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, 4, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 4, -1, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, -1, -1, 3, -1, -1, 0, -1, 3, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 2, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, 2, -1, 0, -1, 0, -1, -1, -1, -1, 5, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, 3, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 5, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 2, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 3, 0, 0, -1, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 4, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, 1, 0, 0, -1, -1, -1, -1, 1, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 2, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, 0, 4, -1, 0, -1, 0, 4, 0, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 1, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 2, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1]\n",
            "-------RUN131-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': 'CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7df3b00f5c90>)', 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, -1, -1, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 4, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 1, 0, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, -1, -1, 1, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, 1, 0, 0, -1, 0, -1, 0, -1, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, 2, 2, 2, -1, -1, 2, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, 1, -1, 0, -1, 0, -1, -1, -1, -1, 1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, 4, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, 3, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 3, -1, 0, 0, -1, -1, -1, 0, -1, -1, 0, -1, 3, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, -1, 4, 0, -1, 4, -1, -1, -1, -1, -1, -1, -1, 0, 1, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, 3, 0, -1, 0, 3, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, 4, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 4, -1, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, 3, -1, -1, 0, -1, 3, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 2, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, 2, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, 3, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, 2, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 2, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 3, 0, 0, -1, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 4, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, -1, 1, 0, -1, -1, 0, -1, -1, 1, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 2, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, 0, 4, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 1, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 2, 0, 0, 0, 1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1]\n",
            "-------RUN132-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': \"CountVectorizer(stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b02d2c50>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, -1, -1, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 4, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 1, 0, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, -1, -1, 1, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, 1, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, 2, -1, 2, -1, -1, 2, -1, 0, -1, -1, 0, -1, 0, 2, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, 1, -1, 0, -1, 0, -1, -1, -1, -1, 1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, 4, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, 3, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 3, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 3, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, 4, -1, -1, -1, -1, -1, -1, -1, 0, 1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 3, 0, 3, 0, 3, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, 4, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 4, -1, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, 1, 0, -1, -1, 3, -1, -1, 0, -1, 3, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 2, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, 2, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, 3, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, 2, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 2, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 3, 0, 0, -1, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 4, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, 1, 0, 0, -1, -1, -1, -1, 1, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, 2, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, 0, 4, -1, 0, -1, -1, 4, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 1, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 2, 0, 0, 0, 1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1]\n",
            "-------RUN133-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': \"CountVectorizer(ngram_range=(1, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b891dd10>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, -1, -1, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 1, 0, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, -1, -1, 1, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, 1, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, 2, 2, 2, -1, -1, 2, -1, 0, -1, -1, 0, -1, 0, 2, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, 1, -1, 0, -1, 0, -1, -1, -1, -1, 1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, 3, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 3, -1, 0, 0, -1, -1, -1, 0, -1, 0, 0, -1, 3, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, 1, -1, -1, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 3, 0, 0, 0, 3, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, 3, -1, -1, 0, -1, 3, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, 2, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, 2, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, 3, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, 2, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, -1, 2, 0, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 3, 0, 0, -1, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, 1, 0, 0, -1, 0, -1, -1, 1, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 2, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 1, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 2, 0, 0, 0, 1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1]\n",
            "-------RUN134-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': \"CountVectorizer(ngram_range=(2, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df4a9008410>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, -1, -1, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 4, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 1, 0, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, -1, -1, 1, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, 1, 0, 0, -1, 0, -1, 0, -1, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, 2, 2, 2, -1, -1, 2, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, 1, -1, 0, -1, 0, -1, -1, -1, -1, 1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, 4, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, 3, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 3, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 3, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, -1, 4, 0, -1, 4, -1, -1, -1, -1, -1, -1, -1, 0, 1, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, 0, 3, -1, 0, -1, 0, 0, -1, -1, -1, 3, 0, -1, 0, 3, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, 4, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 4, -1, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, 1, 0, -1, -1, 3, -1, -1, 0, -1, 3, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 2, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, 2, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, 3, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, 2, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 2, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 3, 0, 0, -1, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 4, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, 1, 0, 0, -1, -1, -1, -1, 1, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 2, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, 0, 4, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 1, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 2, 0, 0, 0, 1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1]\n",
            "-------RUN135-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': \"CountVectorizer(stop_words='english')\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[8, 2, 12, 12, 11, 18, 1, 5, 2, 0, 18, 8, 10, 10, 0, 1, 0, 12, 5, 1, 10, 13, 8, 6, 6, 13, 13, 1, 0, 5, 0, 1, 12, 7, 7, 9, 5, 8, 11, 11, 6, 0, 11, 6, 11, 9, 13, 11, 1, 0, 15, 6, 1, 10, 1, 13, 9, 2, 5, 11, 0, 2, 1, 13, 0, 1, 4, 13, 4, 4, 7, 9, 16, 16, 18, 4, 18, 5, 2, 2, 9, 2, 2, 2, 2, 13, 19, 4, 2, 7, 16, 13, 2, 8, 1, 8, 7, 16, 7, 10, 11, 18, 1, 1, 1, 7, 10, 11, 11, 0, 4, 9, 19, 8, 2, 13, 11, 10, 18, 0, 7, 10, 6, 18, 11, 11, 9, 0, 6, 9, 2, 2, 1, 9, 9, 11, 1, 18, 9, 11, 7, 0, 13, 8, 0, 5, 6, 0, 2, 14, 15, 11, 0, 7, 15, 0, 15, 7, 1, 5, 5, 3, 7, 1, 1, 0, 7, 18, 13, 0, 7, 1, 2, 17, 2, 5, 18, 0, 0, 11, 18, 0, 10, 17, 4, 5, 1, 2, 4, 0, 2, 9, 9, 2, 7, 9, 4, 15, 9, 8, 19, 2, 0, 0, 10, 10, 9, 13, 7, 10, 15, 13, 2, 2, 10, 9, 9, 16, 2, 7, 15, 15, 2, 16, 10, 4, 13, 19, 2, 3, 17, 17, 17, 11, 10, 3, 12, 17, 15, 4, 15, 11, 4, 0, 5, 15, 5, 18, 1, 1, 0, 4, 6, 8, 11, 13, 19, 13, 16, 15, 9, 18, 0, 5, 0, 0, 7, 7, 1, 4, 4, 4, 5, 16, 4, 2, 4, 9, 13, 7, 0, 0, 4, 13, 19, 6, 11, 1, 1, 17, 8, 0, 6, 8, 10, 4, 11, 2, 0, 10, 8, 1, 11, 10, 10, 6, 10, 15, 8, 15, 6, 4, 11, 1, 1, 8, 5, 18, 18, 2, 5, 9, 1, 5, 17, 4, 1, 8, 2, 15, 15, 5, 11, 8, 4, 6, 0, 9, 4, 0, 15, 7, 13, 9, 5, 0, 2, 2, 17, 9, 1, 1, 1, 2, 13, 15, 8, 11, 9, 12, 18, 13, 0, 9, 13, 15, 8, 0, 13, 5, 15, 15, 9, 7, 11, 9, 17, 9, 1, 3, 12, 3, 0, 10, 10, 10, 13, 0, 9, 13, 15, 9, 0, 12, 12, 12, 12, 12, 12, 17, 6, 12, 4, 0, 15, 0, 12, 10, 14, 3, 13, 10, 10, 18, 17, 7, 8, 18, 5, 0, 1, 10, 0, 18, 15, 15, 15, 5, 9, 16, 10, 2, 16, 4, 16, 9, 16, 16, 16, 16, 16, 5, 0, 5, 12, 4, 0, 4, 10, 13, 13, 4, 4, 5, 5, 2, 4, 0, 4, 4, 5, 4, 17, 2, 4, 4, 4, 4, 6, 16, 16, 19, 0, 16, 15, 16, 4, 19, 8, 16, 10, 1, 16, 10, 17, 10, 17, 19, 2, 19, 12, 18, 18, 13, 8, 12, 4, 7, 6, 4, 6, 3, 6, 11, 0, 11, 17, 11, 12, 0, 1, 1, 6, 1, 6, 6, 8, 1, 7, 6, 4, 6, 7, 4, 6, 3, 7, 6, 0, 18, 6, 1, 11, 15, 5, 11, 0, 10, 19, 7, 7, 19, 17, 4, 10, 16, 13, 18, 19, 19, 2, 13, 19, 14, 14, 14, 11, 14, 13, 19, 2, 6, 5, 14, 3, 14, 17, 14, 5, 12, 14, 7, 11, 8, 17, 8, 7, 13, 5, 14, 14, 1, 1, 4, 7, 11, 10, 7, 5, 14, 14, 7, 11, 7, 10, 14, 15, 3, 14, 10, 1, 14, 7, 14, 3, 7, 3, 14, 13, 7, 7, 8, 19, 13, 13, 19, 10, 19, 11, 13, 11, 16, 16, 1, 2, 8, 1, 0, 18, 15, 19, 9, 10, 9, 10, 0, 19, 19, 0, 16, 16, 16, 16, 13, 1, 16, 10, 16, 0, 4, 15, 9, 6, 8, 0, 6, 17, 0, 5, 0, 6, 1, 8, 1, 6, 2, 6, 6, 10, 6, 18, 1, 0, 8, 11, 6, 0, 6, 1, 6, 12, 0, 6, 6, 0, 9, 19, 19, 9, 18, 9, 9, 0, 11, 19, 1, 19, 9, 19, 2, 12, 16, 0, 9, 14, 14, 18, 0, 0, 8, 0, 5, 1, 8, 3, 0, 2, 13, 14, 15, 1, 0, 14, 14, 6, 0, 10, 3, 17, 0, 3, 3, 5, 18, 3, 1, 17, 14, 0, 8, 6, 14, 7, 7, 14, 1, 8, 4, 1, 8, 12, 2, 11, 6, 2, 17, 3, 14, 14, 7, 5, 0, 16, 17, 1, 0, 8, 6, 12, 16, 4, 15, 2, 8, 1, 8, 4, 1, 10, 4, 4, 13, 19, 16, 19, 19, 16, 11, 19, 10, 19, 4, 5, 3, 3, 3, 3, 3, 0, 15, 3, 19, 3, 3, 17, 3, 18, 3, 17, 13, 18, 0, 16, 3, 8, 0, 3, 3, 3, 16, 1, 8, 8, 6, 8, 1, 1, 1, 6, 9, 14, 17, 18, 17, 18, 17, 2, 15, 13, 17, 13, 13, 4, 5, 2, 13, 12, 6, 9, 12, 12, 6, 8, 6, 12, 2, 12, 0, 12, 5, 12, 12, 12, 12, 18, 1, 12, 0, 5, 8, 7, 7, 7, 2, 0, 2, 8, 1, 1, 0, 12, 6, 0, 12, 7, 1, 1, 13, 8, 1, 1, 0, 0, 0, 9, 0, 6, 9, 18, 1, 15, 0, 10, 8, 3, 2, 9, 1, 11, 4, 6, 6, 1, 8, 4, 11, 8, 0, 10, 6, 18, 2, 15, 7, 0, 3, 3, 3, 3, 3, 3, 3, 17, 3, 3, 17, 3, 5, 17, 12, 3, 17, 17, 15, 8, 6, 3, 6, 0, 5, 2, 7, 5, 9, 18, 9, 2, 4, 15, 11, 19, 1, 1, 7, 17, 11, 17, 2, 11, 15, 6, 2, 18, 10, 11, 5, 7, 6, 14, 2, 13, 13, 10, 11, 0, 7, 0, 7, 9, 0, 10, 6, 0, 10, 13, 9, 10, 0, 8, 6, 4, 12, 7, 14, 11, 3, 5, 11, 18, 7, 5, 10, 4, 12, 2, 0, 8, 7, 0, 5, 7, 12, 12, 8, 12, 0, 1, 12, 12, 12, 1, 12, 15, 12, 3, 5, 4, 12, 1, 1, 12, 12, 0, 1, 12, 0, 0, 5, 0, 6, 6, 0, 5, 5, 17, 7, 1, 4, 13, 2, 5, 5, 16, 0, 0, 18, 4, 6, 7, 8, 16, 10, 15, 11, 0, 8, 0, 16, 11, 6, 0, 10, 9, 15, 19, 19, 15, 2, 8, 15, 0, 5, 18, 0, 17, 13, 9, 13, 19, 19, 11, 16, 2, 0, 13, 11, 19, 18, 6, 7, 15, 8, 1, 4, 0, 10, 11, 6, 6, 8, 9, 5, 7, 4, 1, 4, 0, 5, 6, 5, 16, 10, 2, 16, 5, 11, 5, 16, 5, 11, 1, 12, 0, 2, 7, 4, 3, 3, 0, 17, 0, 3, 8, 4, 17, 5, 0, 7, 1, 13, 9, 10, 10, 11, 7, 7, 7, 15, 7, 19, 10, 1, 4, 9, 5, 9, 1, 1, 4, 9, 5, 7, 15, 0, 11, 10, 15, 6, 17, 18, 11, 14, 0, 2, 12, 14, 8, 14, 1, 12, 10, 0, 12, 1, 2, 0, 3, 3, 2, 15, 8, 1, 9, 1, 17, 6, 18, 18, 2, 3, 16, 7, 5, 15, 2, 19, 9, 7, 19, 13, 7, 19, 3, 6, 6, 5, 7, 16, 6, 19, 0, 0, 16, 14, 11, 3, 16, 16, 11, 14, 16, 14, 1, 16, 12, 8, 18, 7, 13, 4, 19, 15, 12, 17, 5, 19, 13, 15, 13, 2, 1, 1, 19, 7, 19, 9, 11, 17, 14, 0, 17, 10, 2, 8, 12, 14, 11, 1, 2, 8, 6, 5, 8, 1, 1, 2, 1, 10, 1, 8, 14, 14, 3, 3, 14, 18, 5, 11, 2, 0, 3, 0, 9, 14, 9, 2, 15, 3, 9, 17, 18, 14, 4, 1, 14, 4, 4, 6, 3, 1, 3, 3, 17, 19, 0, 3, 3, 1, 9, 13, 13, 8, 2, 15, 13, 17, 4, 4, 9, 16, 19, 4, 16, 2, 14, 14, 1, 5, 16, 12, 17, 15, 7, 18, 18, 16, 7, 15, 16, 18, 18, 14, 2, 5, 16, 16, 13, 3, 10, 1, 0, 3, 17, 16, 1, 0, 15, 1, 19, 1, 0, 14, 7, 8, 14, 0, 1, 7, 9, 3, 0, 1, 14, 17, 12, 0, 1, 14, 3, 3, 12, 1, 5, 5, 16, 2, 5, 9, 15, 0, 1, 9, 9, 4, 4, 4, 1, 9, 2, 18, 3, 3, 9, 18, 17, 3, 12, 19, 7, 3, 17, 17, 3, 16, 16, 11, 6, 6, 13, 4, 2, 19, 3, 15, 5, 4, 14, 0, 1, 1, 1, 15, 15, 4, 6, 5, 14, 8, 3, 14, 15, 4, 14, 10, 5, 12, 2, 15, 17, 0, 18, 12, 5, 16, 13, 5, 4, 9, 5, 10, 17, 1, 19, 1, 10, 17, 17, 12, 14, 5, 18, 10, 8, 17, 14, 3, 8, 4, 13, 3, 5, 3, 6, 7, 14, 1, 2, 3, 0, 14, 17, 14, 14, 14, 8, 3, 1, 5, 11, 16, 0, 3, 3, 14, 6, 8, 8, 12, 2, 2, 3, 3, 1, 17, 8, 10, 17, 3, 14, 15, 11, 7]\n",
            "-------RUN136-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': 'CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7df3b00f5c90>)', 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[2, 4, 3, 16, 5, 15, 2, 14, 4, 0, 10, 2, 2, 2, 0, 2, 0, 3, 15, 1, 3, 13, 2, 3, 3, 13, 13, 1, 0, 14, 0, 1, 11, 8, 8, 17, 1, 2, 5, 5, 3, 2, 5, 3, 5, 12, 13, 5, 1, 0, 10, 11, 1, 1, 3, 13, 12, 4, 10, 5, 0, 4, 0, 8, 0, 1, 6, 13, 6, 6, 8, 6, 18, 18, 15, 6, 15, 1, 4, 4, 17, 4, 4, 4, 4, 13, 12, 6, 4, 8, 18, 13, 17, 2, 1, 2, 8, 18, 8, 2, 5, 15, 1, 1, 1, 8, 2, 5, 5, 17, 6, 12, 13, 2, 4, 17, 5, 2, 15, 0, 8, 5, 3, 15, 5, 5, 12, 0, 3, 16, 4, 2, 1, 12, 6, 5, 1, 15, 12, 5, 8, 2, 13, 16, 4, 14, 3, 0, 4, 9, 10, 5, 0, 8, 10, 0, 19, 8, 1, 14, 14, 7, 8, 2, 1, 0, 8, 15, 8, 0, 8, 2, 4, 9, 4, 5, 15, 0, 2, 5, 15, 0, 2, 11, 11, 14, 2, 4, 6, 2, 4, 2, 12, 4, 8, 12, 14, 10, 12, 16, 12, 4, 0, 0, 17, 17, 17, 13, 8, 11, 19, 13, 4, 4, 2, 12, 17, 18, 4, 8, 10, 4, 19, 19, 17, 6, 13, 13, 4, 7, 11, 9, 11, 14, 2, 7, 5, 11, 10, 6, 10, 5, 6, 0, 1, 14, 1, 0, 1, 1, 0, 6, 3, 2, 5, 13, 13, 13, 19, 19, 12, 15, 0, 14, 0, 0, 8, 8, 1, 6, 6, 6, 14, 19, 6, 4, 6, 6, 17, 8, 19, 0, 6, 13, 12, 11, 5, 1, 1, 11, 3, 0, 3, 10, 5, 6, 17, 4, 0, 6, 19, 1, 5, 19, 14, 3, 6, 19, 2, 10, 3, 6, 13, 1, 2, 2, 15, 15, 15, 4, 14, 12, 1, 11, 9, 6, 1, 2, 4, 10, 10, 1, 5, 2, 11, 5, 0, 17, 6, 0, 10, 8, 13, 2, 14, 0, 4, 4, 9, 2, 1, 1, 1, 4, 13, 10, 11, 5, 17, 11, 15, 13, 0, 12, 13, 10, 2, 0, 13, 14, 15, 10, 6, 8, 5, 10, 9, 17, 1, 7, 16, 7, 0, 5, 2, 2, 13, 2, 12, 13, 10, 2, 0, 16, 16, 16, 16, 16, 16, 5, 3, 16, 6, 0, 10, 0, 16, 14, 9, 11, 8, 17, 1, 15, 10, 8, 11, 15, 14, 1, 1, 2, 2, 15, 10, 10, 10, 1, 19, 18, 19, 4, 18, 6, 18, 4, 18, 18, 18, 18, 18, 14, 0, 14, 16, 6, 2, 6, 19, 13, 17, 6, 6, 14, 14, 4, 6, 0, 6, 6, 14, 6, 11, 4, 6, 6, 6, 6, 3, 18, 18, 13, 0, 18, 10, 6, 6, 12, 2, 18, 19, 1, 18, 2, 11, 17, 11, 12, 4, 12, 11, 15, 15, 13, 11, 11, 14, 8, 3, 6, 3, 7, 3, 5, 0, 5, 14, 5, 11, 0, 3, 1, 3, 1, 3, 3, 3, 1, 8, 3, 6, 3, 8, 6, 3, 11, 8, 3, 0, 15, 3, 1, 5, 10, 14, 17, 0, 1, 13, 8, 8, 12, 5, 10, 19, 18, 13, 15, 17, 12, 4, 13, 13, 9, 9, 9, 5, 9, 13, 13, 4, 3, 14, 9, 7, 9, 5, 9, 14, 16, 9, 8, 5, 10, 11, 2, 8, 13, 14, 9, 9, 1, 1, 9, 8, 9, 5, 8, 1, 9, 9, 8, 5, 8, 2, 9, 19, 7, 9, 2, 1, 9, 8, 9, 7, 8, 7, 9, 13, 17, 8, 3, 13, 13, 13, 13, 2, 12, 5, 13, 5, 18, 14, 1, 4, 10, 2, 0, 15, 10, 12, 19, 2, 12, 2, 19, 12, 12, 19, 18, 18, 18, 14, 13, 1, 18, 19, 18, 0, 6, 10, 12, 3, 2, 0, 3, 9, 0, 1, 0, 3, 1, 3, 1, 3, 4, 3, 3, 6, 5, 15, 2, 0, 3, 5, 3, 0, 3, 1, 3, 11, 0, 3, 3, 0, 12, 12, 12, 12, 10, 12, 12, 0, 14, 12, 1, 12, 17, 13, 4, 11, 18, 2, 12, 9, 9, 15, 0, 0, 2, 2, 14, 1, 16, 7, 0, 4, 13, 9, 10, 1, 0, 9, 9, 3, 0, 5, 7, 11, 0, 7, 7, 14, 15, 7, 1, 10, 9, 0, 10, 3, 9, 8, 8, 9, 1, 2, 14, 1, 11, 16, 4, 5, 3, 4, 7, 7, 9, 9, 8, 15, 0, 19, 7, 1, 0, 16, 3, 16, 19, 6, 10, 4, 10, 1, 10, 14, 1, 2, 6, 6, 13, 12, 19, 13, 12, 18, 5, 12, 17, 12, 6, 1, 7, 7, 7, 7, 7, 0, 10, 7, 12, 7, 7, 11, 7, 15, 7, 11, 13, 15, 2, 19, 7, 16, 0, 7, 7, 7, 18, 1, 2, 11, 3, 0, 1, 1, 1, 3, 12, 9, 9, 15, 5, 15, 9, 4, 10, 13, 9, 17, 13, 6, 1, 4, 13, 16, 3, 12, 16, 16, 3, 16, 3, 16, 4, 16, 0, 16, 15, 16, 16, 16, 16, 15, 1, 3, 0, 14, 2, 8, 8, 8, 4, 0, 4, 11, 1, 1, 0, 3, 3, 0, 3, 8, 1, 1, 8, 2, 0, 1, 0, 0, 0, 12, 0, 3, 17, 4, 1, 10, 0, 1, 10, 7, 4, 12, 1, 5, 6, 3, 3, 2, 2, 14, 5, 2, 0, 2, 11, 15, 4, 10, 8, 0, 7, 7, 7, 7, 7, 7, 7, 14, 7, 7, 10, 7, 10, 11, 16, 7, 11, 10, 10, 16, 3, 7, 3, 19, 1, 4, 8, 14, 12, 15, 12, 4, 6, 10, 17, 17, 1, 2, 8, 9, 5, 15, 4, 5, 10, 5, 4, 15, 6, 5, 14, 8, 3, 9, 4, 13, 5, 2, 5, 0, 8, 0, 8, 12, 0, 17, 3, 0, 17, 17, 17, 17, 0, 11, 3, 14, 16, 8, 9, 5, 11, 5, 5, 15, 8, 0, 2, 6, 16, 4, 0, 2, 8, 0, 10, 8, 16, 16, 10, 16, 0, 2, 16, 16, 3, 1, 16, 10, 16, 7, 1, 6, 5, 1, 1, 16, 3, 0, 0, 16, 0, 0, 14, 0, 11, 3, 2, 14, 14, 10, 8, 1, 6, 13, 4, 1, 1, 18, 0, 0, 15, 6, 3, 8, 2, 18, 14, 10, 5, 0, 2, 0, 18, 5, 3, 0, 2, 6, 10, 12, 17, 10, 4, 2, 10, 0, 1, 15, 0, 11, 13, 12, 17, 13, 12, 5, 18, 4, 0, 13, 5, 12, 15, 14, 8, 10, 16, 1, 6, 0, 14, 17, 3, 3, 2, 17, 1, 8, 11, 1, 6, 0, 1, 3, 15, 18, 2, 4, 18, 14, 5, 15, 18, 14, 5, 1, 11, 0, 4, 8, 6, 7, 7, 0, 11, 0, 7, 16, 14, 9, 14, 2, 8, 2, 13, 17, 2, 2, 5, 8, 8, 8, 10, 8, 12, 2, 1, 6, 2, 5, 17, 1, 1, 6, 12, 14, 8, 6, 0, 5, 14, 10, 3, 11, 15, 5, 9, 0, 4, 16, 9, 10, 9, 1, 16, 14, 0, 16, 1, 4, 0, 7, 7, 4, 10, 10, 1, 10, 1, 11, 3, 15, 15, 4, 7, 18, 8, 14, 10, 4, 12, 12, 8, 12, 13, 8, 13, 7, 3, 3, 14, 17, 18, 3, 12, 0, 0, 6, 9, 5, 7, 18, 14, 14, 9, 19, 9, 1, 18, 11, 10, 15, 17, 13, 6, 12, 10, 11, 11, 5, 12, 13, 17, 17, 4, 1, 1, 13, 8, 12, 12, 5, 11, 9, 0, 11, 5, 4, 11, 16, 9, 5, 1, 4, 11, 3, 1, 16, 1, 1, 4, 1, 6, 3, 11, 9, 9, 7, 7, 9, 0, 0, 5, 2, 2, 7, 0, 12, 9, 12, 4, 10, 10, 12, 9, 15, 9, 6, 1, 9, 6, 6, 3, 7, 1, 7, 7, 11, 12, 19, 7, 7, 1, 19, 13, 17, 2, 4, 10, 13, 11, 6, 14, 19, 18, 12, 14, 19, 4, 9, 9, 1, 14, 18, 11, 11, 10, 8, 15, 15, 18, 8, 10, 18, 15, 0, 9, 4, 14, 19, 18, 17, 7, 6, 8, 0, 7, 11, 18, 2, 0, 19, 1, 13, 1, 0, 9, 8, 16, 9, 0, 1, 8, 17, 11, 0, 1, 9, 9, 16, 0, 1, 9, 7, 7, 16, 1, 15, 11, 6, 4, 5, 17, 19, 0, 0, 12, 12, 6, 6, 6, 3, 12, 4, 15, 7, 7, 17, 15, 11, 7, 16, 13, 5, 7, 5, 11, 7, 18, 19, 5, 3, 3, 13, 14, 4, 12, 11, 10, 1, 6, 9, 2, 1, 1, 2, 17, 4, 6, 11, 15, 9, 3, 7, 9, 10, 14, 9, 14, 11, 16, 4, 10, 11, 0, 0, 16, 14, 18, 13, 1, 6, 17, 14, 17, 11, 1, 12, 1, 19, 11, 10, 11, 9, 10, 15, 1, 11, 15, 9, 7, 11, 6, 13, 7, 15, 7, 11, 8, 9, 1, 4, 7, 0, 9, 11, 9, 9, 9, 2, 7, 1, 14, 5, 18, 0, 7, 7, 9, 3, 2, 16, 3, 4, 4, 7, 7, 1, 11, 11, 2, 5, 7, 9, 10, 5, 8]\n",
            "-------RUN137-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': \"CountVectorizer(stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b02d2c50>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[11, 9, 15, 15, 0, 13, 2, 3, 9, 2, 5, 11, 14, 0, 2, 8, 13, 15, 3, 3, 10, 1, 2, 10, 10, 12, 0, 3, 2, 5, 2, 2, 8, 12, 12, 7, 10, 11, 0, 0, 10, 2, 0, 10, 0, 1, 0, 0, 8, 2, 5, 10, 8, 10, 8, 1, 1, 9, 5, 0, 14, 9, 2, 0, 2, 8, 6, 1, 6, 6, 12, 7, 19, 18, 13, 6, 13, 3, 9, 13, 7, 9, 9, 9, 9, 1, 1, 6, 9, 12, 19, 1, 9, 11, 8, 11, 12, 19, 12, 2, 0, 3, 3, 3, 3, 12, 14, 0, 5, 2, 6, 7, 1, 14, 9, 7, 0, 2, 13, 2, 12, 3, 10, 13, 0, 16, 7, 2, 10, 15, 9, 2, 8, 7, 7, 0, 8, 13, 7, 0, 12, 2, 0, 11, 14, 5, 15, 13, 9, 17, 5, 0, 3, 12, 5, 13, 18, 3, 3, 5, 16, 11, 12, 8, 8, 2, 12, 3, 12, 13, 12, 2, 9, 16, 9, 5, 13, 2, 2, 0, 3, 13, 2, 16, 6, 0, 8, 9, 6, 14, 14, 7, 7, 9, 12, 7, 6, 5, 7, 11, 1, 9, 2, 14, 7, 0, 7, 1, 0, 10, 18, 1, 9, 9, 14, 7, 7, 19, 9, 12, 5, 14, 14, 18, 0, 6, 1, 1, 9, 4, 16, 16, 16, 0, 14, 4, 15, 16, 11, 6, 5, 0, 6, 13, 3, 5, 3, 13, 8, 3, 13, 6, 10, 11, 0, 1, 1, 1, 18, 18, 7, 13, 13, 3, 3, 2, 12, 12, 8, 6, 6, 6, 5, 18, 6, 9, 6, 7, 0, 12, 14, 3, 6, 1, 1, 10, 0, 8, 8, 16, 11, 2, 10, 11, 0, 6, 0, 9, 14, 10, 14, 10, 0, 14, 18, 10, 6, 14, 11, 9, 10, 6, 0, 3, 2, 11, 3, 5, 3, 9, 5, 7, 3, 16, 5, 6, 8, 11, 9, 11, 5, 3, 0, 8, 6, 16, 14, 7, 6, 2, 11, 12, 1, 7, 0, 2, 9, 9, 16, 7, 8, 3, 8, 9, 0, 5, 11, 0, 7, 15, 13, 1, 13, 7, 0, 5, 11, 13, 1, 5, 5, 5, 7, 12, 0, 7, 16, 7, 8, 4, 15, 4, 2, 0, 14, 14, 1, 14, 7, 12, 5, 7, 3, 15, 15, 15, 15, 15, 15, 16, 10, 11, 6, 13, 5, 2, 15, 0, 16, 4, 12, 7, 3, 5, 5, 12, 11, 9, 5, 13, 3, 8, 14, 13, 5, 5, 5, 3, 14, 19, 18, 9, 18, 6, 19, 7, 19, 19, 19, 18, 19, 18, 2, 5, 8, 6, 2, 6, 18, 1, 12, 6, 6, 16, 5, 9, 6, 13, 6, 6, 5, 6, 16, 9, 6, 6, 6, 6, 10, 19, 19, 1, 2, 19, 5, 18, 6, 1, 11, 19, 14, 8, 19, 7, 16, 2, 16, 1, 9, 7, 8, 5, 13, 1, 11, 16, 6, 12, 15, 6, 10, 4, 10, 0, 2, 0, 5, 16, 11, 2, 8, 8, 10, 8, 15, 15, 11, 8, 12, 10, 6, 8, 12, 6, 10, 4, 12, 15, 14, 13, 11, 8, 0, 5, 10, 0, 2, 3, 1, 12, 12, 1, 16, 6, 18, 18, 0, 13, 1, 1, 9, 0, 1, 17, 17, 17, 0, 17, 12, 1, 9, 10, 0, 17, 4, 17, 5, 17, 3, 15, 17, 3, 0, 11, 4, 11, 3, 0, 3, 17, 17, 10, 8, 17, 12, 17, 0, 12, 3, 17, 17, 12, 0, 12, 7, 17, 14, 4, 17, 2, 16, 17, 3, 17, 4, 3, 4, 17, 12, 2, 12, 11, 1, 1, 0, 1, 2, 1, 0, 1, 0, 19, 19, 8, 9, 11, 8, 13, 5, 14, 1, 7, 7, 7, 7, 14, 1, 1, 14, 19, 19, 19, 19, 0, 8, 19, 10, 18, 13, 6, 14, 7, 10, 8, 13, 10, 16, 2, 10, 13, 15, 8, 11, 8, 10, 9, 15, 10, 18, 10, 13, 2, 3, 11, 10, 10, 2, 10, 8, 15, 15, 2, 11, 15, 13, 7, 1, 1, 7, 5, 7, 7, 13, 0, 1, 8, 1, 7, 1, 9, 8, 19, 2, 7, 17, 17, 13, 14, 13, 11, 2, 5, 3, 11, 4, 2, 9, 1, 17, 5, 8, 13, 17, 17, 10, 13, 0, 4, 16, 2, 4, 4, 6, 13, 4, 8, 5, 17, 2, 11, 10, 17, 12, 12, 17, 8, 14, 6, 8, 11, 15, 9, 0, 10, 13, 4, 4, 17, 17, 12, 13, 2, 18, 4, 8, 13, 11, 15, 15, 18, 6, 5, 9, 11, 8, 11, 5, 3, 14, 6, 6, 1, 1, 18, 1, 1, 19, 7, 1, 0, 1, 18, 3, 4, 4, 4, 4, 4, 2, 5, 4, 1, 4, 4, 16, 4, 13, 4, 16, 1, 5, 2, 18, 4, 11, 13, 4, 4, 4, 18, 3, 2, 11, 10, 11, 8, 14, 8, 15, 7, 17, 5, 13, 16, 5, 16, 9, 5, 0, 16, 12, 1, 6, 3, 9, 1, 15, 10, 7, 15, 15, 10, 11, 15, 15, 9, 8, 3, 15, 3, 15, 15, 15, 15, 13, 3, 15, 2, 10, 11, 12, 12, 12, 9, 2, 9, 11, 8, 3, 2, 15, 15, 2, 15, 12, 8, 8, 12, 2, 2, 8, 14, 2, 2, 7, 2, 10, 7, 13, 3, 14, 13, 10, 11, 4, 9, 7, 8, 0, 6, 6, 10, 8, 14, 6, 16, 11, 2, 2, 10, 13, 9, 5, 12, 14, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 5, 4, 5, 16, 15, 4, 16, 5, 14, 11, 15, 4, 15, 14, 14, 9, 10, 5, 1, 13, 7, 9, 6, 5, 0, 1, 8, 8, 12, 16, 0, 16, 9, 16, 5, 10, 9, 13, 10, 0, 5, 12, 10, 17, 9, 1, 0, 14, 0, 2, 12, 2, 12, 7, 13, 0, 10, 2, 0, 12, 7, 7, 13, 10, 10, 6, 15, 12, 17, 16, 4, 10, 0, 13, 12, 3, 14, 6, 11, 9, 13, 11, 12, 14, 5, 12, 15, 15, 11, 15, 2, 8, 11, 15, 8, 8, 11, 5, 11, 4, 3, 6, 15, 3, 3, 8, 15, 13, 2, 15, 2, 2, 5, 3, 11, 10, 2, 10, 5, 5, 0, 3, 6, 1, 9, 10, 3, 19, 2, 14, 3, 6, 10, 12, 11, 18, 0, 5, 0, 14, 14, 13, 19, 16, 15, 2, 10, 7, 5, 1, 7, 5, 9, 14, 14, 13, 3, 5, 14, 16, 1, 7, 12, 1, 1, 0, 19, 9, 14, 1, 0, 1, 3, 10, 12, 5, 11, 10, 6, 2, 0, 0, 10, 10, 2, 7, 3, 12, 6, 8, 6, 13, 3, 10, 3, 18, 14, 9, 18, 5, 0, 5, 19, 3, 0, 8, 11, 2, 9, 12, 6, 4, 4, 13, 16, 13, 4, 11, 6, 16, 5, 14, 12, 8, 1, 1, 12, 3, 0, 12, 12, 12, 5, 12, 1, 0, 2, 6, 7, 0, 7, 3, 3, 6, 7, 10, 3, 6, 14, 0, 0, 5, 15, 16, 16, 0, 17, 2, 9, 15, 17, 14, 17, 8, 15, 0, 13, 15, 3, 9, 13, 4, 4, 9, 7, 14, 8, 7, 3, 16, 10, 3, 13, 9, 4, 19, 12, 10, 5, 9, 1, 7, 12, 1, 0, 12, 1, 4, 10, 10, 5, 12, 18, 10, 1, 13, 2, 18, 17, 0, 4, 19, 19, 0, 17, 18, 17, 3, 19, 15, 11, 13, 12, 1, 6, 1, 7, 11, 16, 16, 1, 1, 7, 12, 9, 3, 8, 1, 3, 1, 7, 16, 11, 17, 13, 5, 0, 9, 11, 11, 17, 0, 3, 9, 11, 10, 3, 11, 8, 3, 9, 10, 6, 8, 11, 17, 17, 4, 4, 17, 13, 14, 0, 2, 14, 4, 2, 7, 4, 7, 9, 14, 4, 7, 16, 13, 17, 6, 8, 17, 6, 6, 10, 4, 8, 4, 4, 16, 1, 14, 4, 4, 2, 7, 1, 1, 11, 9, 5, 1, 11, 6, 6, 7, 18, 1, 6, 18, 9, 17, 17, 8, 10, 19, 15, 4, 5, 12, 13, 13, 18, 12, 5, 19, 13, 13, 17, 14, 16, 18, 18, 1, 4, 0, 2, 2, 4, 16, 18, 2, 13, 14, 8, 1, 8, 14, 17, 2, 11, 17, 13, 3, 12, 7, 11, 14, 8, 17, 16, 15, 2, 8, 17, 4, 4, 15, 3, 5, 16, 18, 9, 3, 7, 14, 2, 3, 7, 7, 6, 6, 6, 8, 7, 9, 13, 4, 4, 7, 13, 4, 4, 15, 1, 0, 4, 16, 16, 4, 19, 18, 0, 10, 8, 1, 6, 9, 1, 11, 5, 3, 6, 5, 14, 3, 3, 2, 7, 9, 6, 10, 3, 17, 11, 4, 4, 5, 6, 17, 0, 16, 15, 9, 5, 16, 13, 3, 15, 10, 19, 1, 3, 7, 7, 5, 7, 16, 8, 1, 3, 14, 16, 5, 8, 17, 14, 3, 10, 11, 16, 17, 4, 11, 6, 1, 4, 3, 4, 15, 12, 17, 8, 9, 4, 2, 17, 16, 17, 17, 17, 11, 4, 8, 10, 16, 18, 2, 4, 4, 17, 15, 11, 11, 8, 9, 9, 4, 4, 8, 16, 11, 7, 16, 4, 17, 5, 16, 3]\n",
            "-------RUN138-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': \"CountVectorizer(ngram_range=(1, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b891dd10>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[6, 2, 9, 9, 18, 1, 0, 14, 2, 2, 17, 6, 7, 11, 2, 0, 1, 9, 13, 13, 11, 11, 0, 3, 3, 19, 19, 0, 0, 14, 2, 0, 13, 10, 10, 15, 3, 6, 18, 18, 3, 0, 18, 3, 18, 8, 18, 18, 0, 0, 17, 3, 0, 3, 3, 8, 8, 2, 13, 18, 7, 2, 0, 19, 0, 0, 5, 11, 5, 5, 10, 11, 16, 16, 1, 5, 1, 13, 2, 2, 15, 2, 2, 2, 2, 19, 8, 5, 2, 10, 16, 19, 15, 6, 0, 6, 10, 16, 10, 11, 18, 1, 1, 1, 3, 10, 0, 19, 14, 1, 5, 15, 8, 7, 2, 19, 18, 11, 1, 0, 10, 11, 3, 1, 18, 18, 15, 2, 3, 9, 2, 2, 3, 11, 15, 18, 0, 2, 15, 18, 10, 0, 19, 6, 7, 14, 9, 1, 2, 12, 17, 18, 1, 10, 17, 1, 7, 10, 0, 14, 13, 4, 10, 0, 0, 0, 10, 1, 19, 1, 10, 0, 17, 13, 2, 14, 1, 0, 0, 18, 1, 1, 11, 13, 5, 14, 0, 2, 5, 7, 2, 15, 15, 2, 19, 15, 14, 17, 15, 6, 8, 2, 0, 7, 11, 11, 15, 19, 10, 3, 7, 19, 2, 2, 7, 15, 15, 16, 2, 19, 17, 7, 7, 16, 11, 5, 8, 8, 2, 4, 13, 13, 13, 18, 7, 4, 9, 13, 17, 5, 17, 18, 5, 0, 14, 14, 3, 1, 0, 0, 1, 5, 3, 6, 14, 8, 8, 19, 16, 7, 15, 1, 1, 14, 1, 2, 10, 10, 0, 5, 5, 5, 14, 16, 5, 2, 5, 15, 19, 10, 7, 1, 5, 8, 8, 3, 18, 3, 3, 13, 6, 1, 3, 6, 11, 5, 19, 2, 7, 5, 7, 3, 11, 7, 14, 3, 5, 7, 6, 17, 3, 5, 18, 0, 0, 6, 1, 17, 1, 2, 14, 15, 0, 13, 12, 5, 0, 6, 2, 17, 17, 3, 18, 0, 5, 3, 7, 17, 5, 0, 17, 10, 8, 11, 14, 0, 2, 2, 13, 11, 0, 0, 0, 2, 19, 17, 6, 18, 15, 9, 17, 8, 1, 15, 18, 17, 6, 2, 11, 14, 17, 17, 15, 10, 18, 15, 13, 15, 0, 4, 9, 4, 0, 11, 7, 7, 8, 0, 15, 19, 17, 11, 0, 9, 9, 9, 9, 9, 9, 13, 3, 6, 5, 2, 17, 0, 9, 11, 12, 4, 19, 11, 0, 1, 17, 10, 6, 17, 14, 1, 3, 11, 7, 1, 17, 7, 17, 1, 7, 16, 7, 2, 16, 5, 16, 15, 16, 16, 16, 16, 16, 14, 0, 14, 6, 5, 11, 5, 7, 11, 19, 5, 5, 14, 14, 2, 5, 1, 5, 5, 14, 5, 13, 2, 5, 5, 5, 5, 3, 16, 16, 8, 0, 16, 7, 16, 5, 8, 6, 16, 7, 3, 16, 11, 13, 11, 13, 8, 2, 15, 9, 1, 1, 8, 4, 13, 14, 10, 9, 5, 3, 4, 3, 18, 0, 18, 14, 18, 6, 0, 6, 0, 3, 0, 9, 9, 6, 0, 11, 3, 5, 3, 10, 5, 3, 4, 10, 9, 7, 1, 6, 0, 18, 17, 14, 11, 0, 3, 8, 10, 10, 8, 13, 5, 5, 16, 18, 1, 8, 8, 2, 19, 8, 12, 12, 12, 18, 12, 19, 8, 2, 3, 14, 12, 4, 12, 13, 12, 14, 9, 12, 10, 18, 6, 4, 6, 10, 11, 14, 12, 12, 3, 0, 12, 11, 18, 11, 10, 1, 12, 12, 10, 11, 10, 11, 12, 7, 4, 12, 11, 13, 12, 10, 12, 4, 10, 4, 12, 10, 11, 10, 6, 8, 11, 18, 8, 11, 8, 18, 8, 18, 16, 14, 0, 2, 17, 0, 1, 17, 7, 8, 7, 11, 15, 11, 7, 8, 8, 7, 16, 16, 16, 16, 11, 0, 16, 3, 16, 1, 5, 7, 15, 3, 6, 1, 3, 13, 1, 3, 2, 9, 0, 6, 3, 3, 2, 9, 3, 5, 3, 1, 0, 1, 6, 3, 3, 2, 3, 0, 9, 9, 0, 6, 9, 1, 15, 8, 8, 15, 17, 15, 15, 1, 14, 8, 0, 8, 15, 8, 2, 9, 16, 0, 15, 12, 12, 1, 7, 1, 6, 0, 14, 0, 6, 4, 2, 2, 11, 12, 17, 0, 1, 12, 12, 3, 1, 11, 4, 13, 2, 4, 4, 14, 1, 4, 0, 17, 12, 0, 6, 3, 12, 10, 10, 12, 0, 6, 5, 0, 6, 9, 2, 11, 3, 2, 4, 4, 12, 12, 10, 1, 0, 7, 4, 0, 1, 6, 9, 9, 7, 5, 17, 2, 6, 0, 17, 14, 0, 7, 5, 5, 8, 8, 16, 8, 8, 16, 18, 8, 11, 8, 5, 1, 4, 4, 4, 4, 4, 0, 17, 4, 8, 4, 4, 4, 4, 1, 4, 13, 8, 13, 0, 16, 4, 6, 1, 4, 4, 4, 16, 1, 6, 6, 3, 6, 3, 7, 3, 9, 15, 12, 13, 1, 13, 13, 13, 2, 17, 19, 13, 19, 19, 5, 3, 2, 8, 9, 3, 15, 9, 9, 3, 6, 9, 9, 2, 0, 1, 9, 1, 9, 9, 9, 9, 1, 0, 9, 0, 14, 6, 10, 10, 10, 2, 0, 2, 6, 0, 1, 2, 9, 9, 2, 9, 10, 0, 0, 19, 0, 0, 0, 7, 2, 0, 15, 1, 3, 15, 2, 0, 7, 1, 3, 6, 4, 2, 15, 3, 18, 5, 5, 3, 0, 0, 14, 13, 6, 2, 11, 3, 1, 2, 17, 10, 7, 4, 4, 4, 4, 4, 4, 4, 14, 4, 4, 17, 4, 17, 13, 9, 4, 13, 17, 7, 6, 9, 4, 9, 7, 3, 2, 10, 14, 8, 1, 15, 2, 5, 17, 19, 8, 3, 0, 10, 13, 18, 13, 2, 18, 17, 3, 2, 1, 5, 19, 14, 10, 3, 12, 2, 19, 11, 7, 14, 0, 10, 0, 10, 15, 2, 11, 3, 0, 11, 19, 11, 11, 1, 3, 3, 14, 9, 10, 12, 18, 4, 3, 18, 1, 19, 1, 7, 5, 9, 2, 1, 6, 10, 7, 14, 10, 9, 9, 6, 9, 1, 0, 6, 9, 6, 0, 6, 17, 6, 4, 1, 5, 9, 0, 0, 0, 9, 0, 0, 9, 0, 2, 14, 1, 6, 3, 2, 3, 14, 13, 10, 1, 5, 11, 2, 3, 1, 16, 0, 7, 1, 5, 3, 10, 6, 16, 14, 17, 18, 7, 7, 1, 16, 18, 9, 0, 11, 15, 17, 8, 19, 17, 2, 7, 7, 1, 0, 17, 7, 13, 8, 15, 19, 8, 8, 18, 16, 2, 0, 8, 18, 8, 1, 3, 10, 17, 6, 3, 5, 1, 14, 11, 3, 3, 0, 15, 3, 10, 5, 0, 5, 1, 3, 3, 1, 16, 11, 2, 16, 14, 18, 14, 16, 14, 18, 0, 6, 1, 2, 10, 5, 4, 4, 1, 4, 1, 4, 6, 14, 13, 14, 7, 10, 0, 8, 11, 11, 11, 18, 10, 10, 10, 17, 10, 8, 11, 0, 5, 11, 14, 11, 0, 0, 5, 15, 14, 10, 5, 7, 18, 11, 7, 9, 13, 13, 18, 12, 2, 2, 9, 12, 7, 12, 0, 9, 14, 1, 9, 0, 2, 1, 4, 4, 2, 15, 7, 0, 15, 3, 13, 3, 1, 1, 2, 4, 16, 11, 14, 17, 2, 8, 15, 10, 8, 19, 10, 8, 4, 3, 3, 14, 19, 16, 3, 8, 1, 2, 5, 12, 18, 4, 16, 14, 14, 12, 7, 12, 0, 16, 9, 6, 17, 19, 8, 5, 8, 15, 6, 13, 13, 8, 19, 7, 19, 2, 1, 0, 8, 10, 8, 15, 13, 6, 12, 1, 13, 11, 2, 6, 6, 12, 18, 0, 2, 6, 3, 1, 6, 0, 1, 2, 3, 5, 9, 6, 12, 12, 4, 4, 12, 1, 7, 18, 2, 7, 4, 0, 15, 12, 15, 2, 7, 4, 15, 13, 1, 12, 5, 0, 12, 5, 5, 3, 4, 0, 4, 4, 13, 8, 7, 4, 4, 0, 15, 8, 11, 6, 2, 17, 8, 4, 5, 5, 15, 16, 8, 14, 16, 2, 12, 12, 0, 14, 16, 9, 4, 17, 10, 1, 1, 16, 10, 7, 16, 1, 2, 12, 7, 13, 16, 16, 19, 4, 11, 0, 0, 4, 13, 16, 0, 0, 7, 0, 8, 0, 7, 12, 10, 6, 12, 1, 0, 10, 11, 4, 2, 3, 12, 13, 9, 0, 3, 12, 4, 4, 9, 0, 14, 13, 16, 2, 14, 15, 7, 0, 1, 11, 15, 5, 5, 5, 3, 15, 2, 17, 4, 4, 15, 1, 4, 4, 9, 8, 10, 4, 13, 13, 4, 16, 16, 18, 3, 6, 8, 5, 2, 8, 4, 17, 3, 5, 12, 0, 1, 0, 0, 7, 7, 5, 3, 1, 12, 6, 4, 12, 7, 18, 12, 14, 13, 9, 2, 17, 13, 1, 1, 9, 3, 16, 8, 13, 15, 15, 14, 11, 13, 3, 8, 0, 7, 13, 17, 6, 12, 7, 1, 3, 6, 13, 12, 4, 6, 5, 19, 4, 1, 4, 9, 10, 12, 0, 2, 4, 2, 12, 13, 12, 12, 12, 6, 4, 0, 14, 13, 16, 2, 4, 4, 12, 9, 6, 6, 9, 2, 2, 4, 4, 0, 13, 6, 11, 13, 4, 12, 17, 13, 10]\n",
            "-------RUN139-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=5)', 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': \"CountVectorizer(ngram_range=(2, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df4a9008410>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[3, 6, 10, 10, 5, 0, 13, 0, 6, 0, 18, 3, 16, 13, 3, 3, 0, 10, 1, 1, 13, 13, 3, 2, 2, 11, 11, 1, 0, 18, 0, 1, 1, 12, 12, 7, 2, 3, 5, 5, 2, 13, 5, 3, 5, 9, 11, 5, 1, 3, 7, 2, 1, 2, 10, 11, 9, 6, 18, 5, 0, 6, 0, 13, 0, 1, 4, 13, 4, 4, 12, 9, 17, 17, 0, 4, 0, 1, 6, 6, 7, 6, 19, 19, 19, 11, 9, 4, 6, 12, 17, 11, 19, 3, 1, 3, 12, 17, 12, 13, 5, 12, 1, 0, 1, 12, 16, 5, 5, 13, 4, 9, 11, 16, 6, 19, 5, 13, 0, 3, 12, 13, 2, 18, 5, 5, 9, 6, 2, 9, 6, 19, 1, 9, 7, 5, 1, 6, 9, 5, 12, 13, 11, 14, 16, 7, 10, 0, 6, 8, 7, 5, 0, 12, 18, 0, 16, 12, 1, 2, 2, 14, 12, 3, 1, 0, 12, 0, 12, 0, 12, 13, 6, 8, 6, 7, 18, 0, 3, 5, 0, 0, 13, 8, 14, 5, 3, 6, 4, 3, 16, 13, 9, 19, 12, 9, 4, 7, 9, 3, 9, 6, 0, 16, 13, 13, 19, 11, 12, 2, 16, 11, 6, 6, 16, 9, 19, 17, 19, 12, 7, 16, 16, 17, 13, 4, 11, 9, 19, 15, 8, 8, 8, 5, 16, 14, 10, 14, 3, 4, 7, 5, 4, 0, 1, 7, 1, 0, 1, 1, 0, 4, 2, 3, 5, 11, 11, 11, 17, 16, 7, 0, 0, 5, 0, 6, 12, 12, 1, 4, 4, 4, 7, 17, 4, 19, 4, 9, 11, 12, 16, 0, 4, 11, 9, 2, 5, 1, 1, 1, 3, 0, 2, 7, 13, 7, 5, 19, 16, 4, 16, 2, 13, 16, 16, 2, 4, 16, 3, 16, 2, 4, 11, 1, 1, 3, 1, 18, 0, 19, 7, 7, 1, 2, 8, 4, 1, 3, 6, 7, 18, 2, 5, 3, 4, 2, 16, 7, 4, 0, 7, 12, 11, 13, 5, 0, 19, 6, 8, 7, 1, 0, 1, 6, 11, 7, 14, 5, 7, 14, 18, 9, 0, 9, 11, 7, 3, 6, 11, 2, 18, 7, 7, 12, 5, 7, 8, 19, 1, 15, 14, 14, 0, 13, 16, 16, 11, 3, 9, 11, 7, 13, 0, 10, 10, 10, 10, 10, 10, 5, 2, 3, 7, 6, 7, 0, 10, 4, 8, 14, 11, 13, 0, 18, 18, 13, 14, 18, 18, 0, 1, 13, 16, 0, 18, 16, 18, 0, 16, 17, 16, 6, 17, 4, 17, 19, 17, 17, 17, 17, 17, 16, 0, 4, 10, 4, 13, 4, 16, 11, 11, 4, 4, 5, 18, 6, 4, 0, 4, 4, 18, 4, 8, 6, 4, 4, 4, 4, 2, 17, 17, 9, 0, 17, 7, 17, 4, 9, 3, 17, 16, 1, 17, 13, 14, 13, 14, 9, 6, 9, 10, 18, 18, 11, 14, 10, 4, 12, 10, 4, 2, 15, 2, 5, 3, 5, 7, 5, 10, 0, 3, 3, 2, 3, 10, 10, 4, 1, 13, 2, 4, 2, 12, 4, 2, 14, 12, 10, 16, 18, 10, 1, 5, 7, 2, 5, 0, 2, 9, 12, 12, 9, 5, 7, 4, 17, 11, 18, 11, 9, 6, 11, 11, 8, 8, 8, 5, 8, 11, 11, 6, 2, 5, 8, 15, 8, 5, 8, 1, 10, 8, 12, 5, 3, 14, 3, 12, 11, 18, 8, 8, 2, 1, 8, 13, 8, 13, 12, 0, 8, 8, 12, 13, 12, 13, 8, 16, 15, 8, 13, 1, 15, 12, 8, 15, 12, 14, 15, 11, 13, 12, 3, 11, 11, 11, 11, 13, 9, 5, 11, 11, 17, 17, 1, 6, 3, 3, 0, 18, 7, 9, 16, 13, 9, 13, 16, 9, 9, 16, 17, 17, 17, 17, 11, 1, 17, 2, 17, 0, 4, 16, 9, 2, 3, 0, 2, 8, 0, 2, 6, 10, 1, 3, 1, 2, 19, 10, 2, 4, 2, 0, 13, 0, 3, 5, 2, 6, 2, 1, 10, 14, 0, 3, 10, 0, 9, 9, 9, 9, 18, 9, 19, 0, 5, 9, 1, 9, 7, 11, 6, 10, 17, 13, 9, 8, 8, 18, 16, 6, 3, 3, 5, 1, 3, 14, 0, 6, 11, 8, 18, 1, 0, 8, 8, 2, 0, 13, 15, 8, 6, 15, 15, 4, 0, 7, 1, 18, 8, 0, 3, 2, 8, 12, 12, 8, 1, 3, 4, 1, 14, 10, 6, 5, 2, 6, 15, 15, 8, 8, 12, 0, 0, 16, 14, 1, 0, 3, 10, 10, 16, 4, 7, 6, 3, 1, 3, 7, 1, 16, 4, 4, 11, 9, 17, 11, 9, 17, 7, 9, 13, 9, 4, 1, 15, 15, 15, 15, 15, 0, 7, 15, 9, 15, 15, 14, 15, 0, 15, 14, 11, 18, 3, 17, 15, 3, 0, 15, 15, 15, 17, 1, 3, 3, 2, 3, 1, 1, 1, 10, 9, 8, 8, 18, 5, 18, 8, 6, 18, 11, 18, 11, 11, 4, 2, 6, 11, 10, 2, 9, 10, 10, 2, 14, 10, 10, 6, 3, 0, 10, 0, 14, 10, 14, 10, 18, 1, 10, 0, 2, 3, 12, 12, 12, 6, 0, 6, 14, 1, 0, 6, 10, 10, 6, 10, 12, 1, 1, 13, 3, 0, 1, 16, 0, 0, 19, 0, 2, 19, 6, 1, 16, 0, 2, 3, 14, 6, 9, 1, 5, 7, 4, 10, 3, 3, 4, 5, 3, 0, 13, 2, 18, 19, 7, 12, 0, 15, 15, 15, 15, 15, 15, 15, 7, 15, 15, 7, 15, 18, 18, 14, 15, 14, 18, 16, 3, 10, 15, 10, 16, 1, 6, 2, 7, 9, 0, 19, 6, 7, 7, 5, 11, 1, 3, 13, 18, 5, 18, 6, 5, 7, 2, 6, 0, 4, 5, 18, 12, 2, 8, 6, 11, 13, 13, 5, 0, 12, 0, 12, 9, 6, 13, 2, 0, 13, 11, 13, 13, 0, 2, 2, 4, 10, 12, 8, 5, 14, 2, 5, 0, 12, 0, 16, 7, 14, 6, 0, 3, 12, 16, 7, 12, 10, 10, 3, 10, 0, 3, 10, 10, 10, 1, 10, 7, 3, 14, 0, 4, 10, 1, 1, 10, 10, 0, 3, 10, 0, 6, 5, 0, 14, 2, 19, 2, 18, 7, 12, 1, 4, 11, 6, 2, 0, 17, 0, 0, 0, 4, 2, 12, 3, 17, 5, 7, 5, 0, 16, 0, 17, 5, 10, 0, 2, 7, 7, 9, 19, 7, 6, 3, 16, 0, 1, 18, 16, 14, 11, 9, 11, 11, 9, 5, 17, 19, 3, 11, 5, 9, 0, 2, 12, 7, 3, 2, 4, 0, 5, 7, 2, 2, 3, 7, 2, 12, 14, 1, 4, 0, 1, 2, 0, 17, 13, 6, 17, 5, 5, 18, 17, 2, 5, 1, 14, 0, 19, 12, 4, 14, 15, 0, 14, 0, 15, 14, 4, 8, 8, 16, 12, 3, 11, 9, 13, 13, 5, 12, 12, 12, 18, 12, 9, 13, 1, 4, 13, 5, 13, 0, 0, 4, 7, 2, 12, 7, 0, 5, 13, 7, 10, 14, 18, 5, 8, 6, 6, 10, 8, 7, 8, 1, 10, 2, 0, 14, 1, 6, 0, 14, 14, 6, 7, 16, 1, 7, 1, 5, 2, 12, 18, 6, 15, 17, 13, 2, 7, 6, 9, 9, 12, 9, 11, 12, 11, 15, 2, 2, 18, 13, 17, 2, 9, 0, 6, 4, 8, 5, 15, 17, 17, 5, 8, 16, 8, 0, 17, 10, 3, 18, 13, 11, 4, 9, 7, 14, 14, 5, 9, 11, 7, 13, 6, 1, 1, 11, 12, 9, 9, 5, 1, 8, 0, 14, 13, 6, 14, 14, 8, 5, 13, 6, 14, 2, 0, 3, 1, 0, 6, 1, 4, 10, 14, 8, 8, 14, 15, 8, 0, 0, 5, 19, 16, 14, 0, 9, 15, 9, 6, 16, 14, 9, 8, 18, 8, 4, 1, 8, 4, 4, 2, 15, 1, 15, 15, 14, 9, 16, 15, 15, 1, 16, 11, 11, 3, 6, 7, 11, 14, 4, 7, 19, 17, 9, 4, 17, 19, 8, 8, 1, 2, 17, 10, 14, 18, 12, 0, 18, 17, 12, 16, 17, 0, 6, 8, 19, 2, 17, 17, 11, 15, 4, 13, 0, 14, 1, 17, 3, 0, 16, 1, 11, 1, 16, 8, 13, 3, 8, 0, 1, 12, 13, 14, 0, 1, 15, 8, 10, 0, 1, 8, 15, 15, 10, 1, 18, 1, 17, 6, 5, 19, 16, 0, 0, 9, 9, 4, 4, 4, 3, 9, 6, 18, 14, 15, 19, 0, 14, 15, 10, 9, 13, 15, 5, 2, 15, 17, 17, 5, 2, 3, 11, 4, 6, 9, 14, 7, 1, 4, 8, 3, 0, 0, 3, 7, 16, 4, 2, 0, 8, 3, 14, 15, 7, 4, 8, 5, 1, 10, 6, 7, 10, 18, 0, 14, 2, 17, 11, 1, 7, 19, 16, 13, 1, 13, 9, 1, 16, 14, 7, 10, 7, 1, 0, 2, 14, 18, 15, 15, 14, 4, 11, 15, 0, 15, 10, 12, 8, 1, 6, 14, 6, 8, 1, 8, 8, 8, 3, 15, 1, 2, 5, 17, 6, 15, 15, 8, 10, 3, 14, 10, 19, 6, 15, 15, 1, 5, 14, 13, 5, 15, 8, 18, 5, 12]\n",
            "-------RUN140-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': \"CountVectorizer(stop_words='english')\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, -1, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 1, 0, -1, -1, -1, -1, 1, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 1, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 2, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 2, 2, 2, 2, 2, 2, -1, -1, -1, 0, 0, -1, 0, 2, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 1, -1, 0, -1, 0, -1, -1, -1, -1, 1, 1, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, -1, 1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, -1, -1, 1, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, -1, -1, 1, 0, -1, 0, 1, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 2, 0, 0, 0, -1, 0, 0, -1, 0, 2, -1, 2, 2, 0, 2, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 2, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 2, -1, 0, 2, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, 1, 0, 0, 0, 0, -1, -1, 0, 1, -1, -1, 0, 0, 0, 0, 1, -1, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 1, 0, 0, -1, 0, -1, -1, 1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 1, 0, 0, -1, -1, -1, 0, 1, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, -1, 0, -1, -1, 1, 0, 0, -1, -1, 0, -1, -1, 0, -1, 1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 2, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 2, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, 2, 0, 0, -1, 0, 0, 2, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, 1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1]\n",
            "-------RUN141-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': 'CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7df3b00f5c90>)', 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, -1, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 1, 0, -1, -1, -1, -1, 1, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 1, 0, 0, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 2, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, -1, -1, -1, 0, 0, -1, 0, 2, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 1, -1, 0, -1, 0, -1, -1, -1, -1, 1, 1, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, 1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, -1, -1, 1, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, -1, -1, 1, 0, -1, 0, 1, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 1, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 2, 0, -1, 0, -1, 0, 0, -1, 0, 2, -1, 2, 2, 0, 2, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 2, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 2, 0, -1, 2, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 1, -1, -1, 0, 0, 0, 0, 1, -1, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 1, 0, 0, -1, 0, -1, -1, 1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 2, 0, 0, -1, 0, 2, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 1, 0, 0, -1, -1, -1, 0, 1, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, -1, -1, -1, -1, 1, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 2, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 2, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 2, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, 1, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1]\n",
            "-------RUN142-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': \"CountVectorizer(stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b02d2c50>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, -1, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 1, 0, -1, -1, -1, -1, 1, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 1, 0, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 2, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, -1, -1, -1, 0, 0, -1, 0, 2, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 1, -1, 0, -1, 0, -1, -1, -1, -1, 1, 1, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, 1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, 1, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, -1, -1, 1, 0, -1, 0, 1, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 2, 0, -1, 0, -1, 0, 0, -1, 0, 2, -1, 2, 2, 0, 2, -1, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, 2, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, 0, 2, -1, -1, 2, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 1, -1, -1, 0, 0, 0, 0, 1, -1, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 1, 0, 0, -1, 0, -1, -1, 1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 2, -1, 0, -1, 0, 2, 0, 0, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 1, 0, 0, -1, -1, -1, 0, 1, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, -1, -1, -1, -1, 1, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 2, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 2, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, 1, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1]\n",
            "-------RUN143-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': \"CountVectorizer(ngram_range=(1, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b891dd10>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, -1, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 1, 0, -1, -1, -1, -1, 1, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 1, 0, 0, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 2, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, -1, -1, -1, 0, 0, -1, 0, 2, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 1, -1, 0, -1, 0, -1, -1, -1, -1, 1, 1, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, 1, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, -1, -1, 1, 0, -1, 0, 1, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 1, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 2, 0, -1, 0, -1, 0, 0, -1, 0, 2, -1, 2, 2, 0, 2, -1, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, 2, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, 0, 2, -1, -1, 2, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 1, -1, -1, -1, 0, 0, 0, 1, -1, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 1, 0, 0, -1, 0, -1, -1, 1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 2, -1, 0, -1, 0, 2, 0, 0, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 1, 0, 0, -1, -1, -1, 0, 1, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, -1, -1, -1, -1, 1, 0, 0, -1, -1, -1, -1, -1, 0, -1, 1, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 2, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 2, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 2, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, 1, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1]\n",
            "-------RUN144-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': 'HDBSCAN(min_cluster_size=10, prediction_data=True)', 'Vectorization': \"CountVectorizer(ngram_range=(2, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df4a9008410>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, -1, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 1, 0, -1, -1, -1, -1, 1, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 1, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 2, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, -1, -1, -1, 0, 0, -1, 0, 2, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 1, -1, 0, -1, 0, -1, -1, -1, -1, 1, 1, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, 1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, -1, -1, 1, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, -1, -1, 1, 0, -1, 0, 1, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 1, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 2, 0, -1, 0, -1, 0, 0, -1, 0, 2, -1, 2, 2, 0, 2, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 2, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 2, -1, 0, 2, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 1, -1, -1, 0, 0, 0, 0, 1, -1, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, -1, -1, 1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 1, 0, 0, -1, -1, -1, 0, 1, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, -1, -1, -1, -1, 1, 0, 0, -1, -1, 0, -1, -1, 0, -1, 1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 2, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 2, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 2, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, 1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1]\n",
            "-------RUN145-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': \"CountVectorizer(stop_words='english')\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, -1, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 1, 0, -1, -1, -1, -1, 1, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, 1, 0, 0, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, 1, -1, 0, -1, 0, -1, -1, -1, -1, 1, 1, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, 1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, 1, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 1, 0, -1, 0, 1, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, 1, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 1, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, 1, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 1, -1, -1, -1, 0, 0, 0, 1, -1, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, 1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, 1, 0, 0, -1, -1, -1, 0, 1, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 1, 0, 0, -1, -1, -1, -1, 1, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1]\n",
            "-------RUN146-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': 'CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7df3b00f5c90>)', 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, -1, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, 0, -1, 1, 0, -1, 0, -1, -1, 1, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, 1, 0, 0, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 1, -1, 0, -1, 0, -1, -1, -1, -1, 1, 1, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, 1, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, -1, 1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, 1, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 1, 0, -1, 0, 1, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, 1, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 1, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 1, -1, -1, 0, 0, 0, 0, 1, -1, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 1, 0, -1, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, 0, 1, 0, 0, -1, -1, -1, 0, 1, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 1, 0, 0, -1, 0, -1, -1, 1, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1]\n",
            "-------RUN147-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': \"CountVectorizer(stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b02d2c50>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, -1, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 1, 0, -1, -1, -1, -1, 1, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, 1, 0, 0, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, 1, -1, 0, -1, 0, -1, -1, -1, -1, 1, 1, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, 1, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 1, 0, -1, 0, 1, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, 1, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, 1, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 1, -1, -1, 0, 0, 0, 0, 1, -1, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, 1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 1, 0, 0, -1, -1, -1, 0, 1, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 1, 0, 0, -1, 0, -1, -1, 1, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1]\n",
            "-------RUN148-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': \"CountVectorizer(ngram_range=(1, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b891dd10>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[-1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, 2, 0, -1, -1, -1, 2, 2, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, 2, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, -1, 0, 2, -1, -1, -1, 0, 0, 2, -1, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 3, 0, -1, -1, -1, -1, 3, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, 3, -1, -1, 5, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, 0, 0, 5, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 4, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, -1, 0, 4, -1, 4, 4, -1, 4, -1, -1, -1, 0, 0, -1, 0, 4, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, 3, -1, 0, -1, 5, -1, -1, -1, -1, 3, 3, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, 5, -1, -1, -1, -1, -1, -1, 5, 5, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, 3, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 3, -1, -1, 0, 3, -1, -1, 0, 5, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, -1, 0, -1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 0, 2, -1, -1, 1, -1, -1, -1, -1, -1, 3, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, 1, 1, 1, 1, -1, 0, -1, -1, -1, -1, 1, -1, 1, -1, 1, -1, -1, -1, 0, -1, 1, -1, 0, 1, -1, -1, 3, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 2, 0, 4, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, 4, 4, -1, 4, -1, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, -1, 1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, 2, -1, 0, -1, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, 2, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, -1, -1, 2, 0, -1, -1, -1, -1, 0, -1, 2, -1, 4, -1, -1, -1, 1, -1, 0, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, 4, -1, -1, 4, 0, 0, -1, 4, 0, -1, -1, -1, -1, 1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 3, -1, -1, -1, -1, 0, 0, 3, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, 2, 2, 0, -1, 2, -1, -1, -1, 0, -1, -1, 2, -1, -1, -1, 0, -1, -1, -1, -1, 3, 0, -1, 0, 0, -1, -1, 0, -1, -1, 1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, 0, 0, -1, 4, -1, 0, -1, 0, 4, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, 0, -1, 0, 0, 1, -1, -1, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, 2, 2, -1, -1, -1, 2, -1, 0, 0, -1, -1, -1, 1, 3, -1, 0, -1, -1, -1, 0, 3, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 2, -1, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, 1, -1, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, 1, -1, 1, -1, -1, 0, -1, -1, 1, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 3, 0, 0, -1, -1, -1, -1, 3, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, 1, -1, -1, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, 4, -1, -1, 0, -1, 0, -1, -1, -1, 0, 0, -1, -1, 5, -1, 5, -1, -1, -1, -1, 0, 1, -1, -1, -1, 1, -1, -1, -1, 1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, -1, 5, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, -1, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, 0, -1, -1, 1, -1, 1, 0, 0, -1, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 1, -1, 0, -1, 0, 0, -1, -1, 0, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
            "-------RUN149-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': \"HDBSCAN(metric='manhattan', min_cluster_size=10, prediction_data=True)\", 'Vectorization': \"CountVectorizer(ngram_range=(2, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df4a9008410>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, -1, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, -1, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 1, 0, -1, -1, -1, -1, 1, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, 1, 0, 0, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, 0, -1, 1, -1, 0, -1, 0, -1, -1, -1, -1, 1, 1, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, -1, -1, -1, 0, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, -1, 0, -1, -1, 0, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, 1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 1, 0, -1, 0, 1, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, 1, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, 1, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 1, -1, -1, 0, 0, 0, 0, 1, -1, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 1, 0, -1, -1, 1, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, 1, 0, 0, -1, -1, -1, 0, 1, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, 0, 0, -1, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 1, 0, 0, -1, 0, -1, -1, 1, 0, 0, -1, -1, -1, -1, -1, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1]\n",
            "-------RUN150-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': \"CountVectorizer(stop_words='english')\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, -1, -1, 0, 0, -1, 0, 1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 1, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 1, -1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0]\n",
            "-------RUN151-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': 'CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7df3b00f5c90>)', 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, -1, -1, 0, 0, -1, 0, 1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 1, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 1, -1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0]\n",
            "-------RUN152-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': \"CountVectorizer(stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b02d2c50>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, 0, -1, 1, 0, -1, -1, -1, -1, 1, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 1, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 2, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, -1, -1, -1, 0, 0, -1, 0, 2, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 1, -1, 0, -1, 0, -1, -1, -1, -1, 1, 1, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1, -1, 1, -1, 0, -1, -1, -1, 0, -1, -1, -1, -1, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, -1, 1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 1, 1, 0, -1, 0, 1, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, -1, 1, -1, 0, 0, -1, -1, -1, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, -1, 0, 1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, 2, 0, -1, 0, -1, 0, 0, -1, 0, 2, -1, 2, 2, 0, 2, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 2, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 1, -1, -1, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, -1, -1, 1, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, -1, -1, 1, 0, 0, 0, 0, 0, 0, -1, 0, 1, 0, 0, -1, -1, 0, 0, 1, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, -1, 0, -1, -1, 1, 0, 0, -1, -1, -1, -1, -1, 0, -1, 1, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 2, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 1, -1, 2, 0, 0, 0, 0, 0, 2, 0, -1, 0, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, -1, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, -1, -1, 0]\n",
            "-------RUN153-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': \"CountVectorizer(ngram_range=(1, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b891dd10>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, -1, -1, 0, 0, -1, 0, 1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 1, 0, 0, 0, -1, 0, 0, -1, 0, 1, 0, 1, 1, 0, 1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, 0, 0, 1, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, -1, 0]\n",
            "-------RUN154-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': 'HDBSCAN(prediction_data=True)', 'Vectorization': \"CountVectorizer(ngram_range=(2, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df4a9008410>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[0, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, -1, 0, -1, -1, -1, 0, 0, -1, -1, 0, -1, 0, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, 0, 1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, -1, -1, 0, 0, -1, 0, 1, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, -1, -1, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, -1, 0, 0, -1, -1, -1, -1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 1, 0, 0, 0, -1, 0, 0, -1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, -1, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, 0, 0, 0, -1, -1, -1, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, -1, -1, -1, 0, 1, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, -1, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, 0]\n",
            "-------RUN155-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': \"CountVectorizer(stop_words='english')\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[1, 4, 5, 5, 1, 0, 7, 7, 4, 3, 3, 5, 2, 6, 0, 0, 3, 5, 9, 0, 1, 8, 7, 18, 18, 6, 8, 0, 0, 9, 4, 0, 5, 15, 15, 1, 2, 2, 1, 6, 18, 0, 9, 18, 6, 11, 8, 1, 18, 0, 9, 18, 7, 18, 5, 8, 8, 4, 2, 7, 2, 3, 3, 6, 0, 0, 1, 8, 1, 14, 15, 14, 13, 13, 3, 14, 3, 9, 4, 3, 11, 4, 4, 7, 4, 6, 11, 13, 4, 15, 13, 6, 4, 17, 0, 5, 6, 13, 15, 6, 1, 3, 0, 3, 0, 15, 0, 6, 9, 6, 14, 11, 11, 2, 4, 6, 8, 7, 3, 3, 6, 7, 18, 3, 8, 1, 11, 4, 18, 8, 4, 7, 0, 8, 8, 1, 0, 3, 8, 9, 6, 0, 6, 5, 2, 1, 5, 0, 4, 16, 2, 1, 7, 6, 9, 3, 2, 15, 0, 1, 9, 10, 15, 2, 0, 2, 15, 3, 15, 0, 6, 7, 4, 12, 4, 9, 3, 4, 4, 1, 3, 3, 7, 9, 14, 1, 0, 4, 14, 7, 2, 7, 11, 4, 6, 8, 1, 2, 8, 5, 11, 4, 0, 7, 7, 6, 4, 11, 15, 1, 2, 11, 4, 4, 7, 11, 11, 13, 4, 6, 2, 2, 7, 13, 6, 16, 8, 8, 7, 12, 12, 9, 12, 16, 2, 12, 6, 1, 2, 16, 9, 14, 14, 3, 9, 9, 0, 3, 0, 0, 3, 16, 17, 5, 9, 11, 11, 11, 13, 2, 7, 3, 2, 9, 4, 4, 15, 15, 0, 14, 14, 14, 2, 13, 14, 4, 14, 14, 6, 6, 13, 3, 14, 11, 11, 14, 1, 0, 0, 9, 5, 0, 5, 1, 7, 2, 6, 4, 2, 14, 14, 14, 1, 2, 1, 14, 1, 14, 5, 2, 18, 14, 6, 0, 0, 5, 3, 9, 3, 2, 2, 1, 0, 1, 9, 1, 0, 2, 3, 2, 2, 18, 9, 2, 1, 1, 2, 7, 1, 0, 2, 6, 6, 7, 1, 0, 7, 4, 12, 1, 0, 0, 0, 4, 6, 9, 5, 8, 7, 5, 3, 8, 0, 11, 8, 2, 7, 3, 6, 9, 9, 2, 7, 15, 15, 16, 12, 4, 0, 12, 19, 19, 0, 1, 7, 1, 8, 0, 11, 6, 2, 7, 3, 19, 19, 19, 19, 19, 19, 12, 14, 19, 16, 3, 2, 0, 19, 1, 12, 10, 6, 7, 0, 9, 9, 6, 5, 4, 9, 3, 0, 7, 2, 3, 9, 4, 3, 0, 2, 13, 13, 3, 13, 14, 13, 4, 13, 13, 13, 13, 13, 1, 3, 9, 5, 14, 0, 14, 13, 8, 15, 14, 14, 1, 1, 4, 14, 3, 14, 14, 1, 14, 9, 4, 16, 14, 14, 14, 14, 13, 13, 11, 0, 13, 1, 13, 14, 11, 1, 13, 2, 5, 13, 8, 5, 7, 1, 11, 4, 11, 5, 9, 3, 6, 2, 5, 1, 6, 17, 17, 17, 2, 17, 1, 7, 16, 9, 1, 5, 0, 17, 17, 17, 2, 17, 17, 17, 17, 7, 17, 16, 17, 0, 8, 17, 5, 18, 17, 2, 3, 17, 17, 18, 2, 18, 1, 0, 18, 11, 15, 15, 11, 1, 1, 14, 13, 8, 3, 11, 11, 4, 6, 11, 16, 16, 16, 16, 12, 15, 11, 4, 17, 1, 12, 12, 12, 12, 12, 1, 17, 16, 18, 18, 2, 16, 5, 0, 8, 2, 16, 16, 18, 17, 16, 7, 16, 1, 15, 2, 12, 12, 6, 7, 6, 6, 12, 3, 12, 12, 6, 0, 12, 6, 12, 16, 6, 19, 12, 6, 6, 15, 5, 8, 8, 8, 8, 7, 8, 0, 8, 8, 13, 1, 0, 3, 3, 0, 0, 3, 2, 8, 7, 7, 8, 8, 2, 8, 8, 3, 13, 13, 13, 1, 13, 5, 13, 13, 1, 3, 14, 2, 11, 17, 5, 0, 17, 12, 4, 9, 3, 17, 0, 17, 0, 17, 4, 17, 18, 1, 18, 4, 7, 0, 17, 6, 17, 0, 17, 0, 17, 12, 7, 17, 1, 3, 11, 11, 11, 11, 3, 11, 11, 3, 1, 11, 0, 8, 2, 11, 4, 5, 13, 7, 11, 16, 16, 3, 2, 3, 17, 0, 9, 0, 5, 12, 4, 4, 8, 16, 3, 0, 3, 12, 12, 5, 4, 1, 12, 12, 3, 12, 12, 18, 3, 16, 0, 2, 12, 0, 2, 1, 12, 15, 15, 12, 2, 7, 13, 0, 2, 5, 4, 6, 18, 3, 12, 10, 10, 12, 15, 9, 3, 13, 1, 0, 0, 19, 5, 19, 7, 13, 2, 4, 2, 0, 2, 1, 0, 7, 14, 14, 8, 8, 13, 8, 8, 13, 9, 8, 7, 8, 2, 0, 10, 10, 10, 10, 10, 0, 2, 10, 8, 10, 10, 10, 10, 3, 10, 10, 8, 9, 0, 13, 10, 5, 3, 10, 10, 10, 13, 0, 5, 17, 17, 2, 2, 2, 0, 17, 11, 9, 9, 3, 9, 9, 9, 4, 9, 6, 9, 6, 8, 14, 18, 4, 8, 19, 18, 11, 19, 5, 5, 5, 17, 5, 4, 19, 0, 19, 9, 19, 19, 19, 19, 3, 3, 5, 0, 18, 5, 15, 15, 15, 4, 0, 4, 5, 5, 0, 3, 5, 5, 4, 17, 15, 0, 0, 15, 0, 0, 0, 4, 4, 0, 7, 4, 17, 11, 3, 0, 1, 3, 0, 2, 10, 4, 8, 0, 1, 1, 5, 5, 0, 2, 1, 12, 5, 0, 7, 5, 3, 4, 9, 6, 3, 10, 10, 10, 10, 10, 10, 10, 1, 10, 10, 12, 10, 2, 10, 5, 10, 1, 10, 2, 5, 1, 10, 17, 2, 18, 4, 18, 9, 11, 3, 4, 4, 2, 2, 6, 11, 0, 0, 15, 9, 14, 9, 4, 9, 2, 18, 4, 3, 1, 1, 1, 15, 18, 16, 4, 6, 8, 7, 9, 0, 15, 0, 15, 11, 4, 7, 18, 0, 7, 6, 6, 7, 3, 2, 18, 1, 19, 6, 12, 1, 10, 1, 1, 4, 6, 7, 1, 1, 19, 4, 3, 5, 15, 3, 1, 6, 19, 5, 2, 19, 0, 0, 5, 5, 17, 0, 5, 10, 5, 10, 3, 1, 1, 0, 0, 5, 5, 0, 0, 5, 4, 3, 9, 2, 5, 5, 7, 1, 9, 9, 15, 3, 14, 6, 3, 18, 0, 13, 0, 7, 3, 14, 1, 15, 5, 13, 15, 2, 6, 2, 2, 2, 13, 6, 17, 0, 1, 8, 2, 11, 4, 16, 4, 2, 2, 0, 0, 4, 2, 5, 11, 11, 6, 11, 11, 9, 13, 4, 0, 11, 8, 11, 3, 18, 18, 2, 5, 0, 10, 0, 1, 7, 18, 18, 7, 11, 18, 15, 14, 0, 1, 3, 17, 18, 3, 13, 7, 4, 13, 9, 16, 9, 13, 1, 1, 0, 5, 3, 4, 0, 14, 10, 10, 3, 1, 3, 10, 5, 16, 12, 1, 7, 15, 0, 11, 8, 6, 7, 6, 15, 6, 15, 12, 6, 8, 7, 7, 16, 7, 15, 6, 3, 0, 16, 7, 1, 6, 2, 2, 1, 8, 2, 17, 1, 9, 1, 12, 4, 4, 19, 12, 5, 9, 0, 19, 1, 3, 10, 0, 3, 3, 12, 10, 4, 2, 2, 0, 2, 0, 1, 1, 6, 3, 4, 10, 13, 7, 1, 16, 4, 11, 11, 15, 11, 6, 15, 11, 10, 18, 18, 1, 15, 13, 18, 11, 3, 4, 1, 16, 16, 10, 13, 1, 1, 16, 13, 16, 0, 13, 1, 2, 3, 15, 8, 14, 11, 2, 5, 1, 9, 8, 6, 7, 6, 4, 0, 0, 8, 6, 11, 11, 9, 2, 12, 3, 1, 15, 3, 5, 5, 12, 8, 6, 4, 5, 18, 18, 5, 5, 3, 4, 0, 14, 19, 5, 16, 12, 12, 10, 12, 3, 7, 1, 7, 7, 2, 0, 11, 12, 11, 4, 2, 1, 11, 9, 3, 12, 8, 0, 16, 14, 14, 14, 10, 0, 10, 10, 10, 11, 0, 10, 10, 0, 7, 8, 8, 5, 4, 2, 6, 1, 16, 1, 7, 13, 11, 1, 13, 4, 16, 16, 0, 1, 13, 5, 16, 9, 18, 3, 3, 13, 15, 2, 13, 3, 4, 16, 2, 0, 13, 13, 6, 10, 14, 0, 0, 10, 10, 13, 0, 0, 2, 2, 8, 0, 2, 12, 7, 5, 12, 2, 0, 15, 8, 5, 4, 0, 12, 9, 5, 0, 0, 12, 12, 12, 19, 0, 9, 0, 14, 4, 6, 7, 2, 4, 0, 8, 8, 14, 14, 14, 17, 8, 2, 3, 12, 10, 4, 3, 10, 10, 19, 11, 15, 10, 9, 9, 10, 13, 13, 8, 1, 5, 8, 14, 4, 11, 5, 2, 15, 14, 16, 0, 3, 0, 0, 7, 7, 14, 5, 1, 16, 17, 19, 12, 2, 16, 16, 1, 1, 19, 4, 2, 1, 3, 15, 19, 18, 13, 8, 0, 1, 11, 1, 6, 9, 0, 8, 0, 1, 9, 9, 5, 16, 2, 3, 0, 5, 12, 16, 12, 5, 1, 6, 10, 3, 10, 5, 6, 1, 0, 4, 12, 3, 1, 1, 12, 12, 12, 2, 12, 0, 18, 19, 13, 0, 10, 10, 12, 17, 2, 5, 5, 4, 4, 10, 10, 0, 1, 10, 8, 9, 10, 10, 1, 15, 15]\n",
            "-------RUN156-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': 'CountVectorizer(tokenizer=<__main__.LemmaTokenizer object at 0x7df3b00f5c90>)', 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[1, 6, 4, 4, 1, 0, 5, 5, 6, 6, 3, 4, 5, 1, 5, 0, 3, 4, 11, 0, 1, 7, 5, 17, 17, 9, 7, 0, 0, 11, 19, 0, 4, 10, 10, 1, 5, 12, 2, 1, 17, 0, 11, 17, 1, 9, 7, 1, 17, 0, 11, 17, 5, 17, 4, 7, 7, 6, 3, 11, 1, 6, 5, 7, 0, 0, 1, 7, 14, 14, 10, 14, 13, 13, 3, 14, 3, 11, 6, 3, 8, 19, 6, 6, 6, 9, 9, 13, 19, 10, 13, 9, 6, 16, 5, 4, 10, 13, 10, 1, 1, 11, 3, 3, 0, 10, 0, 10, 11, 6, 14, 7, 9, 5, 19, 9, 7, 5, 3, 6, 10, 5, 17, 11, 7, 2, 7, 19, 17, 7, 19, 6, 0, 7, 8, 1, 0, 3, 7, 1, 10, 0, 1, 4, 6, 1, 4, 0, 6, 2, 8, 1, 3, 10, 11, 3, 8, 10, 0, 1, 11, 12, 10, 4, 0, 3, 10, 11, 10, 0, 10, 5, 6, 2, 6, 11, 3, 19, 6, 1, 3, 3, 5, 11, 14, 1, 0, 19, 14, 5, 6, 5, 7, 19, 10, 7, 1, 8, 7, 12, 7, 19, 0, 5, 9, 1, 6, 9, 10, 16, 8, 9, 19, 19, 5, 9, 9, 13, 19, 10, 8, 6, 5, 13, 1, 2, 7, 7, 6, 2, 2, 11, 2, 1, 5, 18, 4, 12, 8, 8, 8, 14, 14, 3, 11, 11, 0, 3, 0, 0, 3, 8, 16, 4, 1, 9, 9, 9, 13, 8, 7, 3, 0, 11, 19, 6, 10, 10, 0, 14, 14, 14, 12, 13, 14, 6, 14, 14, 9, 10, 5, 3, 14, 9, 9, 14, 1, 3, 0, 11, 4, 0, 4, 14, 5, 8, 1, 6, 5, 14, 14, 14, 1, 5, 1, 14, 1, 14, 4, 6, 17, 14, 1, 0, 0, 4, 3, 11, 11, 6, 1, 1, 0, 14, 11, 12, 0, 5, 6, 8, 8, 17, 7, 5, 11, 1, 3, 5, 16, 0, 8, 10, 9, 9, 1, 5, 5, 6, 2, 7, 0, 0, 0, 6, 9, 8, 12, 7, 5, 4, 3, 7, 0, 7, 7, 8, 5, 3, 9, 1, 11, 12, 8, 10, 10, 8, 2, 6, 3, 2, 18, 18, 0, 1, 5, 1, 7, 0, 9, 9, 1, 9, 3, 18, 18, 18, 18, 18, 18, 2, 12, 18, 8, 3, 8, 0, 18, 1, 2, 12, 9, 7, 0, 11, 11, 10, 4, 6, 11, 3, 3, 5, 1, 3, 11, 6, 3, 3, 5, 13, 5, 3, 13, 14, 13, 6, 13, 13, 13, 13, 13, 1, 5, 11, 4, 14, 0, 14, 5, 9, 10, 14, 14, 1, 1, 6, 14, 3, 14, 14, 1, 14, 11, 6, 8, 14, 14, 14, 14, 13, 13, 9, 0, 13, 8, 8, 14, 9, 4, 13, 5, 4, 13, 7, 4, 5, 12, 9, 6, 9, 4, 11, 3, 9, 8, 4, 1, 10, 16, 16, 16, 8, 16, 1, 5, 8, 8, 1, 4, 5, 16, 16, 16, 4, 16, 16, 16, 16, 7, 16, 8, 16, 0, 7, 16, 2, 17, 16, 3, 3, 16, 16, 17, 8, 17, 1, 6, 17, 9, 10, 10, 9, 2, 8, 14, 13, 9, 3, 9, 9, 19, 9, 9, 2, 2, 2, 1, 2, 10, 9, 19, 16, 1, 2, 2, 2, 2, 2, 1, 16, 2, 17, 17, 12, 8, 4, 0, 7, 5, 2, 2, 17, 4, 2, 7, 2, 1, 10, 5, 2, 2, 10, 1, 10, 1, 2, 3, 2, 2, 1, 0, 2, 10, 2, 2, 10, 18, 2, 10, 5, 10, 4, 7, 7, 7, 7, 5, 7, 0, 7, 7, 13, 1, 0, 3, 3, 4, 0, 11, 8, 7, 8, 5, 7, 7, 5, 7, 7, 5, 13, 13, 13, 1, 13, 4, 13, 5, 8, 3, 14, 8, 8, 16, 4, 0, 16, 2, 0, 17, 3, 16, 0, 16, 0, 16, 6, 16, 17, 1, 17, 6, 5, 10, 16, 1, 16, 0, 16, 4, 16, 2, 5, 16, 12, 6, 9, 9, 9, 8, 3, 9, 9, 3, 1, 9, 4, 7, 8, 9, 19, 4, 13, 5, 9, 2, 2, 11, 5, 3, 16, 0, 1, 0, 4, 2, 6, 6, 7, 2, 3, 0, 3, 2, 2, 4, 6, 1, 2, 2, 6, 2, 2, 17, 3, 8, 0, 8, 2, 0, 12, 16, 2, 10, 10, 2, 12, 5, 13, 5, 12, 4, 19, 1, 17, 3, 2, 15, 15, 2, 10, 11, 3, 13, 12, 0, 0, 18, 4, 18, 8, 13, 8, 6, 8, 0, 4, 1, 0, 5, 14, 14, 7, 7, 13, 7, 7, 13, 7, 7, 5, 7, 8, 3, 15, 15, 15, 15, 12, 0, 8, 15, 7, 15, 15, 15, 15, 3, 15, 12, 7, 11, 0, 13, 15, 4, 3, 15, 12, 15, 13, 0, 4, 16, 16, 4, 5, 3, 0, 16, 8, 11, 11, 11, 11, 11, 11, 19, 11, 9, 11, 9, 7, 14, 17, 19, 9, 18, 17, 8, 18, 18, 4, 4, 16, 4, 6, 18, 0, 18, 11, 18, 18, 18, 18, 11, 3, 4, 0, 17, 4, 10, 10, 10, 6, 0, 6, 4, 4, 0, 6, 4, 4, 19, 16, 10, 0, 0, 10, 0, 0, 0, 6, 6, 0, 7, 6, 16, 8, 3, 0, 1, 3, 5, 4, 15, 19, 7, 0, 1, 1, 4, 4, 0, 4, 1, 11, 4, 0, 5, 12, 3, 6, 11, 10, 5, 15, 15, 15, 15, 15, 15, 15, 12, 15, 15, 2, 15, 3, 15, 4, 15, 12, 12, 5, 4, 12, 15, 16, 6, 17, 19, 17, 11, 9, 3, 6, 19, 8, 8, 1, 9, 0, 4, 10, 11, 2, 11, 6, 11, 8, 17, 6, 3, 1, 1, 1, 10, 17, 2, 6, 9, 1, 5, 11, 0, 10, 0, 10, 8, 6, 5, 17, 0, 5, 9, 1, 7, 3, 5, 17, 1, 18, 10, 2, 12, 15, 1, 1, 19, 10, 5, 1, 8, 18, 6, 3, 4, 10, 3, 8, 10, 18, 4, 3, 18, 0, 0, 4, 4, 16, 0, 4, 15, 18, 15, 3, 12, 12, 0, 3, 4, 4, 0, 4, 4, 0, 6, 11, 3, 4, 4, 5, 1, 11, 11, 10, 3, 14, 9, 6, 17, 0, 13, 0, 3, 1, 14, 1, 10, 4, 13, 10, 8, 1, 5, 5, 3, 13, 11, 16, 0, 1, 8, 8, 9, 9, 8, 6, 5, 5, 0, 0, 6, 3, 4, 9, 7, 10, 9, 9, 11, 13, 19, 0, 9, 7, 9, 11, 17, 17, 8, 4, 0, 12, 0, 0, 5, 17, 17, 5, 8, 17, 10, 12, 3, 8, 3, 11, 17, 3, 13, 5, 6, 13, 11, 2, 11, 13, 1, 1, 0, 4, 3, 6, 0, 14, 12, 12, 3, 12, 3, 12, 4, 2, 2, 12, 5, 10, 0, 9, 7, 10, 5, 1, 10, 10, 10, 8, 10, 7, 5, 5, 8, 5, 1, 1, 3, 10, 8, 7, 1, 10, 8, 3, 12, 7, 8, 16, 12, 11, 1, 2, 6, 19, 18, 2, 4, 11, 0, 18, 1, 3, 12, 0, 3, 3, 2, 12, 6, 8, 12, 12, 8, 0, 1, 12, 10, 3, 6, 15, 13, 5, 1, 8, 19, 9, 9, 10, 9, 9, 10, 9, 15, 17, 17, 1, 10, 13, 17, 9, 3, 6, 1, 2, 2, 15, 13, 1, 1, 2, 13, 2, 0, 13, 12, 12, 3, 10, 7, 14, 7, 8, 4, 12, 11, 7, 9, 1, 10, 19, 0, 0, 7, 10, 9, 9, 11, 3, 2, 3, 12, 0, 6, 4, 4, 2, 7, 1, 6, 4, 17, 17, 4, 4, 3, 19, 0, 14, 18, 18, 2, 2, 2, 15, 2, 3, 5, 1, 6, 5, 8, 0, 7, 2, 7, 6, 3, 12, 9, 11, 3, 2, 7, 0, 2, 14, 14, 14, 15, 0, 15, 15, 12, 9, 5, 15, 15, 0, 8, 9, 7, 4, 19, 8, 9, 12, 8, 12, 8, 13, 9, 1, 13, 19, 2, 2, 0, 1, 13, 4, 12, 11, 17, 11, 3, 13, 10, 8, 13, 3, 6, 2, 6, 0, 13, 13, 9, 15, 14, 0, 0, 15, 12, 13, 0, 0, 6, 4, 7, 0, 3, 2, 0, 4, 2, 5, 0, 10, 7, 4, 6, 0, 2, 11, 4, 0, 0, 2, 2, 2, 18, 3, 11, 0, 14, 6, 1, 9, 6, 19, 0, 7, 7, 14, 14, 14, 16, 7, 6, 6, 2, 15, 6, 3, 15, 15, 18, 9, 10, 15, 11, 11, 15, 13, 13, 7, 1, 4, 7, 14, 19, 9, 4, 8, 10, 14, 2, 0, 3, 0, 5, 8, 5, 14, 4, 11, 2, 16, 18, 2, 8, 2, 2, 1, 12, 18, 6, 8, 12, 3, 10, 18, 17, 13, 7, 0, 8, 9, 1, 1, 11, 0, 7, 0, 1, 11, 11, 4, 8, 12, 11, 0, 12, 2, 2, 2, 12, 8, 9, 15, 3, 15, 12, 10, 2, 0, 6, 2, 6, 2, 12, 2, 2, 2, 5, 2, 3, 17, 18, 13, 0, 15, 15, 2, 16, 5, 4, 4, 6, 19, 15, 15, 0, 1, 12, 7, 11, 12, 12, 1, 2, 10]\n",
            "-------RUN157-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': \"CountVectorizer(stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b02d2c50>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[6, 8, 3, 14, 6, 1, 3, 2, 8, 10, 2, 5, 10, 4, 1, 1, 2, 6, 7, 1, 6, 0, 5, 19, 19, 4, 0, 1, 1, 7, 1, 1, 6, 16, 16, 10, 3, 9, 6, 4, 19, 1, 7, 19, 4, 0, 0, 6, 19, 1, 7, 19, 3, 19, 3, 0, 0, 8, 2, 4, 10, 2, 3, 4, 1, 1, 10, 0, 6, 18, 16, 6, 12, 12, 2, 18, 2, 7, 8, 2, 10, 8, 8, 5, 8, 4, 0, 12, 8, 16, 12, 0, 8, 13, 3, 1, 4, 12, 16, 4, 4, 7, 3, 2, 3, 16, 1, 4, 7, 4, 6, 0, 0, 5, 8, 4, 0, 2, 2, 2, 4, 4, 19, 7, 0, 6, 0, 8, 19, 6, 8, 5, 1, 0, 5, 6, 3, 2, 0, 6, 4, 1, 4, 9, 5, 10, 6, 1, 8, 15, 9, 10, 2, 4, 7, 2, 5, 16, 3, 10, 7, 9, 16, 3, 1, 2, 16, 4, 16, 1, 4, 3, 8, 11, 5, 7, 2, 1, 1, 4, 2, 2, 4, 7, 6, 10, 3, 8, 18, 5, 5, 5, 0, 8, 4, 0, 10, 10, 0, 9, 0, 8, 1, 2, 4, 4, 8, 0, 16, 13, 5, 0, 8, 8, 5, 0, 0, 12, 8, 16, 2, 5, 5, 12, 4, 15, 0, 0, 5, 11, 11, 7, 11, 15, 5, 14, 6, 9, 5, 15, 7, 18, 18, 2, 7, 7, 1, 2, 1, 1, 2, 15, 13, 3, 10, 0, 0, 0, 12, 5, 0, 4, 1, 7, 1, 1, 16, 4, 3, 18, 18, 18, 10, 12, 18, 8, 18, 18, 4, 4, 5, 2, 18, 0, 0, 6, 6, 3, 3, 7, 13, 1, 6, 6, 4, 5, 4, 5, 2, 18, 18, 18, 6, 5, 10, 18, 10, 10, 13, 10, 19, 18, 4, 3, 1, 5, 2, 7, 7, 2, 10, 6, 3, 6, 7, 9, 1, 3, 8, 9, 2, 19, 7, 3, 7, 6, 2, 5, 13, 1, 5, 4, 4, 5, 10, 3, 5, 5, 11, 6, 1, 1, 1, 8, 4, 7, 9, 4, 5, 6, 7, 0, 1, 0, 0, 10, 5, 2, 4, 10, 7, 9, 5, 16, 16, 15, 11, 5, 3, 11, 14, 14, 1, 6, 10, 10, 0, 3, 0, 4, 10, 4, 2, 14, 14, 14, 14, 14, 14, 14, 6, 14, 15, 2, 15, 1, 14, 10, 11, 9, 4, 0, 1, 7, 7, 4, 3, 8, 7, 2, 3, 3, 10, 2, 7, 10, 2, 3, 5, 12, 5, 2, 12, 18, 12, 5, 12, 12, 12, 12, 12, 10, 2, 7, 3, 18, 1, 18, 5, 0, 16, 18, 18, 6, 7, 8, 18, 2, 18, 18, 10, 18, 7, 8, 15, 18, 18, 18, 18, 12, 12, 0, 1, 12, 10, 12, 18, 0, 6, 12, 10, 3, 12, 0, 3, 5, 9, 0, 8, 0, 3, 7, 2, 4, 9, 14, 10, 4, 13, 13, 13, 9, 13, 6, 5, 15, 15, 6, 13, 3, 13, 13, 13, 3, 13, 13, 13, 13, 4, 13, 15, 13, 3, 0, 13, 6, 19, 13, 2, 2, 13, 13, 19, 5, 19, 10, 1, 19, 0, 16, 16, 0, 6, 9, 18, 12, 0, 2, 0, 0, 8, 4, 0, 15, 15, 15, 15, 11, 16, 0, 8, 6, 4, 11, 11, 11, 11, 11, 6, 13, 15, 19, 19, 9, 9, 3, 1, 19, 3, 15, 15, 19, 13, 15, 4, 15, 6, 16, 2, 11, 11, 4, 4, 4, 10, 11, 2, 11, 11, 4, 1, 11, 4, 11, 15, 4, 14, 11, 4, 4, 16, 9, 0, 0, 0, 0, 3, 0, 6, 0, 6, 12, 10, 3, 2, 2, 3, 1, 7, 5, 0, 5, 5, 0, 6, 5, 0, 0, 2, 12, 12, 12, 10, 12, 3, 12, 12, 10, 1, 18, 10, 5, 13, 13, 1, 13, 11, 1, 19, 2, 13, 3, 13, 3, 13, 8, 13, 19, 10, 19, 8, 3, 1, 13, 4, 13, 1, 13, 3, 13, 6, 2, 13, 6, 1, 0, 0, 0, 0, 2, 0, 5, 2, 10, 0, 3, 0, 5, 0, 8, 3, 12, 5, 0, 15, 15, 7, 2, 2, 13, 1, 7, 1, 13, 11, 1, 8, 0, 15, 2, 1, 1, 11, 11, 6, 1, 6, 11, 11, 2, 11, 11, 19, 2, 15, 1, 9, 11, 1, 9, 6, 11, 16, 16, 11, 9, 5, 12, 3, 9, 6, 8, 4, 19, 2, 15, 17, 17, 11, 16, 7, 2, 5, 9, 3, 1, 14, 6, 14, 5, 12, 7, 8, 5, 3, 7, 10, 1, 5, 18, 18, 0, 0, 12, 0, 0, 12, 7, 0, 4, 0, 5, 3, 17, 17, 17, 17, 9, 1, 7, 17, 0, 17, 17, 17, 17, 2, 17, 9, 0, 7, 1, 12, 17, 13, 2, 17, 9, 17, 12, 1, 13, 13, 13, 3, 3, 13, 3, 13, 5, 7, 7, 7, 7, 7, 7, 8, 7, 4, 7, 4, 0, 18, 19, 8, 0, 14, 19, 0, 14, 14, 3, 14, 13, 6, 8, 14, 3, 14, 7, 14, 14, 14, 14, 7, 2, 6, 1, 19, 3, 16, 16, 16, 8, 1, 8, 6, 3, 1, 8, 6, 6, 8, 6, 16, 3, 3, 16, 1, 1, 3, 5, 1, 1, 5, 8, 13, 0, 2, 1, 10, 2, 3, 9, 17, 8, 0, 3, 6, 10, 13, 13, 3, 5, 15, 11, 13, 1, 1, 6, 2, 8, 7, 4, 2, 17, 17, 17, 17, 17, 17, 17, 9, 17, 17, 1, 17, 2, 17, 6, 17, 9, 9, 2, 9, 6, 17, 13, 5, 19, 8, 19, 7, 0, 2, 5, 8, 5, 5, 4, 0, 1, 3, 16, 7, 6, 7, 8, 7, 5, 19, 8, 2, 6, 4, 10, 16, 19, 15, 8, 4, 6, 5, 7, 1, 16, 1, 16, 0, 8, 4, 19, 1, 4, 4, 4, 0, 2, 3, 19, 10, 14, 4, 11, 6, 17, 6, 6, 8, 4, 2, 10, 10, 14, 8, 2, 3, 16, 2, 5, 4, 14, 14, 2, 14, 1, 3, 14, 14, 13, 3, 14, 17, 14, 17, 2, 9, 6, 1, 3, 6, 6, 1, 3, 6, 1, 2, 7, 2, 6, 6, 5, 6, 7, 7, 4, 3, 18, 4, 2, 19, 3, 12, 1, 2, 4, 18, 6, 16, 5, 12, 16, 7, 4, 2, 5, 2, 12, 4, 13, 1, 10, 5, 5, 0, 8, 15, 8, 3, 5, 1, 1, 8, 2, 9, 0, 0, 4, 0, 0, 7, 12, 8, 1, 0, 19, 0, 7, 19, 19, 2, 14, 3, 9, 1, 6, 5, 19, 19, 5, 0, 19, 16, 9, 3, 10, 2, 3, 19, 2, 12, 5, 8, 12, 7, 15, 7, 12, 10, 6, 1, 14, 2, 5, 4, 18, 9, 9, 2, 9, 2, 9, 9, 15, 11, 10, 5, 16, 1, 0, 0, 4, 3, 4, 16, 4, 16, 11, 4, 0, 4, 3, 15, 5, 16, 4, 3, 3, 15, 0, 10, 4, 5, 2, 6, 19, 5, 13, 9, 7, 6, 11, 8, 8, 14, 11, 6, 7, 3, 14, 10, 2, 9, 1, 2, 2, 14, 9, 8, 5, 9, 9, 5, 3, 6, 9, 4, 2, 1, 17, 12, 4, 10, 15, 8, 0, 0, 16, 0, 4, 16, 0, 17, 19, 19, 10, 16, 12, 19, 0, 1, 8, 10, 15, 15, 17, 12, 10, 7, 15, 12, 15, 1, 12, 6, 9, 2, 16, 0, 18, 0, 5, 14, 6, 7, 0, 4, 10, 4, 8, 3, 1, 0, 4, 0, 0, 6, 9, 11, 2, 9, 16, 2, 9, 14, 11, 0, 4, 8, 9, 19, 19, 14, 3, 2, 8, 3, 18, 14, 14, 15, 11, 11, 17, 15, 2, 2, 6, 5, 5, 9, 1, 0, 11, 0, 8, 2, 9, 0, 7, 2, 11, 0, 1, 15, 18, 18, 18, 17, 3, 17, 17, 9, 0, 1, 17, 17, 1, 5, 0, 0, 9, 8, 7, 4, 9, 15, 9, 5, 12, 0, 15, 12, 8, 15, 15, 1, 10, 12, 9, 9, 7, 19, 7, 2, 12, 16, 10, 12, 2, 1, 15, 5, 3, 5, 12, 4, 17, 18, 1, 1, 9, 9, 12, 3, 1, 5, 3, 0, 3, 5, 11, 4, 9, 11, 2, 1, 16, 0, 14, 8, 1, 11, 7, 14, 1, 6, 11, 11, 11, 14, 3, 7, 1, 18, 8, 4, 5, 5, 8, 1, 0, 0, 18, 18, 18, 13, 0, 5, 2, 11, 17, 8, 2, 17, 17, 14, 0, 16, 17, 7, 7, 17, 12, 12, 0, 6, 3, 0, 18, 8, 0, 14, 2, 16, 18, 15, 1, 2, 1, 3, 5, 2, 18, 9, 7, 15, 13, 14, 11, 10, 15, 15, 10, 9, 14, 8, 5, 6, 2, 16, 14, 19, 12, 0, 1, 10, 0, 10, 10, 7, 3, 0, 3, 10, 7, 7, 3, 15, 9, 7, 1, 9, 11, 15, 11, 9, 6, 4, 17, 2, 17, 6, 4, 11, 3, 2, 11, 2, 15, 9, 11, 11, 11, 5, 11, 3, 19, 14, 12, 1, 17, 17, 11, 13, 9, 14, 6, 8, 8, 17, 17, 3, 6, 9, 0, 7, 9, 9, 10, 16, 16]\n",
            "-------RUN158-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': \"CountVectorizer(ngram_range=(1, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df3b891dd10>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[3, 8, 7, 15, 3, 1, 7, 7, 8, 9, 5, 7, 9, 2, 1, 1, 4, 3, 5, 1, 3, 0, 7, 17, 17, 2, 0, 1, 1, 5, 8, 1, 3, 16, 16, 12, 7, 9, 6, 2, 17, 1, 5, 17, 2, 0, 0, 2, 17, 1, 5, 17, 7, 17, 13, 0, 0, 8, 4, 2, 9, 4, 7, 2, 1, 1, 12, 2, 3, 14, 16, 14, 18, 18, 4, 14, 4, 5, 8, 4, 12, 8, 8, 12, 8, 2, 0, 19, 8, 16, 18, 0, 8, 13, 7, 1, 2, 19, 16, 2, 2, 5, 1, 4, 7, 16, 1, 2, 5, 2, 3, 12, 0, 19, 8, 12, 0, 4, 4, 4, 2, 2, 17, 5, 0, 3, 0, 8, 17, 3, 8, 4, 1, 0, 12, 3, 7, 4, 0, 2, 2, 1, 2, 3, 4, 9, 3, 1, 8, 6, 10, 5, 4, 2, 5, 4, 10, 16, 7, 9, 5, 3, 16, 9, 1, 4, 16, 5, 16, 1, 2, 7, 8, 6, 8, 5, 5, 1, 1, 2, 5, 4, 2, 5, 14, 5, 7, 8, 14, 7, 4, 12, 0, 8, 2, 0, 9, 10, 0, 9, 0, 8, 1, 4, 2, 2, 12, 0, 16, 13, 10, 0, 8, 8, 7, 12, 0, 18, 8, 16, 10, 4, 4, 19, 2, 10, 0, 0, 12, 6, 6, 5, 6, 2, 7, 15, 3, 3, 10, 10, 5, 14, 14, 4, 5, 10, 1, 4, 1, 1, 4, 10, 13, 9, 5, 0, 0, 0, 19, 19, 0, 5, 1, 5, 1, 8, 16, 2, 7, 14, 14, 14, 9, 19, 14, 8, 14, 14, 2, 2, 19, 1, 14, 0, 0, 3, 3, 7, 7, 5, 13, 1, 3, 9, 2, 10, 2, 8, 4, 14, 14, 14, 3, 9, 9, 14, 14, 9, 9, 9, 17, 14, 2, 7, 1, 9, 4, 5, 5, 4, 9, 12, 7, 3, 6, 3, 1, 9, 8, 9, 9, 17, 5, 7, 3, 3, 4, 10, 13, 1, 10, 2, 2, 2, 2, 7, 4, 8, 6, 3, 1, 7, 1, 8, 2, 12, 3, 2, 10, 3, 5, 0, 1, 0, 0, 12, 9, 4, 2, 5, 5, 9, 12, 16, 16, 10, 6, 12, 7, 6, 15, 15, 1, 2, 9, 9, 0, 7, 0, 2, 9, 2, 5, 15, 15, 15, 15, 15, 15, 15, 3, 15, 10, 4, 10, 1, 15, 2, 6, 11, 2, 0, 1, 5, 5, 2, 3, 8, 5, 4, 7, 9, 9, 4, 5, 12, 4, 7, 19, 18, 19, 4, 19, 14, 18, 12, 19, 18, 18, 18, 18, 9, 4, 10, 3, 14, 1, 14, 19, 0, 16, 14, 14, 3, 5, 8, 14, 4, 14, 14, 5, 14, 5, 8, 10, 14, 14, 14, 14, 18, 18, 0, 1, 18, 12, 12, 14, 0, 9, 18, 4, 7, 18, 0, 3, 2, 3, 0, 8, 0, 7, 5, 5, 2, 10, 3, 9, 2, 13, 13, 13, 10, 13, 3, 4, 10, 10, 3, 13, 7, 13, 13, 13, 9, 13, 13, 13, 13, 2, 13, 10, 13, 7, 0, 13, 3, 17, 13, 4, 4, 13, 13, 17, 10, 18, 2, 1, 17, 0, 16, 16, 0, 3, 12, 14, 19, 0, 5, 0, 0, 8, 2, 0, 6, 6, 6, 2, 6, 16, 0, 8, 13, 5, 6, 6, 6, 6, 6, 3, 13, 6, 17, 17, 9, 10, 9, 1, 17, 7, 6, 6, 17, 13, 10, 2, 6, 2, 16, 7, 6, 6, 2, 2, 2, 2, 6, 4, 6, 6, 2, 1, 6, 2, 6, 6, 2, 15, 6, 2, 2, 16, 3, 0, 0, 0, 0, 7, 0, 7, 0, 0, 18, 18, 7, 4, 4, 7, 1, 5, 10, 0, 12, 7, 0, 12, 4, 0, 0, 4, 18, 18, 18, 18, 18, 9, 18, 19, 10, 1, 14, 9, 12, 13, 13, 1, 13, 6, 1, 17, 4, 13, 1, 13, 7, 13, 8, 13, 17, 12, 17, 8, 7, 1, 13, 2, 13, 1, 13, 7, 13, 3, 4, 13, 3, 1, 0, 0, 0, 10, 4, 12, 12, 4, 12, 0, 1, 0, 12, 0, 8, 3, 18, 7, 0, 6, 6, 5, 4, 4, 13, 1, 10, 1, 13, 6, 8, 8, 0, 6, 4, 1, 1, 6, 6, 3, 1, 2, 6, 6, 4, 6, 6, 17, 4, 10, 1, 10, 6, 1, 9, 3, 6, 16, 16, 6, 9, 7, 18, 7, 9, 3, 8, 2, 17, 4, 6, 11, 11, 6, 16, 5, 4, 19, 3, 7, 1, 15, 3, 15, 19, 18, 10, 8, 9, 7, 9, 10, 1, 4, 14, 14, 0, 0, 19, 0, 0, 18, 12, 0, 2, 0, 10, 7, 11, 11, 11, 11, 11, 1, 10, 11, 0, 11, 11, 11, 11, 4, 11, 11, 0, 5, 1, 19, 11, 13, 5, 11, 11, 11, 19, 1, 9, 13, 13, 9, 7, 4, 7, 13, 12, 5, 5, 5, 5, 5, 5, 8, 5, 2, 5, 2, 0, 14, 17, 8, 0, 15, 17, 12, 15, 15, 13, 15, 13, 3, 8, 15, 7, 15, 5, 15, 15, 15, 15, 5, 4, 3, 1, 17, 7, 16, 16, 16, 8, 1, 8, 3, 7, 1, 8, 3, 3, 8, 13, 16, 7, 7, 16, 1, 1, 7, 8, 1, 1, 12, 8, 13, 12, 4, 1, 9, 4, 7, 9, 11, 8, 0, 7, 3, 9, 3, 13, 7, 4, 6, 5, 9, 1, 7, 3, 4, 12, 4, 2, 4, 11, 11, 11, 11, 11, 11, 11, 9, 11, 11, 1, 11, 4, 11, 3, 11, 3, 11, 4, 9, 3, 11, 13, 4, 17, 8, 17, 10, 0, 4, 12, 8, 10, 10, 2, 0, 1, 9, 16, 5, 14, 5, 8, 5, 10, 17, 8, 5, 3, 2, 5, 16, 17, 6, 8, 2, 2, 12, 5, 1, 16, 1, 16, 0, 8, 2, 17, 1, 2, 2, 12, 0, 4, 9, 17, 18, 15, 2, 6, 3, 11, 3, 3, 8, 2, 7, 9, 10, 15, 8, 9, 15, 16, 4, 10, 2, 15, 15, 4, 15, 1, 7, 15, 15, 13, 7, 15, 11, 15, 11, 4, 3, 3, 1, 7, 3, 3, 1, 1, 3, 1, 4, 5, 4, 3, 3, 4, 3, 5, 3, 2, 5, 14, 2, 8, 17, 7, 18, 1, 4, 5, 14, 3, 16, 12, 19, 16, 12, 2, 4, 9, 4, 18, 2, 13, 1, 9, 12, 10, 0, 12, 10, 8, 9, 4, 1, 1, 5, 4, 3, 0, 0, 2, 0, 0, 2, 18, 8, 1, 0, 17, 0, 5, 17, 17, 10, 15, 7, 10, 1, 3, 10, 17, 17, 7, 12, 17, 16, 3, 7, 12, 4, 5, 17, 5, 19, 9, 8, 19, 5, 6, 5, 18, 5, 3, 1, 15, 4, 8, 2, 14, 3, 11, 4, 3, 4, 3, 9, 6, 6, 10, 4, 16, 1, 0, 0, 2, 7, 2, 16, 2, 16, 10, 2, 0, 2, 7, 10, 0, 16, 12, 7, 7, 10, 12, 3, 16, 10, 9, 2, 17, 10, 13, 3, 5, 3, 6, 8, 8, 15, 6, 3, 6, 7, 15, 2, 4, 3, 1, 4, 4, 15, 11, 8, 10, 9, 3, 10, 7, 3, 9, 5, 4, 8, 11, 18, 7, 3, 10, 8, 0, 12, 16, 0, 2, 16, 0, 11, 17, 17, 5, 16, 19, 17, 0, 1, 8, 18, 6, 10, 11, 18, 18, 3, 6, 19, 6, 1, 18, 3, 9, 5, 16, 0, 14, 0, 12, 15, 3, 5, 0, 2, 12, 2, 8, 7, 1, 0, 2, 0, 0, 3, 4, 6, 4, 3, 16, 8, 9, 15, 6, 0, 2, 8, 3, 17, 17, 15, 7, 4, 8, 1, 14, 15, 15, 6, 6, 6, 11, 6, 1, 4, 2, 12, 4, 10, 1, 12, 6, 0, 8, 4, 12, 0, 5, 5, 6, 0, 1, 6, 14, 14, 14, 11, 7, 11, 11, 3, 0, 7, 11, 11, 1, 12, 0, 0, 9, 8, 10, 2, 9, 10, 10, 12, 18, 0, 10, 19, 8, 6, 6, 1, 10, 18, 3, 10, 5, 17, 5, 4, 19, 16, 10, 18, 5, 8, 10, 12, 3, 19, 19, 2, 11, 14, 1, 1, 11, 3, 18, 7, 1, 4, 9, 0, 1, 9, 6, 7, 9, 6, 7, 1, 16, 0, 15, 8, 1, 6, 6, 15, 1, 3, 6, 6, 6, 15, 7, 5, 1, 14, 8, 2, 12, 9, 8, 1, 0, 0, 14, 14, 14, 13, 12, 4, 5, 6, 11, 12, 4, 11, 11, 15, 0, 16, 11, 5, 17, 11, 19, 19, 0, 3, 13, 0, 14, 8, 0, 15, 10, 16, 14, 10, 1, 5, 1, 7, 12, 4, 14, 3, 5, 6, 9, 15, 6, 12, 6, 6, 18, 9, 15, 8, 10, 3, 4, 16, 15, 17, 18, 0, 1, 12, 12, 9, 12, 5, 7, 0, 7, 9, 3, 5, 7, 10, 9, 5, 1, 9, 6, 6, 6, 9, 10, 2, 11, 5, 11, 3, 2, 6, 7, 8, 6, 8, 6, 3, 6, 6, 6, 9, 6, 7, 17, 15, 19, 1, 11, 11, 6, 13, 9, 15, 3, 8, 8, 11, 11, 1, 3, 11, 0, 5, 11, 11, 9, 16, 16]\n",
            "-------RUN159-------\n",
            "{'Topic extraction': '<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>', 'Dimensionality Reduction': 'PCA(n_components=10)', 'Clustering': 'KMeans(n_clusters=20)', 'Vectorization': \"CountVectorizer(ngram_range=(2, 2), stop_words='english',\\n                tokenizer=<__main__.LemmaTokenizer object at 0x7df4a9008410>)\", 'Representation': 'KeyBERTInspired()'}\n",
            "BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7df4a91f6e50>, hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\n",
            "[2, 9, 6, 12, 2, 0, 6, 4, 9, 13, 3, 4, 4, 10, 0, 0, 4, 2, 14, 0, 2, 7, 4, 18, 18, 10, 7, 0, 0, 14, 0, 0, 2, 10, 10, 13, 6, 2, 5, 13, 18, 0, 14, 18, 7, 1, 7, 7, 18, 0, 14, 18, 6, 18, 12, 7, 1, 9, 3, 7, 3, 3, 6, 7, 0, 0, 11, 7, 2, 17, 10, 17, 16, 16, 3, 17, 3, 14, 3, 3, 1, 9, 9, 4, 9, 7, 1, 16, 9, 10, 16, 7, 1, 12, 6, 0, 10, 16, 10, 13, 11, 13, 6, 6, 6, 10, 0, 10, 14, 13, 17, 1, 7, 4, 9, 13, 7, 4, 3, 3, 13, 13, 18, 13, 7, 2, 1, 9, 18, 1, 9, 4, 0, 1, 8, 2, 6, 3, 1, 7, 13, 0, 7, 2, 4, 11, 2, 0, 9, 5, 8, 11, 4, 13, 14, 3, 4, 10, 6, 11, 14, 2, 10, 2, 0, 3, 10, 13, 10, 0, 10, 6, 9, 5, 9, 14, 3, 0, 0, 11, 13, 3, 13, 14, 17, 11, 6, 9, 17, 4, 4, 1, 1, 9, 10, 1, 11, 8, 1, 2, 1, 9, 0, 4, 13, 13, 9, 1, 10, 12, 4, 1, 9, 9, 4, 1, 1, 16, 9, 10, 4, 4, 4, 16, 13, 11, 7, 1, 4, 5, 5, 14, 5, 11, 4, 19, 2, 2, 8, 8, 14, 17, 17, 3, 14, 14, 0, 3, 0, 0, 3, 8, 12, 12, 14, 7, 1, 7, 16, 4, 1, 13, 0, 14, 0, 0, 10, 10, 6, 17, 17, 17, 4, 16, 17, 9, 17, 17, 10, 13, 4, 3, 17, 1, 1, 2, 7, 6, 6, 14, 12, 0, 2, 2, 7, 8, 13, 4, 4, 17, 17, 17, 7, 4, 11, 17, 11, 17, 2, 3, 18, 17, 7, 6, 0, 2, 3, 14, 13, 3, 11, 11, 6, 2, 14, 2, 6, 4, 3, 8, 3, 6, 7, 6, 8, 2, 4, 4, 11, 0, 8, 10, 10, 8, 11, 6, 4, 9, 5, 2, 0, 6, 0, 9, 10, 8, 2, 7, 4, 2, 3, 7, 0, 1, 7, 8, 4, 3, 7, 14, 14, 8, 8, 10, 10, 8, 5, 13, 6, 5, 19, 19, 0, 11, 4, 11, 7, 0, 1, 7, 8, 1, 3, 19, 19, 19, 19, 19, 19, 19, 2, 19, 8, 3, 8, 0, 19, 11, 5, 8, 7, 1, 0, 14, 14, 10, 2, 9, 14, 3, 6, 2, 13, 3, 14, 13, 3, 6, 4, 16, 4, 3, 16, 17, 16, 9, 16, 16, 16, 16, 11, 11, 4, 14, 2, 17, 0, 17, 4, 7, 10, 17, 17, 2, 11, 9, 17, 3, 17, 17, 11, 17, 14, 9, 8, 17, 17, 17, 17, 16, 16, 1, 0, 16, 11, 16, 17, 1, 2, 16, 4, 6, 16, 7, 2, 4, 2, 1, 9, 1, 6, 14, 13, 7, 8, 2, 11, 13, 12, 12, 12, 8, 12, 11, 4, 11, 8, 2, 12, 6, 12, 12, 12, 6, 12, 12, 12, 13, 7, 12, 8, 12, 6, 7, 12, 2, 18, 12, 4, 3, 12, 12, 7, 8, 11, 11, 0, 18, 1, 10, 10, 1, 11, 8, 17, 16, 7, 3, 1, 1, 9, 7, 7, 5, 5, 5, 11, 5, 10, 1, 9, 12, 11, 5, 5, 5, 5, 5, 11, 12, 5, 18, 18, 8, 8, 2, 0, 7, 4, 5, 5, 18, 12, 5, 7, 5, 7, 10, 6, 5, 5, 13, 13, 10, 11, 5, 3, 5, 5, 13, 0, 5, 13, 5, 5, 13, 19, 5, 10, 13, 10, 2, 7, 7, 7, 7, 6, 1, 6, 7, 7, 16, 11, 6, 3, 3, 6, 0, 3, 4, 1, 4, 1, 1, 7, 4, 1, 1, 4, 16, 16, 16, 11, 16, 2, 16, 6, 11, 3, 17, 4, 8, 12, 12, 0, 12, 5, 0, 14, 3, 12, 6, 12, 6, 12, 9, 12, 18, 11, 18, 9, 6, 13, 12, 13, 12, 0, 12, 6, 12, 2, 4, 12, 2, 0, 1, 1, 1, 1, 3, 1, 1, 3, 11, 1, 0, 1, 4, 1, 9, 2, 16, 4, 1, 5, 5, 13, 4, 3, 12, 0, 11, 0, 12, 5, 0, 9, 7, 5, 3, 0, 3, 5, 5, 2, 0, 10, 5, 5, 3, 5, 5, 18, 3, 8, 0, 8, 5, 0, 8, 11, 5, 10, 10, 5, 2, 4, 16, 6, 2, 2, 9, 7, 18, 3, 5, 15, 15, 5, 10, 14, 3, 4, 2, 6, 0, 19, 2, 19, 4, 16, 8, 9, 8, 6, 3, 11, 0, 4, 17, 17, 7, 1, 16, 7, 1, 16, 7, 1, 4, 1, 8, 6, 15, 15, 15, 15, 8, 0, 8, 15, 1, 15, 15, 15, 15, 3, 15, 15, 1, 14, 0, 16, 15, 12, 3, 15, 8, 15, 16, 0, 12, 12, 12, 3, 6, 12, 6, 12, 1, 14, 14, 3, 14, 14, 14, 9, 14, 7, 14, 13, 7, 17, 18, 9, 7, 19, 18, 1, 19, 19, 12, 8, 12, 2, 9, 19, 6, 19, 14, 19, 19, 19, 19, 3, 6, 2, 0, 18, 12, 10, 10, 10, 9, 0, 9, 2, 6, 0, 3, 2, 2, 9, 12, 10, 6, 6, 10, 0, 0, 6, 9, 0, 0, 1, 13, 12, 1, 3, 0, 11, 3, 6, 8, 15, 9, 1, 6, 7, 11, 2, 12, 0, 12, 11, 14, 12, 0, 6, 2, 3, 9, 14, 10, 4, 15, 15, 15, 15, 15, 15, 15, 2, 15, 15, 0, 15, 3, 15, 12, 15, 2, 15, 4, 2, 2, 15, 12, 4, 18, 9, 18, 14, 1, 3, 1, 9, 8, 8, 13, 1, 0, 6, 10, 14, 17, 14, 9, 14, 8, 18, 9, 3, 2, 11, 11, 10, 18, 5, 9, 7, 7, 4, 14, 0, 10, 0, 10, 1, 3, 4, 18, 0, 4, 13, 13, 1, 3, 2, 18, 11, 19, 13, 5, 2, 15, 2, 11, 9, 10, 6, 11, 8, 19, 9, 3, 12, 10, 3, 8, 10, 19, 12, 3, 19, 0, 6, 19, 19, 12, 6, 12, 15, 19, 15, 6, 2, 2, 0, 6, 2, 2, 0, 0, 2, 0, 3, 14, 4, 2, 2, 4, 6, 14, 14, 10, 6, 17, 7, 3, 6, 6, 16, 0, 3, 13, 17, 2, 10, 1, 16, 10, 8, 13, 4, 4, 3, 16, 7, 12, 0, 11, 8, 8, 1, 9, 8, 9, 4, 4, 0, 0, 13, 3, 2, 7, 1, 10, 7, 1, 7, 16, 9, 0, 7, 7, 1, 13, 18, 18, 8, 8, 6, 8, 0, 0, 4, 18, 18, 4, 1, 18, 10, 2, 6, 11, 3, 13, 18, 3, 16, 4, 9, 16, 14, 5, 14, 16, 11, 7, 0, 19, 3, 9, 13, 17, 15, 15, 4, 2, 3, 8, 8, 11, 5, 11, 4, 10, 0, 7, 1, 13, 6, 7, 10, 10, 10, 8, 10, 1, 13, 6, 8, 1, 10, 13, 13, 6, 8, 1, 11, 13, 8, 3, 2, 7, 4, 12, 2, 14, 7, 5, 9, 9, 19, 5, 2, 14, 6, 19, 11, 3, 2, 0, 4, 3, 8, 15, 9, 8, 2, 2, 8, 6, 2, 2, 13, 3, 0, 15, 16, 4, 11, 8, 9, 1, 1, 10, 1, 7, 10, 1, 15, 18, 18, 11, 10, 16, 18, 1, 3, 9, 11, 5, 5, 15, 16, 11, 11, 5, 16, 5, 0, 16, 2, 2, 3, 10, 7, 17, 1, 8, 2, 2, 14, 1, 7, 13, 10, 9, 6, 0, 1, 13, 1, 1, 14, 3, 5, 3, 2, 10, 3, 2, 2, 5, 7, 13, 3, 2, 18, 18, 12, 6, 6, 9, 6, 17, 19, 19, 5, 5, 5, 15, 5, 3, 4, 7, 4, 4, 8, 0, 1, 5, 1, 9, 4, 8, 1, 14, 3, 5, 1, 0, 5, 17, 17, 17, 15, 6, 15, 15, 2, 1, 4, 15, 15, 0, 1, 7, 1, 2, 9, 8, 7, 2, 8, 8, 1, 16, 1, 11, 16, 9, 5, 5, 0, 11, 16, 2, 8, 14, 18, 13, 3, 16, 10, 4, 16, 3, 0, 5, 4, 6, 16, 16, 7, 15, 17, 0, 0, 15, 2, 16, 0, 0, 4, 6, 1, 0, 4, 5, 13, 8, 5, 6, 0, 10, 1, 8, 9, 0, 5, 14, 12, 0, 2, 5, 5, 5, 19, 6, 14, 0, 17, 9, 13, 4, 4, 9, 0, 1, 1, 17, 17, 17, 12, 1, 4, 3, 5, 15, 9, 3, 15, 15, 19, 1, 10, 15, 14, 14, 15, 16, 16, 7, 2, 12, 7, 17, 9, 1, 8, 4, 10, 17, 5, 0, 3, 0, 6, 4, 4, 17, 2, 13, 5, 12, 19, 5, 11, 11, 5, 11, 2, 19, 9, 8, 2, 3, 0, 19, 18, 16, 1, 0, 11, 1, 11, 11, 14, 6, 1, 6, 11, 14, 14, 6, 8, 2, 13, 0, 2, 5, 5, 5, 8, 11, 7, 15, 13, 15, 2, 13, 5, 6, 3, 5, 3, 11, 13, 5, 5, 5, 4, 5, 6, 18, 19, 16, 0, 15, 15, 5, 12, 4, 12, 2, 9, 9, 15, 15, 6, 13, 15, 1, 14, 8, 15, 2, 5, 10]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df_metrics1\",\n  \"rows\": 149,\n  \"fields\": [\n    {\n      \"column\": \"Run\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 46.80418258389171,\n        \"min\": 1.0,\n        \"max\": 160.0,\n        \"num_unique_values\": 149,\n        \"samples\": [\n          78.0,\n          21.0,\n          129.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 149,\n        \"samples\": [\n          \"BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<bertopic.backend._sentencetransformers.SentenceTransformerBackend object at 0x7df359de5810>, hdbscan_model=KMeans(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\",\n          \"BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<bertopic.backend._sentencetransformers.SentenceTransformerBackend object at 0x7df35af05950>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=UMAP(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\",\n          \"BERTopic(calculate_probabilities=True, ctfidf_model=ClassTfidfTransformer(...), embedding_model=<bertopic.backend._use.USEBackend object at 0x7df35b8254d0>, hdbscan_model=HDBSCAN(...), language=None, low_memory=True, min_topic_size=10, n_gram_range=(1, 1), nr_topics=None, representation_model=KeyBERTInspired(...), seed_topic_list=None, top_n_words=10, umap_model=PCA(...), vectorizer_model=CountVectorizer(...), verbose=False, zeroshot_min_similarity=0.7, zeroshot_topic_list=None)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"u_mass_coherence\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2582827337718019,\n        \"min\": -0.8791316046419374,\n        \"max\": 1.2000844762374166e-12,\n        \"num_unique_values\": 142,\n        \"samples\": [\n          -0.17016561118453516,\n          -0.7334666748863037,\n          -0.5411911611101137\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"c_v_coherence\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07703048633041497,\n        \"min\": 0.2800683347652146,\n        \"max\": 0.6342199903892216,\n        \"num_unique_values\": 149,\n        \"samples\": [\n          0.4053470029377184,\n          0.43298867609288694,\n          0.5335068199343356\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df_metrics1"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-f46dac10-cce9-43e5-a6ad-f0bab8ea7f62\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Run</th>\n",
              "      <th>Model</th>\n",
              "      <th>u_mass_coherence</th>\n",
              "      <th>c_v_coherence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>BERTopic(calculate_probabilities=True, ctfidf_...</td>\n",
              "      <td>-0.817523</td>\n",
              "      <td>0.464426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>BERTopic(calculate_probabilities=True, ctfidf_...</td>\n",
              "      <td>-0.621999</td>\n",
              "      <td>0.361650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.0</td>\n",
              "      <td>BERTopic(calculate_probabilities=True, ctfidf_...</td>\n",
              "      <td>-0.786082</td>\n",
              "      <td>0.466844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.0</td>\n",
              "      <td>BERTopic(calculate_probabilities=True, ctfidf_...</td>\n",
              "      <td>-0.709494</td>\n",
              "      <td>0.497112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>BERTopic(calculate_probabilities=True, ctfidf_...</td>\n",
              "      <td>-0.221516</td>\n",
              "      <td>0.478240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>156.0</td>\n",
              "      <td>BERTopic(calculate_probabilities=True, ctfidf_...</td>\n",
              "      <td>-0.672964</td>\n",
              "      <td>0.367068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>157.0</td>\n",
              "      <td>BERTopic(calculate_probabilities=True, ctfidf_...</td>\n",
              "      <td>-0.491039</td>\n",
              "      <td>0.400427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>158.0</td>\n",
              "      <td>BERTopic(calculate_probabilities=True, ctfidf_...</td>\n",
              "      <td>-0.625908</td>\n",
              "      <td>0.376987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>159.0</td>\n",
              "      <td>BERTopic(calculate_probabilities=True, ctfidf_...</td>\n",
              "      <td>-0.592832</td>\n",
              "      <td>0.469385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>160.0</td>\n",
              "      <td>BERTopic(calculate_probabilities=True, ctfidf_...</td>\n",
              "      <td>-0.325151</td>\n",
              "      <td>0.360818</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>149 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f46dac10-cce9-43e5-a6ad-f0bab8ea7f62')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f46dac10-cce9-43e5-a6ad-f0bab8ea7f62 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f46dac10-cce9-43e5-a6ad-f0bab8ea7f62');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-00f487dc-2a81-4abe-b974-91685eeb75ba\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-00f487dc-2a81-4abe-b974-91685eeb75ba')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-00f487dc-2a81-4abe-b974-91685eeb75ba button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_a140ae99-7e32-4534-878a-1391f319fd67\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_metrics1')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_a140ae99-7e32-4534-878a-1391f319fd67 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_metrics1');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "       Run                                              Model  \\\n",
              "0      1.0  BERTopic(calculate_probabilities=True, ctfidf_...   \n",
              "1      2.0  BERTopic(calculate_probabilities=True, ctfidf_...   \n",
              "2      3.0  BERTopic(calculate_probabilities=True, ctfidf_...   \n",
              "3      4.0  BERTopic(calculate_probabilities=True, ctfidf_...   \n",
              "4      5.0  BERTopic(calculate_probabilities=True, ctfidf_...   \n",
              "..     ...                                                ...   \n",
              "144  156.0  BERTopic(calculate_probabilities=True, ctfidf_...   \n",
              "145  157.0  BERTopic(calculate_probabilities=True, ctfidf_...   \n",
              "146  158.0  BERTopic(calculate_probabilities=True, ctfidf_...   \n",
              "147  159.0  BERTopic(calculate_probabilities=True, ctfidf_...   \n",
              "148  160.0  BERTopic(calculate_probabilities=True, ctfidf_...   \n",
              "\n",
              "     u_mass_coherence  c_v_coherence  \n",
              "0           -0.817523       0.464426  \n",
              "1           -0.621999       0.361650  \n",
              "2           -0.786082       0.466844  \n",
              "3           -0.709494       0.497112  \n",
              "4           -0.221516       0.478240  \n",
              "..                ...            ...  \n",
              "144         -0.672964       0.367068  \n",
              "145         -0.491039       0.400427  \n",
              "146         -0.625908       0.376987  \n",
              "147         -0.592832       0.469385  \n",
              "148         -0.325151       0.360818  \n",
              "\n",
              "[149 rows x 4 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "i=0\n",
        "metrics = {\n",
        "    'Run': [],\n",
        "    'Model': [],\n",
        "    'u_mass_coherence': [],\n",
        "    'c_v_coherence': []\n",
        "}\n",
        "\n",
        "df_metrics1 = pd.DataFrame(metrics)\n",
        "for p in pipelines:\n",
        "\n",
        "  print(f'-------RUN{i}-------')\n",
        "  print(meta[i])\n",
        "  i=i+1\n",
        "  print(p)\n",
        "  topics, _ = p.fit_transform(comments)\n",
        "\n",
        "  coherence_metrics=calculate_coherence_metrics(p,comments,topics)\n",
        "\n",
        "  df_metrics1 = pd.concat([df_metrics1, pd.DataFrame([{'Run':i,'Model':p,'u_mass_coherence': coherence_metrics['u_mass_coherence'],'c_v_coherence': coherence_metrics['c_v_coherence']}])], ignore_index=True)\n",
        "df_metrics1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2p3w085-B2rd"
      },
      "outputs": [],
      "source": [
        "df_metrics1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xg5vf9De_OHU"
      },
      "outputs": [],
      "source": [
        "print(meta[4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_yHZqvB_cgp"
      },
      "outputs": [],
      "source": [
        "mymodel=df_metrics1.iloc[4][\"Model\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8pjqtMRUQHV"
      },
      "outputs": [],
      "source": [
        "df_metrics1.sort_values(by=['c_v_coherence'], ascending=False).head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8MJ8mTBC1zJ"
      },
      "source": [
        "From the above results we select the model with the highest c_v_coherence and with a mass_coherence close to zero. choosing the best configuration is a tradeoff between those metrics but the final decision is based human judgement after data visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "PSru_B7-Ckcj",
        "outputId": "69440b46-9f7e-4f0e-d4e9-d15b0943998a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-3 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-3 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-3 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-3 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-3 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-3 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-3 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-3 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-3 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-3 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-3 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-3 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HDBSCAN(min_cluster_size=10, prediction_data=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>HDBSCAN</div></div><div><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>HDBSCAN(min_cluster_size=10, prediction_data=True)</pre></div> </div></div></div></div>"
            ],
            "text/plain": [
              "HDBSCAN(min_cluster_size=10, prediction_data=True)"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mymodel.hdbscan_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNKibYH3BfPn"
      },
      "outputs": [],
      "source": [
        "mymodel.min_topic_size=15\n",
        "mymodel.fit_transform(comments)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dboH4WQlBvma"
      },
      "source": [
        "## Topics visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "unBE9b0FHvYy",
        "outputId": "d6964a17-6fe5-4ddd-f51e-da16fde5a565"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"mymodel\",\n  \"rows\": 1552,\n  \"fields\": [\n    {\n      \"column\": \"Document\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1552,\n        \"samples\": [\n          \"excellent  excellent advice as patients  we feel awkward questioning or secondguessing a doctor  or even getting a second opinion while some doctors get annoyed  asking questions and getting secondopinions are essential to getting a good diagnosis  figuring out a good treatment  and getting good care  unfortunately  even with insurance  doing all of that can be hard making appointments is difficult  as the economics of healthcare encourages doctors to overbook and to spend as little time as possible with patients many doctors do nt take any insurance or have moved into extremely expensive vip practices  and others  who do take insurance  wo nt see patients who do nt have the particular insurance that the doctor accepts  moreover  finding reliable data and ratings on doctors is nearly impossible  people will say   just google the doctor   but that rarely  if ever  gives reliable results  even once you scroll down below the paidemptyform web sites that inevitably come up first in those searches  and doing all of this while sick is even harder  but completely agree with op s point \",\n          \"i do nt get step 3 before you ship how much  insurance  i get is the declared value  essentially  i m paying for my item again with ups \",\n          \"awesome  i ll remember this next time i m in the 1990s \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Topic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10,\n        \"min\": -1,\n        \"max\": 34,\n        \"num_unique_values\": 36,\n        \"samples\": [\n          24,\n          14,\n          29\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 36,\n        \"samples\": [\n          \"24_good_check_used_nt\",\n          \"14_essentially_say_customer_provide\",\n          \"29_use_cell phone_research_used\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Representation\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Representative_Docs\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Top_n_words\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 36,\n        \"samples\": [\n          \"good - check - used - nt - checked - better - sure - near - went - want\",\n          \"essentially - say - customer - provide - important - charge - liability - bigger box - like - ship\",\n          \"use - cell phone - research - used - result wa - example - practical - gt researcher - researcher - method\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Probability\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.34477609691571803,\n        \"min\": 0.010653537268671633,\n        \"max\": 1.0,\n        \"num_unique_values\": 1157,\n        \"samples\": [\n          0.2264883746677671,\n          0.05925329442316865,\n          0.7853118885501224\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Representative_document\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-254afb71-e928-4480-94aa-6576419cd246\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Document</th>\n",
              "      <th>Topic</th>\n",
              "      <th>Name</th>\n",
              "      <th>Representation</th>\n",
              "      <th>Representative_Docs</th>\n",
              "      <th>Top_n_words</th>\n",
              "      <th>Probability</th>\n",
              "      <th>Representative_document</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>informative post and truly helpful it s easy t...</td>\n",
              "      <td>5</td>\n",
              "      <td>5_wwwsrhnoaagovffc_http wwwsrhnoaagovffc_help ...</td>\n",
              "      <td>[wwwsrhnoaagovffc, http wwwsrhnoaagovffc, help...</td>\n",
              "      <td>[with the impending northeast storm  there s a...</td>\n",
              "      <td>wwwsrhnoaagovffc - http wwwsrhnoaagovffc - hel...</td>\n",
              "      <td>0.532638</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>actually that is such a good tip it s not what...</td>\n",
              "      <td>1</td>\n",
              "      <td>1_rbreadit good_rbreadit_post written_thanks tip</td>\n",
              "      <td>[rbreadit good, rbreadit, post written, thanks...</td>\n",
              "      <td>[thanks op , i totally forgot goggle my drive ...</td>\n",
              "      <td>rbreadit good - rbreadit - post written - than...</td>\n",
              "      <td>0.462085</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>do nt put it in the fridge with the peel on   ...</td>\n",
              "      <td>28</td>\n",
              "      <td>28_peel turn_peel way_s_overripening spoiling</td>\n",
              "      <td>[peel turn, peel way, s, overripening spoiling...</td>\n",
              "      <td>[ysk ozone destroy ethylene gas helping keep f...</td>\n",
              "      <td>peel turn - peel way - s - overripening spoili...</td>\n",
              "      <td>0.165413</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>this probably wo nt come up that often  but i ...</td>\n",
              "      <td>28</td>\n",
              "      <td>28_peel turn_peel way_s_overripening spoiling</td>\n",
              "      <td>[peel turn, peel way, s, overripening spoiling...</td>\n",
              "      <td>[ysk ozone destroy ethylene gas helping keep f...</td>\n",
              "      <td>peel turn - peel way - s - overripening spoili...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>just in general with usbc be careful what you ...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1_advice_try_remindme_scrap</td>\n",
              "      <td>[advice, try, remindme, scrap, cheap, copy, us...</td>\n",
              "      <td>[just as there is a difference between the com...</td>\n",
              "      <td>advice - try - remindme - scrap - cheap - copy...</td>\n",
              "      <td>0.412432</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1547</th>\n",
              "      <td>so if 73  are nt installed properly  is there ...</td>\n",
              "      <td>24</td>\n",
              "      <td>24_good_check_used_nt</td>\n",
              "      <td>[good, check, used, nt, checked, better, sure,...</td>\n",
              "      <td>[also  for children around two and older   a p...</td>\n",
              "      <td>good - check - used - nt - checked - better - ...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1548</th>\n",
              "      <td>certified child passenger safety technician he...</td>\n",
              "      <td>24</td>\n",
              "      <td>24_good_check_used_nt</td>\n",
              "      <td>[good, check, used, nt, checked, better, sure,...</td>\n",
              "      <td>[also  for children around two and older   a p...</td>\n",
              "      <td>good - check - used - nt - checked - better - ...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1549</th>\n",
              "      <td>or you can vent all your problems right here o...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1_advice_try_remindme_scrap</td>\n",
              "      <td>[advice, try, remindme, scrap, cheap, copy, us...</td>\n",
              "      <td>[just as there is a difference between the com...</td>\n",
              "      <td>advice - try - remindme - scrap - cheap - copy...</td>\n",
              "      <td>0.719516</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1550</th>\n",
              "      <td>i think this is usa only  yes it s similar to ...</td>\n",
              "      <td>4</td>\n",
              "      <td>4_inactive fix_noticed_download_question</td>\n",
              "      <td>[inactive fix, noticed, download, question, gr...</td>\n",
              "      <td>[ kiwix   http  wwwkiwixorgwikimain_page  is a...</td>\n",
              "      <td>inactive fix - noticed - download - question -...</td>\n",
              "      <td>0.039347</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1551</th>\n",
              "      <td>http  wwwcrisistextlineorg designed for teens ...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1_advice_try_remindme_scrap</td>\n",
              "      <td>[advice, try, remindme, scrap, cheap, copy, us...</td>\n",
              "      <td>[just as there is a difference between the com...</td>\n",
              "      <td>advice - try - remindme - scrap - cheap - copy...</td>\n",
              "      <td>0.208780</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1552 rows × 8 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-254afb71-e928-4480-94aa-6576419cd246')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-254afb71-e928-4480-94aa-6576419cd246 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-254afb71-e928-4480-94aa-6576419cd246');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-99ab6f1a-646d-4825-8fa7-85ad39d3a304\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-99ab6f1a-646d-4825-8fa7-85ad39d3a304')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-99ab6f1a-646d-4825-8fa7-85ad39d3a304 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                               Document  Topic  \\\n",
              "0     informative post and truly helpful it s easy t...      5   \n",
              "1     actually that is such a good tip it s not what...      1   \n",
              "2     do nt put it in the fridge with the peel on   ...     28   \n",
              "3     this probably wo nt come up that often  but i ...     28   \n",
              "4     just in general with usbc be careful what you ...     -1   \n",
              "...                                                 ...    ...   \n",
              "1547  so if 73  are nt installed properly  is there ...     24   \n",
              "1548  certified child passenger safety technician he...     24   \n",
              "1549  or you can vent all your problems right here o...     -1   \n",
              "1550  i think this is usa only  yes it s similar to ...      4   \n",
              "1551  http  wwwcrisistextlineorg designed for teens ...     -1   \n",
              "\n",
              "                                                   Name  \\\n",
              "0     5_wwwsrhnoaagovffc_http wwwsrhnoaagovffc_help ...   \n",
              "1      1_rbreadit good_rbreadit_post written_thanks tip   \n",
              "2         28_peel turn_peel way_s_overripening spoiling   \n",
              "3         28_peel turn_peel way_s_overripening spoiling   \n",
              "4                          -1_advice_try_remindme_scrap   \n",
              "...                                                 ...   \n",
              "1547                              24_good_check_used_nt   \n",
              "1548                              24_good_check_used_nt   \n",
              "1549                       -1_advice_try_remindme_scrap   \n",
              "1550           4_inactive fix_noticed_download_question   \n",
              "1551                       -1_advice_try_remindme_scrap   \n",
              "\n",
              "                                         Representation  \\\n",
              "0     [wwwsrhnoaagovffc, http wwwsrhnoaagovffc, help...   \n",
              "1     [rbreadit good, rbreadit, post written, thanks...   \n",
              "2     [peel turn, peel way, s, overripening spoiling...   \n",
              "3     [peel turn, peel way, s, overripening spoiling...   \n",
              "4     [advice, try, remindme, scrap, cheap, copy, us...   \n",
              "...                                                 ...   \n",
              "1547  [good, check, used, nt, checked, better, sure,...   \n",
              "1548  [good, check, used, nt, checked, better, sure,...   \n",
              "1549  [advice, try, remindme, scrap, cheap, copy, us...   \n",
              "1550  [inactive fix, noticed, download, question, gr...   \n",
              "1551  [advice, try, remindme, scrap, cheap, copy, us...   \n",
              "\n",
              "                                    Representative_Docs  \\\n",
              "0     [with the impending northeast storm  there s a...   \n",
              "1     [thanks op , i totally forgot goggle my drive ...   \n",
              "2     [ysk ozone destroy ethylene gas helping keep f...   \n",
              "3     [ysk ozone destroy ethylene gas helping keep f...   \n",
              "4     [just as there is a difference between the com...   \n",
              "...                                                 ...   \n",
              "1547  [also  for children around two and older   a p...   \n",
              "1548  [also  for children around two and older   a p...   \n",
              "1549  [just as there is a difference between the com...   \n",
              "1550  [ kiwix   http  wwwkiwixorgwikimain_page  is a...   \n",
              "1551  [just as there is a difference between the com...   \n",
              "\n",
              "                                            Top_n_words  Probability  \\\n",
              "0     wwwsrhnoaagovffc - http wwwsrhnoaagovffc - hel...     0.532638   \n",
              "1     rbreadit good - rbreadit - post written - than...     0.462085   \n",
              "2     peel turn - peel way - s - overripening spoili...     0.165413   \n",
              "3     peel turn - peel way - s - overripening spoili...     1.000000   \n",
              "4     advice - try - remindme - scrap - cheap - copy...     0.412432   \n",
              "...                                                 ...          ...   \n",
              "1547  good - check - used - nt - checked - better - ...     1.000000   \n",
              "1548  good - check - used - nt - checked - better - ...     1.000000   \n",
              "1549  advice - try - remindme - scrap - cheap - copy...     0.719516   \n",
              "1550  inactive fix - noticed - download - question -...     0.039347   \n",
              "1551  advice - try - remindme - scrap - cheap - copy...     0.208780   \n",
              "\n",
              "      Representative_document  \n",
              "0                       False  \n",
              "1                       False  \n",
              "2                       False  \n",
              "3                        True  \n",
              "4                       False  \n",
              "...                       ...  \n",
              "1547                    False  \n",
              "1548                    False  \n",
              "1549                    False  \n",
              "1550                    False  \n",
              "1551                    False  \n",
              "\n",
              "[1552 rows x 8 columns]"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mymodel.get_document_info(comments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        },
        "id": "HYvPk3uzFwad",
        "outputId": "4f127f90-622c-4d17-eb6d-729a99e7c79b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"013b364b-2ba0-43db-9672-6f199dacd7b7\" class=\"plotly-graph-div\" style=\"height:650px; width:650px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"013b364b-2ba0-43db-9672-6f199dacd7b7\")) {                    Plotly.newPlot(                        \"013b364b-2ba0-43db-9672-6f199dacd7b7\",                        [{\"customdata\":[[0,\"print | http | simple | know | mail\",122],[1,\"rbreadit good | rbreadit | post written | thanks tip | did\",122],[2,\"nt | nt know | doe | issue | use\",85],[3,\"help | need | understand | month | make mistake\",75],[4,\"inactive fix | noticed | download | question | great feature\",68],[5,\"wwwsrhnoaagovffc | http wwwsrhnoaagovffc | help clear | trouble | help\",53],[6,\"realtime | email | free | important | realworld\",42],[7,\"nt use | nt | issue | use | fine\",41],[8,\"just | app | folder | restore | version\",40],[9,\"digit number | add digit | digit | digit time | 2 digit\",37],[10,\"virus | worked | thing | faster | use\",36],[11,\"just search | searched | search | search did | webpage available\",33],[12,\"justtrust awesome | justtrust | hot justtrust | little shop | doe work\",33],[13,\"payment | insurance | legal | health insurance | government\",32],[14,\"essentially | say | customer | provide | important\",31],[15,\"ram | free | saved 300 | ssd | bought\",28],[16,\"online storage | checkup 2gb | extra gb | extra 2gb | http wwwgooglecomsettingsstorage\",28],[17,\"using | used | friend | wa | list\",27],[18,\"nt pay | google | phone number | http | did nt\",27],[19,\"text | ve | kid | v | zork\",27],[20,\"lost | age | need | nt | home nt\",26],[21,\"possible question | question googled | question | man | policy open\",24],[22,\"nt | nt nutrient | let recommend | stuff | link\",22],[23,\"need rid | file | permission | hide | remove\",22],[24,\"good | check | used | nt | checked\",22],[25,\"missing | use | month | download | second\",21],[26,\"life change | ll end | invalidates | kid die | having\",19],[27,\"internet said | service aka | need | news | stuff\",19],[28,\"peel turn | peel way | s | overripening spoiling | fridge\",18],[29,\"use | cell phone | research | used | result wa\",17],[30,\"check | apply | actually | know | say\",17],[31,\"mind posted | baby | monolike | case | doner\",16],[32,\"question got | site fervently | ll know | lol like | seminar lol\",15],[33,\"mega | mb mega | kilo mega | example dsl | dsl use\",13],[34,\"site exist | delete | nah site | website ve | site check\",13]],\"hovertemplate\":\"\\u003cb\\u003eTopic %{customdata[0]}\\u003c\\u002fb\\u003e\\u003cbr\\u003e%{customdata[1]}\\u003cbr\\u003eSize: %{customdata[2]}\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#B0BEC5\",\"size\":[122,122,85,75,68,53,42,41,40,37,36,33,33,32,31,28,28,27,27,27,26,24,22,22,22,21,19,19,18,17,17,16,15,13,13],\"sizemode\":\"area\",\"sizeref\":0.07625,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":2}},\"mode\":\"markers\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[7.18176,-0.1240732,18.24572,-0.94479907,-1.2652429,-0.033899773,8.082453,26.002281,-3.443027,-2.6124444,17.816607,-1.3297935,-0.43443668,-1.4158138,9.50955,-3.6228235,-3.680728,17.817059,-1.573151,0.16884302,18.379154,-0.38808468,26.203482,-1.5987507,18.125702,-2.9716058,7.181835,9.510012,26.28516,-0.87156236,8.082504,-1.1312752,0.03589333,-2.709951,-1.8009125],\"xaxis\":\"x\",\"y\":[18.674883,5.7313776,4.246457,25.053425,6.7007117,6.1182623,0.33861014,-1.4807154,1.4547622,2.2703145,-1.6776853,6.3721066,5.976317,25.52238,15.919747,1.2744205,1.2148182,-1.6915226,25.679523,6.587021,4.3797097,6.5980573,-1.2798357,6.4755163,4.126305,1.9111378,18.67481,15.919215,-1.1980438,24.979689,0.33858183,25.240316,5.988061,2.172173,6.686012],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"rgb(36,36,36)\"},\"error_y\":{\"color\":\"rgb(36,36,36)\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"baxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.6}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"rgb(237,237,237)\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"rgb(217,217,217)\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"colorscale\":{\"diverging\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"sequential\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"sequentialminus\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]]},\"colorway\":[\"#1F77B4\",\"#FF7F0E\",\"#2CA02C\",\"#D62728\",\"#9467BD\",\"#8C564B\",\"#E377C2\",\"#7F7F7F\",\"#BCBD22\",\"#17BECF\"],\"font\":{\"color\":\"rgb(36,36,36)\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}},\"shapedefaults\":{\"fillcolor\":\"black\",\"line\":{\"width\":0},\"opacity\":0.3},\"ternary\":{\"aaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"baxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"\"},\"visible\":false,\"range\":[-4.232837152481079,30.227934074401855]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"\"},\"visible\":false,\"range\":[-1.9452509880065918,29.531451988220216]},\"legend\":{\"tracegroupgap\":0,\"itemsizing\":\"constant\"},\"margin\":{\"t\":60},\"title\":{\"font\":{\"size\":22,\"color\":\"Black\"},\"text\":\"\\u003cb\\u003eIntertopic Distance Map\\u003c\\u002fb\\u003e\",\"y\":0.95,\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"hoverlabel\":{\"font\":{\"size\":16,\"family\":\"Rockwell\"},\"bgcolor\":\"white\"},\"width\":650,\"height\":650,\"sliders\":[{\"active\":0,\"pad\":{\"t\":50},\"steps\":[{\"args\":[{\"marker.color\":[[\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 0\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 1\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 2\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 3\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 4\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 5\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 6\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 7\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 8\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 9\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 10\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 11\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 12\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 13\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 14\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 15\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 16\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 17\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 18\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 19\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 20\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 21\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 22\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 23\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 24\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 25\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 26\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 27\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 28\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 29\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 30\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 31\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\",\"#B0BEC5\"]]}],\"label\":\"Topic 32\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\",\"#B0BEC5\"]]}],\"label\":\"Topic 33\",\"method\":\"update\"},{\"args\":[{\"marker.color\":[[\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"#B0BEC5\",\"red\"]]}],\"label\":\"Topic 34\",\"method\":\"update\"}]}],\"shapes\":[{\"line\":{\"color\":\"#CFD8DC\",\"width\":2},\"type\":\"line\",\"x0\":12.997548460960388,\"x1\":12.997548460960388,\"y0\":-1.9452509880065918,\"y1\":29.531451988220216},{\"line\":{\"color\":\"#9E9E9E\",\"width\":2},\"type\":\"line\",\"x0\":-4.232837152481079,\"x1\":30.227934074401855,\"y0\":13.793100500106812,\"y1\":13.793100500106812}],\"annotations\":[{\"showarrow\":false,\"text\":\"D1\",\"x\":-4.232837152481079,\"y\":13.793100500106812,\"yshift\":10},{\"showarrow\":false,\"text\":\"D2\",\"x\":12.997548460960388,\"xshift\":10,\"y\":29.531451988220216}]},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('013b364b-2ba0-43db-9672-6f199dacd7b7');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "mymodel.visualize_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "id": "yEczEqoLYLC-",
        "outputId": "2cb94775-39b5-451c-f8f9-99761ec4405d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"2762839d-7d24-4f04-b0d7-cb3278a4c29f\" class=\"plotly-graph-div\" style=\"height:750px; width:1000px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"2762839d-7d24-4f04-b0d7-cb3278a4c29f\")) {                    Plotly.newPlot(                        \"2762839d-7d24-4f04-b0d7-cb3278a4c29f\",                        [{\"marker\":{\"color\":\"#D55E00\"},\"orientation\":\"h\",\"x\":[0.19376492500305176,0.19493891298770905,0.19971004128456116,0.21800631284713745,0.22746668756008148],\"y\":[\"mail  \",\"know  \",\"simple  \",\"http  \",\"print  \"],\"type\":\"bar\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"marker\":{\"color\":\"#0072B2\"},\"orientation\":\"h\",\"x\":[0.24192333221435547,0.24980655312538147,0.2698506712913513,0.31664326786994934,0.330261766910553],\"y\":[\"did  \",\"thanks tip  \",\"post written  \",\"rbreadit  \",\"rbreadit good  \"],\"type\":\"bar\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"marker\":{\"color\":\"#CC79A7\"},\"orientation\":\"h\",\"x\":[0.4484441876411438,0.4535982310771942,0.4601718783378601,0.47984129190444946,0.49026811122894287],\"y\":[\"use  \",\"issue  \",\"doe  \",\"nt know  \",\"nt  \"],\"type\":\"bar\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"marker\":{\"color\":\"#E69F00\"},\"orientation\":\"h\",\"x\":[0.19313237071037292,0.193812757730484,0.19409456849098206,0.1967093050479889,0.25279247760772705],\"y\":[\"make mistake  \",\"month  \",\"understand  \",\"need  \",\"help  \"],\"type\":\"bar\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"marker\":{\"color\":\"#56B4E9\"},\"orientation\":\"h\",\"x\":[0.3141576945781708,0.31587424874305725,0.3386147618293762,0.37410789728164673,0.4034113883972168],\"y\":[\"great feature  \",\"question  \",\"download  \",\"noticed  \",\"inactive fix  \"],\"type\":\"bar\",\"xaxis\":\"x5\",\"yaxis\":\"y5\"},{\"marker\":{\"color\":\"#009E73\"},\"orientation\":\"h\",\"x\":[0.35281410813331604,0.36450865864753723,0.37398967146873474,0.3788403570652008,0.3835133910179138],\"y\":[\"help  \",\"trouble  \",\"help clear  \",\"http wwwsrhnoaagovffc  \",\"wwwsrhnoaagovffc  \"],\"type\":\"bar\",\"xaxis\":\"x6\",\"yaxis\":\"y6\"},{\"marker\":{\"color\":\"#F0E442\"},\"orientation\":\"h\",\"x\":[0.2331336885690689,0.24091580510139465,0.24111893773078918,0.25994962453842163,0.28008708357810974],\"y\":[\"realworld  \",\"important  \",\"free  \",\"email  \",\"realtime  \"],\"type\":\"bar\",\"xaxis\":\"x7\",\"yaxis\":\"y7\"},{\"marker\":{\"color\":\"#D55E00\"},\"orientation\":\"h\",\"x\":[0.27636826038360596,0.2894153594970703,0.33315062522888184,0.34679535031318665,0.3557037115097046],\"y\":[\"fine  \",\"use  \",\"issue  \",\"nt  \",\"nt use  \"],\"type\":\"bar\",\"xaxis\":\"x8\",\"yaxis\":\"y8\"},{\"marker\":{\"color\":\"#0072B2\"},\"orientation\":\"h\",\"x\":[0.22498854994773865,0.2405199110507965,0.242908775806427,0.2472064197063446,0.2591313123703003],\"y\":[\"version  \",\"restore  \",\"folder  \",\"app  \",\"just  \"],\"type\":\"bar\",\"xaxis\":\"x9\",\"yaxis\":\"y9\"},{\"marker\":{\"color\":\"#CC79A7\"},\"orientation\":\"h\",\"x\":[0.3643900752067566,0.3773782551288605,0.39027267694473267,0.39357423782348633,0.39825356006622314],\"y\":[\"2 digit  \",\"digit time  \",\"digit  \",\"add digit  \",\"digit number  \"],\"type\":\"bar\",\"xaxis\":\"x10\",\"yaxis\":\"y10\"},{\"marker\":{\"color\":\"#E69F00\"},\"orientation\":\"h\",\"x\":[0.1968013495206833,0.20210543274879456,0.20745085179805756,0.20970924198627472,0.2284919023513794],\"y\":[\"use  \",\"faster  \",\"thing  \",\"worked  \",\"virus  \"],\"type\":\"bar\",\"xaxis\":\"x11\",\"yaxis\":\"y11\"},{\"marker\":{\"color\":\"#56B4E9\"},\"orientation\":\"h\",\"x\":[0.4098166525363922,0.40987831354141235,0.42322856187820435,0.4248217046260834,0.44270196557044983],\"y\":[\"webpage available  \",\"search did  \",\"search  \",\"searched  \",\"just search  \"],\"type\":\"bar\",\"xaxis\":\"x12\",\"yaxis\":\"y12\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.175],\"showgrid\":true},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.7555555555555555,1.0],\"showgrid\":true},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.275,0.45],\"showgrid\":true},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.7555555555555555,1.0],\"showgrid\":true},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.55,0.7250000000000001],\"showgrid\":true},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.7555555555555555,1.0],\"showgrid\":true},\"xaxis4\":{\"anchor\":\"y4\",\"domain\":[0.825,1.0],\"showgrid\":true},\"yaxis4\":{\"anchor\":\"x4\",\"domain\":[0.7555555555555555,1.0],\"showgrid\":true},\"xaxis5\":{\"anchor\":\"y5\",\"domain\":[0.0,0.175],\"showgrid\":true},\"yaxis5\":{\"anchor\":\"x5\",\"domain\":[0.37777777777777777,0.6222222222222222],\"showgrid\":true},\"xaxis6\":{\"anchor\":\"y6\",\"domain\":[0.275,0.45],\"showgrid\":true},\"yaxis6\":{\"anchor\":\"x6\",\"domain\":[0.37777777777777777,0.6222222222222222],\"showgrid\":true},\"xaxis7\":{\"anchor\":\"y7\",\"domain\":[0.55,0.7250000000000001],\"showgrid\":true},\"yaxis7\":{\"anchor\":\"x7\",\"domain\":[0.37777777777777777,0.6222222222222222],\"showgrid\":true},\"xaxis8\":{\"anchor\":\"y8\",\"domain\":[0.825,1.0],\"showgrid\":true},\"yaxis8\":{\"anchor\":\"x8\",\"domain\":[0.37777777777777777,0.6222222222222222],\"showgrid\":true},\"xaxis9\":{\"anchor\":\"y9\",\"domain\":[0.0,0.175],\"showgrid\":true},\"yaxis9\":{\"anchor\":\"x9\",\"domain\":[0.0,0.24444444444444446],\"showgrid\":true},\"xaxis10\":{\"anchor\":\"y10\",\"domain\":[0.275,0.45],\"showgrid\":true},\"yaxis10\":{\"anchor\":\"x10\",\"domain\":[0.0,0.24444444444444446],\"showgrid\":true},\"xaxis11\":{\"anchor\":\"y11\",\"domain\":[0.55,0.7250000000000001],\"showgrid\":true},\"yaxis11\":{\"anchor\":\"x11\",\"domain\":[0.0,0.24444444444444446],\"showgrid\":true},\"xaxis12\":{\"anchor\":\"y12\",\"domain\":[0.825,1.0],\"showgrid\":true},\"yaxis12\":{\"anchor\":\"x12\",\"domain\":[0.0,0.24444444444444446],\"showgrid\":true},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Topic 0\",\"x\":0.0875,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Topic 1\",\"x\":0.36250000000000004,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Topic 2\",\"x\":0.6375000000000001,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Topic 3\",\"x\":0.9125,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Topic 4\",\"x\":0.0875,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.6222222222222222,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Topic 5\",\"x\":0.36250000000000004,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.6222222222222222,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Topic 6\",\"x\":0.6375000000000001,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.6222222222222222,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Topic 7\",\"x\":0.9125,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.6222222222222222,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Topic 8\",\"x\":0.0875,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.24444444444444446,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Topic 9\",\"x\":0.36250000000000004,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.24444444444444446,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Topic 10\",\"x\":0.6375000000000001,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.24444444444444446,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Topic 11\",\"x\":0.9125,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.24444444444444446,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"font\":{\"size\":22,\"color\":\"Black\"},\"text\":\"Topic Word Scores\",\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"hoverlabel\":{\"font\":{\"size\":16,\"family\":\"Rockwell\"},\"bgcolor\":\"white\"},\"showlegend\":false,\"width\":1000,\"height\":750},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('2762839d-7d24-4f04-b0d7-cb3278a4c29f');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Visualize top topic keywords\n",
        "mymodel.visualize_barchart(top_n_topics=12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "id": "F0gvSwm7a6Uh",
        "outputId": "0fb6847c-9889-4254-b2b0-d74ca8b90941"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"f358947f-cef5-4a0f-b6cb-70da9b02f14a\" class=\"plotly-graph-div\" style=\"height:725px; width:1000px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f358947f-cef5-4a0f-b6cb-70da9b02f14a\")) {                    Plotly.newPlot(                        \"f358947f-cef5-4a0f-b6cb-70da9b02f14a\",                        [{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"x\":[0.0,0.9130990721835424,0.9130990721835424,0.0],\"xaxis\":\"x\",\"y\":[-5.0,-5.0,-15.0,-15.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(255,65,54)\"},\"mode\":\"lines\",\"x\":[0.0,0.8783177650986767,0.8783177650986767,0.0],\"xaxis\":\"x\",\"y\":[-25.0,-25.0,-35.0,-35.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(255,65,54)\"},\"mode\":\"lines\",\"x\":[0.8783177650986767,0.9353974916251079,0.9353974916251079,0.0],\"xaxis\":\"x\",\"y\":[-30.0,-30.0,-45.0,-45.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(0,116,217)\"},\"mode\":\"lines\",\"x\":[0.9130990721835424,1.0112351127515242,1.0112351127515242,0.9353974916251079],\"xaxis\":\"x\",\"y\":[-10.0,-10.0,-37.5,-37.5],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(35,205,205)\"},\"mode\":\"lines\",\"x\":[0.0,0.9062090340106712,0.9062090340106712,0.0],\"xaxis\":\"x\",\"y\":[-65.0,-65.0,-75.0,-75.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(35,205,205)\"},\"mode\":\"lines\",\"x\":[0.9062090340106712,0.9352605346919115,0.9352605346919115,0.0],\"xaxis\":\"x\",\"y\":[-70.0,-70.0,-85.0,-85.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(35,205,205)\"},\"mode\":\"lines\",\"x\":[0.0,0.9569500018414573,0.9569500018414573,0.9352605346919115],\"xaxis\":\"x\",\"y\":[-55.0,-55.0,-77.5,-77.5],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(133,20,75)\"},\"mode\":\"lines\",\"x\":[0.0,0.948698442115624,0.948698442115624,0.0],\"xaxis\":\"x\",\"y\":[-95.0,-95.0,-105.0,-105.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(0,116,217)\"},\"mode\":\"lines\",\"x\":[0.9569500018414573,1.0037142470557403,1.0037142470557403,0.948698442115624],\"xaxis\":\"x\",\"y\":[-66.25,-66.25,-100.0,-100.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(255,220,0)\"},\"mode\":\"lines\",\"x\":[0.0,0.9383363717755218,0.9383363717755218,0.0],\"xaxis\":\"x\",\"y\":[-125.0,-125.0,-135.0,-135.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(255,220,0)\"},\"mode\":\"lines\",\"x\":[0.0,0.9590253999950745,0.9590253999950745,0.9383363717755218],\"xaxis\":\"x\",\"y\":[-115.0,-115.0,-130.0,-130.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(255,220,0)\"},\"mode\":\"lines\",\"x\":[0.0,0.8303363501712009,0.8303363501712009,0.0],\"xaxis\":\"x\",\"y\":[-155.0,-155.0,-165.0,-165.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(255,220,0)\"},\"mode\":\"lines\",\"x\":[0.0,0.8702524384616224,0.8702524384616224,0.8303363501712009],\"xaxis\":\"x\",\"y\":[-145.0,-145.0,-160.0,-160.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(255,220,0)\"},\"mode\":\"lines\",\"x\":[0.0,0.8768123154999198,0.8768123154999198,0.0],\"xaxis\":\"x\",\"y\":[-175.0,-175.0,-185.0,-185.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(255,220,0)\"},\"mode\":\"lines\",\"x\":[0.8702524384616224,0.9450587260279759,0.9450587260279759,0.8768123154999198],\"xaxis\":\"x\",\"y\":[-152.5,-152.5,-180.0,-180.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(255,220,0)\"},\"mode\":\"lines\",\"x\":[0.9590253999950745,0.9894897602057826,0.9894897602057826,0.9450587260279759],\"xaxis\":\"x\",\"y\":[-122.5,-122.5,-166.25,-166.25],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(0,116,217)\"},\"mode\":\"lines\",\"x\":[1.0037142470557403,1.0211628394019114,1.0211628394019114,0.9894897602057826],\"xaxis\":\"x\",\"y\":[-83.125,-83.125,-144.375,-144.375],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(40,35,35)\"},\"mode\":\"lines\",\"x\":[0.0,0.8256070362522312,0.8256070362522312,0.0],\"xaxis\":\"x\",\"y\":[-195.0,-195.0,-205.0,-205.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"x\":[0.0,0.7310188371614054,0.7310188371614054,0.0],\"xaxis\":\"x\",\"y\":[-235.0,-235.0,-245.0,-245.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"x\":[0.0,0.8595554364190265,0.8595554364190265,0.7310188371614054],\"xaxis\":\"x\",\"y\":[-225.0,-225.0,-240.0,-240.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"x\":[0.8595554364190265,0.9434016669334369,0.9434016669334369,0.0],\"xaxis\":\"x\",\"y\":[-232.5,-232.5,-255.0,-255.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(61,153,112)\"},\"mode\":\"lines\",\"x\":[0.0,0.9555988703424649,0.9555988703424649,0.9434016669334369],\"xaxis\":\"x\",\"y\":[-215.0,-215.0,-243.75,-243.75],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(255,65,54)\"},\"mode\":\"lines\",\"x\":[0.0,0.9263617187964186,0.9263617187964186,0.0],\"xaxis\":\"x\",\"y\":[-275.0,-275.0,-285.0,-285.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(255,65,54)\"},\"mode\":\"lines\",\"x\":[0.0,0.9782458032839035,0.9782458032839035,0.9263617187964186],\"xaxis\":\"x\",\"y\":[-265.0,-265.0,-280.0,-280.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(255,65,54)\"},\"mode\":\"lines\",\"x\":[0.0,0.9217020561944509,0.9217020561944509,0.0],\"xaxis\":\"x\",\"y\":[-295.0,-295.0,-305.0,-305.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(255,65,54)\"},\"mode\":\"lines\",\"x\":[0.9217020561944509,0.9666518343508499,0.9666518343508499,0.0],\"xaxis\":\"x\",\"y\":[-300.0,-300.0,-315.0,-315.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(255,65,54)\"},\"mode\":\"lines\",\"x\":[0.9782458032839035,0.9813603316237247,0.9813603316237247,0.9666518343508499],\"xaxis\":\"x\",\"y\":[-272.5,-272.5,-307.5,-307.5],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(255,65,54)\"},\"mode\":\"lines\",\"x\":[0.0,0.931308468139644,0.931308468139644,0.0],\"xaxis\":\"x\",\"y\":[-325.0,-325.0,-335.0,-335.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(255,65,54)\"},\"mode\":\"lines\",\"x\":[0.931308468139644,0.9421095169926759,0.9421095169926759,0.0],\"xaxis\":\"x\",\"y\":[-330.0,-330.0,-345.0,-345.0],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(255,65,54)\"},\"mode\":\"lines\",\"x\":[0.9813603316237247,0.9861631724761255,0.9861631724761255,0.9421095169926759],\"xaxis\":\"x\",\"y\":[-290.0,-290.0,-337.5,-337.5],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(0,116,217)\"},\"mode\":\"lines\",\"x\":[0.9555988703424649,1.020679487459743,1.020679487459743,0.9861631724761255],\"xaxis\":\"x\",\"y\":[-229.375,-229.375,-313.75,-313.75],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(0,116,217)\"},\"mode\":\"lines\",\"x\":[0.8256070362522312,1.0385600739437046,1.0385600739437046,1.020679487459743],\"xaxis\":\"x\",\"y\":[-200.0,-200.0,-271.5625,-271.5625],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(0,116,217)\"},\"mode\":\"lines\",\"x\":[1.0211628394019114,1.0577096684347536,1.0577096684347536,1.0385600739437046],\"xaxis\":\"x\",\"y\":[-113.75,-113.75,-235.78125,-235.78125],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"rgb(0,116,217)\"},\"mode\":\"lines\",\"x\":[1.0112351127515242,1.0879102208468494,1.0879102208468494,1.0577096684347536],\"xaxis\":\"x\",\"y\":[-23.75,-23.75,-174.765625,-174.765625],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"autosize\":false,\"height\":725,\"hovermode\":\"closest\",\"showlegend\":false,\"width\":1000,\"xaxis\":{\"mirror\":\"allticks\",\"rangemode\":\"tozero\",\"showgrid\":false,\"showline\":true,\"showticklabels\":true,\"ticks\":\"outside\",\"type\":\"linear\",\"zeroline\":false},\"yaxis\":{\"mirror\":\"allticks\",\"rangemode\":\"tozero\",\"showgrid\":false,\"showline\":true,\"showticklabels\":true,\"tickmode\":\"array\",\"ticks\":\"outside\",\"ticktext\":[\"25_missing_use_month\",\"33_mega_mb mega_kilo mega\",\"18_nt pay_google_phone number\",\"9_digit number_add digit_di...\",\"32_question got_site ferven...\",\"21_possible question_questi...\",\"1_rbreadit good_rbreadit_po...\",\"19_text_ve_kid\",\"11_just search_searched_sea...\",\"12_justtrust awesome_justtr...\",\"26_life change_ll end_inval...\",\"34_site exist_delete_nah site\",\"23_need rid_file_permission\",\"30_check_apply_actually\",\"4_inactive fix_noticed_down...\",\"6_realtime_email_free\",\"0_print_http_simple\",\"8_just_app_folder\",\"16_online storage_checkup 2...\",\"27_internet said_service ak...\",\"14_essentially_say_customer\",\"31_mind posted_baby_monolike\",\"13_payment_insurance_legal\",\"3_help_need_understand\",\"2_nt_nt know_doe\",\"29_use_cell phone_research\",\"20_lost_age_need\",\"24_good_check_used\",\"5_wwwsrhnoaagovffc_http www...\",\"17_using_used_friend\",\"10_virus_worked_thing\",\"22_nt_nt nutrient_let recom...\",\"15_ram_free_saved 300\",\"7_nt use_nt_issue\",\"28_peel turn_peel way_s\"],\"tickvals\":[-5.0,-15.0,-25.0,-35.0,-45.0,-55.0,-65.0,-75.0,-85.0,-95.0,-105.0,-115.0,-125.0,-135.0,-145.0,-155.0,-165.0,-175.0,-185.0,-195.0,-205.0,-215.0,-225.0,-235.0,-245.0,-255.0,-265.0,-275.0,-285.0,-295.0,-305.0,-315.0,-325.0,-335.0,-345.0],\"type\":\"linear\",\"zeroline\":false,\"range\":[-350.0,0.0]},\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"title\":{\"font\":{\"size\":22,\"color\":\"Black\"},\"text\":\"\\u003cb\\u003eHierarchical Clustering\\u003c\\u002fb\\u003e\",\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"hoverlabel\":{\"font\":{\"size\":16,\"family\":\"Rockwell\"},\"bgcolor\":\"white\"},\"plot_bgcolor\":\"#ECEFF1\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f358947f-cef5-4a0f-b6cb-70da9b02f14a');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "mymodel.visualize_hierarchy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "6JVuUZELbC8F",
        "outputId": "f56b591a-b6c3-4070-e1c7-6a7baa129e38"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"a300a4e6-7157-4bc8-8d04-15d5f68ec629\" class=\"plotly-graph-div\" style=\"height:800px; width:800px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"a300a4e6-7157-4bc8-8d04-15d5f68ec629\")) {                    Plotly.newPlot(                        \"a300a4e6-7157-4bc8-8d04-15d5f68ec629\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"x\":[\"0_print_http_simple\",\"1_rbreadit good_rbreadit_po...\",\"2_nt_nt know_doe\",\"3_help_need_understand\",\"4_inactive fix_noticed_down...\",\"5_wwwsrhnoaagovffc_http www...\",\"6_realtime_email_free\",\"7_nt use_nt_issue\",\"8_just_app_folder\",\"9_digit number_add digit_di...\",\"10_virus_worked_thing\",\"11_just search_searched_sea...\",\"12_justtrust awesome_justtr...\",\"13_payment_insurance_legal\",\"14_essentially_say_customer\",\"15_ram_free_saved 300\",\"16_online storage_checkup 2...\",\"17_using_used_friend\",\"18_nt pay_google_phone number\",\"19_text_ve_kid\",\"20_lost_age_need\",\"21_possible question_questi...\",\"22_nt_nt nutrient_let recom...\",\"23_need rid_file_permission\",\"24_good_check_used\",\"25_missing_use_month\",\"26_life change_ll end_inval...\",\"27_internet said_service ak...\",\"28_peel turn_peel way_s\",\"29_use_cell phone_research\",\"30_check_apply_actually\",\"31_mind posted_baby_monolike\",\"32_question got_site ferven...\",\"33_mega_mb mega_kilo mega\",\"34_site exist_delete_nah site\"],\"y\":[\"0_print_http_simple\",\"1_rbreadit good_rbreadit_po...\",\"2_nt_nt know_doe\",\"3_help_need_understand\",\"4_inactive fix_noticed_down...\",\"5_wwwsrhnoaagovffc_http www...\",\"6_realtime_email_free\",\"7_nt use_nt_issue\",\"8_just_app_folder\",\"9_digit number_add digit_di...\",\"10_virus_worked_thing\",\"11_just search_searched_sea...\",\"12_justtrust awesome_justtr...\",\"13_payment_insurance_legal\",\"14_essentially_say_customer\",\"15_ram_free_saved 300\",\"16_online storage_checkup 2...\",\"17_using_used_friend\",\"18_nt pay_google_phone number\",\"19_text_ve_kid\",\"20_lost_age_need\",\"21_possible question_questi...\",\"22_nt_nt nutrient_let recom...\",\"23_need rid_file_permission\",\"24_good_check_used\",\"25_missing_use_month\",\"26_life change_ll end_inval...\",\"27_internet said_service ak...\",\"28_peel turn_peel way_s\",\"29_use_cell phone_research\",\"30_check_apply_actually\",\"31_mind posted_baby_monolike\",\"32_question got_site ferven...\",\"33_mega_mb mega_kilo mega\",\"34_site exist_delete_nah site\"],\"z\":[[1.0,0.23402116,0.13275623,0.107456036,0.35973054,0.1605194,0.21262424,0.23707628,0.33721876,0.15186492,0.015062239,0.27455592,0.22877358,0.33810633,0.39020795,0.31346652,0.41353953,0.04364121,0.36135042,0.15836251,0.068687476,0.25529057,0.16168812,0.19661182,0.063663326,0.31368887,0.4440771,0.38066465,0.027729996,0.005547587,0.25358158,0.08344943,0.15227626,0.112007745,0.2703499],[0.23402116,0.99999976,0.29742542,0.18113232,0.49051526,0.3405612,0.18860863,0.14546773,0.21066844,0.27186638,0.1432612,0.52668905,0.4975909,0.11988134,0.10845266,0.19517417,0.29703778,0.3425377,0.2151851,0.48672795,0.29554427,0.2596759,0.34816658,0.24527675,0.17055306,0.17448151,0.32647204,0.1512395,0.24285659,0.12118694,0.13589038,0.18667793,0.46140146,0.22460312,0.30173331],[0.13275623,0.29742542,1.0,0.41376102,0.19015893,0.2533073,0.04566907,0.13911547,0.08636752,0.0947129,0.07795068,0.06847217,0.2210421,0.3579477,0.19306701,0.10706644,0.037376456,0.15081576,0.31371677,0.05426537,0.30773413,0.13434954,0.090731494,0.018991698,0.5361709,0.10565874,0.24307261,0.102838755,0.076672025,0.17845744,0.114485584,0.24684477,0.12283714,0.02634361,0.061736036],[0.107456036,0.18113232,0.41376102,0.99999976,0.15858918,0.10024379,0.14298259,0.022140533,0.06785677,-0.013939496,0.33002934,0.025365911,0.057565607,0.569293,0.15834144,0.13370964,0.016328882,0.16130237,0.19916832,0.0083342865,0.11222389,0.07534554,0.04504668,0.010687663,0.17665507,0.07235132,0.071575016,0.077544436,0.012580376,0.2973216,0.12698668,0.5227069,0.23843974,-0.03988065,0.035462838],[0.35973054,0.49051526,0.19015893,0.15858918,1.0,0.2896893,0.27260354,0.054631185,0.37654465,0.18701613,0.07719759,0.56498766,0.35670817,0.29937094,0.13485228,0.26275668,0.37815976,0.055832636,0.41843614,0.38551903,0.09229438,0.15969387,0.1583723,0.3420143,0.113073125,0.34481955,0.283763,0.1995526,0.06811954,0.03385873,0.27714664,0.112802505,0.38996148,0.23535362,0.39726326],[0.1605194,0.3405612,0.2533073,0.10024379,0.2896893,0.99999994,0.193019,0.20942114,0.13761996,0.09227273,0.17365424,0.20941919,0.2883566,0.1286124,0.13833994,0.10020397,0.10145032,0.16347685,0.17154838,0.15021965,0.22782713,0.20334888,0.19562964,0.11442888,0.20135519,0.1607272,0.16345245,0.209784,0.09216867,0.092775375,0.090002686,0.14390379,0.10414477,0.09038379,0.0874063],[0.21262424,0.18860863,0.04566907,0.14298259,0.27260354,0.193019,1.0000002,0.06025187,0.2914697,0.029680738,0.31394178,0.33427545,0.086430594,0.15952908,0.18835409,0.2721003,0.3094771,0.06858294,0.17779356,0.21027552,0.1382803,0.016675338,0.074893646,0.39920005,0.056509048,0.28496686,0.15057816,0.15044141,0.06557935,-0.008869555,0.52351654,0.33491993,0.017856516,0.15756139,0.27262533],[0.23707628,0.14546773,0.13911547,0.022140533,0.054631185,0.20942114,0.06025187,1.0,0.018073954,0.062407337,0.054446768,0.059428763,0.1432161,-0.029456556,0.13394374,0.02542983,0.01908899,0.17555046,-0.0043127667,0.046093397,0.02165189,0.077225685,0.3506516,0.03292724,0.120485894,0.09898165,0.09503811,0.21809414,0.19157359,0.08708302,0.09816727,0.0014420561,0.13718982,-0.032619607,-0.014553353],[0.33721876,0.21066844,0.08636752,0.06785677,0.37654465,0.13761996,0.2914697,0.018073954,1.0,0.027278356,0.0395771,0.22131106,0.09139644,0.15800026,0.29976895,0.27361396,0.5360657,0.034300014,0.15183142,0.23104814,0.034301136,0.034475446,0.035792537,0.14865354,0.09978935,0.3692875,0.19135188,0.1878132,0.054237507,0.037624303,0.3308648,0.03163001,0.11335549,0.15437232,0.2590467],[0.15186492,0.27186638,0.0947129,-0.013939496,0.18701613,0.09227273,0.029680738,0.062407337,0.027278356,0.9999999,-0.0115673635,0.18244624,0.14464733,0.0053446796,-0.008875806,0.07717318,0.102864936,0.056289054,0.19630787,0.117381975,0.03977844,0.12941128,0.08552462,0.08223803,0.04371324,0.110353135,0.20661181,0.06388429,0.14607072,0.18271118,0.004493758,0.024429237,0.33004087,0.41207743,0.068340495],[0.015062239,0.1432612,0.07795068,0.33002934,0.07719759,0.17365424,0.31394178,0.054446768,0.0395771,-0.0115673635,1.0,0.07279451,0.059030373,0.111586615,0.059443686,0.033174172,0.03620533,0.49411583,0.022705847,-0.011006746,0.033275925,-0.06395839,0.16432561,0.13719085,0.070535496,-0.023460628,-0.015631469,-0.0031073773,0.21400669,0.14240402,0.059588328,0.39465392,0.020212475,-0.012071505,0.029390588],[0.27455592,0.52668905,0.06847217,0.025365911,0.56498766,0.20941919,0.33427545,0.059428763,0.22131106,0.18244624,0.07279451,1.0000001,0.2648382,0.10791957,0.1042172,0.30404538,0.3861498,0.12017891,0.22020514,0.45976925,0.07085661,0.121104,0.12539285,0.5258319,0.10385946,0.25803202,0.29408917,0.07123195,0.06300269,-0.0018555708,0.2947514,0.100076124,0.18358457,0.26454797,0.3887153],[0.22877358,0.4975909,0.2210421,0.057565607,0.35670817,0.2883566,0.086430594,0.1432161,0.09139644,0.14464733,0.059030373,0.2648382,1.0,0.07662173,0.13601452,0.15162334,0.16996852,0.12127921,0.20622346,0.21034923,0.1612108,0.14700067,0.20743245,0.10666665,0.19279148,0.2249453,0.2755639,0.23554114,0.17132697,0.08620929,0.075035065,0.10716823,0.27278018,0.18897186,0.14420697],[0.33810633,0.11988134,0.3579477,0.569293,0.29937094,0.1286124,0.15952908,-0.029456556,0.15800026,0.0053446796,0.111586615,0.10791957,0.07662173,0.99999994,0.29566517,0.14695469,0.09246981,-0.023676736,0.43347907,-0.045599796,0.15700594,0.096147016,-0.030670155,0.07610887,0.16067979,0.13228048,0.20110285,0.11278568,-0.111208454,0.131907,0.12854476,0.27388805,0.08604927,0.027676843,0.20546336],[0.39020795,0.10845266,0.19306701,0.15834144,0.13485228,0.13833994,0.18835409,0.13394374,0.29976895,-0.008875806,0.059443686,0.1042172,0.13601452,0.29566517,1.0,0.36884594,0.22913662,0.07781813,0.34114993,0.00750909,0.09756999,-0.06166785,0.0816916,0.0609154,0.17560595,0.21750905,0.33274025,0.6693239,0.1553038,0.030641906,0.1622732,0.108534515,-0.031871133,0.035876557,0.13897341],[0.31346652,0.19517417,0.10706644,0.13370964,0.26275668,0.10020397,0.2721003,0.02542983,0.27361396,0.07717318,0.033174172,0.30404538,0.15162334,0.14695469,0.36884594,0.99999994,0.40180632,0.0021402184,0.21976966,0.14470467,0.03511265,0.057573132,0.04565849,0.12227321,0.07250607,0.30145687,0.14312586,0.22399779,0.19502816,0.04458066,0.3585406,0.13687353,0.043013252,0.17288025,0.102552295],[0.41353953,0.29703778,0.037376456,0.016328882,0.37815976,0.10145032,0.3094771,0.01908899,0.5360657,0.102864936,0.03620533,0.3861498,0.16996852,0.09246981,0.22913662,0.40180632,1.0,0.06387729,0.16011155,0.23416205,0.023394633,0.066780664,0.061910074,0.26086605,0.115333974,0.57472277,0.27954346,0.26299003,0.10378873,0.012953598,0.38198274,0.030157018,0.114203714,0.36383015,0.28073496],[0.04364121,0.3425377,0.15081576,0.16130237,0.055832636,0.16347685,0.06858294,0.17555046,0.034300014,0.056289054,0.49411583,0.12017891,0.12127921,-0.023676736,0.07781813,0.0021402184,0.06387729,0.99999994,0.010205311,0.11132657,0.1859774,0.029204816,0.36271274,0.045271233,0.04964374,-0.011370657,0.09060244,0.0446089,0.3434462,0.038410157,-0.01947686,0.24294218,0.14461173,0.0046472386,0.0143491905],[0.36135042,0.2151851,0.31371677,0.19916832,0.41843614,0.17154838,0.17779356,-0.0043127667,0.15183142,0.19630787,0.022705847,0.22020514,0.20622346,0.43347907,0.34114993,0.21976966,0.16011155,0.010205311,1.0,0.0874805,0.11884555,0.11701378,0.13076542,0.1674411,0.14234008,0.21247503,0.3890558,0.22505078,0.056102283,0.052417755,0.115134045,0.13761392,0.0865473,0.11240344,0.29274347],[0.15836251,0.48672795,0.05426537,0.0083342865,0.38551903,0.15021965,0.21027552,0.046093397,0.23104814,0.117381975,-0.011006746,0.45976925,0.21034923,-0.045599796,0.00750909,0.14470467,0.23416205,0.11132657,0.0874805,0.9999998,0.051969312,0.016751673,0.10214736,0.17936283,0.020903297,0.110141225,0.31756222,-0.005634673,0.03826073,-0.09670744,0.21444075,0.03708737,0.17760095,0.13001399,0.23265372],[0.068687476,0.29554427,0.30773413,0.11222389,0.09229438,0.22782713,0.1382803,0.02165189,0.034301136,0.03977844,0.033275925,0.07085661,0.1612108,0.15700594,0.09756999,0.03511265,0.023394633,0.1859774,0.11884555,0.051969312,1.0000005,0.12524025,0.23091231,0.005395255,0.17589623,-0.012990848,0.057000294,0.08885757,0.14813371,0.019952405,0.002595555,0.19648877,0.0030671563,0.009465046,0.09795499],[0.25529057,0.2596759,0.13434954,0.07534554,0.15969387,0.20334888,0.016675338,0.077225685,0.034475446,0.12941128,-0.06395839,0.121104,0.14700067,0.096147016,-0.06166785,0.057573132,0.066780664,0.029204816,0.11701378,0.016751673,0.12524025,0.99999976,0.19396138,0.018429382,0.113234974,0.062996104,0.21195148,0.08260763,-0.009336552,0.12844197,0.077473,0.05511701,0.2097463,0.048672035,0.12850402],[0.16168812,0.34816658,0.090731494,0.04504668,0.1583723,0.19562964,0.074893646,0.3506516,0.035792537,0.08552462,0.16432561,0.12539285,0.20743245,-0.030670155,0.0816916,0.04565849,0.061910074,0.36271274,0.13076542,0.10214736,0.23091231,0.19396138,1.0,0.059782512,-0.0015033893,0.01194412,0.07493998,0.15824112,0.4326469,0.05003623,-0.01658849,0.053723365,0.16877376,0.07077747,0.064474784],[0.19661182,0.24527675,0.018991698,0.010687663,0.3420143,0.11442888,0.39920005,0.03292724,0.14865354,0.08223803,0.13719085,0.5258319,0.10666665,0.07610887,0.0609154,0.12227321,0.26086605,0.045271233,0.1674411,0.17936283,0.005395255,0.018429382,0.059782512,1.0000002,0.03703953,0.24973562,0.1404599,0.103771925,0.023085967,-0.003002526,0.37632883,0.08150092,0.054191716,0.061820325,0.46232218],[0.063663326,0.17055306,0.5361709,0.17665507,0.113073125,0.20135519,0.056509048,0.120485894,0.09978935,0.04371324,0.070535496,0.10385946,0.19279148,0.16067979,0.17560595,0.07250607,0.115333974,0.04964374,0.14234008,0.020903297,0.17589623,0.113234974,-0.0015033893,0.03703953,1.0,0.047921665,0.26478255,0.104373924,0.00871802,0.091057956,0.16324735,0.23813689,0.099549145,-0.0181198,0.055598598],[0.31368887,0.17448151,0.10565874,0.07235132,0.34481955,0.1607272,0.28496686,0.09898165,0.3692875,0.110353135,-0.023460628,0.25803202,0.2249453,0.13228048,0.21750905,0.30145687,0.57472277,-0.011370657,0.21247503,0.110141225,-0.012990848,0.062996104,0.01194412,0.24973562,0.047921665,1.0000001,0.17588195,0.38826573,0.08353928,0.1820241,0.2870692,0.030135833,0.13292077,0.46757066,0.19812447],[0.4440771,0.32647204,0.24307261,0.071575016,0.283763,0.16345245,0.15057816,0.09503811,0.19135188,0.20661181,-0.015631469,0.29408917,0.2755639,0.20110285,0.33274025,0.14312586,0.27954346,0.09060244,0.3890558,0.31756222,0.057000294,0.21195148,0.07493998,0.1404599,0.26478255,0.17588195,0.9999999,0.2359969,-0.022019405,0.0741945,0.14187759,0.13528693,0.13471323,0.13041438,0.31919062],[0.38066465,0.1512395,0.102838755,0.077544436,0.1995526,0.209784,0.15044141,0.21809414,0.1878132,0.06388429,-0.0031073773,0.07123195,0.23554114,0.11278568,0.6693239,0.22399779,0.26299003,0.0446089,0.22505078,-0.005634673,0.08885757,0.08260763,0.15824112,0.103771925,0.104373924,0.38826573,0.2359969,0.99999976,0.13741481,0.05026313,0.123502895,0.055378657,0.067098156,0.12746692,0.21273112],[0.027729996,0.24285659,0.076672025,0.012580376,0.06811954,0.09216867,0.06557935,0.19157359,0.054237507,0.14607072,0.21400669,0.06300269,0.17132697,-0.111208454,0.1553038,0.19502816,0.10378873,0.3434462,0.056102283,0.03826073,0.14813371,-0.009336552,0.4326469,0.023085967,0.00871802,0.08353928,-0.022019405,0.13741481,0.9999999,0.09556083,0.018893266,0.04463646,0.10212162,0.062240392,0.004407486],[0.005547587,0.12118694,0.17845744,0.2973216,0.03385873,0.092775375,-0.008869555,0.08708302,0.037624303,0.18271118,0.14240402,-0.0018555708,0.08620929,0.131907,0.030641906,0.04458066,0.012953598,0.038410157,0.052417755,-0.09670744,0.019952405,0.12844197,0.05003623,-0.003002526,0.091057956,0.1820241,0.0741945,0.05026313,0.09556083,0.99999964,0.06719041,0.23098114,0.24626438,0.15996398,-0.045689534],[0.25358158,0.13589038,0.114485584,0.12698668,0.27714664,0.090002686,0.52351654,0.09816727,0.3308648,0.004493758,0.059588328,0.2947514,0.075035065,0.12854476,0.1622732,0.3585406,0.38198274,-0.01947686,0.115134045,0.21444075,0.002595555,0.077473,-0.01658849,0.37632883,0.16324735,0.2870692,0.14187759,0.123502895,0.018893266,0.06719041,1.0,0.101671375,0.04424604,0.09462799,0.26245213],[0.08344943,0.18667793,0.24684477,0.5227069,0.112802505,0.14390379,0.33491993,0.0014420561,0.03163001,0.024429237,0.39465392,0.100076124,0.10716823,0.27388805,0.108534515,0.13687353,0.030157018,0.24294218,0.13761392,0.03708737,0.19648877,0.05511701,0.053723365,0.08150092,0.23813689,0.030135833,0.13528693,0.055378657,0.04463646,0.23098114,0.101671375,1.0000002,0.06683762,0.05143197,0.079733275],[0.15227626,0.46140146,0.12283714,0.23843974,0.38996148,0.10414477,0.017856516,0.13718982,0.11335549,0.33004087,0.020212475,0.18358457,0.27278018,0.08604927,-0.031871133,0.043013252,0.114203714,0.14461173,0.0865473,0.17760095,0.0030671563,0.2097463,0.16877376,0.054191716,0.099549145,0.13292077,0.13471323,0.067098156,0.10212162,0.24626438,0.04424604,0.06683762,0.99999976,0.17765383,0.12510759],[0.112007745,0.22460312,0.02634361,-0.03988065,0.23535362,0.09038379,0.15756139,-0.032619607,0.15437232,0.41207743,-0.012071505,0.26454797,0.18897186,0.027676843,0.035876557,0.17288025,0.36383015,0.0046472386,0.11240344,0.13001399,0.009465046,0.048672035,0.07077747,0.061820325,-0.0181198,0.46757066,0.13041438,0.12746692,0.062240392,0.15996398,0.09462799,0.05143197,0.17765383,1.0,0.063830666],[0.2703499,0.30173331,0.061736036,0.035462838,0.39726326,0.0874063,0.27262533,-0.014553353,0.2590467,0.068340495,0.029390588,0.3887153,0.14420697,0.20546336,0.13897341,0.102552295,0.28073496,0.0143491905,0.29274347,0.23265372,0.09795499,0.12850402,0.064474784,0.46232218,0.055598598,0.19812447,0.31919062,0.21273112,0.004407486,-0.045689534,0.26245213,0.079733275,0.12510759,0.063830666,0.9999999]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"x: %{x}\\u003cbr\\u003ey: %{y}\\u003cbr\\u003eSimilarity Score: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\"},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"Similarity Score\"}},\"colorscale\":[[0.0,\"rgb(247,252,240)\"],[0.125,\"rgb(224,243,219)\"],[0.25,\"rgb(204,235,197)\"],[0.375,\"rgb(168,221,181)\"],[0.5,\"rgb(123,204,196)\"],[0.625,\"rgb(78,179,211)\"],[0.75,\"rgb(43,140,190)\"],[0.875,\"rgb(8,104,172)\"],[1.0,\"rgb(8,64,129)\"]]},\"margin\":{\"t\":60},\"title\":{\"font\":{\"size\":22,\"color\":\"Black\"},\"text\":\"\\u003cb\\u003eSimilarity Matrix\\u003c\\u002fb\\u003e\",\"y\":0.95,\"x\":0.55,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"hoverlabel\":{\"font\":{\"size\":16,\"family\":\"Rockwell\"},\"bgcolor\":\"white\"},\"width\":800,\"height\":800,\"showlegend\":true,\"legend\":{\"title\":{\"text\":\"Trend\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('a300a4e6-7157-4bc8-8d04-15d5f68ec629');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "mymodel.visualize_heatmap()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "WP_aMCFHbIfy",
        "outputId": "9d0bebec-7f03-4ce4-fb59-22e1535ec636"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"63b465b4-702d-4935-81f5-32ed8eabba05\" class=\"plotly-graph-div\" style=\"height:500px; width:800px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"63b465b4-702d-4935-81f5-32ed8eabba05\")) {                    Plotly.newPlot(                        \"63b465b4-702d-4935-81f5-32ed8eabba05\",                        [{\"hovertext\":\"\\u003cb\\u003eTopic -1\\u003c\\u002fb\\u003e:advice_try_remindme_scrap_cheap_co\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.3457436,0.2489681,0.24051258,0.22343549,0.20949422,0.20867065,0.1976678,0.18941073,0.1852465,0.18169364],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e:print_http_simple_know_mail_print m\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.22746669,0.21800631,0.19971004,0.19493891,0.19376493,0.19105323,0.18070778,0.18029198,0.16375282,0.16049443],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e:rbreadit good_rbreadit_post written\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.33026177,0.31664327,0.26985067,0.24980655,0.24192333,0.23436359,0.22838268,0.22631001,0.22531366,0.22274187],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 2\\u003c\\u002fb\\u003e:nt_nt know_doe_issue_use_problem_ni\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.4902681,0.4798413,0.46017188,0.45359823,0.4484442,0.43898195,0.4323308,0.43205005,0.43067846,0.43017536],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 3\\u003c\\u002fb\\u003e:help_need_understand_month_make mis\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.25279248,0.1967093,0.19409457,0.19381276,0.19313237,0.18919408,0.17525907,0.1726223,0.1703377,0.17010535],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 4\\u003c\\u002fb\\u003e:inactive fix_noticed_download_quest\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.4034114,0.3741079,0.33861476,0.31587425,0.3141577,0.31145027,0.29776642,0.29634392,0.2871461,0.284795],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 5\\u003c\\u002fb\\u003e:wwwsrhnoaagovffc_http wwwsrhnoaagov\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.3835134,0.37884036,0.37398967,0.36450866,0.3528141,0.35141507,0.33992195,0.33077502,0.3280473,0.32531384],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 6\\u003c\\u002fb\\u003e:realtime_email_free_important_realw\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.28008708,0.25994962,0.24111894,0.2409158,0.23313369,0.22874509,0.22853509,0.22841392,0.22642139,0.22157228],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 7\\u003c\\u002fb\\u003e:nt use_nt_issue_use_fine_fuel nt_ga\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.3557037,0.34679535,0.33315063,0.28941536,0.27636826,0.2621597,0.25770432,0.25280082,0.24480018,0.23847009],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 8\\u003c\\u002fb\\u003e:just_app_folder_restore_version_tha\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.2591313,0.24720642,0.24290878,0.24051991,0.22498855,0.2209292,0.21804965,0.21574248,0.21268654,0.20787936],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 9\\u003c\\u002fb\\u003e:digit number_add digit_digit_digit \",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.39825356,0.39357424,0.39027268,0.37737826,0.36439008,0.35458255,0.33026665,0.32366306,0.31185728,0.30903232],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 10\\u003c\\u002fb\\u003e:virus_worked_thing_faster_use_usin\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.2284919,0.20970924,0.20745085,0.20210543,0.19680135,0.18709487,0.18140823,0.1739209,0.16805246,0.1589692],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 11\\u003c\\u002fb\\u003e:just search_searched_search_search\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.44270197,0.4248217,0.42322856,0.4098783,0.40981665,0.390613,0.37675223,0.37360197,0.37212163,0.35500735],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 12\\u003c\\u002fb\\u003e:justtrust awesome_justtrust_hot ju\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.2813841,0.27406883,0.24139088,0.18872678,0.18810335,0.18466407,0.18419312,0.18309605,0.1767527,0.17020765],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 13\\u003c\\u002fb\\u003e:payment_insurance_legal_health ins\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.30093563,0.2887705,0.26796216,0.250031,0.23156385,0.21476203,0.20803845,0.20365159,0.19778642,0.19136143],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 14\\u003c\\u002fb\\u003e:essentially_say_customer_provide_i\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.2951401,0.29017553,0.28240478,0.27986655,0.27570084,0.27338856,0.27115673,0.26652,0.26195836,0.2597007],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 15\\u003c\\u002fb\\u003e:ram_free_saved 300_ssd_bought_got_\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.39723426,0.31247556,0.31205058,0.30331856,0.30099815,0.2970118,0.2956592,0.2693932,0.2601433,0.25843114],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 16\\u003c\\u002fb\\u003e:online storage_checkup 2gb_extra g\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.40337723,0.4021967,0.39404178,0.3908453,0.37059563,0.3499477,0.34951657,0.34537882,0.34474474,0.3373788],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 17\\u003c\\u002fb\\u003e:using_used_friend_wa_list_read tit\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.3293169,0.21628812,0.20342264,0.19341913,0.18901104,0.1829595,0.16549721,0.153823,0.14291286,0.13837145],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 18\\u003c\\u002fb\\u003e:nt pay_google_phone number_http_di\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.2243447,0.21644679,0.20353612,0.1956721,0.18564156,0.18190941,0.1813162,0.17743835,0.17683828,0.17471185],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 19\\u003c\\u002fb\\u003e:text_ve_kid_v_zork_just_looked faq\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.32390666,0.3107473,0.26891088,0.26738176,0.26259857,0.25299588,0.24614373,0.23682076,0.22546577,0.22518983],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 20\\u003c\\u002fb\\u003e:lost_age_need_nt_home nt_5 lost_yo\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.28508028,0.27288282,0.24805759,0.21432737,0.21098758,0.20551439,0.20351249,0.20339361,0.19785449,0.19489953],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 21\\u003c\\u002fb\\u003e:possible question_question googled\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.22593996,0.21890569,0.2054652,0.20041578,0.20028399,0.19772446,0.19633923,0.1819914,0.1813522,0.18098855],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 22\\u003c\\u002fb\\u003e:nt_nt nutrient_let recommend_stuff\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.24630241,0.2438778,0.24334395,0.23020084,0.21871448,0.2138081,0.2115844,0.195277,0.19198392,0.1724738],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 23\\u003c\\u002fb\\u003e:need rid_file_permission_hide_remo\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.29979503,0.2991914,0.28179222,0.28073245,0.27723068,0.2636944,0.25542963,0.2435658,0.24334651,0.2373527],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 24\\u003c\\u002fb\\u003e:good_check_used_nt_checked_better_\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.27203903,0.27190757,0.24181028,0.23405837,0.22556135,0.22013682,0.21935093,0.20604563,0.20121166,0.1874285],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 25\\u003c\\u002fb\\u003e:missing_use_month_download_second_\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.3302619,0.28913826,0.2876091,0.2797376,0.27919033,0.27739358,0.27333224,0.2701259,0.26718327,0.25755793],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 26\\u003c\\u002fb\\u003e:life change_ll end_invalidates_kid\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.25085068,0.24836928,0.2256476,0.22455885,0.21316336,0.20722148,0.20453,0.20437247,0.19829816,0.18823798],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 27\\u003c\\u002fb\\u003e:internet said_service aka_need_new\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.26904133,0.23348221,0.22137293,0.19630125,0.19349658,0.18575895,0.18496317,0.17958829,0.17487942,0.17209418],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 28\\u003c\\u002fb\\u003e:peel turn_peel way_s_overripening \",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.239861,0.22433962,0.21672854,0.20716113,0.20221484,0.2002781,0.1999643,0.19597211,0.19132884,0.19124654],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 29\\u003c\\u002fb\\u003e:use_cell phone_research_used_resul\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.3616376,0.32902658,0.29751405,0.26943845,0.25176173,0.24497771,0.22376132,0.22352189,0.22087649,0.2134442],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 30\\u003c\\u002fb\\u003e:check_apply_actually_know_say_stuf\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.286665,0.25724375,0.2454176,0.24180725,0.24160185,0.23669465,0.23020819,0.22978461,0.22674173,0.22485891],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 31\\u003c\\u002fb\\u003e:mind posted_baby_monolike_case_don\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.30944803,0.30450255,0.3028896,0.29873306,0.29548386,0.29472953,0.29206184,0.28971428,0.28223607,0.28002915],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 32\\u003c\\u002fb\\u003e:question got_site fervently_ll kno\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.37259173,0.35524976,0.34268552,0.34076288,0.3369498,0.31644112,0.315037,0.3058561,0.29670724,0.28770995],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 33\\u003c\\u002fb\\u003e:mega_mb mega_kilo mega_example dsl\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.25792027,0.23307084,0.2128295,0.2003926,0.1997504,0.18274206,0.17858852,0.17626072,0.16356623,0.14106616],\"type\":\"scatter\"},{\"hovertext\":\"\\u003cb\\u003eTopic 34\\u003c\\u002fb\\u003e:site exist_delete_nah site_website\",\"line\":{\"color\":\"black\",\"width\":1.5},\"mode\":\"lines+lines\",\"name\":\"\",\"opacity\":0.1,\"x\":[1,2,3,4,5,6,7,8,9,10],\"y\":[0.35423684,0.33649138,0.33305627,0.33060843,0.32771558,0.31963345,0.3188401,0.31807247,0.31759143,0.31696284],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"range\":[0,10],\"tick0\":1,\"dtick\":2,\"title\":{\"text\":\"Term Rank\"}},\"title\":{\"font\":{\"size\":22,\"color\":\"Black\"},\"text\":\"\\u003cb\\u003eTerm score decline per Topic\\u003c\\u002fb\\u003e\",\"y\":0.9,\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"hoverlabel\":{\"font\":{\"size\":16,\"family\":\"Rockwell\"},\"bgcolor\":\"white\"},\"showlegend\":false,\"width\":800,\"height\":500,\"yaxis\":{\"title\":{\"text\":\"c-TF-IDF score\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('63b465b4-702d-4935-81f5-32ed8eabba05');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "mymodel.visualize_term_rank()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "9a92a65034734e89864f78824b626562",
            "a406dfd875f8410b9036dd84153194fd",
            "162f6058036540fab3262e8071f1c1a5",
            "9cac561b06e7474ab0f461f4d64c8d1b",
            "0baf3344ff014ef09fecf22f6e61d1d5",
            "b5d77f5298984faaae11e9037afabf08",
            "e3ee2295f6914116b0dd184716a6a734",
            "afd79e6c885745a78cd168525b5a68ae",
            "fa598420f86040ff8fc7fd94ee90aa06",
            "e71dfbee0d434b279d9385d55ae5c32f",
            "20475e729e72460197fb1f07d0280521"
          ]
        },
        "id": "bwX8WKQFbQbL",
        "outputId": "c04792e9-249d-4b49-f512-b88adc80f314"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a92a65034734e89864f78824b626562",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/49 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = sentence_model.encode(comments, show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwIG53s_EOD9"
      },
      "outputs": [],
      "source": [
        "mymodel.save(\"/content/drive/MyDrive/Drive Blog NLP/best_model_reddit_bertopic_14_Juin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c3SDA57UeRZ"
      },
      "outputs": [],
      "source": [
        "my_model = BERTopic.load(\"/content/drive/MyDrive/Drive Blog NLP/my_model_reddit_bertopic\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0baf3344ff014ef09fecf22f6e61d1d5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1618eede9af54d10a3a23fd6444b563c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "162f6058036540fab3262e8071f1c1a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afd79e6c885745a78cd168525b5a68ae",
            "max": 49,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fa598420f86040ff8fc7fd94ee90aa06",
            "value": 49
          }
        },
        "1c0b1b9f4ca9437a95306bc2b56811da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36bc5b99bb6049c78b8fbb2cbd972aaf",
            "placeholder": "​",
            "style": "IPY_MODEL_fa95fd2b0a7a485aa08a71dc600864d0",
            "value": "Batches: 100%"
          }
        },
        "20475e729e72460197fb1f07d0280521": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e3bca59c32744408eddf50121def79a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36bc5b99bb6049c78b8fbb2cbd972aaf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c86388ebc5a4c47a3f1f977d80561eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57619b86608345c68c4848d7120ea87d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f6593e5784942eaa431a8f15579be79": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7557d831a2d543b6b41ab0ef09f5f0a4",
            "placeholder": "​",
            "style": "IPY_MODEL_e7c00645df224d8993932c50fe17bab5",
            "value": " 49/49 [00:46&lt;00:00,  4.60it/s]"
          }
        },
        "718f00e7417e4f1da90767a562c605e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c2439ead24bc4d3e95b49e621b6c2b43",
              "IPY_MODEL_dbaa582ff84149248120e95c36c344eb",
              "IPY_MODEL_5f6593e5784942eaa431a8f15579be79"
            ],
            "layout": "IPY_MODEL_1618eede9af54d10a3a23fd6444b563c"
          }
        },
        "7557d831a2d543b6b41ab0ef09f5f0a4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d0161c060464fb7911ac278d2e677ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "820ed13a53e445ebb6b94d7c99567b8e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bab0975ced34ccd851f11b85f55ce7b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a92a65034734e89864f78824b626562": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a406dfd875f8410b9036dd84153194fd",
              "IPY_MODEL_162f6058036540fab3262e8071f1c1a5",
              "IPY_MODEL_9cac561b06e7474ab0f461f4d64c8d1b"
            ],
            "layout": "IPY_MODEL_0baf3344ff014ef09fecf22f6e61d1d5"
          }
        },
        "9cac561b06e7474ab0f461f4d64c8d1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e71dfbee0d434b279d9385d55ae5c32f",
            "placeholder": "​",
            "style": "IPY_MODEL_20475e729e72460197fb1f07d0280521",
            "value": " 49/49 [00:42&lt;00:00,  6.37it/s]"
          }
        },
        "a078f22b38f3411998c5eec014ecc2f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1c0b1b9f4ca9437a95306bc2b56811da",
              "IPY_MODEL_ab31b7fcc24e49c8b1ced738ad2f8b2a",
              "IPY_MODEL_a640fbad053e4f0cb5aafbb805ed27f1"
            ],
            "layout": "IPY_MODEL_df27605719fb4454b2e4dd38f179bdae"
          }
        },
        "a406dfd875f8410b9036dd84153194fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5d77f5298984faaae11e9037afabf08",
            "placeholder": "​",
            "style": "IPY_MODEL_e3ee2295f6914116b0dd184716a6a734",
            "value": "Batches: 100%"
          }
        },
        "a640fbad053e4f0cb5aafbb805ed27f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d0161c060464fb7911ac278d2e677ee",
            "placeholder": "​",
            "style": "IPY_MODEL_4c86388ebc5a4c47a3f1f977d80561eb",
            "value": " 49/49 [00:44&lt;00:00,  6.16it/s]"
          }
        },
        "ab31b7fcc24e49c8b1ced738ad2f8b2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8bab0975ced34ccd851f11b85f55ce7b",
            "max": 49,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_57619b86608345c68c4848d7120ea87d",
            "value": 49
          }
        },
        "afd79e6c885745a78cd168525b5a68ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5d77f5298984faaae11e9037afabf08": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be51c4a94e0e42f687f351146b8626cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c2439ead24bc4d3e95b49e621b6c2b43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_820ed13a53e445ebb6b94d7c99567b8e",
            "placeholder": "​",
            "style": "IPY_MODEL_ef049747a4e642dfaf5ab0811a44113a",
            "value": "Batches: 100%"
          }
        },
        "dbaa582ff84149248120e95c36c344eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e3bca59c32744408eddf50121def79a",
            "max": 49,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be51c4a94e0e42f687f351146b8626cf",
            "value": 49
          }
        },
        "df27605719fb4454b2e4dd38f179bdae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3ee2295f6914116b0dd184716a6a734": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e71dfbee0d434b279d9385d55ae5c32f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7c00645df224d8993932c50fe17bab5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef049747a4e642dfaf5ab0811a44113a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa598420f86040ff8fc7fd94ee90aa06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fa95fd2b0a7a485aa08a71dc600864d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}